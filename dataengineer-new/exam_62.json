{"pageProps":{"questions":[{"id":"SM8mgMShCw2e5B2zikKN","answer":"B","timestamp":"2020-03-22 16:37:00","answer_description":"","topic":"1","answer_images":[],"isMC":true,"discussion":[{"timestamp":"1599029700.0","poster":"GHN74","comment_id":"171799","comments":[{"timestamp":"1633340280.0","poster":"sergio6","content":"Information in social profiles are public","comment_id":"457000","comments":[{"poster":"sergio6","content":"according to the privacy settings and shareable informations","timestamp":"1633340400.0","upvote_count":"1","comment_id":"457002"}],"upvote_count":"1"}],"upvote_count":"40","content":"A is incorrect as you need to work with the data you have available \nC is an optimisation not a solution \nD is ethically incorrect and invasion to privacy, there could be several legal implications with this\nB although oversimplified but is a workable solution"},{"content":"We have labelled data that contains whether a loan application is accepted or defaulted - So Classification Problem Data.\n\nWe need to predict (Default Rates for applicants) - I think whether application will be granted or defaulted. - So Binary Classification.\n\nNo option matches the answer. - if we mark 'B' - It should be Logistic Regression, Instead of Linear Regression","poster":"sumanshu","timestamp":"1617735480.0","upvote_count":"21","comment_id":"329871","comments":[{"comment_id":"504131","upvote_count":"23","comments":[{"poster":"cchen8181","comment_id":"900636","upvote_count":"1","content":"Correct approach is to use logistic regression to predict default/not default, and then take the confidence/probability of the outcome as the \"default rate\". Linear regression doesn't make sense since we are not given a default rate label in our data, we are just given the labels default vs no default.","timestamp":"1684371540.0"},{"timestamp":"1678215120.0","poster":"Aaronn14","upvote_count":"2","comment_id":"832204","content":"You cannot predict rate. You predict a realization, which is either default or not. This question is terribly written."}],"poster":"szefco","content":"Question says: \"to predict default RATES for credit applicants\".\nIt is not binary classification, so Linear Regression would work here. \nI think B is correct answer.","timestamp":"1639819440.0"}]},{"comment_id":"1338848","content":"Selected Answer: A\nA. Because the dataset is just of granted loans of the business, but is needed a grater database where to train the model in order to get aceptable results","timestamp":"1736521680.0","upvote_count":"1","poster":"Skyw4lk3r"},{"poster":"nairoh","comment_id":"1297319","timestamp":"1728892860.0","content":"\"social\" does not mean Social Media. This could be linked to demograhic data, si it could improve the score.","upvote_count":"1"},{"content":"Selected Answer: B\nTo predict default rates for credit applicants using the labeled dataset of granted loan applications, the most appropriate course of action would be:\n\nB. Train a linear regression to predict a credit default risk score.\n\nHere's the rationale for this approach:\n\nAppropriate Model for Prediction: Linear regression is a common statistical method used for predictive modeling, particularly when the outcome variable (in this case, the likelihood of default) is continuous. In the context of credit scoring, linear regression can be used to predict a risk score that represents the probability of default.\n\nUtilization of Labeled Data: Since you already have a labeled dataset containing information on loans that have been granted and whether they have defaulted, you can use this data to train the regression model. This historical data provides the model with examples of borrower characteristics and their corresponding default outcomes.","upvote_count":"5","timestamp":"1703160360.0","poster":"TVH_Data_Engineer","comment_id":"1102412"},{"timestamp":"1701694560.0","content":"Selected Answer: B\nB. Train a linear regression to predict a credit default risk score.","poster":"rocky48","upvote_count":"3","comment_id":"1087644"},{"poster":"gaurav0480","comment_id":"984141","timestamp":"1692328200.0","content":"What would be the target variable if B is correct i.e. training a linear regression model? Default/No-Default is a categorical variable one cannot train a linear regression model with this target variable","upvote_count":"1"},{"comment_id":"965539","poster":"FDS1993","content":"Selected Answer: C\nC - it is a typical approach in credit loans. \nKeeping only the accepted loans leads to a bias in the application","upvote_count":"1","timestamp":"1690544520.0"},{"timestamp":"1690106640.0","content":"Selected Answer: B\nLinear regression is not the good way to solve such a problem, but you can totally apply linear regression to solve a classification problem. Just set the labels to numeric values 0 and 1 and linear regression will try to predict a value inbetween and round to the closest label (0 or 1). \n\nTotally not the way to go about it, but actually it's possible.","upvote_count":"2","comment_id":"960319","poster":"Mathew106"},{"comments":[{"comment_id":"1288929","timestamp":"1727251860.0","upvote_count":"1","poster":"baimus","content":"Credit scores are numbers, so this is a regression. Whether or not a client defaults could be a classification, but the option specifies the use of scores, which is fine."},{"content":"D. Matching loan applicants with their social profiles to enable feature engineering is not recommended as it raises privacy concerns and may not be legal in some jurisdictions. Additionally, social profiles may not be a good indicator of creditworthiness, and relying on them may introduce bias or discrimination.","upvote_count":"1","comment_id":"880468","poster":"Oleksandr0501","timestamp":"1682432340.0"}],"upvote_count":"1","content":"Selected Answer: D\nCannot be B. This is logistic regression, not linear regression.\n\nD is the only acceptable option.\nSocial profile can include things like high or low income, for example.\nWhen you apply for a credit you usually have to give this information, so totally legal.","timestamp":"1679685060.0","poster":"juliobs","comment_id":"849572"},{"content":"Because there is no option to know what dataset schema is even though B is needed for this question's purpose Nobody can't select B. So there is none to answer","upvote_count":"1","timestamp":"1677725940.0","poster":"jin0","comment_id":"826439"},{"upvote_count":"1","comment_id":"809532","content":"all options are wrong: Still in favour of B\nA: ofc its good to have more data but its not clear how much data we have\nB: Linear can be a workable approach but current situation is not for linear approach, decision tree, random forest etc can be good for it. \nC: DAta should be unbiased, removing bias is negative for tranining","poster":"musumusu","timestamp":"1676465280.0"},{"poster":"Besss","upvote_count":"2","comments":[{"poster":"21c17b3","content":"default rates is classification probability","comment_id":"1087692","timestamp":"1701698940.0","upvote_count":"1"}],"content":"Selected Answer: B\ndefault rates can be predicted with linear regression.","timestamp":"1674396420.0","comment_id":"784375"},{"comment_id":"756146","timestamp":"1672022580.0","poster":"Whoswho","upvote_count":"2","content":"Answer should actually be a logistic Regression model"},{"upvote_count":"3","timestamp":"1670328660.0","poster":"zellck","comment_id":"736787","content":"Selected Answer: B\nB is the answer."},{"comment_id":"732583","poster":"ladistar","content":"The question asks about default RATES, as in you are predicting a continuous variable, not a discrete one (classification). This is a regression problem, so choice B.","timestamp":"1669897080.0","upvote_count":"2"},{"upvote_count":"8","content":"I used to be a Credit Risk modeler and I think this question is stupid.","comment_id":"713903","timestamp":"1667918340.0","poster":"woyaolai"},{"comment_id":"699892","poster":"Azlijaffar","comments":[{"comment_id":"699893","upvote_count":"2","poster":"Azlijaffar","content":"So answer should be C i think.","timestamp":"1666267920.0"}],"timestamp":"1666267920.0","upvote_count":"2","content":"Data that you have is binary - Defaulted or not. You want default rates - Linear Regression. HOWEVER. The data that you have is for \"ALREADY GRANTED\" loan applications and whether they have defaulted or not.\nBut you want to \"train a model to predict default rates for credit applicants\", which would include applicants who would not be granted the loan. \nIf you just work on that dataset your model will not be as accurate as it wont have considered profiles of applicants that normally would not be granted those loans in the first place. Am I missing something here?"},{"poster":"devaid","upvote_count":"1","content":"Selected Answer: B\nAnswer B. \nYou can't assume it's a classification model, since the label is not specified. The \"whether these applications have been defaulted\" can be used just as a categorial variable (dummy) that impacts in the final numeric value, what is the default rate.","timestamp":"1665794580.0","comment_id":"695073"},{"content":"Selected Answer: B\nPredict default rate, it seem to be numberic value , it is not y Or n\nso it is regression\ndefault risk score = default rate","upvote_count":"1","comment_id":"673237","timestamp":"1663587840.0","poster":"John_Pongthorn"},{"comment_id":"653671","timestamp":"1661812260.0","poster":"ducc","content":"Selected Answer: B\nI did work for a real project\nWe always use linear regression or AUC for predicting default in loan processes.\nSo I go for B","upvote_count":"3"},{"timestamp":"1661408460.0","upvote_count":"1","content":"Selected Answer: B\nI vote for B","comment_id":"651669","poster":"ducc"},{"comment_id":"627560","content":"Selected Answer: B\nB or D, and I think B because the credit score already means a profile and no more data given but approved applications and defaults.","timestamp":"1657042080.0","upvote_count":"3","poster":"changsu"},{"poster":"cualquiernick","comment_id":"621068","content":"A is incorrect, there is no need of more data\nB Possiby correct answer is the default rates are numeric (int)\nC Not realy a solution\nD) Maybe the right answer, but not sure what social profiles has to be in relation of default payments.....\nD","upvote_count":"1","timestamp":"1655998200.0"},{"timestamp":"1655330220.0","upvote_count":"1","comment_id":"616985","poster":"hello_world404","content":"I agree it's B"},{"content":"Selected Answer: B\nB is correct","poster":"Georgesaju","upvote_count":"1","timestamp":"1653349920.0","comment_id":"606367"},{"comment_id":"578917","timestamp":"1648733940.0","upvote_count":"1","content":"Selected Answer: D\nfeaturing engineering folks (IMHO)","poster":"wences"},{"content":"Selected Answer: B\nB is correct option","upvote_count":"2","poster":"Mr_Stark","comment_id":"571499","timestamp":"1647765480.0"},{"comment_id":"518484","timestamp":"1641495120.0","content":"Selected Answer: C\nWe have only loan acceptance data, we do not have information about folks not getting loans thus our data is biased.","upvote_count":"5","poster":"medeis_jar"},{"content":"we dont have to predict risk score,we have to predict default rates, B and C are not correct.\nWe need to get data of person where the credit score is available and do train the dataset.Social profiles I believe is about persons.So manipulate data already present in loan department (risk score) also present and other data,Use feature engineering to predict","comment_id":"513275","upvote_count":"2","poster":"prasanna77","timestamp":"1640862360.0"},{"timestamp":"1640713620.0","comment_id":"511400","content":"Selected Answer: D\nA and B dont make sense - labelled dataset.\nC doesnt either, What should we do with the applications? ðŸ™‚\nD looks correct, in order to find new features for the model.","upvote_count":"3","poster":"MaxNRG"},{"timestamp":"1640254260.0","content":"Selected Answer: D\nA-It does not suggest a data size problem.\nB-We need a score. Linear regression cannot be an option.\nC-We need accepted loan applications.\nD-Complex to implement, but the most correct answer.","comment_id":"507711","poster":"Jlozano","upvote_count":"2","comments":[{"upvote_count":"3","content":"SCORE is continuous. So linear regression is right.","poster":"Deepakd","timestamp":"1649004000.0","comment_id":"580381"}]},{"timestamp":"1639979940.0","upvote_count":"1","comment_id":"505229","poster":"kishanu","content":"I guess it is A, as I think we have data insufficiency.\nThe Labeled data is for classification, where the application is granted or not, whereas the question is to train a model to predict the credit score which is a regression model.\nSo, we need to increase the size of the dataset, could be adding more features, labels etc.."},{"content":"D. \nA - Incorrect. There's no indication that size was the problem. \nB - Incorrect. The model is likely to be a classification model. So linear is not the best fit. \nC - Incorrect. You wouldn't need declined applications. How can you default without getting an approved application. It's not relevant.","upvote_count":"6","comment_id":"474560","timestamp":"1636420260.0","poster":"JayZeeLee"},{"poster":"omar_bahrain","upvote_count":"2","comment_id":"345887","timestamp":"1619758500.0","content":"A is the correct answer. please read:\nhttps://www.yields.io/blog/how-to-fix-bias-in-machine-learning-algorithms/"},{"upvote_count":"3","comments":[{"upvote_count":"2","timestamp":"1618862400.0","comment_id":"339126","content":"D, you have to do feature engineering to prepare data for Linear regression first , don't think its B because you don't have label for default rates.","poster":"[Removed]"}],"comment_id":"186334","content":"D you don't have the defaulted rate in hand only defaulted or not... it's classification","timestamp":"1600965540.0","poster":"zxing233"},{"poster":"haroldbenites","content":"B is correct","comment_id":"162552","upvote_count":"5","timestamp":"1597967760.0"},{"comment_id":"145539","poster":"hamzam","content":"I think B is the best answer","upvote_count":"4","timestamp":"1595921460.0"},{"timestamp":"1593306180.0","poster":"yxyj","comment_id":"121499","content":"D is right","upvote_count":"2"},{"timestamp":"1593137700.0","poster":"dambilwa","content":"Question not worded properly. Option [B] looks closest","upvote_count":"6","comment_id":"120101"},{"timestamp":"1590089520.0","comment_id":"93575","upvote_count":"5","poster":"arnabbis4u","content":"Answer B"},{"upvote_count":"1","poster":"[Removed]","comments":[{"upvote_count":"4","poster":"[Removed]","timestamp":"1584943260.0","comment_id":"67185","content":"Selected - B"},{"content":"Confused about the answer","timestamp":"1584891480.0","comment_id":"66992","upvote_count":"1","poster":"[Removed]"}],"content":"Ambiguos - B/D","timestamp":"1584891420.0","comment_id":"66991"}],"answer_ET":"B","answers_community":["B (66%)","D (17%)","Other"],"unix_timestamp":1584891420,"exam_id":10,"question_images":[],"question_id":311,"choices":{"D":"Match loan applicants with their social profiles to enable feature engineering.","A":"Increase the size of the dataset by collecting additional data.","B":"Train a linear regression to predict a credit default risk score.","C":"Remove the bias from the data and collect applications that have been declined loans."},"url":"https://www.examtopics.com/discussions/google/view/17256-exam-professional-data-engineer-topic-1-question-91/","question_text":"You work for a bank. You have a labelled dataset that contains information on already granted loan application and whether these applications have been defaulted. You have been asked to train a model to predict default rates for credit applicants.\nWhat should you do?"},{"id":"iYVatHSg3z7HeeD6ISbf","answer":"D","answer_ET":"D","answer_description":"","topic":"1","answers_community":["D (100%)"],"question_images":[],"choices":{"B":"Cloud Bigtable","A":"Cloud Spanner","D":"Cloud SQL","C":"Cloud Firestore"},"question_id":312,"question_text":"You need to migrate a 2TB relational database to Google Cloud Platform. You do not have the resources to significantly refactor the application that uses this database and cost to operate is of primary concern.\nWhich service do you select for storing and serving your data?","discussion":[{"comment_id":"66993","timestamp":"1600781940.0","content":"Answer - D","poster":"[Removed]","upvote_count":"24"},{"upvote_count":"13","comment_id":"68753","timestamp":"1601261460.0","poster":"[Removed]","content":"Answer: D\nDescription: Cloud SQl cheap and relational DB."},{"timestamp":"1693633980.0","content":"Selected Answer: D\nCloud SQL: max storage for shared core = 3TB and for dedicated core = up to 64TB\n\nOnly use Spanner if we need autoscale (Note that Cloud SQL could scale too but not automatic yet) or the size is too big (as above) or 4/5 9s HA (Cloud SQL is only 99.95)","comment_id":"826623","upvote_count":"2","poster":"midgoo"},{"upvote_count":"1","poster":"zellck","comment_id":"736783","timestamp":"1686046140.0","content":"Selected Answer: D\nD is the answer."},{"content":"Selected Answer: D\nCloud SQL is relational DB (pg mssql, mysql)","comment_id":"693119","upvote_count":"1","poster":"Nirca","timestamp":"1681306140.0"},{"content":"Answer - D","upvote_count":"1","comment_id":"614930","timestamp":"1670759700.0","poster":"Dhass"},{"poster":"homaj","content":"Selected Answer: D\nanswer D","timestamp":"1668198900.0","upvote_count":"1","comment_id":"600278"},{"upvote_count":"3","timestamp":"1641048540.0","content":"Vote for D","comment_id":"395963","poster":"sumanshu"},{"comment_id":"308357","timestamp":"1631388660.0","poster":"daghayeghi","content":"D:\nhttps://cloud.google.com/sql/docs/features","upvote_count":"1"},{"timestamp":"1623897120.0","content":"D, cloud SQL is a relational database; if > 10tb, then choose spanner","poster":"GypsyMonkey","upvote_count":"3","comment_id":"246191"},{"content":"D\nCloud SQL supports MySQL 5.6 or 5.7, and provides up to 624 GB of RAM and 30 TB of data storage, with the option to automatically increase the storage size as needed.","comment_id":"163383","poster":"atnafu2020","upvote_count":"3","timestamp":"1613982000.0","comments":[{"upvote_count":"5","timestamp":"1653357660.0","content":"64TB AS OF TODAY","comment_id":"485592","poster":"Abhi16820"}]},{"upvote_count":"3","timestamp":"1613872620.0","comment_id":"162553","poster":"haroldbenites","content":"D is correct. Obviously"},{"comment_id":"76130","upvote_count":"3","comments":[{"upvote_count":"1","timestamp":"1724514900.0","poster":"Preetmehta1234","content":"Nope. Now, the Dedicated core is Up to 64 TB\nhttps://cloud.google.com/sql/docs/quotas","comment_id":"1158078"},{"upvote_count":"13","poster":"taepyung","content":"At this moment, Cloud SQL is providing up to 30,720GB(about 30TB)\nSo I think it's D.","comment_id":"80602","timestamp":"1603863240.0"},{"comments":[{"content":"Well explained I can confirmed.","timestamp":"1718636340.0","poster":"LaxmanTiwari","upvote_count":"1","comment_id":"1099094"}],"timestamp":"1623415620.0","comment_id":"241007","upvote_count":"5","content":"Another consideration is that Cloud SQL uses standard databases like MySQL, PostgreSQL and now MS SQL. Cloud Spanner is a proprietary product of Google and does some things differently than typical databases (no stored procedures and triggers). So migrating to Cloud Spanner makes application refactoring necessary. So Cloud SQL is the answer.","poster":"xrun"},{"timestamp":"1606036260.0","comment_id":"93784","content":"Sorry , I think it's D \nhttps://cloud.google.com/sql/docs/features\n(Cloud SQL supports MySQL 5.6 or 5.7, and provides up to 416 GB of RAM and 30 TB of data storage, with the option to automatically increase the storage size as needed.)","poster":"Barniyah","upvote_count":"5"}],"timestamp":"1603042080.0","poster":"Barniyah","content":"But cloud SQL storage is limited to several hundreds of GB's for all instances \nand we need 2TB.\nSo, Cloud spanner is much closer to this, with the exception of the cost"}],"exam_id":10,"answer_images":[],"isMC":true,"timestamp":"2020-03-22 16:39:00","url":"https://www.examtopics.com/discussions/google/view/17257-exam-professional-data-engineer-topic-1-question-92/","unix_timestamp":1584891540},{"id":"kpph2DySAB4RJYoBzpmF","answer":"C","answer_description":"","answer_ET":"C","topic":"1","answers_community":["C (68%)","B (32%)"],"question_images":[],"choices":{"D":"Increase the size of your existing cluster twice and execute your analytics workload on your new resized cluster.","C":"Add a second cluster to an existing instance with a single-cluster routing, use live-traffic app profile for your regular workload and batch-analytics profile for the analytics workload.","B":"Add a second cluster to an existing instance with a multi-cluster routing, use live-traffic app profile for your regular workload and batch-analytics profile for the analytics workload.","A":"Export Bigtable dump to GCS and run your analytical job on top of the exported files."},"question_id":313,"question_text":"You're using Bigtable for a real-time application, and you have a heavy load that is a mix of read and writes. You've recently identified an additional use case and need to perform hourly an analytical job to calculate certain statistics across the whole database. You need to ensure both the reliability of your production application as well as the analytical workload.\nWhat should you do?","discussion":[{"timestamp":"1662398520.0","comments":[{"timestamp":"1724247840.0","content":"full example here: https://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve","poster":"nadavw","upvote_count":"1","comment_id":"1270162"},{"upvote_count":"2","poster":"HarshKothari21","content":"Agreed :)","timestamp":"1662657360.0","comment_id":"663836"},{"poster":"[Removed]","comment_id":"715750","timestamp":"1668139080.0","content":"\"When you use a single cluster\", here we are creating a 2nd cluster, so we'll be using 2 different clusters. We want to redirect analysis jobs to the 2nd cluster, and the other job to the 1st cluster. Thus, I think that D is more adequate","comments":[{"timestamp":"1670391840.0","comment_id":"737462","content":"Option D didnt say to create a new cluster, rather it said to increase the size of the cluster. There is a difference. Hence c is the correct answer to run the batch processing in a single cluster mode","poster":"somilaseeja","upvote_count":"2"}],"upvote_count":"1"}],"upvote_count":"22","comment_id":"660396","content":"Answer is C\n\nWhen you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\n\nhttps://cloud.google.com/bigtable/docs/replication-overview#use-cases","poster":"[Removed]"},{"poster":"aewis","comment_id":"951486","content":"Selected Answer: C\nIt was actually illustrated here\nhttps://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve","upvote_count":"5","timestamp":"1689335160.0"},{"comments":[{"content":"(an example of multicluster in this case would be 4 clusters, 2 for the transactional load and 2 for the analytical load)","upvote_count":"1","poster":"baimus","timestamp":"1727253780.0","comment_id":"1288935"}],"comment_id":"1288934","timestamp":"1727253720.0","poster":"baimus","upvote_count":"1","content":"Selected Answer: C\nTo answer some confusion - \"single cluster routing\" is routing to one cluster per profile, rather than having failover options per profile. So we have two clusters, but it's not multicluster, because we have two profiles, so it's one cluster per profile, so \"single cluster routing\". We COULD use multicluster, but none of the answers give the steps required to do so, so the assumption as to be that we're using single."},{"poster":"47767f9","content":"Selected Answer: B\nB better than C. Multi-cluster routing to handle failovers automatically. Reference: https://cloud.google.com/bigtable/docs/replication-settings#regional-failover","timestamp":"1719739320.0","upvote_count":"1","comment_id":"1239585"},{"timestamp":"1711537320.0","poster":"opt_sub","content":"Selected Answer: B\nB is correct. \nTwo different job profiles to redirect trafiic to two different cluster. C is incorrect because there is no tpoint in creating app profile for two different workloads in the same cluster. One cluster handles writes and another handle reads.","comment_id":"1184013","upvote_count":"2"},{"content":"Selected Answer: C\nIIt is C:\n\"Workload isolation:\nUsing separate app profiles lets you use different routing policies for different purposes. For example, consider a situation when you want to prevent a batch read job (workload A) from increasing CPU usage on clusters that handle an application's steady reads and writes (workload B). You can create an app profile for workload B that routes to a cluster group that excludes one cluster. Then you create an app profile for workload A that specifies single-cluster routing to the cluster that workload B doesn't send requests to.\n\nYou can change the settings for one application or function without affecting other applications that connect to the same data.\"\nhttps://cloud.google.com/bigtable/docs/app-profiles","timestamp":"1702108380.0","upvote_count":"3","poster":"carbino","comment_id":"1091501"},{"upvote_count":"3","comments":[{"content":"I see what you say on C but the question states high availability how do you handle that with option C when you have a single region cluster hence answer needs to be with multi-region cluster - To configure your instance for a high availability (HA) use case, create a new app profile that uses multi-cluster routing, or update the default app profile to use multi-cluster routing.","comments":[{"comment_id":"1201232","upvote_count":"1","poster":"zevexWM","timestamp":"1713950280.0","content":"It actually addresses the issue of High availability in that same link if you scroll down a bit more. \nhttps://cloud.google.com/bigtable/docs/replication-settings#high-availability"},{"poster":"A4M","timestamp":"1683187680.0","upvote_count":"1","content":"i meant single-cluster routing","comment_id":"889284"}],"timestamp":"1683187620.0","upvote_count":"1","poster":"A4M","comment_id":"889282"}],"timestamp":"1681489200.0","content":"Selected Answer: C\nhttps://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve","comment_id":"870326","poster":"DevShah"},{"content":"Selected Answer: C\nC. This is exactly the example in the documentation.\nhttps://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve","comments":[{"comment_id":"870324","poster":"DevShah","upvote_count":"1","timestamp":"1681489080.0","content":"Correct \n2 jobs >> 2 cluster\n3 jobs >> 3 cluster\napp profiles with single-cluster routing used to route to specific cluster\nJob1 >> Cluster 1\nJob2 >> Cluster 2 ....."}],"comment_id":"849579","timestamp":"1679686260.0","upvote_count":"3","poster":"juliobs"},{"content":"Answer B: \nreason 1: If you don' t have any cost constraint use multi-cluster routing, \nreason 2: Single cluster is less scalable as we need high scalability i would go with B","timestamp":"1676472300.0","upvote_count":"1","comment_id":"809656","poster":"musumusu"},{"content":"Selected Answer: C\nI am going for C?","comment_id":"799090","poster":"samdhimal","timestamp":"1675622160.0","upvote_count":"1"},{"content":"Selected Answer: C\nWhen you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\n\nSingle cluster routing - You can use single-cluster routing for this use case if you don't want your Bigtable cluster to automatically fail over if a zone or region becomes unavailable. \n\nMulti-cluster routing - If you want Bigtable to automatically fail over to one region if your application cannot reach the other region, use multi-cluster routing.","comment_id":"751943","timestamp":"1671605160.0","poster":"slade_wilson","upvote_count":"2"},{"content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve\nWhen you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.","upvote_count":"2","poster":"zellck","comment_id":"736781","timestamp":"1670328480.0"},{"poster":"Siant_137","content":"Answer is C\n\n\"When you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\"\n\nhttps://cloud.google.com/bigtable/docs/replication-overview#batch-vs-serve","comment_id":"734571","upvote_count":"2","timestamp":"1670085840.0"},{"poster":"sfsdeniso","comment_id":"725845","content":"Answer is C","timestamp":"1669295160.0","upvote_count":"1"},{"comment_id":"724127","poster":"dish11dish","content":"Selected Answer: B\nOption B is correct\n\nAn app profile specifies the routing policy that Bigtable should use for each request.\n\nSingle-cluster routing routes all requests to 1 cluster in your instance. If that cluster becomes unavailable, you must manually fail over to another cluster.\n\nMulti-cluster routing automatically routes requests to the nearest cluster in an instance. If the cluster becomes unavailable, traffic automatically fails over to the nearest cluster that is available. Bigtable considers clusters in a single region to be equidistant, even though they are in different zones. You can configure an app profile to route to any cluster in an instance, or you can specify a cluster group that tells the app profile to route to only some of the clusters in the instance.\n\nCluster group routing sends requests to the nearest available cluster within a cluster group that you specify in the app profile settings.\n\nReference:-https://cloud.google.com/bigtable/docs/app-profiles#routing","upvote_count":"3","timestamp":"1669099440.0"},{"comment_id":"723493","content":"Selected Answer: C\nhttps://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve\n\n\n\"When you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\"\n\nIt is C.","upvote_count":"2","timestamp":"1669034940.0","poster":"piotrpiskorski"},{"content":"Selected Answer: C\nC - \"With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\" - https://cloud.google.com/bigtable/docs/replication-overview#batch-vs-serve","upvote_count":"1","poster":"gudiking","comment_id":"721211","timestamp":"1668765360.0"},{"poster":"cloudmon","content":"Selected Answer: B\nIt's B\nhttps://cloud.google.com/bigtable/docs/replication-overview#app-profiles\nhttps://cloud.google.com/bigtable/docs/replication-overview#routing-policies","upvote_count":"2","timestamp":"1667762640.0","comment_id":"712597"},{"content":"Selected Answer: B\nIt is B . Add one cluster means you have multi clusters so you use Multi-cluster routing","timestamp":"1666969260.0","comment_id":"706567","upvote_count":"3","poster":"hauhau"},{"content":"Selected Answer: B\nI meant to vote B not C","comment_id":"704273","poster":"MisuLava","upvote_count":"3","timestamp":"1666743600.0"},{"timestamp":"1666743540.0","upvote_count":"4","content":"Selected Answer: C\nhow can you use multi-cluster routing when you want to go to old instance with the regular load and to the new one with the analytical node ? MULTI will go to the nearest one...\n\nMulti-cluster routing automatically routes requests to the nearest cluster in an instance. If the cluster becomes unavailable, traffic automatically fails over to the nearest cluster that is available. Bigtable considers clusters in a single region to be equidistant, even though they are in different zones. You can configure an app profile to route to any cluster in an instance, or you can specify a cluster group that tells the app profile to route to only some of the clusters in the instance.","poster":"MisuLava","comment_id":"704272"},{"comment_id":"692883","content":"Selected Answer: C\nAnswer is C","poster":"somnathmaddi","upvote_count":"1","timestamp":"1665565320.0"},{"timestamp":"1662206460.0","upvote_count":"2","content":"Selected Answer: C\nC. Add a second cluster to an existing instance with a single-cluster routing, use live-traffic app profile for your regular workload and batch-analytics profile for the analytics workload.","poster":"AWSandeep","comment_id":"658407"}],"exam_id":10,"answer_images":[],"isMC":true,"timestamp":"2022-09-03 14:01:00","url":"https://www.examtopics.com/discussions/google/view/79773-exam-professional-data-engineer-topic-1-question-93/","unix_timestamp":1662206460},{"id":"eHqUIrjqt2d2uolI4K0o","exam_id":10,"question_text":"You are designing an Apache Beam pipeline to enrich data from Cloud Pub/Sub with static reference data from BigQuery. The reference data is small enough to fit in memory on a single worker. The pipeline should write enriched results to BigQuery for analysis. Which job type and transforms should this pipeline use?","topic":"1","unix_timestamp":1584434040,"answer_description":"","answer_images":[],"timestamp":"2020-03-17 09:34:00","answer_ET":"C","answer":"C","question_id":314,"answers_community":["C (95%)","5%"],"discussion":[{"comment_id":"65093","timestamp":"1600324440.0","content":"Why not C? Without BigQueryIO how can data be written back to BigQuery?","poster":"rickywck","upvote_count":"31","comments":[{"upvote_count":"8","poster":"xq","content":"C should be right","timestamp":"1600596120.0","comment_id":"66253"}]},{"content":"Answer: C\nDescription: Sideinput for Bigquery data","comment_id":"68756","timestamp":"1601262120.0","upvote_count":"16","poster":"[Removed]"},{"comments":[{"content":"https://beam.apache.org/documentation/programming-guide/#side-inputs","upvote_count":"2","timestamp":"1719432960.0","comment_id":"1106420","poster":"JOKKUNO"}],"comment_id":"1106419","timestamp":"1719432900.0","content":"Side inputs\nIn addition to the main input PCollection, you can provide additional inputs to a ParDo transform in the form of side inputs. A side input is an additional input that your DoFn can access each time it processes an element in the input PCollection. When you specify a side input, you create a view of some other data that can be read from within the ParDo transformâ€™s DoFn while processing each element.\n\nSide inputs are useful if your ParDo needs to inject additional data when processing each element in the input PCollection, but the additional data needs to be determined at runtime (and not hard-coded). Such values might be determined by the input data, or depend on a different branch of your pipeline.","upvote_count":"2","poster":"JOKKUNO"},{"timestamp":"1708028220.0","comment_id":"981909","poster":"piyush7777","upvote_count":"1","content":"Why not side-output?"},{"poster":"TQM__9MD","timestamp":"1707029760.0","content":"Selected Answer: B\nB. Use multi-cluster routing to add a second cluster to the existing instance, utilizing a live traffic app profile for the regular workload and a batch analytics profile for the analytical workload.","upvote_count":"1","comment_id":"971694"},{"content":"Selected Answer: C\nThe answer is C. It's a trap so that you answer A because of batch vs streaming but you need BigQueryIO. On the other hand, streaming is absolutely redundant here and will incur extra costs. C is right but would be better with batch.","upvote_count":"2","poster":"Mathew106","timestamp":"1706011920.0","comment_id":"960327"},{"timestamp":"1688327340.0","comment_id":"764042","upvote_count":"1","content":"A is the Answer.\n A. Batch job, PubSubIO, side-inputs","poster":"Siadd"},{"upvote_count":"4","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/dataflow/docs/tutorials/ecommerce-java#side-input-pattern\nIn streaming analytics applications, data is often enriched with additional information that might be useful for further analysis. For example, if you have the store ID for a transaction, you might want to add information about the store location. This additional information is often added by taking an element and bringing in information from a lookup table.","poster":"zellck","comment_id":"736770","timestamp":"1686045600.0"},{"comment_id":"675307","comments":[{"upvote_count":"1","content":"dear can you please help, i have some questions about how to prepare the cerification exam using this questionnaire. this is my email cmayola@yahoo.fr, ping me to have some conversation","comment_id":"704162","poster":"chrismayola","timestamp":"1682454600.0"}],"content":"Selected Answer: C\nI got this question on sept 2022. Answer is C","timestamp":"1679422800.0","upvote_count":"3","poster":"sedado77"},{"upvote_count":"1","poster":"alex12441","comment_id":"539710","content":"Selected Answer: C\nAnswer: C","timestamp":"1659524640.0"},{"timestamp":"1657126620.0","poster":"medeis_jar","content":"Selected Answer: C\nI vote for C, because data will come from Pub/Sub, so it should be streaming, we'll need PubSubIO to be able to read from PubSub and BigQueryIO to be able to write to BigQuery, finally the side-inputs pattern let us enrich data","comment_id":"518488","upvote_count":"5"},{"poster":"MaxNRG","content":"Selected Answer: C\nStatic reference data from BigQuery will go as side-inputs and data from pub-sub will go as streaming data using PubSubIO and finally BigQueryIO is required to push the final data to BigQuery","comment_id":"511411","timestamp":"1656431520.0","upvote_count":"4"},{"poster":"JG123","timestamp":"1653611640.0","content":"Ans: C","upvote_count":"1","comment_id":"487789"},{"poster":"pals_muthu","comment_id":"432918","timestamp":"1645959600.0","upvote_count":"6","content":"Answer is C,\nYou need pubsubIO and BigQueryIO for streaming data and writing enriched data back to BigQuery. side-inputs are a way to enrich the data\nhttps://cloud.google.com/architecture/e-commerce/patterns/slow-updating-side-inputs"},{"upvote_count":"3","content":"I choose C, because data will come from Pub/Sub, so it should be streaming, we'll need PubSubIO to be able to read from PubSub y BigQueryIO to be able to write to BigQuery, finally the side-inputs pattern let us enrich data\nhttps://beam.apache.org/releases/javadoc/2.4.0/org/apache/beam/sdk/io/gcp/pubsub/PubsubIO.html\nhttps://cloud.google.com/architecture/e-commerce/patterns/slow-updating-side-inputs\nhttps://beam.apache.org/releases/javadoc/2.3.0/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html","timestamp":"1645233960.0","comment_id":"427114","poster":"Meuter"},{"content":"C:\nwe have to use Streaming job because of Pub/Sub, and side-input thanks to static reference data. and we have to leverage BigQueryIO since finally we want to write data to BigQuery. then C is the correct answer.","timestamp":"1631389560.0","comment_id":"308371","upvote_count":"2","poster":"daghayeghi"},{"comments":[{"poster":"funtoosh","content":"How you are going to write back to BQ?","comment_id":"294613","upvote_count":"1","timestamp":"1629400800.0"}],"poster":"someshsehgal","timestamp":"1629366060.0","upvote_count":"1","comment_id":"294162","content":"Correct A. batch is cost-effective and no need to go for streaming"},{"upvote_count":"2","comment_id":"284530","timestamp":"1628215260.0","poster":"someshsehgal","content":"Correct A: \nThere are two points to defend it. \na. Its batch hence free of cost\nb. static reference data hence no need to opt for streaming"},{"content":"After so much confusion I think correct answer is B..reason\n1.PubSub is always used for streaming input..no reason to mention\n2.JdbcIO can be used by apache beam to connect bigquery to use a lookup\n3.Sideoutput as a result to put in bigquery","comment_id":"200391","upvote_count":"2","timestamp":"1618476660.0","poster":"arghya13"},{"poster":"haroldbenites","timestamp":"1613872800.0","comment_id":"162556","content":"C is correct","upvote_count":"2"},{"content":"Questions says - with static reference data---so there is no need for streaming, Choice A seems correct.","comment_id":"146431","upvote_count":"2","poster":"AJKumar","comments":[{"poster":"zonc","timestamp":"1612442580.0","content":"But A answer doesn't have BigQueryIO.","comments":[{"comment_id":"156431","poster":"AJKumar","comments":[{"comment_id":"164792","content":"Without BQIO how could you send data back to BQ?","upvote_count":"3","timestamp":"1614138300.0","poster":"dragon123"},{"comment_id":"157143","poster":"zonc","content":"I don't think so. There are an input for streaming data from BigQuery and a side-input for BigQuery.","upvote_count":"1","timestamp":"1613213520.0"}],"upvote_count":"2","content":"\"The reference data is small enough to fit in memory on a single worker\" does not require BQIO. This is a trick question, A is the obvious answer.","timestamp":"1613142300.0"}],"comment_id":"150358","upvote_count":"3"}],"timestamp":"1611924420.0"},{"comment_id":"67003","poster":"[Removed]","upvote_count":"5","content":"Correct - C","timestamp":"1600783560.0"},{"poster":"Rajokkiyam","upvote_count":"5","timestamp":"1600744500.0","comment_id":"66805","content":"Answer :C"}],"isMC":true,"question_images":[],"choices":{"D":"Streaming job, PubSubIO, BigQueryIO, side-outputs","A":"Batch job, PubSubIO, side-inputs","C":"Streaming job, PubSubIO, BigQueryIO, side-inputs","B":"Streaming job, PubSubIO, JdbcIO, side-outputs"},"url":"https://www.examtopics.com/discussions/google/view/16841-exam-professional-data-engineer-topic-1-question-94/"},{"id":"Q2ckjy3t6JAH7NlyjRRN","answer_description":"","answer_images":[],"isMC":true,"discussion":[{"poster":"jvg637","comments":[{"comments":[{"comment_id":"194351","timestamp":"1617718020.0","content":"https://cloud.google.com/bigtable/docs/monitoring-instance","poster":"dabrat","upvote_count":"3"}],"content":"Storage utilization (% max) \nThe percentage of the cluster's storage capacity that is being used. The capacity is based on the number of nodes in your cluster.\n\nIn general, do not use more than 70% of the hard limit on total storage, so you have room to add more data. If you do not plan to add significant amounts of data to your instance, you can use up to 100% of the hard limit.\n\nImportant: If any cluster in an instance exceeds the hard limit on the amount of storage per node, writes to all clusters in that instance will fail until you add nodes to each cluster that is over the limit. Also, if you try to remove nodes from a cluster, and the change would cause the cluster to exceed the hard limit on storage, Cloud Bigtable will deny the request.\nIf you are using more than the recommended percentage of the storage limit, add nodes to the cluster. You can also delete existing data, but deleted data takes up more space, not less, until a compaction occurs.","timestamp":"1617718020.0","upvote_count":"4","poster":"dabrat","comment_id":"194350"},{"upvote_count":"1","comment_id":"457495","poster":"sergio6","timestamp":"1649135460.0","content":"Adding nodes to the cluster In Bigtable scales linearly the performances both read and write\nhttps://cloud.google.com/bigtable/docs/performance#typical-workloads"}],"comment_id":"66229","content":"Answer is C & D.\nC â€“> Adding more nodes to a cluster (not replication) can improve the write performance https://cloud.google.com/bigtable/docs/performance\nD â€“> since Google recommends adding nodes when storage utilization is > 70% https://cloud.google.com/bigtable/docs/modifying-instance#nodes","timestamp":"1600589160.0","upvote_count":"53"},{"comments":[{"content":"No. it is C, D. \"You have a data pipeline that writes data to Cloud Bigtable using well-designed row keys.\"\nwhy are you monitoring read anyway? you are just writing.","upvote_count":"14","timestamp":"1608558900.0","poster":"ch3n6","comment_id":"115536"}],"comment_id":"76160","timestamp":"1603048800.0","upvote_count":"10","poster":"Barniyah","content":"Key visualizer is bigtable metric , So A and B incorrect\nstorage utilization also bigtable metric , So D incorrect\nThe question want you to monitor pipeline metrics (which is dataflow metrics) , in our case we can only monitor latency .\nThe answer will be : C & E"},{"poster":"TVH_Data_Engineer","comment_id":"1214197","upvote_count":"1","timestamp":"1732096380.0","content":"Selected Answer: BC\nThe question focus is on writing. BC is correct. when the writing pressure is above 100, it is time to increase. same logic with C"},{"content":"why not B ?","timestamp":"1692105480.0","comments":[{"content":"i am feeling to go with B and D. In option C, when latency is low, latency can be low for write operation for other reason. \nbut in option B, its showing clearly when write pressure more than 100. But why no one is talking about B","poster":"musumusu","comment_id":"820594","timestamp":"1692879840.0","upvote_count":"2"}],"comment_id":"809677","poster":"musumusu","upvote_count":"1"},{"poster":"RoshanAshraf","content":"Selected Answer: CD\nKey visualizer is Metrics for Performance issues. Ruled out\nStorage and Write Operations ; C and D","upvote_count":"3","timestamp":"1689989820.0","comment_id":"783910"},{"timestamp":"1686045360.0","upvote_count":"3","poster":"zellck","comment_id":"736767","content":"Selected Answer: CD\nCD is the answer.\n\nhttps://cloud.google.com/bigtable/docs/monitoring-instance#disk\nStorage utilization (% max) \n- The percentage of the cluster's storage capacity that is being used. The capacity is based on the number of nodes in your cluster.\nIn general, do not use more than 70% of the hard limit on total storage, so you have room to add more data."},{"content":"Selected Answer: CD\nWell-designed row key : A B are not nessary\nWrite : CD both are involved in the question the most.","timestamp":"1678875360.0","upvote_count":"2","comment_id":"669649","poster":"John_Pongthorn"},{"timestamp":"1672825260.0","content":"Answer: CD\nhttps://cloud.google.com/bigtable/docs/scaling","comment_id":"626898","upvote_count":"2","poster":"Fezo"},{"content":"Selected Answer: CD\nas explained by MaxNRG","upvote_count":"2","comment_id":"518490","poster":"medeis_jar","timestamp":"1657126860.0"},{"timestamp":"1656431700.0","upvote_count":"3","content":"Selected Answer: CD\nD: In general, do not use more than 70% of the hard limit on total storage, so you have room to add more data. If you do not plan to add significant amounts of data to your instance, you can use up to 100% of the hard limit\nC: If this value is frequently at 100%, you might experience increased latency. Add nodes to the cluster to reduce the disk load percentage.\nThe key visualizer metrics options, suggest other things other than increase the cluster size.\nhttps://cloud.google.com/bigtable/docs/monitoring-instance","poster":"MaxNRG","comment_id":"511418"},{"upvote_count":"1","comment_id":"504078","content":"Selected Answer: CD\nCD.\n\nI agree with jvg637","poster":"hendrixlives","timestamp":"1655530860.0"},{"content":"Selected Answer: AD\nfrom https://cloud.google.com/bigtable/docs/monitoring-instance\nDisk load - If this value is frequently at 100%, you might experience increased latency. Add nodes to the cluster to reduce the disk load percentage.\nStorage utilization (% max) - In general, do not use more than 70% of the hard limit on total storage, so you have room to add more data. If you do not plan to add significant amounts of data to your instance, you can use up to 100% of the hard limit.","upvote_count":"2","poster":"StefanoG","timestamp":"1653833880.0","comment_id":"490029"},{"content":"I am Voting for CD","poster":"KokkiKumar","upvote_count":"2","timestamp":"1652225940.0","comment_id":"475912"},{"comment_id":"458307","timestamp":"1649253540.0","content":"Answer should be D & E","comments":[{"timestamp":"1665135000.0","content":"Why are you monitoring read operations, when youre supposed to write? why E?","upvote_count":"1","comment_id":"582336","poster":"tavva_prudhvi"}],"poster":"u_t_s","upvote_count":"1"},{"content":"D--> 70% is the the recommended percentage of the cluster's storage capacity that is being used, If you are using more than 70% storage limit, add nodes to the cluster \nhttps://cloud.google.com/bigtable/quotas#storage-per-node\nhttps://cloud.google.com/bigtable/docs/monitoring-instance#disk\nE--> 100 ms is an order of magnitude lower latency than Google claimed (<10ms)\nhttps://cloud.google.com/bigtable/docs/performance#typical-workloads","timestamp":"1649134980.0","upvote_count":"2","poster":"sergio6","comment_id":"457489"},{"upvote_count":"1","comment_id":"427949","timestamp":"1645348260.0","poster":"hauhau","content":"BC\nD:you can just add node, not cluster\nThe percentage of the cluster's storage capacity that is being used. The capacity is based on the number of nodes in your cluster.(https://cloud.google.com/bigtable/docs/monitoring-instance)\nAfter you create a Cloud Bigtable instance, you can update any of the following settings without any downtime:\n\n(The number of nodes in each cluster)\nhttps://cloud.google.com/bigtable/docs/modifying-instance"},{"comment_id":"396027","timestamp":"1641052620.0","comments":[{"timestamp":"1650240720.0","upvote_count":"2","comment_id":"463783","content":"Answer is C, D. \nB is not correct, because B is Key Visualizer, it means the row key needs re-design again.","poster":"squishy_fishy"},{"upvote_count":"3","poster":"sumanshu","timestamp":"1641052680.0","comment_id":"396028","content":"Vote for C & D, \nOption B eliminated, as Row are well defined (as per question) - so no need of key-visualizer"}],"content":"B, C , D - all three looks okay to me","upvote_count":"2","poster":"sumanshu"},{"comment_id":"308394","poster":"daghayeghi","timestamp":"1631390760.0","upvote_count":"3","content":"C, D:\nhttps://cloud.google.com/bigtable/docs/monitoring-instance"},{"comment_id":"192841","upvote_count":"4","timestamp":"1617531960.0","poster":"ajay1709","content":"C : To Improve write.\nD : As more storage is required"},{"upvote_count":"2","poster":"haroldbenites","comment_id":"162567","timestamp":"1613873580.0","content":"A, D is correct \nA: https://cloud.google.com/bigtable/docs/keyvis-metrics\nD: It makes sense unlike the others"},{"poster":"Rajokkiyam","content":"Well Defined RowKey So A & B out of scope.\nWith Increase in cluster Size you can only reduce the latency of Write operations. Read latency doesn't have any impact with increasing the cluster size. Ready latency attributes to Cluster replication so Option E is also ruled out. Answer C & D","comment_id":"130259","timestamp":"1610161620.0","comments":[{"content":"\"Well designed\" indeed excludes A&B as these refer to heavy loads on specific key ranges","upvote_count":"3","poster":"Ward010","timestamp":"1616848560.0","comment_id":"188312"}],"upvote_count":"7"},{"poster":"norwayping","content":"c.d https://cloud.google.com/bigtable/docs/scaling","upvote_count":"2","timestamp":"1609153680.0","comment_id":"121760"},{"comment_id":"68757","content":"Answer: C, D","poster":"[Removed]","timestamp":"1601262780.0","upvote_count":"4"},{"upvote_count":"4","poster":"[Removed]","timestamp":"1600783920.0","comments":[{"upvote_count":"3","content":"Selected - CD","timestamp":"1600833660.0","poster":"[Removed]","comment_id":"67186"}],"comment_id":"67006","content":"Answer - CD"},{"timestamp":"1600325280.0","upvote_count":"1","poster":"rickywck","content":"Why not BC? A is about Read performance.","comments":[{"poster":"Rajokkiyam","timestamp":"1600744680.0","upvote_count":"5","comment_id":"66806","content":"B is out of Scope because KeyVisualizer is used to identify the skew in data and help us to re-deisgn the rowkey. Since its already confirmed \"Well Defined Rowkey\" KeyViz is out of scope."}],"comment_id":"65100"}],"question_id":315,"url":"https://www.examtopics.com/discussions/google/view/16842-exam-professional-data-engineer-topic-1-question-95/","topic":"1","question_images":[],"answers_community":["CD (82%)","Other"],"exam_id":10,"timestamp":"2020-03-17 09:48:00","choices":{"C":"Monitor the latency of write operations. Increase the size of the Cloud Bigtable cluster when there is a sustained increase in write latency.","E":"Monitor latency of read operations. Increase the size of the Cloud Bigtable cluster of read operations take longer than 100 ms.","D":"Monitor storage utilization. Increase the size of the Cloud Bigtable cluster when utilization increases above 70% of max capacity.","B":"Review Key Visualizer metrics. Increase the size of the Cloud Bigtable cluster when the Write pressure index is above 100.","A":"Review Key Visualizer metrics. Increase the size of the Cloud Bigtable cluster when the Read pressure index is above 100."},"answer":"CD","answer_ET":"CD","unix_timestamp":1584434880,"question_text":"You have a data pipeline that writes data to Cloud Bigtable using well-designed row keys. You want to monitor your pipeline to determine when to increase the size of your Cloud Bigtable cluster. Which two actions can you take to accomplish this? (Choose two.)"}],"exam":{"isBeta":false,"name":"Professional Data Engineer","isMCOnly":true,"id":10,"lastUpdated":"15 Feb 2025","isImplemented":true,"numberOfQuestions":319,"provider":"Google"},"currentPage":63},"__N_SSP":true}