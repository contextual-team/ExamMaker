{"pageProps":{"questions":[{"id":"5jni3XsrsF6AEKSksoUa","answer":"A","answers_community":["A (82%)","D (18%)"],"unix_timestamp":1662141480,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/79529-exam-professional-data-engineer-topic-1-question-194/","discussion":[{"timestamp":"1679830200.0","content":"A and D are both good. I go for A because we have high volume and easy to scale and optmize cost","upvote_count":"6","poster":"lucaluca1982","comment_id":"850941"},{"comment_id":"1240087","timestamp":"1719827100.0","content":"Selected Answer: D\nD is the answer.","upvote_count":"2","poster":"kajitsu","comments":[{"comment_id":"1272061","content":"There is no need for a composer to call a Python API only - it's an overkill.","poster":"nadavw","upvote_count":"2","timestamp":"1724574660.0"}]},{"comment_id":"1052275","timestamp":"1698097020.0","upvote_count":"3","content":"Answer is D, at work we use solution A for low volume of Pub/Sub messages and Cloud function, and using D Composer for high volume Pub/Sub messages.","poster":"squishy_fishy"},{"content":"Answer A:\nassume, Company wants to buy immediately in same second if stock goes down or up. \nSomehow, it is connected to PubSub as SINK connector, then immediately there is PUSH to subcriber (cloud function) that is connected to their python API (internal application) that makes the purchase.","comment_id":"814117","timestamp":"1676813220.0","upvote_count":"4","poster":"musumusu"},{"comment_id":"763420","timestamp":"1672621020.0","poster":"AzureDP900","content":"A. Use a Pub/Sub push subscription to trigger a Cloud Function to pass the data to the Python API.","upvote_count":"1"},{"timestamp":"1669653840.0","comment_id":"729434","poster":"zellck","upvote_count":"3","content":"Selected Answer: A\nA is the answer."},{"upvote_count":"4","poster":"GCPCloudArchitectUser","comment_id":"714817","timestamp":"1668037800.0","content":"Selected Answer: A\nBecause trading platform requires securely transmission to queuing \nIf you use cloud compose then we need some other job to trigger composer … would that be cloud composer api or cloud function …"},{"timestamp":"1664426760.0","upvote_count":"1","poster":"TNT87","comment_id":"682342","content":"https://cloud.google.com/functions/docs/calling/pubsub"},{"timestamp":"1662614160.0","upvote_count":"4","poster":"TNT87","content":"Selected Answer: A\nAns A\nhttps://cloud.google.com/functions/docs/calling/pubsub#deployment","comment_id":"663139"},{"comment_id":"661144","upvote_count":"3","content":"Selected Answer: A\nA because D is stupidly high latency","timestamp":"1662463080.0","poster":"YorelNation"},{"comment_id":"659758","poster":"nwk","upvote_count":"1","timestamp":"1662356040.0","content":"Vote A, can't see the need for composer"},{"comment_id":"659092","poster":"soichirokawa","timestamp":"1662280680.0","upvote_count":"3","content":"A might be enough. Cloud composer will be an overkill"},{"comment_id":"658047","upvote_count":"4","poster":"AWSandeep","content":"A. Use a Pub/Sub push subscription to trigger a Cloud Function to pass the data to the Python API.","timestamp":"1662177480.0"},{"content":"Selected Answer: D\nD is a more recommend way by Google, IMO.","poster":"ducc","comments":[{"content":"I agree, at work use solution A for low volume of Pub/Sub messages and function, and using Composer for high volume Pub/Sub messages.","timestamp":"1698096960.0","comment_id":"1052273","poster":"squishy_fishy","upvote_count":"2"}],"upvote_count":"1","timestamp":"1662170040.0","comment_id":"657968"},{"timestamp":"1662141480.0","upvote_count":"1","comments":[{"timestamp":"1662169980.0","poster":"ducc","upvote_count":"2","content":"Composer support exception and retry for complex pipeline.\nD might be correct","comment_id":"657967"}],"content":"A. more sense to me.","comment_id":"657687","poster":"PhuocT"}],"choices":{"C":"Write an application that makes a queue in a NoSQL database.","B":"Write an application hosted on a Compute Engine instance that makes a push subscription to the Pub/Sub topic.","A":"Use a Pub/Sub push subscription to trigger a Cloud Function to pass the data to the Python API.","D":"Use Cloud Composer to subscribe to a Pub/Sub topic and call the Python API."},"question_text":"An online brokerage company requires a high volume trade processing architecture. You need to create a secure queuing system that triggers jobs. The jobs will run in Google Cloud and call the company's Python API to execute trades. You need to efficiently implement a solution. What should you do?","isMC":true,"answer_images":[],"question_id":106,"timestamp":"2022-09-02 19:58:00","answer_description":"","answer_ET":"A","exam_id":10,"question_images":[]},{"id":"EXzOYR0uoP5XW2oSzmcv","topic":"1","answer_images":[],"answer_description":"","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/79645-exam-professional-data-engineer-topic-1-question-195/","unix_timestamp":1662170160,"question_id":107,"question_text":"Your company wants to be able to retrieve large result sets of medical information from your current system, which has over 10 TBs in the database, and store the data in new tables for further query. The database must have a low-maintenance architecture and be accessible via SQL. You need to implement a cost-effective solution that can support data analytics for large result sets. What should you do?","exam_id":10,"timestamp":"2022-09-03 03:56:00","answer_ET":"B","choices":{"A":"Use Cloud SQL, but first organize the data into tables. Use JOIN in queries to retrieve data.","D":"Use Cloud Spanner to replicate the data across regions. Normalize the data in a series of tables.","C":"Use a MySQL cluster installed on a Compute Engine managed instance group for scalability.","B":"Use BigQuery as a data warehouse. Set output destinations for caching large queries."},"discussion":[{"poster":"AWSandeep","upvote_count":"8","comment_id":"658048","timestamp":"1677823260.0","content":"Selected Answer: B\nB. Use BigQuery as a data warehouse. Set output destinations for caching large queries."},{"timestamp":"1718985420.0","poster":"MaxNRG","content":"Selected Answer: B\nOption B is the best approach - use BigQuery as a data warehouse, and set output destinations for caching large queries.\n\nThe key reasons why BigQuery fits the requirements:\n\nIt is a fully managed data warehouse built to scale to handle massive datasets and perform fast SQL analytics\nIt has a low maintenance architecture with no infrastructure to manage\nSQL capabilities allow easy querying of the medical data\nOutput destinations allow configurable caching for fast retrieval of large result sets\nIt provides a very cost-effective solution for these large scale analytics use cases\nIn contrast, Cloud Spanner and Cloud SQL would not scale as cost effectively for 10TB+ data volumes. Self-managed MySQL on Compute Engine also requires more maintenance. Hence, leveraging BigQuery as a fully managed data warehouse is the optimal solution here.","comment_id":"1102826","upvote_count":"3"},{"poster":"AzureDP900","timestamp":"1688252280.0","upvote_count":"2","content":"B. Use BigQuery as a data warehouse. Set output destinations for caching large queries. Most Voted","comment_id":"763421"},{"poster":"zellck","comment_id":"729428","upvote_count":"3","content":"Selected Answer: B\nB is the answer.","timestamp":"1685284440.0"},{"content":"Answer B.\nhttps://cloud.google.com/bigquery/docs/query-overview","comment_id":"666721","timestamp":"1678615020.0","upvote_count":"4","poster":"TNT87"},{"comment_id":"657969","timestamp":"1677815760.0","upvote_count":"2","poster":"ducc","content":"Selected Answer: B\nB is correct"}],"answer":"B","answers_community":["B (100%)"],"question_images":[]},{"id":"SgR70gnOWGGee326gLpt","isMC":true,"question_id":108,"topic":"1","answer":"C","url":"https://www.examtopics.com/discussions/google/view/79646-exam-professional-data-engineer-topic-1-question-196/","answer_description":"","answer_images":[],"question_images":[],"timestamp":"2022-09-03 03:57:00","exam_id":10,"question_text":"You have 15 TB of data in your on-premises data center that you want to transfer to Google Cloud. Your data changes weekly and is stored in a POSIX-compliant source. The network operations team has granted you 500 Mbps bandwidth to the public internet. You want to follow Google-recommended practices to reliably transfer your data to Google Cloud on a weekly basis. What should you do?","unix_timestamp":1662170220,"discussion":[{"comment_id":"729379","upvote_count":"9","timestamp":"1716906420.0","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#storage-transfer-service-for-large-transfers-of-on-premises-data\nLike gsutil, Storage Transfer Service for on-premises data enables transfers from network file system (NFS) storage to Cloud Storage. Although gsutil can support small transfer sizes (up to 1 TB), Storage Transfer Service for on-premises data is designed for large-scale transfers (up to petabytes of data, billions of files).","poster":"zellck"},{"content":"answer C, \nTo avoid confustion: Install Storage Transfer Service is always on EXTERNAL OR NON GOOGLE service or data centre to connect google service.","poster":"musumusu","comment_id":"814123","timestamp":"1724067540.0","upvote_count":"7"},{"content":"Selected Answer: C\nC is the Answer as we need weekly run Storage transfer service has the feature to schedule.","poster":"Prudvi3266","timestamp":"1729522800.0","upvote_count":"3","comment_id":"876656"},{"upvote_count":"3","comment_id":"725895","poster":"NicolasN","timestamp":"1716552000.0","content":"Selected Answer: C\nThe fact that it's about a POSIX source makes necessary the set up of Storage Transfer Service agents. \nThis detail limits [C] to be the correct answer, since it's the data center hosting the files where the agent must be installed.\n--\nSome excerpts:\n(an older version of documentation was definite)\n\"The following is a high-level overview of how Transfer service for on-premises data works:\n 1.Install Docker and run a small piece of software, called an agent, in your private data center. \"\nSource: https://web.archive.org/web/20210529161414/https://cloud.google.com/storage-transfer/docs/on-prem-overview\n--\n\"Storage Transfer Service agents are applications running inside a Docker container, that coordinate with Storage Transfer Service to read data from POSIX file system sources, and/or write data to POSIX file system sinks.\nIf your transfer does not involve a POSIX file system, you do not need to set up agents.\"\nSource: https://cloud.google.com/storage-transfer/docs/managing-on-prem-agents"},{"comment_id":"725629","upvote_count":"2","timestamp":"1716527940.0","poster":"Atnafu","content":"C\nStorage Transfer Service agents are applications running inside a Docker container, that coordinate with Storage Transfer Service to read data from POSIX file system sources, and/or write data to POSIX file system sinks.\n\nhttps://cloud.google.com/storage-transfer/docs/managing-on-prem-agents#:~:text=Storage%20Transfer%20Service%20agents,agents%20on%20your%20servers."},{"comment_id":"676100","content":"why can't it be D?","comments":[{"content":"Installation is done for the source which is in your data centre, and not in GCP.","poster":"zellck","comment_id":"729382","upvote_count":"2","timestamp":"1716906660.0"}],"upvote_count":"1","timestamp":"1711117080.0","poster":"namo621"},{"upvote_count":"2","comment_id":"667483","poster":"Wasss123","timestamp":"1710287580.0","content":"Selected Answer: C\nI vote for C"},{"upvote_count":"2","comment_id":"666699","timestamp":"1710235620.0","content":"Ans C \nhttps://cloud.google.com/storage-transfer/docs/overview","poster":"TNT87"},{"comment_id":"665462","timestamp":"1710087300.0","comments":[{"upvote_count":"2","comment_id":"725216","timestamp":"1716468960.0","poster":"gudiking","content":"If you install the software for on-premise data center on a Google Cloud VM, then it's not on-premise, it's on GCP, so it can't access your on-premise data."}],"poster":"MounicaN","upvote_count":"3","content":"can you help with difference between c and d ?"},{"timestamp":"1709446500.0","content":"Selected Answer: C\nC. Install Storage Transfer Service for on-premises data in your data center, and then configure a weekly transfer job.","comment_id":"658050","upvote_count":"2","poster":"AWSandeep"},{"upvote_count":"2","content":"Selected Answer: C\nC is correct","timestamp":"1709438220.0","comment_id":"657971","poster":"ducc"}],"choices":{"A":"Use Cloud Scheduler to trigger the gsutil command. Use the -m parameter for optimal parallelism.","C":"Install Storage Transfer Service for on-premises data in your data center, and then configure a weekly transfer job.","B":"Use Transfer Appliance to migrate your data into a Google Kubernetes Engine cluster, and then configure a weekly transfer job.","D":"Install Storage Transfer Service for on-premises data on a Google Cloud virtual machine, and then configure a weekly transfer job."},"answer_ET":"C","answers_community":["C (100%)"]},{"id":"2DaMHklP9eiTdoIAUUV7","discussion":[{"timestamp":"1667828940.0","comments":[{"comment_id":"1052282","content":"Will you change your answer if the answer D says dataset instead of table?","timestamp":"1698097920.0","upvote_count":"2","poster":"squishy_fishy"},{"poster":"Mcloudgirl","upvote_count":"2","content":"Your explanation is perfect, thanks","comment_id":"714993","timestamp":"1668064140.0"}],"content":"Selected Answer: B\nWe exclude [C[ as non ACID and [D] for being invalid (location is configured on Dataset level, not Table).\nThen, let's focus on \"minimal human intervention in case of a failure\" requirement in order to eliminate one answer among [A] and [B]. \nBasically, we have to compare point-in-time recovery with high availability. It doesn't matter whether it's about MySQL or PostgreSQL since both databases support those features.\n- Point-in-time recovery logs are created automatically, but restoring an instance in case of failure requires manual steps (described here: https://cloud.google.com/sql/docs/mysql/backup-recovery/pitr#perform-pitr)\n- High availability, in case of failure requires no human intervention: \"If an HA-configured instance becomes unresponsive, Cloud SQL automatically switches to serving data from the standby instance.\" (from https://cloud.google.com/sql/docs/postgres/high-availability#failover-overview)\nSo answer [B] wins.","poster":"NicolasN","upvote_count":"43","comment_id":"713071"},{"content":"Selected Answer: B\nIts B because of HA\ncant be A because point in time recovery still requires human intervention","upvote_count":"1","poster":"edre","timestamp":"1721324340.0","comment_id":"1250594"},{"content":"Selected Answer: B\nThe best option to meet the ACID compliance and minimal human intervention requirements is to configure a Cloud SQL for PostgreSQL instance with high availability enabled.\n\nKey reasons:\n\nCloud SQL for PostgreSQL provides full ACID compliance, unlike Bigtable which provides only atomicity and consistency guarantees.\nEnabling high availability removes the need for manual failover as Cloud SQL will automatically failover to a standby replica if the leader instance goes down.\nPoint-in-time recovery in MySQL requires manual intervention to restore data if needed.\nBigQuery does not provide transactional guarantees required for an ACID database.\nTherefore, a Cloud SQL for PostgreSQL instance with high availability meets the ACID and minimal intervention requirements best. The automatic failover will ensure availability and uptime without administrative effort.","upvote_count":"2","poster":"MaxNRG","comment_id":"1102912","timestamp":"1703187840.0"},{"content":"Selected Answer: D\nI vote for D - BigQuery with multi region configuration. \nAccording to https://cloud.google.com/bigquery/docs/introduction , BigQuery support ACID and automatically replicated for high availability.\n\"\"\"BigQuery stores data using a columnar storage format that is optimized for analytical queries. BigQuery presents data in tables, rows, and columns and provides full support for database transaction semantics (ACID). BigQuery storage is automatically replicated across multiple locations to provide high availability.\"\"\"","poster":"[Removed]","comment_id":"976427","upvote_count":"2","timestamp":"1691573760.0"},{"comment_id":"961460","content":"Selected Answer: B\nOption B","upvote_count":"1","timestamp":"1690194120.0","poster":"vamgcp"},{"upvote_count":"2","timestamp":"1676813700.0","content":"Answer B,\nACID -compliant database are Spanner and CloudSQL\nOption A could be the answer if they setup a secondary or failure replicas and auto maintenance window that could trigger in non business hours. \nOption B, does not explain about extra replica but in postgresql Highavailablity option means the same extra replicas instances are available for emergency.","comment_id":"814121","poster":"musumusu"},{"content":"B. Configure a Cloud SQL for PostgreSQL instance with high availability enabled.","upvote_count":"1","comment_id":"763423","poster":"AzureDP900","timestamp":"1672621320.0"},{"poster":"zellck","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/sql/docs/postgres/high-availability#HA-configuration\nThe purpose of an HA configuration is to reduce downtime when a zone or instance becomes unavailable. This might happen during a zonal outage, or when an instance runs out of memory. With HA, your data continues to be available to client applications.","timestamp":"1669652580.0","upvote_count":"3","comment_id":"729374"},{"timestamp":"1669241100.0","upvote_count":"1","comment_id":"725409","poster":"samirzubair","content":"I voted for B"},{"upvote_count":"1","poster":"John_Pongthorn","timestamp":"1664263080.0","comment_id":"680477","content":"Selected Answer: B\nB it is exact anwer."},{"upvote_count":"2","content":"Selected Answer: B\nAns B\nPostgres is highly ACID compliant as compared to Mysql","timestamp":"1662967680.0","poster":"TNT87","comment_id":"666700"},{"poster":"Remi2021","comment_id":"665378","upvote_count":"2","content":"Selected Answer: B\ncloud sql with high availability enabled is enough","timestamp":"1662811500.0"},{"comment_id":"658054","content":"Selected Answer: B\nB. Configure a Cloud SQL for PostgreSQL instance with high availability enabled.","upvote_count":"1","poster":"AWSandeep","timestamp":"1662178800.0"},{"poster":"ducc","content":"Selected Answer: B\nI voted for B","comment_id":"657972","upvote_count":"1","timestamp":"1662170220.0"}],"question_id":109,"answer_ET":"B","answer":"B","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/79647-exam-professional-data-engineer-topic-1-question-197/","question_images":[],"exam_id":10,"topic":"1","question_text":"You are designing a system that requires an ACID-compliant database. You must ensure that the system requires minimal human intervention in case of a failure.\nWhat should you do?","timestamp":"2022-09-03 03:57:00","answers_community":["B (97%)","3%"],"isMC":true,"answer_description":"","unix_timestamp":1662170220,"choices":{"C":"Configure a Bigtable instance with more than one cluster.","B":"Configure a Cloud SQL for PostgreSQL instance with high availability enabled.","A":"Configure a Cloud SQL for MySQL instance with point-in-time recovery enabled.","D":"Configure a BigQuery table with a multi-region configuration."}},{"id":"4poQTTvawlwsNki1LS3v","question_images":[],"exam_id":10,"url":"https://www.examtopics.com/discussions/google/view/79648-exam-professional-data-engineer-topic-1-question-198/","answer_description":"","timestamp":"2022-09-03 04:00:00","answer_images":[],"discussion":[{"comment_id":"658057","content":"Selected Answer: D\nD. Use Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the service project.\n\nShared VPC requires that you designate a host project to which networks and subnetworks belong and a service project, which is attached to the host project. When Cloud Composer participates in a Shared VPC, the Cloud Composer environment is in the service project.\n\nReference:\nhttps://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc","timestamp":"1662179100.0","upvote_count":"17","poster":"AWSandeep"},{"timestamp":"1737878700.0","upvote_count":"1","content":"Selected Answer: D\nUsing the host project to deploy services is a bad practice and should only be used if the service doesn't support shared VPC. It may be easier networking wise, but that's why it's the wrong answer. Cloud Composer supports Shaved VPC, so it should go in it's own project.","comment_id":"1346841","poster":"loki82"},{"content":"Selected Answer: C\nPlace in host project for network connectivity","comment_id":"1306667","upvote_count":"1","poster":"8284a4c","timestamp":"1730672460.0"},{"content":"Selected Answer: C\nThe recommended approach is to place Cloud Composer resources in the host project of the Shared VPC. This centralizes network management, simplifies connectivity, and enhances security by adhering to the principle of least privilege.","timestamp":"1730462520.0","upvote_count":"2","comment_id":"1305782","poster":"ToiToi"},{"comment_id":"1217339","timestamp":"1716542460.0","content":"Selected Answer: C\nPlacing Cloud Composer resources in the service project can lead to more complex network configurations and management overhead compared to placing them in the host project, which is designed to manage Shared VPC resources.","upvote_count":"2","poster":"TVH_Data_Engineer"},{"comment_id":"961449","comments":[{"timestamp":"1699310760.0","content":"https://cloud.google.com/composer/docs/composer-2/configure-shared-vpc#shared-vpc-guidelines","poster":"spicebits","comment_id":"1064317","upvote_count":"1"}],"upvote_count":"2","poster":"vamgcp","content":"Please correct if I am wrong.. I think it is Option C coz I feel Option D is incorrect because placing the Cloud Composer resources in the service project would not allow you to access resources in the host project.","timestamp":"1690193340.0"},{"comments":[{"poster":"ckanaar","comment_id":"1012394","timestamp":"1695219060.0","content":"That's answer D though.","upvote_count":"2"}],"poster":"Ender_H","comment_id":"915547","upvote_count":"1","content":"Selected Answer: A\n✅ Use Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the service project.\n\n- Cloud Composer is a managed Apache Airflow service. It is an open-source tool that programmatically author, schedule, and monitor pipelines, which fits your needs perfectly.\n- In a Shared VPC configuration, Cloud Composer resources should be placed in the service project. This provides network isolation while still allowing the Cloud Composer environment to communicate with resources in the host project.\n- With Shared VPC, the host project's network (including its subnets and secondary IP ranges) is shared by other service projects, which promotes network peering, and it's compliant with the networking considerations of GKE.","timestamp":"1685980560.0"},{"content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc\nShared VPC enables organizations to establish budgeting and access control boundaries at the project level while allowing for secure and efficient communication using private IPs across those boundaries. In the Shared VPC configuration, Cloud Composer can invoke services hosted in other Google Cloud projects in the same organization without exposing services to the public internet.","upvote_count":"3","comment_id":"729322","poster":"zellck","comments":[{"upvote_count":"1","content":"I thought it was C, but after reading docs I realized it is D. \nIn D case we can have isolation and reduce project complicity(It could be overlapped resources in Host project, and it is harder to restrict composer access for Host project resources)","poster":"unnamed12355","comment_id":"848934","timestamp":"1679631060.0"},{"upvote_count":"2","comment_id":"763424","content":"Agreed","poster":"AzureDP900","comments":[{"comments":[{"upvote_count":"1","timestamp":"1682919660.0","comment_id":"885899","poster":"Oleksandr0501","content":"D it is, as per doc link, provided by users. thx"}],"poster":"Oleksandr0501","content":"agreed to what..","upvote_count":"1","comment_id":"885888","timestamp":"1682917920.0"}],"timestamp":"1672621440.0"},{"upvote_count":"2","timestamp":"1677284940.0","comment_id":"821052","content":"why not option C? if composer in host project it will be easier to connect one or more service project with it.","poster":"musumusu"}],"timestamp":"1669649400.0"},{"timestamp":"1669275060.0","content":"D\nShared VPC requires that you designate a host project to which networks and subnetworks belong and a service project, which is attached to the host project. \nhttps://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc#:~:text=This%20page%20describes,the%20service%20project.","upvote_count":"2","comment_id":"725632","poster":"Atnafu"},{"poster":"ducc","timestamp":"1662170400.0","comment_id":"657974","content":"Selected Answer: D\nD according to documentation\n\nShared VPC requires that you designate a host project to which networks and subnetworks belong and a service project, which is attached to the host project. When Cloud Composer participates in a Shared VPC, the Cloud Composer environment is in the service project.\n\nhttps://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc#set_up_shared_vpc_and_attach_the_service_project","upvote_count":"2"}],"choices":{"C":"Use Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the host project.","B":"Use Dataflow for your workflow pipelines. Use shell scripts to schedule workflows.","D":"Use Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the service project.","A":"Use Dataflow for your workflow pipelines. Use Cloud Run triggers for scheduling."},"isMC":true,"answers_community":["D (79%)","C (17%)","3%"],"question_text":"You are implementing workflow pipeline scheduling using open source-based tools and Google Kubernetes Engine (GKE). You want to use a Google managed service to simplify and automate the task. You also want to accommodate Shared VPC networking considerations. What should you do?","answer":"D","unix_timestamp":1662170400,"topic":"1","question_id":110,"answer_ET":"D"}],"exam":{"isMCOnly":true,"numberOfQuestions":319,"isBeta":false,"lastUpdated":"15 Feb 2025","provider":"Google","isImplemented":true,"name":"Professional Data Engineer","id":10},"currentPage":22},"__N_SSP":true}