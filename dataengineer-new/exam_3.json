{"pageProps":{"questions":[{"id":"mNLQXrgZrsR68l5GqxX1","choices":{"C":"Establish a Cloud Interconnect between all remote data centers and Google.","B":"Have the data acquisition devices publish data to Cloud Pub/Sub.","D":"Write a Cloud Dataflow pipeline that aggregates all data in session windows.","A":"Deploy small Kafka clusters in your data centers to buffer events."},"timestamp":"2020-03-22 13:58:00","question_images":[],"answer_ET":"B","answer":"B","question_text":"You operate a logistics company, and you want to improve event delivery reliability for vehicle-based sensors. You operate small data centers around the world to capture these events, but leased lines that provide connectivity from your event collection infrastructure to your event processing infrastructure are unreliable, with unpredictable latency. You want to address this issue in the most cost-effective way. What should you do?","answers_community":["B (70%)","11%","Other"],"exam_id":10,"answer_images":[],"answer_description":"","topic":"1","isMC":true,"discussion":[{"upvote_count":"31","poster":"[Removed]","timestamp":"1584881880.0","content":"Should be B","comment_id":"66937"},{"comment_id":"73991","content":"C.\nThis is a tricky one. The issue here is the unreliable connection between data collection and data processing infrastructure, and to resolve it in a cost-effective manner. However, it also mentions that the company is using leased lines. I think replacing the leased lines with Cloud InterConnect would solve the problem, and hopefully not be an added expense. \nhttps://cloud.google.com/interconnect/docs/concepts/overview","timestamp":"1586766000.0","upvote_count":"22","poster":"Ganshank","comments":[{"comment_id":"101809","content":"Yea, this would definitely solve the issue, but it's not \"the most cost-effective way\". I think PubSub is the correct answer.","poster":"serg3d","upvote_count":"7","timestamp":"1591206900.0"},{"timestamp":"1605799320.0","content":"the question also talks about a cost effective way...","poster":"snamburi3","upvote_count":"3","comment_id":"222886"},{"content":"I agree, C is the only choice that addresses the problem. The problem is caused by leased line. How come pub/sub service can resolve it? Pub/sub will still use the leased line","comment_id":"114014","upvote_count":"5","poster":"sh2020","timestamp":"1592575260.0"},{"timestamp":"1625522700.0","comment_id":"399486","upvote_count":"7","poster":"awssp12345","content":"DEFINITELY NOT COST EFFECT. C IS THE WORST CHOICE."},{"comments":[{"upvote_count":"1","comment_id":"760769","content":"Thanks for this explanation. I was also going for C initially. But you convinced me that B is the correct answer.","poster":"Catweazle1983","timestamp":"1672302060.0"}],"poster":"fire558787","upvote_count":"17","content":"Disagree. OK, the problem is between the data centers and Google's data centers. However, Cloud Interconnect costs money. If the devices write to PubSub instead, they would bypass their own data centers and write directly to Google. It seems that connectivity is not a problem for the collecting devices, since it says that data is collected in local data centers. So my guess goes towards PubSub rather than Interconnect.","comment_id":"426345","timestamp":"1629206100.0"},{"poster":"Yiouk","upvote_count":"1","content":"C. Can you imagine changing the software in all sensors to use PupSub instead of the existing one? This is out of scope of the question.","comment_id":"951406","timestamp":"1689326640.0"}]},{"poster":"grshankar9","timestamp":"1737240060.0","upvote_count":"1","content":"Selected Answer: B\nThe data acquisition devices are spread across the globe. How many cloud interconnects would this require? Option C is definitely not cost effective. Option B seems to make sense as it suggests publishing from the acquisition device to Pub/Sub and not from the collection infrastructure. The problem clearly stated the connection from collection infrastructure to the processing infrastructure was unreliable.","comment_id":"1342784"},{"timestamp":"1718631660.0","content":"Selected Answer: B\nOption B: Have the data acquisition devices publish data to Cloud Pub/Sub.\n\nRationale:\n\nManaged Service: Cloud Pub/Sub is a fully managed service, reducing the operational overhead compared to managing Kafka clusters.\nReliability and Scalability: Cloud Pub/Sub can handle high volumes of data with low latency and provides built-in mechanisms for reliable message delivery, even in the face of intermittent connectivity.\nCost-Effective: Cloud Pub/Sub offers a pay-as-you-go pricing model, which can be more cost-effective than setting up and maintaining dedicated network infrastructure like Cloud Interconnect.\nGlobal Availability: Cloud Pub/Sub is available globally and can handle data from multiple regions efficiently.","upvote_count":"1","poster":"Anudeep58","comment_id":"1231954"},{"timestamp":"1702730580.0","comment_id":"1098165","poster":"Nandababy","upvote_count":"1","content":"Even with Cloud Pub/Sub, unpredictable latency or delays could still occur due to the unreliable leased lines connecting your event collection infrastructure and event processing infrastructure. While Cloud Pub/Sub offers reliable message delivery within its own network, the handoff to your processing infrastructure is still dependent on the leased lines.\nReplacing leased lines with Cloud Interconnect could potentially resolve the overall issue of unpredictable latency in event processing pipeline but it could be unnecessary expense provided data centers distributed world wide.\nCloud Pub/Sub along with other optimization techniques like Cloud VPN or edge computing might be sufficient."},{"timestamp":"1691921040.0","comment_id":"979916","poster":"FP77","content":"Selected Answer: C\nI don't know why B is the most voted. The issue here is unreliable connectivity and C is the perfect use-case for that","upvote_count":"1"},{"upvote_count":"1","content":"its says with unpredictable latency and here no need to worry about connection \nSo B is the right one","comment_id":"973025","poster":"NeoNitin","timestamp":"1691239440.0"},{"timestamp":"1688782140.0","comment_id":"946080","content":"Selected Answer: C\nThe question is misleading. But should be C since it addresses the unpredictablility and latency directly.","poster":"ZZHZZH","upvote_count":"1"},{"poster":"musumusu","upvote_count":"2","content":"Best answer is A, By using Kafka, you can buffer the events in the data centers until a reliable connection is established with the event processing infrastructure.\nBut go with B, its google asking :P","timestamp":"1676558880.0","comment_id":"810774","comments":[{"upvote_count":"1","timestamp":"1677258360.0","poster":"musumusu","content":"I read this question again, I wanna answer C. Buying Data acquisition devices and set them up with sensor, i dont think its practical approach. Imagine, Adruino is cheapest IOT available in market for 15 dollars, but who will open the sensor box and install it .. omg,, its a big job. This question depends if IOT devices that are attached to sensor needs to be programmed. Big Headache right. Use google cloud connect to deal with current situation. Or reprogramme IOT if they have connected with sensors.","comment_id":"820752"}]},{"poster":"ayush_1995","timestamp":"1674969000.0","upvote_count":"10","comment_id":"791408","content":"Selected Answer: B\nB. Have the data acquisition devices publish data to Cloud Pub/Sub. This would provide a reliable messaging service for your event data, allowing you to ingest and process your data in a timely manner, regardless of the reliability of the leased lines. Cloud Pub/Sub also offers automatic retries and fault-tolerance, which would further improve the reliability of your event delivery. Additionally, using Cloud Pub/Sub would allow you to easily scale up or down your event processing infrastructure as needed, which would help to minimize costs."},{"upvote_count":"2","comment_id":"781635","poster":"desertlotus1211","content":"Are they talking about GCP in this question?\nWhere is the event processing infrastructure?\n\nAnswer A, might be correct!","timestamp":"1674164220.0"},{"upvote_count":"1","poster":"PrashantGupta1616","timestamp":"1672117380.0","comment_id":"758145","content":"Selected Answer: B\npub/sub is region is a global service\nIt's important to note that the term \"global\" in this context refers to the geographical scope of the service"},{"poster":"NicolasN","timestamp":"1671113760.0","upvote_count":"3","content":"Selected Answer: A\nAs usual the answer is hidden somewhere in the Google Cloud Blog:\n\"In the case of our automotive company, the data is already stored and processed in local data centers in different regions. This happens by streaming all sensor data from the cars via MQTT to local Kafka Clusters that leverage Confluent‚Äôs MQTT Proxy.\"\n\"This integration from devices to a local Kafka cluster typically is its own standalone project, because you need to handle IoT-specific challenges like constrained devices and unreliable networks.\"\n\nüîó https://cloud.google.com/blog/products/ai-machine-learning/enabling-connected-transformation-with-apache-kafka-and-tensorflow-on-google-cloud-platform","comments":[{"upvote_count":"2","content":"The question is asking from the on-premise infrastructure, which already has the data, to the event processing infrastructure, which is in the GCP, is unreliable.... \n\nit not asking from the sensors to the on-premise...","comment_id":"781630","poster":"desertlotus1211","timestamp":"1674164100.0","comments":[{"comment_id":"781636","timestamp":"1674164280.0","poster":"desertlotus1211","content":"I might have to retract my answer... Are they talking about GCP in this question?\nwhere is the event processing infrastructure?","upvote_count":"1"}]}],"comment_id":"746169"},{"poster":"zellck","comments":[{"timestamp":"1672430280.0","upvote_count":"1","poster":"AzureDP900","content":"yes it is B. Have the data acquisition devices publish data to Cloud Pub/Sub.","comment_id":"762281"}],"content":"Selected Answer: B\nB is the answer.","comment_id":"735880","timestamp":"1670239800.0","upvote_count":"2"},{"content":"yeah, changing whole architecture arround the world for the use of pub/sub is so much more cost efficient than Cloud Interconnect (which is like 3k$).. \n\nIt's C.","comments":[{"content":"It's not a Cloud Interconnect, it's a lot of interconnect ones per data center, PUB/SUB addresses all the requirements. Its B","poster":"odacir","upvote_count":"1","comments":[{"content":"ALSO, the problem it's no t your connection, its the connectivity BT your event collection infrastructure to your event processing infrastructure, so PUSUB it's perfect for this","comment_id":"738244","upvote_count":"1","timestamp":"1670438640.0","poster":"odacir"}],"timestamp":"1670438460.0","comment_id":"738241"},{"content":"Wouldn't using cloud interconnect also result in amendments to each of the data center around the world? I don't see why there would be a huge architecture change when using PubSub, the publishers would just need to push messages directly to pubsub, instead of pushing to their own cost center.\n\nAlso, if the script for pushing messages can be standardised, the data centers can share it around to","timestamp":"1670461140.0","comment_id":"738494","upvote_count":"1","poster":"jkhong"}],"timestamp":"1669040880.0","poster":"piotrpiskorski","comment_id":"723607","upvote_count":"1"},{"comment_id":"668639","content":"Selected Answer: B\nCloud Pub/Sub, it supports batch & streaming , push and pull capabilities \nAnswer B","poster":"TNT87","timestamp":"1663134660.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1661046120.0","comment_id":"649569","content":"It has to be B.","poster":"t11"},{"comment_id":"647279","poster":"rr4444","timestamp":"1660580700.0","upvote_count":"2","content":"Selected Answer: D\nFeels like everyone is wrong. \n\nA. Deploy small Kafka clusters in your data centers to buffer events.\n - Silly in a GCP cloudnative context, plus they have messaging infra anyway\nB. Have the data acquisition devices publish data to Cloud Pub/Sub.\n - They have messaging infra, so why? Unless they want to replace, it, but that doesn't change the issue\nC. Establish a Cloud Interconnect between all remote data centers and Google.\n - Wrong, because Interconnect is basically a leased line. There must be some telecoms issue with it, which we can assume is unresolvable e.g. long distance remote locations and sometimes water ingress, and the telco can't justify sorting it yet, or is slow to, or something. Leased lines usually don't come with awful internet connectivity, so sound physical connectivity issue. Sure, an Interconnect is better, more direct, but a leased line should be bullet proof. \nD. Write a Cloud Dataflow pipeline that aggregates all data in session windows.\n - The only way to address dodgy/delayed data delivery"},{"comment_id":"632720","content":"The answer must be C. To me the question is more about having to re-use existing Infrastructure as much as you can and try to only fix the problem that is mentioned, that is what they mean as cost optimization, so there is no need to overthink. Since there is no issue with event collection centers, so there is really no need to change up to Pub/Sub. Using Cloud Interconnect, they switch from unreliable network to a fully reliable Cloud network. That should solve the problem here.","upvote_count":"1","timestamp":"1658095920.0","poster":"NM1212"},{"upvote_count":"1","timestamp":"1641497280.0","poster":"medeis_jar","content":"Selected Answer: B\nskip processing and sending through multiple endpoints and ingest raw messages from sensors in GCP","comment_id":"518522"},{"content":"Selected Answer: B\nB: the most cost-effective way - Pub/Sub","comment_id":"516249","comments":[{"poster":"MaxNRG","comment_id":"516255","timestamp":"1641276840.0","content":"improve event delivery reliability","upvote_count":"1"}],"poster":"MaxNRG","upvote_count":"2","timestamp":"1641275940.0"},{"upvote_count":"1","poster":"sergio6","comment_id":"488049","content":"Selected Answer: C\nA. On edge solution, it doesn't resolve the network latency problem\nB. like A\nD: It's impossible define the minimum gap duration of the session window","timestamp":"1638009180.0"},{"poster":"mjb65","comment_id":"483132","content":"I've reas the text a fifth time now and all of your comments, folks... mmmh, they do not mention that the processing infrastructure is loacated at GCP, in this case A seems technically correct as buffering solution, otherwise I would go with B as modern actuators can use MQTT --> Cloud IoT Core --> PubSub (???)","upvote_count":"2","timestamp":"1637485740.0"},{"timestamp":"1635976380.0","content":"Should be B. The most cost effective (cheapest) way is to use PubSub. It can handle messages with high latency.","comment_id":"472305","poster":"szefco","upvote_count":"1"},{"poster":"ManojT","comment_id":"459862","timestamp":"1633836960.0","upvote_count":"3","content":"Answer A: Problem on unreliable connection so may be data lose. To avoid it use small kafka which would buffer data and keep it up for x number of days to replay in case data lost."},{"poster":"sergio6","comment_id":"458717","upvote_count":"2","timestamp":"1633607580.0","content":"have you looked at the costs of cloud interconnect?\nPricing example: Dedicated Interconnect connections for a single month \ncapacity 30 Gbps (3 x 10-Gbps circuit) 3 x 10-Gbps circuit x 24 hrs @ $2.328 per hour x 30 days = $5,028.48\nhttps://cloud.google.com/network-connectivity/docs/interconnect/pricing#pricing-example"},{"content":"the issue is between Events Collect Datacenters and Analytics Datacenters, \nA : setting up a Kafka wont change anything since the problem is due to the unreliable connection.\nC : with Google InterConnect it would be expensive \nD : Dataflow would be the step to do after using Pub/sub.\nB : Seem to be the correct one, but still more expensive to use Cloud infrastructure instead of On-prem only","comment_id":"453880","poster":"Ysance_AGS","timestamp":"1632896340.0","upvote_count":"2"},{"timestamp":"1628490660.0","poster":"sandipk91","upvote_count":"2","content":"option A is the most cost effective way","comment_id":"421960"},{"comment_id":"417321","content":"\"leased lines that provide connectivity from your event collection infrastructure to your event processing infrastructure are unreliable.\" \n\n\nThe leased lines are used only when i'm sending the event to the datacenters. If i use cloud pub/sub, it will not be used or will be more reliable as Google datacenters are highly available. I also think having a cloud pub sub topic will be WAAAY cheaper than interconnecting some datacenters around the world so it's very cost effective.\n\nIMO the answer is B","upvote_count":"6","poster":"MrCastro","timestamp":"1627641900.0","comments":[{"timestamp":"1636632720.0","poster":"Abhi16820","comment_id":"476188","upvote_count":"1","content":"I totally agree with you."},{"timestamp":"1677683880.0","content":"The question did not say the Data Processing Center is in the Google Cloud. So IoT to Pubsub then to on-premise event processing infrastructure?","poster":"SuperVee","comment_id":"826009","upvote_count":"1"}]},{"comments":[{"content":"Vote for A, all other solution involve Cloud","comment_id":"396972","poster":"sumanshu","timestamp":"1625237460.0","upvote_count":"4"}],"content":"Both - collection infrastructure & event processing infrastructure are ON-PREMISES","comment_id":"396969","upvote_count":"3","poster":"sumanshu","timestamp":"1625237220.0"},{"comment_id":"308878","timestamp":"1615556280.0","poster":"daghayeghi","content":"C:\nI agree, C is the only choice that addresses the problem. The problem is caused by leased line. How come pub/sub service can resolve it? Pub/sub will still use the leased line\n\nhttps://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview","upvote_count":"2"},{"timestamp":"1611025620.0","comments":[{"content":"In saying that pub/sub could be cost effective since you don't need to setup physical hardware soo ehhhh","upvote_count":"2","comment_id":"270889","poster":"lammingtons","timestamp":"1611025800.0"}],"content":"They mention the problem is between the event collection data center and the analytics data center. \nPub/Sub would solve the issue if the issue was between the vehicles and the collection centers (But this is a GCP service!).\nCloud interconnect is totally irrelevant since there is no mention of GCP at all.\nSo A seems the most correct.","upvote_count":"2","comment_id":"270888","poster":"lammingtons"},{"poster":"xrun","upvote_count":"3","content":"C. The question is, however, not very specific. It says that the company operates its own data centers. But the connection between event processing and event reciving infrastructure is unreliable. It doesn't say which part of it is in the GCP. Maybe none? However, if some of the infrastructure is in the GCP, then cloud interconnect should help.","comment_id":"241058","timestamp":"1607703780.0"},{"comment_id":"216021","upvote_count":"5","poster":"Cloud_Enthusiast","timestamp":"1604934600.0","content":"Answer is C: Cloud Interconnect will connect all data centres.\nhttps://cloud.google.com/network-connectivity/docs/interconnect#:~:text=Cloud%20Interconnect%20extends%20your%20on,through%20a%20supported%20service%20provider. \n\"Cloud Interconnect extends your on-premises network to Google's network through a highly available, low latency connection. You can use Dedicated Interconnect to connect directly to Google or use Partner Interconnect to connect to Google through a supported service provider\" ...It will not require leased lines.Hence C is the most correct answer"},{"content":"This is tricky question and I will go with Option A - \"Deploy small Kafka clusters in your data centers to buffer events.\"\nOption B: Pub/Sub - This indeed solves the problem but going with the question it does not mention anywhere that they are looking for a google cloud solution. They have on prem. DCs across the world only. Also, Pub/Sub has limitation of retention period of 7 days only. \nC: Cloud Interconnect. This will again solve the problem but the question asks for \"Cost Effective\" solution and definitely Cloud Interconnect will be a costly solution. We can check from pricing calculator \nD: Data Flow : Although latency issues are sometimes addressed by Dataflow but session windows does not gel well to the requirement. So this rules out at first place.","upvote_count":"3","poster":"Alasmindas","comment_id":"201801","timestamp":"1602986160.0"},{"timestamp":"1602654480.0","comment_id":"199561","poster":"aleedrew","upvote_count":"4","comments":[{"content":"And B will be ok if they move both data collection AND data processing into GCP.","comment_id":"826018","timestamp":"1677684240.0","poster":"SuperVee","upvote_count":"1"}],"content":"I think it's a toss up between A and C. The question does not talk about using GCP services and points out that it has it's own data centers. I would choose A if they are not moving to use GCP. C would be the best choice if they are migrating their processing to GCP."},{"upvote_count":"2","poster":"Tanmoyk","content":"Should be C , below are the reason \n1. Data can be collected by Pub/Sub and then can be processed by Dataflow using windowing, but windowing is only helpful where latency is in predictable range, where latency is unpredictable which is true in this case this solution is not appropriate.","timestamp":"1602065820.0","comment_id":"195102"},{"content":"A - makes sense, the issue is between event collection infrastructure and event processing infrastructure. And to make sure events are not lost, Kafka is better solution since the retention of event data is not limited to 7 days as in the case of PUB/Sub.","comment_id":"189008","timestamp":"1601295240.0","poster":"DeepakKhattar","upvote_count":"4"},{"content":"Should be B. By replacing their current infrastructure for collection of the events with Cloud Pub/Sub they will be able to address both the reliability & latency issues for collection to processing.","comment_id":"185015","poster":"SteelWarrior","timestamp":"1600837680.0","upvote_count":"1"},{"timestamp":"1600403340.0","poster":"Diqtator","comment_id":"181371","content":"The problem is about the connection between vehicles and their own datacenter. Not between their datacenter and google. Therefore I'd say B, Pub/Sub.","upvote_count":"4"},{"timestamp":"1600053240.0","comment_id":"179040","poster":"Tanmoyk","content":"C should be the proper solution , this clearly a connectivity issue and as latency is irregular setting custom window for data collection will be difficult.","upvote_count":"2"},{"comment_id":"174733","timestamp":"1599414960.0","poster":"oku","upvote_count":"1","content":"B: data acquisition device (USB, Ethernet, PCI, etc) with pub/Sub will sortout in cost effective way"},{"upvote_count":"2","timestamp":"1598135040.0","comment_id":"163965","poster":"atnafu2020","content":"C\nit's partner interconnection via lease\nCloud Interconnect provides low latency, highly available connections that enable you to reliably transfer data between your on-premises and Google Cloud Virtual Private Cloud (VPC) networks. Also, Cloud Interconnect connections provide internal IP address communication, which means internal IP addresses are directly accessible from both networks.\n\nCloud Interconnect offers two options for extending your on-premises network. Dedicated Interconnect provides a direct physical connection between your on-premises network and Google's network. Partner Interconnect provides connectivity between your on-premises and VPC networks through a supported service provider."},{"upvote_count":"3","timestamp":"1598027040.0","comment_id":"163078","content":"B is correct.\nPub/sub guarantee one send sucessfully at least.","poster":"haroldbenites"},{"timestamp":"1596357360.0","comment_id":"148990","poster":"RP123","content":"C\nAs leased line word is used.","upvote_count":"2"},{"poster":"tprashanth","comment_id":"134274","upvote_count":"2","timestamp":"1594670040.0","content":"Answer seems to be C followed by B. But as it asked for only 1 answer, I've to choose C (first step to take out unreliable connection)\nhttps://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview"},{"timestamp":"1594010640.0","upvote_count":"1","content":"B pub/sub is the answer.","comment_id":"127430","poster":"Rajuuu"},{"comment_id":"121228","upvote_count":"3","content":"B..Pub/Sub - cost effective and ensure reliable message delivery, which is the key issue to be addressed","timestamp":"1593263820.0","poster":"PRC"},{"poster":"dambilwa","comment_id":"120565","upvote_count":"2","timestamp":"1593177360.0","content":"Option [B] - Pub-Sub appears to be correct...\nThere is network unreliability between event collection centers (Logistics Hubs) & event processing centers (Small Data centers). I doubt if Cloud Interconnect would be helpful"},{"timestamp":"1585378440.0","comment_id":"68780","poster":"[Removed]","upvote_count":"1","content":"Answer: B\nDescription: Pubsub is global service with high message delivery capacity"},{"comment_id":"67007","content":"Latency.... Answer DataFlow.","upvote_count":"1","timestamp":"1584894060.0","comments":[{"upvote_count":"1","timestamp":"1585630320.0","comment_id":"69719","content":"Sorry Typo It should be PubSub","poster":"Rajokkiyam"}],"poster":"Rajokkiyam"}],"url":"https://www.examtopics.com/discussions/google/view/17249-exam-professional-data-engineer-topic-1-question-112/","question_id":16,"unix_timestamp":1584881880},{"id":"VHlAn1SxWIMrYElM1GfL","answer_images":[],"isMC":true,"timestamp":"2020-03-17 12:12:00","question_id":17,"question_images":[],"answer":"C","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/16856-exam-professional-data-engineer-topic-1-question-113/","answers_community":["C (81%)","A (19%)"],"unix_timestamp":1584443520,"topic":"1","answer_ET":"C","exam_id":10,"discussion":[{"upvote_count":"26","timestamp":"1584443520.0","comment_id":"65159","comments":[{"poster":"AzureDP900","comment_id":"762283","content":"C. Dialogflow Enterprise Editio","upvote_count":"1","timestamp":"1672430340.0"}],"poster":"rickywck","content":"should be C, since we need to recognize both voice and intent"},{"comments":[{"comment_id":"415375","timestamp":"1627383120.0","content":"Cloud Speech-to-Text API just converts speech to text. You will have text files as an output and then the requirement is to \"interpret customer voice commands and issue an order to the backend systems\". This is not achieved by having text files.\n\nI would go with option C, since Dialogflow can interpret the commands (intents) and integrates other applications e.g. backend systems.","upvote_count":"8","poster":"hdmi_switch"},{"content":"shuld be C, the key is interpret customer voice commands","comment_id":"1013765","poster":"exnaniantwort","timestamp":"1695368160.0","upvote_count":"1"},{"poster":"HarshKothari21","upvote_count":"2","comment_id":"663024","content":"Question also says \"in-home assistants, such as Google Home\". the idea here is to provide assistance which involves Dialog. \n\nI would go with option C","timestamp":"1662600000.0"}],"content":"Option A - Cloud Speech-to-Text API. \nThe question is just asking to \" interpret customer voice commands\" .. it does not mention anything related to sentiment analysis so NLP is not required. DialogFlow is more of a chat bot services typically suited for a \"Service Desk\" kind of setup - where clients will call a centralized helpdesk and automation is achieved through Chat bot services like - google Dialog flow","poster":"Alasmindas","upvote_count":"19","comment_id":"216388","timestamp":"1604988300.0"},{"timestamp":"1737240180.0","content":"Selected Answer: C\nWhile both are Google Cloud services related to speech processing, the key difference is that Cloud Speech-to-Text API is solely focused on transcribing spoken language into text, while DialogFlow is a more comprehensive platform that not only transcribes speech but also interprets the meaning of the conversation, allowing you to build conversational AI applications with features like intent recognition and entity extraction","comment_id":"1342788","poster":"grshankar9","upvote_count":"1"},{"upvote_count":"1","timestamp":"1727294100.0","content":"This is a queston from Actual exam question from Google's Professional Data Engineer so C makes sense. This is not an AWS question","comment_id":"1289153","poster":"LR2023"},{"comment_id":"1224380","content":"Selected Answer: A\nThe question clearly states \"voice commands\" which is a term for short (few words long at most) well-defined phrases to be recognized. No need for a dialog.\nEven if I were to use Dialogflow, I would use ES instead of CX (new name for Enterprise Edition), no fancy features are required for this.","poster":"AlizCert","upvote_count":"1","timestamp":"1717537200.0"},{"upvote_count":"1","content":"Ans C . main thing is that question is saying\" customer voice commands \" there is no need to sentimental analysis of language so thats why.\n\nC. Dialogflow Enterprise Edition\n\nDialogflow is a powerful natural language understanding platform developed by Google. It allows you to build conversational interfaces, interpret user voice commands, and integrate with various platforms and devices like Google Home. The \"Enterprise Edition\" provides additional features and support for more complex use cases, making it a good choice for a retailer looking to integrate with in-home assistants and handle customer voice commands effectively.","comment_id":"973028","poster":"NeoNitin","timestamp":"1691239740.0"},{"poster":"juliobs","comment_id":"843735","timestamp":"1679228700.0","content":"Selected Answer: C\nAnswer is C. However Google Assistant Conversational Actions will be sunsetted on June 13, 2023.","upvote_count":"3"},{"comment_id":"808974","timestamp":"1676421960.0","upvote_count":"2","content":"Selected Answer: C\nhttps://cloud.google.com/dialogflow/es/docs/integrations/aog","poster":"techtitan"},{"comment_id":"781640","content":"I think the answer is A: Speech to Text.\nYou want to interpret what a user say... Diagflow is text to speech, not what the question asked for...\n\nThoughts?","timestamp":"1674164640.0","poster":"desertlotus1211","upvote_count":"1"},{"comment_id":"758146","timestamp":"1672117500.0","poster":"PrashantGupta1616","upvote_count":"1","content":"Selected Answer: A\nThe question is just asking to \" interpret customer voice commands\" so A is out of the box solution"},{"comments":[{"poster":"odacir","upvote_count":"5","comment_id":"750100","timestamp":"1671472440.0","content":"I change my mind, it's C.\nhttps://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps"}],"poster":"odacir","upvote_count":"1","timestamp":"1670439180.0","comment_id":"738255","content":"Selected Answer: A\nEnable voice control\n\nImplement voice commands such as ‚Äúturn the volume up,‚Äù and voice search such as saying ‚Äúwhat is the temperature in Paris?‚Äù Combine this with the Text-to-Speech API to deliver voice-enabled experiences in IoT (Internet of Things) applications.\nhttps://cloud.google.com/speech-to-text#section-9"},{"upvote_count":"3","poster":"zellck","timestamp":"1670239680.0","comment_id":"735879","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/dialogflow/es/docs\nDialogflow is a natural language understanding platform that makes it easy to design and integrate a conversational user interface into your mobile app, web application, device, bot, interactive voice response system, and so on. Using Dialogflow, you can provide new and engaging ways for users to interact with your product.\n\nDialogflow can analyze multiple types of input from your customers, including text or audio inputs (like from a phone or voice recording). It can also respond to your customers in a couple of ways, either through text or with synthetic speech."},{"upvote_count":"2","poster":"TNT87","timestamp":"1663134900.0","content":"https://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps \n\nDialogflow is the answer","comment_id":"668642"},{"timestamp":"1658212740.0","content":"Selected Answer: C\nDialogflow provides a seamless integration with Google Assistant. This integration has the following advantages: You can use the same Dialogflow agent to power Google Assistant and other integrations. Dialogflow agents provide Google Cloud enterprise-grade security, privacy, support, and SLAs","upvote_count":"3","comment_id":"633401","poster":"Smaks"},{"content":"dialog","poster":"Vip777","timestamp":"1650631380.0","upvote_count":"1","comment_id":"589986"},{"poster":"Vip777","comment_id":"589965","upvote_count":"1","timestamp":"1650629160.0","content":"speech"},{"timestamp":"1648209540.0","content":"It should be D. INTERPRET customer voice commands and issue an order to the backend systems. Option C is usually applied for conversation. But in this case, it is not a conversation.","comments":[{"poster":"[Removed]","timestamp":"1658880000.0","comment_id":"637689","content":"it can totally be a conversation. if the system is attempting to interpret voice commands, it may need clarifying questions, which would in turn be a conversation. it's not D because autoML NL analyzes TEXT. it would have to work in conjunction with speech to text API. therefore the answer is dialogflow.","upvote_count":"1"}],"comment_id":"574983","upvote_count":"1","poster":"PJG_worm"},{"content":"Selected Answer: C\nrecognize voice and intent\nhttps://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps","timestamp":"1641497400.0","upvote_count":"4","poster":"medeis_jar","comment_id":"518524"},{"timestamp":"1641276120.0","upvote_count":"6","poster":"MaxNRG","comment_id":"516250","content":"Selected Answer: C\nC: Dialogflow Enterprise Edition is an end-to-end development suite for building conversational interfaces for websites, mobile applications, popular messaging platforms, and IoT devices. You can use it to build interfaces (e.g., chatbots) that are capable of natural and rich interactions between your users and your business. It is powered by machine learning to recognize the intent and context of what a user says, allowing your conversational interface to provide highly efficient and accurate responses.\nhttps://cloud.google.com/dialogflow/\nDialogflow API V2 is the new iteration of our developer API. The new API integrates Google Cloud Speech-to-Text, enabling developers to send audio directly to Dialogflow for combined speech recognition and natural language understanding.\nhttps://dialogflow.com/v2-faq\nhttps://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps"},{"upvote_count":"3","poster":"MasterOfTheUniverse","content":"Selected Answer: A\nSpeech-to-text is mentioned in Googles use cases. \nIt is used for transcription.\nAutoML is still mentioned though which is opening up other services to be included.\n\nThe question is not so fair and clear.\nhttps://cloud.google.com/speech-to-text#section-9","timestamp":"1640083020.0","comment_id":"506034"},{"content":"Selected Answer: C\nDialogflow is able to understand the customer's voice commands and send an order to the backend systems","timestamp":"1638009360.0","upvote_count":"3","comment_id":"488051","poster":"sergio6"},{"comment_id":"435649","upvote_count":"4","timestamp":"1630339800.0","poster":"manocha_01887","content":"D\nA Dialogflow Agent is a virtual agent that participates in a conversation. Conversations can be audio or text based.\n\nSullivan, Dan. Official Google Cloud Certified Professional Data Engineer Study Guide (p. 274). Wiley. Kindle Edition."},{"poster":"sumanshu","timestamp":"1625237640.0","upvote_count":"3","content":"Vote for C - Dialogflow","comment_id":"396973"},{"poster":"tikna","comment_id":"264293","timestamp":"1610313840.0","content":"C as per latest info\nhttps://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps","upvote_count":"4"},{"content":"Lots of the discussion here is driven by the assumption that you need to parse the voice into text, then interpret the text. I'm pretty sure digital assistants have the voice-to-text built in, so the only task is to probably classify text into specific categories based on the things you might want to do in the back end. D.","timestamp":"1609900680.0","poster":"derplau","comments":[{"content":"Not A - it's not translating or transcribing\nNot B - not Entity sentiment analysis\nNot C - Not for Predefined set of requests such as banking systems\nSo answer is D","upvote_count":"1","poster":"rskrishvm","timestamp":"1610154540.0","comment_id":"262932"}],"upvote_count":"2","comment_id":"260698"},{"comment_id":"222888","content":"I am picking C. This Dialogflow article clearly mention \"voice commands\": https://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps","poster":"snamburi3","upvote_count":"2","timestamp":"1605799920.0"},{"poster":"kavs","upvote_count":"2","timestamp":"1605690780.0","content":"Dialog flow internally uses spech to text n text to speech..in Google docs section there is a sample dialog flow on ordering pizza which is same use case for retailer","comment_id":"221707"},{"timestamp":"1604934780.0","comment_id":"216024","content":"Answer is C: https://cloud.google.com/dialogflow/?utm_source=google&utm_medium=cpc&utm_campaign=japac-IN-all-en-dr-bkws-all-pkws-trial-e-dr-1009137&utm_content=text-ad-none-none-DEV_c-CRE_438601776949-ADGP_Hybrid%20%7C%20AW%20SEM%20%7C%20BKWS%20~%20T2%20%7C%20EXA%20%7C%20AI%20ML%20%7C%20M%3A1%20%7C%20IN%20%7C%20en%20%7C%20dialogflow%20%7C%20general%20-%20PKWS-KWID_43700049544741737-kwd-464371085200&userloc_9062009-network_g&utm_term=KW_google%20cloud%20dialogflow&&gclid=EAIaIQobChMIprbgo-317AIVRFRgCh3z8gChEAAYASAAEgLiY_D_BwE","poster":"Cloud_Enthusiast","upvote_count":"4"},{"poster":"aleedrew","upvote_count":"2","comment_id":"199564","content":"I am gravitating toward C: https://medium.com/swlh/chapter-6-how-to-build-a-google-home-app-with-dialogflow-overview-4549d92d8d6a. But it does say other home assistants so could be A too.","timestamp":"1602654780.0"},{"upvote_count":"2","comment_id":"196515","poster":"kino2020","content":"should be C\nDetect intent with audio output\nApplications often need a bot to talk back to the end-user. Dialogflow can use Cloud Text-to-Speech powered by DeepMind WaveNet to generate speech responses from your agent. This conversion from intent text responses to audio is known as audio output, speech synthesis, text-to-speech, or TTS.\n\nThis guide provides an example that uses audio for both input and output when detecting an intent. This use case is common when developing apps that communicate with users via a purely audio interface.\n\nFor a list of supported languages, see the TTS column on the Languages page.\nhttps://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts","timestamp":"1602222780.0"},{"comment_id":"181376","poster":"Diqtator","upvote_count":"2","comments":[{"content":"I agree that answer A is the best. Look at the \"enable voice control\" use case at this link: https://cloud.google.com/speech-to-text. This question is about in-home assistants (Alexa, GoogleHome, etc.) From my review of Dialogflow, it is much more chatbot related.","upvote_count":"2","comment_id":"188988","poster":"GregDT","timestamp":"1601293800.0"},{"poster":"karthik89","comment_id":"294812","content":"you need to issue an order based on voice command also, so dialogflow is ideal","upvote_count":"3","timestamp":"1613803080.0"}],"content":"I think it should be A.\n\nDialogflow seems to be more of an assistant/chatbot when looking at what integrations it can do. All we really need is to translate what item and how many the customer orders, which i think A would do good enough.\nhttps://cloud.google.com/dialogflow/es/docs/integrations","timestamp":"1600404240.0"},{"timestamp":"1598136780.0","content":"C\nA Dialogflow agent is a virtual agent that handles conversations with your end-users. It is a natural language understanding module that understands the nuances of human language. Dialogflow translates end-user text or audio during a conversation to structured data that your apps and services can understand.\nBy using speech recognition to convert audio and video into perfectly-accurate text, businesses large and small can help automate processes like:\n* Complaint analysis.\n* Call (and caller) categorization.\n* Repeat call analysis.\n* Demographic analysis.\n* Legal compliance.\n* NPS analysis.Net Promoter Score (NPS) measures overall customer loyalty toward a brand.\nAnalytical data can be get from speech to text.","poster":"atnafu2020","comment_id":"163976","upvote_count":"4","comments":[{"upvote_count":"1","timestamp":"1632002400.0","comment_id":"447291","poster":"Ral17","content":"Why not D?"}]},{"comment_id":"163079","upvote_count":"1","poster":"haroldbenites","content":"A is correct.\nSpeech to text is to process voice.","timestamp":"1598027160.0"},{"upvote_count":"3","content":"C\nhttps://en.wikipedia.org/wiki/Dialogflow#:~:text=Dialogflow%20is%20a%20natural%20language,response%20systems%2C%20and%20so%20on.\nhttps://cloud.google.com/dialogflow/docs/basics (User interactions with integration)","comment_id":"134278","timestamp":"1594670460.0","poster":"tprashanth"},{"timestamp":"1594444500.0","poster":"Rajuuu","comment_id":"131902","upvote_count":"5","content":"Answer C. \nDialogflow is used to build interfaces (such as chatbots and conversational IVR) that enable natural and rich interactions between your users and your business."},{"poster":"dg63","comment_id":"128977","content":"\"A\" is the best logical answer. First step is to accept the voice command.","upvote_count":"1","timestamp":"1594126800.0"},{"poster":"dass","comment_id":"121642","content":"Option A should be the correct one as customer voice order needs to be interpreted","timestamp":"1593321960.0","upvote_count":"2"},{"upvote_count":"5","comment_id":"69722","content":"Answer C","timestamp":"1585630740.0","poster":"Rajokkiyam"},{"upvote_count":"5","content":"Answer: C\nDescription: Dialogflow is used to do voice analytics on human computer interaction","comment_id":"68781","timestamp":"1585378620.0","poster":"[Removed]"},{"timestamp":"1584883500.0","poster":"[Removed]","comment_id":"66944","content":"should be C","upvote_count":"8"}],"question_text":"You are a retailer that wants to integrate your online sales capabilities with different in-home assistants, such as Google Home. You need to interpret customer voice commands and issue an order to the backend systems. Which solutions should you choose?","choices":{"D":"AutoML Natural Language","B":"Cloud Natural Language API","A":"Speech-to-Text API","C":"Dialogflow Enterprise Edition"}},{"id":"b0UP5cBfHz0JSxoOQfy6","discussion":[{"content":"Answer should be B","timestamp":"1631668560.0","upvote_count":"30","poster":"madhu1171","comment_id":"64129"},{"content":"Answer - B","comment_id":"66945","upvote_count":"12","timestamp":"1632310140.0","poster":"[Removed]"},{"comment_id":"1342790","timestamp":"1737240360.0","poster":"grshankar9","content":"Selected Answer: B\nCloud Composer is considered suitable across multiple cloud providers, as it is built on Apache Airflow, which allows for workflow orchestration across different cloud environments and even on-premises data centers, making it a good choice for multi-cloud strategies; however, its tightest integration is with Google Cloud Platform services.","upvote_count":"1"},{"content":"Selected Answer: B\nNo other option is aimed for this purpose","comment_id":"911421","poster":"forepick","upvote_count":"3","timestamp":"1732991460.0"},{"poster":"juliobs","content":"Selected Answer: B\nAirflow","timestamp":"1726741620.0","comment_id":"843736","upvote_count":"1"},{"comment_id":"758147","content":"Selected Answer: B\nCloud Composer is Airflow","upvote_count":"2","poster":"PrashantGupta1616","timestamp":"1719457560.0"},{"comment_id":"738258","upvote_count":"3","poster":"odacir","timestamp":"1717779420.0","content":"Selected Answer: B\nCloud Composer is Airflow, It's made for this job."},{"upvote_count":"4","poster":"zellck","timestamp":"1717579440.0","comments":[{"timestamp":"1719770460.0","poster":"AzureDP900","upvote_count":"2","comment_id":"762286","content":"Cloud composer is right"}],"comment_id":"735875","content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/composer/docs/concepts/overview\nCloud Composer is a fully managed workflow orchestration service, enabling you to create, schedule, monitor, and manage workflows that span across clouds and on-premises data centers."},{"upvote_count":"1","comment_id":"518527","timestamp":"1688664660.0","poster":"medeis_jar","content":"Selected Answer: B\nhttps://cloud.google.com/composer/"},{"comments":[{"upvote_count":"1","poster":"MaxNRG","content":"Option A is wrong as Cloud Scheduler is a fully managed enterprise-grade cron job scheduler. It is not a multi-cloud orchestration tool.\nOption B is wrong as Google Cloud Dataflow is a fully managed service for strongly consistent, parallel data-processing pipelines. It does not support multi-cloud handling.\nOption D is wrong as Google Cloud Dataproc is a fast, easy to use, managed Spark and Hadoop service for distributed data processing.","comment_id":"516253","timestamp":"1688443560.0"}],"content":"Selected Answer: B\nB:\nCloud Composer is a fully managed workflow orchestration service that empowers you to author, schedule, and monitor pipelines that span across clouds and on-premises data centers.\nhttps://cloud.google.com/composer/\nCloud Composer can help create workflows that connect data, processing, and services across clouds, giving you a unified data environment.\nBuilt on the popular Apache Airflow open source project and operated using the Python programming language, Cloud Composer is free from lock-in and easy to use.\nCloud Composer gives you the ability to connect your pipeline through a single orchestration tool whether your workflow Eves on-premises, in multiple clouds, or fully within GCP. The ability to author, schedule, and monitor your workflows in a unified manner means you can break down the silos in your environment and focus less on infrastructure.","upvote_count":"6","timestamp":"1688443500.0","comment_id":"516252","poster":"MaxNRG"},{"content":"Selected Answer: B\nAnswer: B","timestamp":"1685072040.0","comment_id":"487092","upvote_count":"3","poster":"JG123"},{"poster":"sandipk91","comment_id":"421964","timestamp":"1675931700.0","content":"Cloud composer","upvote_count":"3"},{"timestamp":"1672678500.0","comment_id":"396975","upvote_count":"5","content":"Vote for B","poster":"sumanshu"},{"comment_id":"308886","upvote_count":"4","poster":"daghayeghi","content":"B:\nHybrid and multi-cloud\nEase your transition to the cloud or maintain a hybrid data environment by orchestrating workflows that cross between on-premises and the public cloud. Create workflows that connect data, processing, and services across clouds to give you a unified data environment.\nhttps://cloud.google.com/composer#section-2","timestamp":"1662983340.0"},{"timestamp":"1659844380.0","comment_id":"285314","upvote_count":"3","content":"Correct B: without any doubt.","poster":"someshsehgal"},{"content":"B-Cloud Composer works on a multicloud environment","poster":"arghya13","timestamp":"1652937180.0","upvote_count":"4","comment_id":"222516"},{"content":"there can not be any simple question like this to choose the right answer as \"Cloud Composer\". I really feel someone must have deliberately selecting the wrong answers in Exam topics to confuse people....","timestamp":"1652155620.0","poster":"Alasmindas","comment_id":"216390","upvote_count":"5"},{"upvote_count":"4","comment_id":"216026","content":"Composer is the obvious answer. so B","poster":"Cloud_Enthusiast","timestamp":"1652102100.0"},{"comments":[{"upvote_count":"1","poster":"squishy_fishy","content":"Ha ha.. I know.","timestamp":"1681765020.0","comment_id":"463704"}],"comment_id":"194785","poster":"Darlee","upvote_count":"8","timestamp":"1649299440.0","content":"How come the `Correct Answer` so ridiculous WRONG?"},{"poster":"haroldbenites","comment_id":"163080","timestamp":"1645467960.0","upvote_count":"3","content":"B is correct obviously."},{"poster":"Rajuuu","content":"Open source Cloud Composer .B","comment_id":"127443","timestamp":"1641452400.0","upvote_count":"4"},{"comment_id":"68782","upvote_count":"7","content":"Answer: B\nDescription: Cloud Composer uses airflow which is open source and can help to orchestrate jobs","poster":"[Removed]","timestamp":"1632805140.0"},{"poster":"Rajokkiyam","upvote_count":"4","timestamp":"1632320520.0","comment_id":"67009","content":"Answer B : Cloud Composer"}],"url":"https://www.examtopics.com/discussions/google/view/16628-exam-professional-data-engineer-topic-1-question-114/","unix_timestamp":1584242160,"choices":{"C":"Cloud Dataprep","A":"Cloud Dataflow","B":"Cloud Composer","D":"Cloud Dataproc"},"question_images":[],"question_id":18,"answer_description":"","timestamp":"2020-03-15 04:16:00","answer":"B","answer_ET":"B","topic":"1","answers_community":["B (100%)"],"question_text":"Your company has a hybrid cloud initiative. You have a complex data pipeline that moves data between cloud provider services and leverages services from each of the cloud providers. Which cloud-native service should you use to orchestrate the entire pipeline?","isMC":true,"exam_id":10,"answer_images":[]},{"id":"xYzzzp26e9fjWAP95ONX","discussion":[{"content":"I feel the answer really should be Create an authorized view on the BigQuery table to control data access, and provide third-party companies with access to that view.","comment_id":"684964","timestamp":"1696266900.0","poster":"LP_PDE","upvote_count":"23"},{"content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/bigquery/docs/analytics-hub-introduction\nnalytics Hub is a data exchange platform that enables you to share data and insights at scale across organizational boundaries with a robust security and privacy framework.\n\nAs an Analytics Hub publisher, you can monetize data by sharing it with your partner network or within your own organization in real time. Listings let you share data without replicating the shared data. You can build a catalog of analytics-ready data sources with granular permissions that let you deliver data to the right audiences. You can also manage subscriptions to your listings.","timestamp":"1701772500.0","upvote_count":"10","poster":"zellck","comment_id":"735852"},{"content":"Selected Answer: A\nOption A: This option is correct because Analytics Hub is a managed service that provides a centralized repository for data assets. You can use Analytics Hub to share data with other Google Cloud Platform services, as well as with third-party companies","comment_id":"964487","timestamp":"1722067260.0","upvote_count":"2","poster":"vamgcp"},{"comment_id":"810805","poster":"musumusu","upvote_count":"1","content":"You are preparing for exam: \nCreating a view and share with 3rd party is best and cheapest. \nThen create a separate dataset to share it cost less than using paid service for data access i.e analytics hub where you create data access policies\nyou choose, its just making me craazy","timestamp":"1708096020.0","comments":[{"timestamp":"1708794720.0","comment_id":"820761","content":"One main reason you should use analytics hub, when you want control over 3 party activites and you want to monetize ( to make money ) by sharing BQ dataset.","poster":"musumusu","upvote_count":"1"}]},{"timestamp":"1705953540.0","comment_id":"784646","upvote_count":"2","content":"Selected Answer: A\nShared datasets are collections of tables and views in BigQuery defined by a data publisher and make up the unit of cross-project / cross-organizational sharing. Data subscribers get an opaque, read-only, linked dataset inside their project and VPC perimeter that they can combine with their own datasets and connect to solutions from Google Cloud or our partners. For example, a retailer might create a single exchange to share demand forecasts to the 1,000‚Äôs of vendors in their supply chain‚Äìhaving joined historical sales data with weather, web clickstream, and Google Trends data in their own BigQuery project, then sharing real-time outputs via Analytics Hub. The publisher can add metadata, track subscribers, and see aggregated usage metrics.","poster":"lool"},{"content":"A. Use Analytics Hub to control data access, and provide third party companies with access to the dataset.","poster":"AzureDP900","timestamp":"1703966460.0","comment_id":"762288","upvote_count":"1"},{"content":"Selected Answer: A\nhttps://cloud.google.com/analytics-hub","poster":"odacir","upvote_count":"2","timestamp":"1701975600.0","comment_id":"738262"},{"upvote_count":"1","timestamp":"1700941200.0","poster":"Atnafu","comment_id":"727053","content":"A\nMultiple choose listed wrongly \nCorrect one \nA. \nCreate an authorized view on the BigQuery table to control data access, and provide third-party companies with access to that view.\nB.\nUse Cloud Scheduler to export the data on a regular basis to Cloud Storage, and provide third-party companies with access to the bucket.\nc.\nCreate a separate dataset in BigQuery that contains the relevant data to share, and provide third-party companies with access to the new dataset.\nD.\nCreate a Cloud Dataflow job that reads the data in frequent time intervals, and writes it to the relevant BigQuery dataset or Cloud Storage bucket for third-party companies to use."},{"timestamp":"1698501720.0","poster":"ajayrtk","upvote_count":"2","content":"no option is correct\nthis is correct answer -Create an authorised view on the BigQuery table to control data access, and provide third-party companies with access to that view.","comment_id":"706515"},{"timestamp":"1693743060.0","content":"Selected Answer: A\nA. Use Analytics Hub to control data access, and provide third party companies with access to the dataset.","poster":"AWSandeep","comment_id":"658421","upvote_count":"1"},{"poster":"damaldon","upvote_count":"1","content":"Answer A.\nAs an Analytics Hub user, you can perform the following tasks:\n\n As an Analytics Hub publisher, you can monetize data by sharing it with your partner network or within your own organization in real time. Listings let you share data without replicating the shared data. You can build a catalog of analytics-ready data sources with granular permissions that let you deliver data to the right audiences.\n\n As an Analytics Hub subscriber, you can discover the data that you are looking for, combine shared data with your existing data, and leverage the built-in features of BigQuery. When you subscribe to a listing, a linked dataset is created in your project.\n\n As an Analytics Hub viewer, you can browse through the datasets that you have access to in Analytics Hub and request the publisher to access the shared data.\n\n As an Analytics Hub administrator, you can create data exchanges that enable data sharing, and then give permissions to data publishers and subscribers to access these data exchanges.\nhttps://cloud.google.com/bigquery/docs/analytics-hub-introduction","timestamp":"1693666800.0","comment_id":"657534"}],"url":"https://www.examtopics.com/discussions/google/view/79459-exam-professional-data-engineer-topic-1-question-115/","unix_timestamp":1662130800,"choices":{"D":"Create a Dataflow job that reads the data in frequent time intervals, and writes it to the relevant BigQuery dataset or Cloud Storage bucket for third-party companies to use.","C":"Create a separate dataset in BigQuery that contains the relevant data to share, and provide third-party companies with access to the new dataset.","A":"Use Analytics Hub to control data access, and provide third party companies with access to the dataset.","B":"Use Cloud Scheduler to export the data on a regular basis to Cloud Storage, and provide third-party companies with access to the bucket."},"question_images":[],"question_id":19,"timestamp":"2022-09-02 17:00:00","answer_description":"","answer":"A","answer_ET":"A","topic":"1","answers_community":["A (100%)"],"question_text":"You use a dataset in BigQuery for analysis. You want to provide third-party companies with access to the same dataset. You need to keep the costs of data sharing low and ensure that the data is current. Which solution should you choose?","isMC":true,"exam_id":10,"answer_images":[]},{"id":"37v4qZJSLE0YHL7LyntO","answer":"BD","question_text":"Your company is in the process of migrating its on-premises data warehousing solutions to BigQuery. The existing data warehouse uses trigger-based change data capture (CDC) to apply updates from multiple transactional database sources on a daily basis. With BigQuery, your company hopes to improve its handling of\nCDC so that changes to the source systems are available to query in BigQuery in near-real time using log-based CDC streams, while also optimizing for the performance of applying changes to the data warehouse. Which two steps should they take to ensure that changes are available in the BigQuery reporting table with minimal latency while reducing compute overhead? (Choose two.)","unix_timestamp":1662179100,"choices":{"B":"Insert each new CDC record and corresponding operation type to a staging table in real time.","C":"Periodically DELETE outdated records from the reporting table.","A":"Perform a DML INSERT, UPDATE, or DELETE to replicate each individual CDC record in real time directly on the reporting table.","D":"Periodically use a DML MERGE to perform several DML INSERT, UPDATE, and DELETE operations at the same time on the reporting table.","E":"Insert each new CDC record and corresponding operation type in real time to the reporting table, and use a materialized view to expose only the newest version of each unique record."},"question_id":20,"answers_community":["BD (79%)","12%","6%"],"topic":"1","discussion":[{"timestamp":"1662362940.0","poster":"YorelNation","upvote_count":"15","content":"Selected Answer: BD\nTo aim for minimal latency while reducing compute overhead:\n\nB. Insert each new CDC record and corresponding operation type to a staging table in real time.\n\nD. Periodically use a DML MERGE to perform several DML INSERT, UPDATE, and DELETE operations at the same time on the reporting table. (all statements comes from the staging table)","comment_id":"659856"},{"poster":"musumusu","comment_id":"820766","upvote_count":"6","timestamp":"1677258960.0","content":"B&D\nTricks here: Always choose google recommended approach, Use data first in Staging table then merge with original tables."},{"poster":"Nirca","timestamp":"1696584300.0","upvote_count":"2","comments":[{"content":"A isn't correct, as the requirement is \"reducing compute overhead\"\nB isn't correct, as there is no mention of a \"staging table\" in the scenario\nD isn't correct as it's done periodically, and the requirement is \"near real-time\"","timestamp":"1724315820.0","comment_id":"1270577","poster":"nadavw","upvote_count":"1"}],"content":"Selected Answer: CE\nI'm going for E &C - this in the only solution with low TCO. \nE - is the best way to work with CDC when real nearline data is needed BQ snapshots can be online! . & C - is good practice to delete old records.","comment_id":"1026391"},{"comment_id":"760073","timestamp":"1672244280.0","poster":"dconesoko","content":"Selected Answer: BD\nwith both the delta table and the main table changes could be queried in near realtime, by using a view that unions both tables and queries the laters record for the given key, eventually the delta table should be merged into the main table and truncated. Google recently introduced datastream that would take away all these headaches.","upvote_count":"2"},{"content":"Selected Answer: BD\nThe solution is B and D. I perform a similar task in my work, and this is the best way to do it at scale with BigQuery.","comment_id":"738265","timestamp":"1670440080.0","poster":"odacir","upvote_count":"2"},{"content":"Selected Answer: BD\nBD is the answer.\n\nhttps://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture#overview_of_cdc_data_replication\nDelta tables contain all change events for a particular table since the initial load. Having all change events available can be valuable for identifying trends, the state of the entities that a table represents at a particular moment, or change frequency.\n\nThe best way to merge data frequently and consistently is to use a MERGE statement, which lets you combine multiple INSERT, UPDATE, and DELETE statements into a single atomic operation.","comment_id":"735849","timestamp":"1670236320.0","poster":"zellck","upvote_count":"4"},{"comment_id":"723309","timestamp":"1669020240.0","content":"I really can‚Äôt find a correct combination of answers. I'm between the following alternatives, but with no one fitting:\n1Ô∏è‚É£ [B] and [D]: That's a proposed solution, but as a cost-optimized approach (along with an extra step to \"Periodically DELETE outdated records from the STAGING table\" - more details on my subsequent reply). Also, I can't imagine how an answer with the word \"Periodically\" may be compatible with the \"minimal latency\" requirement.\n2Ô∏è‚É£ [E] and [C]: It could be a valid approach, but near-real time requirement would demand also for a materialized view refresh. And it seems to contradict the \"reducing compute overhead\" req.\n3Ô∏è‚É£ [A] standalone: Provides immediate results but is far from compute-optimized.","comments":[{"content":"The previous guidelines were here: \nüîó https://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture#immediate_consistency_approach\nThere were two approaches:\n1Ô∏è‚É£ Immediate consistency approach\n2Ô∏è‚É£ Cost-optimized approach\nFor approach 1Ô∏è‚É£, which is the objective of this question, it proposes:\n a. Insert CDC data into a delta table in BigQuery => that's answer [B]\n b. Create a BigQuery view that joins the main and delta tables and finds the most recent row => there' no answer that fits\nFor approach 2Ô∏è‚É£ it proposes:\n a. Insert CDC data into a delta table in BigQuery => that's answer [B]\n b. Merge delta table changes into the main table and periodically purge merged rows from the delta table - Run Merge statement on a regular interval => that's answer [D]","timestamp":"1669020540.0","poster":"NicolasN","upvote_count":"5","comment_id":"723315"},{"timestamp":"1669020480.0","poster":"NicolasN","upvote_count":"4","content":"Nowadays (Nov. 2022) I don't expect to confront this question in a real exam with this set of answers since the more recent documentation proposes the use of Datastream.\nüîó https://cloud.google.com/blog/products/data-analytics/real-time-cdc-replication-bigquery","comment_id":"723314"}],"poster":"NicolasN","upvote_count":"2"},{"content":"B and E. Typically in a Data Warehouse you don't delete date. Data Warehouse should store full history to see how the data changed over time. All the solutions with 'DELETE' should not be used as this goes against being able to access the history of the data.","upvote_count":"2","timestamp":"1667255400.0","poster":"beanz00","comment_id":"708755"},{"upvote_count":"1","timestamp":"1665434160.0","content":"https://www.striim.com/blog/oracle-to-google-bigquery/","poster":"TNT87","comment_id":"691495"},{"timestamp":"1663137240.0","comment_id":"668668","upvote_count":"1","poster":"TNT87","content":"https://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture#data_latency"},{"comment_id":"668660","upvote_count":"1","timestamp":"1663136820.0","content":"https://docs.streamsets.com/platform-datacollector/latest/datacollector/UserGuide/Destinations/GBigQuery.html","poster":"TNT87"},{"content":"Selected Answer: BD\nB and D \nhttps://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture","poster":"Wasss123","upvote_count":"4","comment_id":"668399","timestamp":"1663100820.0"},{"content":"Selected Answer: BC\nB D you have to do the both to get it done.\nTo merge process, you have to perform between the report table and stage a table","upvote_count":"2","comment_id":"667866","timestamp":"1663064700.0","poster":"John_Pongthorn"},{"comment_id":"663099","poster":"Remi2021","content":"Answers are tricky, official documentation suggests Dataflow or Datafusion path as well as inclusion of DataStreams\nhttps://cloud.google.com/blog/products/data-analytics/real-time-cdc-replication-bigquery","timestamp":"1662611460.0","upvote_count":"1"},{"timestamp":"1662499860.0","poster":"changsu","content":"Selected Answer: CE\nIt costs more to update/delete.","comment_id":"661664","upvote_count":"2"},{"poster":"ducc","timestamp":"1662179100.0","upvote_count":"1","content":"Selected Answer: BE\nBE is correct\nBig Query only need to capture change, no need DELETE, UPDATE","comment_id":"658058"}],"question_images":[],"exam_id":10,"url":"https://www.examtopics.com/discussions/google/view/79672-exam-professional-data-engineer-topic-1-question-116/","timestamp":"2022-09-03 06:25:00","isMC":true,"answer_description":"","answer_images":[],"answer_ET":"BD"}],"exam":{"provider":"Google","numberOfQuestions":319,"name":"Professional Data Engineer","isBeta":false,"isImplemented":true,"id":10,"lastUpdated":"15 Feb 2025","isMCOnly":true},"currentPage":4},"__N_SSP":true}