{"pageProps":{"questions":[{"id":"p91Z5Ms166Qq2lTgSwrF","timestamp":"2020-03-18 01:46:00","discussion":[{"upvote_count":"39","comment_id":"68171","comments":[{"timestamp":"1688142420.0","poster":"AzureDP900","content":"Thank you for detailed explanation. C is right","comment_id":"762820","upvote_count":"1"}],"timestamp":"1601044740.0","poster":"[Removed]","content":"Correct: C\n\nIf you create a Dataproc cluster with internal IP addresses only, attempts to access the Internet in an initialization action will fail unless you have configured routes to direct the traffic through a NAT or a VPN gateway. Without access to the Internet, you can enable Private Google Access, and place job dependencies in Cloud Storage; cluster nodes can download the dependencies from Cloud Storage from internal IPs."},{"content":"Should be C:\n\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/init-actions","upvote_count":"12","comment_id":"65403","poster":"rickywck","timestamp":"1600382760.0"},{"comment_id":"1335507","timestamp":"1735814400.0","content":"Selected Answer: C\nA: Incorrect, Proxy allows for connection to a Cloud SQL instance, which unless you have the dependencies stored there (doesn't seem viable or smart), would achieve nothing.\n\nB: Incorrect, will allow for a connection to the internet to be made for installing the dependencies, however this goes against the companies security policies so should not be considered.\n\nC: Correct, only one that makes sense and is best practise (see https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/init-actions)\n\nD: Incorrect, provides access to a shared VPC network, however doesn't necessarily provide a way to access the dependencies. And even if it did, would go against company security policy.","poster":"b3e59c2","upvote_count":"1"},{"upvote_count":"1","comment_id":"1176345","content":"Selected Answer: C\nc looks good","poster":"gcpdataeng","timestamp":"1726641120.0"},{"timestamp":"1711296960.0","comment_id":"1015876","content":"Selected Answer: C\nSecurity Compliance: This option aligns with your company's security policies, which prohibit public Internet access from Cloud Dataproc nodes. Placing the dependencies in a Cloud Storage bucket within your VPC security perimeter ensures that the data remains within your private network.\n\nVPC Security: By placing the dependencies within your VPC security perimeter, you maintain control over network access and can restrict access to the necessary nodes only.\n\nDataproc Initialization Action: You can use a custom initialization action or script to fetch and install the dependencies from the secure Cloud Storage bucket to the Dataproc cluster nodes during startup.\n\nBy copying the dependencies to a secure Cloud Storage bucket and using an initialization action to install them on the Dataproc nodes, you can meet your security requirements while providing the necessary dependencies to your cluster.","poster":"barnac1es","upvote_count":"3"},{"timestamp":"1707216540.0","content":"Selected Answer: C\nC is correct","upvote_count":"1","comment_id":"973660","poster":"knith66"},{"content":"Selected Answer: C\nC seems good","poster":"charline","comment_id":"838239","upvote_count":"1","timestamp":"1694627340.0"},{"upvote_count":"2","comment_id":"812962","content":"Answer C,\nIt needs practical experience to understand this question. You create cluster with some package/software i.e dependencies such as python packages that you store in .zip file, then you save a jar file to run the cluster as an application such as you need java while running spark session. and some config yaml file. \nThese dependencies you can save in bucket and can use to configure cluster from external window , sdk or api. without going into UI. \nThen you need to use VPC to access these files","poster":"musumusu","timestamp":"1692356160.0"},{"timestamp":"1685447640.0","poster":"zellck","comment_id":"731554","upvote_count":"1","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network#and_vpc-sc_networks\nWith VPC Service Controls, administrators can define a security perimeter around resources of Google-managed services to control communication to and between those services."},{"content":"Without access to the internet, you can enable Private Google Access and place job dependencies in Cloud Storage; cluster nodes can download the dependencies from Cloud Storage from internal IPs.","comment_id":"634398","timestamp":"1674288060.0","poster":"DataEngineer_WideOps","upvote_count":"1"},{"upvote_count":"2","poster":"medeis_jar","content":"Selected Answer: C\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network#create_a_cloud_dataproc_cluster_with_internal_ip_address_only","timestamp":"1657357380.0","comment_id":"520141"},{"comment_id":"507447","poster":"Prabusankar","timestamp":"1655936220.0","content":"When creating a Dataproc cluster, you can specify initialization actions in executables or scripts that Dataproc will run on all nodes in your Dataproc cluster immediately after the cluster is set up. Initialization actions often set up job dependencies, such as installing Python packages, so that jobs can be submitted to the cluster without having to install dependencies when the jobs are run","upvote_count":"3"},{"poster":"JG123","comment_id":"486435","upvote_count":"1","content":"Correct: C","timestamp":"1653445380.0"},{"content":"c it is!","comment_id":"152791","upvote_count":"2","poster":"clouditis","timestamp":"1612753200.0"},{"comment_id":"70615","upvote_count":"2","poster":"Rajokkiyam","timestamp":"1601698080.0","content":"Should be C"},{"content":"Should be C","poster":"[Removed]","timestamp":"1600749600.0","comment_id":"66830","upvote_count":"2"},{"comment_id":"65524","timestamp":"1600409820.0","poster":"jvg637","content":"I think the correct answer might be C instead, due to https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network#create_a_cloud_dataproc_cluster_with_internal_ip_address_only","upvote_count":"4"}],"url":"https://www.examtopics.com/discussions/google/view/16899-exam-professional-data-engineer-topic-1-question-158/","question_text":"You need to deploy additional dependencies to all nodes of a Cloud Dataproc cluster at startup using an existing initialization action. Company security policies require that Cloud Dataproc nodes do not have access to the Internet so public initialization actions cannot fetch resources. What should you do?","question_id":66,"exam_id":10,"answer":"C","answer_images":[],"answer_description":"","choices":{"C":"Copy all dependencies to a Cloud Storage bucket within your VPC security perimeter","D":"Use Resource Manager to add the service account used by the Cloud Dataproc cluster to the Network User role","B":"Use an SSH tunnel to give the Cloud Dataproc cluster access to the Internet","A":"Deploy the Cloud SQL Proxy on the Cloud Dataproc master"},"question_images":[],"isMC":true,"unix_timestamp":1584492360,"answers_community":["C (100%)"],"answer_ET":"C","topic":"1"},{"id":"CSqyenY90vwiBkhGGlTA","isMC":true,"answer_description":"","question_text":"You need to choose a database for a new project that has the following requirements:\n✑ Fully managed\n✑ Able to automatically scale up\n✑ Transactionally consistent\n✑ Able to scale up to 6 TB\n✑ Able to be queried using SQL\nWhich database do you choose?","answer_images":[],"discussion":[{"content":"Correct: A\nIt asks for scaling up which can be done in cloud sql, horizontal scaling is not possible in cloud sql\nAutomatic storage increase\nIf you enable this setting, Cloud SQL checks your available storage every 30 seconds. If the available storage falls below a threshold size, Cloud SQL automatically adds additional storage capacity. If the available storage repeatedly falls below the threshold size, Cloud SQL continues to add storage until it reaches the maximum of 30 TB.","poster":"[Removed]","timestamp":"1585154700.0","upvote_count":"35","comments":[{"timestamp":"1598619240.0","poster":"google_learner123","comment_id":"168458","content":"C - CloudSQL does not scale automatically.","comments":[{"content":"Cloud SQL can automatically scale up storage capacity when you are near your limit","comment_id":"188979","poster":"zxing233","timestamp":"1601292780.0","comments":[{"comment_id":"690411","content":"it does not say about type of scaling, Cloud SQL scale up automatically with storage, that should works","poster":"dmzr","timestamp":"1665337980.0","upvote_count":"4"}],"upvote_count":"5"}],"upvote_count":"9"},{"timestamp":"1594065840.0","comment_id":"128239","upvote_count":"7","comments":[{"content":"Have you really worked on GCP?","comment_id":"354869","upvote_count":"15","poster":"dem2021","timestamp":"1620750240.0"},{"comment_id":"188977","upvote_count":"2","timestamp":"1601292660.0","poster":"zxing233","content":"https://cloud.google.com/sql it is fully managed"},{"poster":"Gcpyspark","content":"Google documentation says \"Cloud SQL is a fully-managed database service that helps you set up, maintain, manage, and administer your relational databases on Google Cloud Platform.\"","upvote_count":"8","timestamp":"1608482880.0","comment_id":"248819"},{"poster":"hightech","timestamp":"1669388040.0","content":"Cloud Sql is a fully managed service by Google","comment_id":"726867","upvote_count":"5"}],"content":"C:- Cloud SQL is not fully managed as that is one of the requirement.","poster":"Rajuuu"},{"content":"A. Cloud SQL There is no need of Spanner","timestamp":"1672511340.0","poster":"AzureDP900","comment_id":"762825","upvote_count":"1"}],"comment_id":"68173"},{"poster":"[Removed]","upvote_count":"14","content":"Should be C.","timestamp":"1584859320.0","comments":[{"timestamp":"1584929640.0","poster":"[Removed]","upvote_count":"8","comment_id":"67146","content":"May be A"}],"comment_id":"66831"},{"timestamp":"1738873440.0","poster":"Siahara","upvote_count":"1","comment_id":"1352648","content":"Selected Answer: C\nC. Cloud Spanner and an explanation of the other options:\nWhy Cloud Spanner is the best fit:\nFully managed service: Cloud Spanner is fully managed by Google, simplifying database administration.\nAutomatic scaling It handles scaling seamlessly, both horizontally and vertically.\nTransactional consistency: Cloud Spanner is known for its strong transactional consistency, including globally distributed transactions.\nScalable up to 6 TB (and beyond): It easily accommodates your 6 TB requirement and can scale much larger if needed.\nSQL Support: Cloud Spanner offers a familiar SQL interface."},{"content":"Selected Answer: C\nAlthough the type of automatic scaling isn't specified, Cloud SQL does not allow for outright dynamic capacity auto scaling, so I believe the answer would be C","upvote_count":"1","poster":"b3e59c2","timestamp":"1735815240.0","comment_id":"1335510"},{"upvote_count":"4","poster":"Rav761","content":"Selected Answer: C\nC. Cloud Spanner\n\nHere's why:\n\nFully Managed: Cloud Spanner is a fully managed database service provided by Google Cloud.\n\nAutomatic Scaling: It automatically scales horizontally to handle increased workloads and data volumes.\n\nTransactional Consistency: Cloud Spanner provides strong transactional consistency with support for ACID transactions.\n\nScalability: It can easily scale up to and beyond 6 TB while maintaining performance and consistency.\n\nSQL Queries: Cloud Spanner supports SQL queries, making it compatible with existing SQL-based analytics and applications.","timestamp":"1735250340.0","comment_id":"1332122"},{"content":"Selected Answer: C\nI would go to C.","poster":"SamuelTsch","timestamp":"1730101560.0","upvote_count":"3","comment_id":"1303868"},{"poster":"baimus","comment_id":"1291908","timestamp":"1727778480.0","upvote_count":"2","content":"Selected Answer: C\nAll the arguments below can essentially be boiled down to two questions. 1) is Cloud SQL fully managed? (yes), 2) Does it autoscale? It depends, is the answer. The question is horrifyingly worded, as it comes down to an ambiguity coinflip. I'm going with C, it feels like it's a better fit."},{"upvote_count":"2","content":"Selected Answer: A\nDid not refer to global, thus not spanner. A is correct","poster":"JamesKarianis","comment_id":"1263492","timestamp":"1723294620.0"},{"comment_id":"1247653","timestamp":"1720939620.0","content":"Selected Answer: C\nTransactional consistency: spanner provides strong consistency across rows, regions, and continents.","upvote_count":"1","poster":"Anudeep58"},{"upvote_count":"1","content":"Answer : A\nCloud SQL and Cloud Spanner are the options for the questions. But here as per requirement they don't need horizontal scaling, they want manager SQL instance and it should support 6 TB of storage. Cloud SQL can support up to 64 TB of storage\nhttps://cloud.google.com/sql/docs/quotas#:~:text=Cloud%20SQL%20storage%20limits,core%3A%20Up%20to%203%20TB.","timestamp":"1709353020.0","poster":"mothkuri","comment_id":"1163914"},{"content":"Selected Answer: A\nNot horizontal scaling is required, cloud SQL will work for 10 TB","upvote_count":"2","poster":"cuadradobertolinisebastiancami","timestamp":"1708022220.0","comment_id":"1151232"},{"timestamp":"1707365400.0","comment_id":"1144035","upvote_count":"1","poster":"williamvinct","content":"i will go with A, since the question doesnt specifically say it should be scaling horizontally"},{"comment_id":"1082840","poster":"arturido","timestamp":"1701196140.0","upvote_count":"2","content":"Selected Answer: A\n\"Able to scale up to 6 TB\" -seems to be the key \nit looks like autoscaling is related to storage - possible in case of Cloud SQL","comments":[{"content":"no way , u can automaticly scale the Cloud SQL , please read the documents of Cloud SQL, Spanner is the solution .","upvote_count":"1","poster":"LaxmanTiwari","timestamp":"1703053500.0","comment_id":"1101302"}]},{"content":"Selected Answer: C\nSpanner is consistent and fully-managed \nhttps://cloud.google.com/spanner/docs/transactions?hl=en","poster":"tibuenoc","timestamp":"1700474940.0","comment_id":"1075310","upvote_count":"1"},{"comment_id":"1074088","upvote_count":"2","timestamp":"1700319660.0","poster":"DataFrame","content":"Selected Answer: A\nA seems to be correct because of the scaling factor of 6 TB because cloud sql easily supports up to 40 TB and obviously there is a limitation of GLOBALLY MULTI REGIONAL which is nothing to do with question. Hence A seems more closer."},{"content":"Selected Answer: C\nGuys - this is 1000% C. \".....automatically scale up\" Cloud SQL needs restart! this is not the solution. Only Spanner (and BQ) are true automatically scale up","comment_id":"1053616","upvote_count":"2","comments":[{"timestamp":"1701425160.0","poster":"Zepopo","content":"What problem in restart? There are no any constraints about this, but there directly writes: \"Fully managed\"","comment_id":"1085133","upvote_count":"1"}],"poster":"Nirca","timestamp":"1698228420.0"},{"poster":"kcl10","comment_id":"1023932","upvote_count":"1","content":"Selected Answer: C\nC: Cloud Spanner\n\nWhy not A: Cloud SQL?\nNo \"automatically\" scale up feature","timestamp":"1696336560.0"},{"poster":"barnac1es","upvote_count":"1","content":"Selected Answer: C\nFully Managed: Cloud Spanner is a fully managed database service provided by Google Cloud, which means you don't have to worry about managing infrastructure, updates, or backups.\n\nAutomatic Scaling: Cloud Spanner can automatically scale both horizontally and vertically to handle increased workloads and data volume. It can handle databases ranging from a few gigabytes to multi-terabyte scale.\n\nTransactionally Consistent: Cloud Spanner offers strong transactional consistency, making it suitable for applications that require ACID compliance.\n\nSQL Querying: You can query Cloud Spanner databases using SQL, which is a familiar query language for many developers and analysts.\n\nGiven your requirements, Cloud Spanner is designed to meet the need for a fully managed, scalable, transactionally consistent, and SQL-accessible database solution.","timestamp":"1695565380.0","comment_id":"1015888"},{"upvote_count":"1","poster":"Xubaca","content":"Selected Answer: C\nAble to automatically scale up = Memory and CPU\nAble to scale up to 6 TB = Disk\nCorrect is C, because Cloud SQL no scale Memory and CPU.","comment_id":"991482","timestamp":"1693144020.0"},{"comment_id":"953294","content":"Selected Answer: A\nCloud SQL can also automatically scale up storage capacity when you are near your limit.\nhttps://cloud.google.com/sql/?utm_source=google&utm_medium=cpc&utm_campaign=japac-HK-all-en-dr-BKWS-all-super-trial-PHR-dr-1605216&utm_content=text-ad-none-none-DEV_c-CRE_652279903175-ADGP_Hybrid%20%7C%20BKWS%20-%20BRO%20%7C%20Txt%20~%20Databases_Cloud%20SQL_gcp%20horizontal%20sql_main-KWID_43700076522535474-kwd-2005739330106&userloc_2344-network_g&utm_term=KW_google%20sql%20horizontal&gclid=Cj0KCQjwqs6lBhCxARIsAG8YcDjWaS5NbG3remUHnHQ7EFK-wJNsF0I_IvePKHF0mHaiBK3_-eM7Z-UaAqQeEALw_wcB&gclsrc=aw.ds#section-2:~:text=Cloud%20SQL%20can%20also%20automatically%20scale%20up%20storage%20capacity%20when%20you%20are%20near%20your%20limit.","timestamp":"1689509820.0","upvote_count":"2","poster":"wan2three"},{"poster":"vaga1","timestamp":"1687259640.0","upvote_count":"2","content":"Selected Answer: A\nThe requirements does not mention high-availability, and Cloud SQL is up to 64TB now \nhttps://cloud.google.com/sql/docs/quotas#storage_limits","comment_id":"928382"},{"content":"I feel this is a 'fit-for-purpose' question, and TB's large Cloud-SQL is enough. The up-to-6TB is a sign for me to not go for Cloud-Spanner.","upvote_count":"1","comment_id":"917815","poster":"WillemHendr","timestamp":"1686201000.0"},{"poster":"Adswerve","upvote_count":"1","timestamp":"1681677060.0","content":"Selected Answer: C\nC Spanner\n\nSpanner scales automatically:\nhttps://cloud.google.com/blog/products/databases/cloud-database-scales-instance-sizes-easily/\n\"Automatically right-size Spanner instances with the new Autoscaler\"\n\nCloud SQL doesn't scale automatically. Here's a tutorial by a Google employee which shows manual steps to scale Cloud SQL:\nhttps://cloud.google.com/community/tutorials/horizontally-scale-mysql-database-backend-with-google-cloud-sql-and-proxysql","comment_id":"872152"},{"timestamp":"1681542300.0","poster":"streeeber","comment_id":"870720","content":"Selected Answer: C\nI would say C, because A does not offer autoscaling.","upvote_count":"2"},{"upvote_count":"3","content":"Selected Answer: A\nPeople here are rejecting Cloud SQL because it does not scale up CPU and MEM automatically, but Cloud Spanner also don't (not without hosting the Autoscaler tool yourself).","comment_id":"854610","timestamp":"1680105120.0","poster":"juliobs"},{"poster":"musumusu","comment_id":"812972","timestamp":"1676725260.0","upvote_count":"3","content":"Answer A: CloudSQL\nDon't confuse anymore, \nSQL needs, Cloud sql or Spanner. \nnothing mentioned that data should be highly available or scalable or increasing in future So, don't go with Spanner. \nIdeal use case, Cloud Sql can handle UP TO 30TB. \nOther two options are no-sql so don't go."},{"content":"Selected Answer: A\nCloud SQL is enough here. Internet scale is not required. So Cloud spanner is incorrect.","comment_id":"809853","timestamp":"1676486700.0","poster":"certmonkey","upvote_count":"2"},{"poster":"desertlotus1211","comment_id":"789213","content":"Answer is C: https://cloud.google.com/architecture/autoscaling-cloud-spanner","timestamp":"1674781200.0","upvote_count":"2"},{"comment_id":"734228","content":"Selected Answer: C\nC because only cloud spanner fit requirement: Able to automatically scale up","timestamp":"1670046300.0","upvote_count":"3","poster":"hauhau"},{"comment_id":"731547","content":"Selected Answer: A\nA is the answer.","timestamp":"1669816200.0","upvote_count":"3","poster":"zellck"},{"poster":"NicolasN","comment_id":"722102","content":"Selected Answer: A\nThe whole discussion is around the bullet \"Able to automatically scale up\"\nIf this was referring explicitly to memory or CPU scaling. answer [A] would be incorrect.\nBut the subsequent bullet \"Able to scale up to 6 TB\" makes it clear (at least for me) that the former is referring to storage scaling.","timestamp":"1668874380.0","upvote_count":"4"},{"poster":"ducc","upvote_count":"2","content":"Selected Answer: A\nIt not required scale up horizontally and global end point\nSo A is correct","comment_id":"656704","timestamp":"1662070320.0"},{"timestamp":"1647790260.0","comments":[{"timestamp":"1658412180.0","comment_id":"634644","upvote_count":"2","poster":"alecuba16","content":"Scale up -> more computation resources, cloud sql only scales in storage, not in performance/cpu"}],"content":"Selected Answer: A\nA - Because you don't need to scale horizontally","comment_id":"571704","poster":"BISeniorConsultant","upvote_count":"2"},{"poster":"rbeeraka","content":"Selected Answer: A\nCloud SQL can scale up and fully managed DB service.","timestamp":"1646438400.0","comment_id":"561098","upvote_count":"3"},{"content":"Selected Answer: C\n✑ Able to automatically scale-up\n✑ Transactionally consistent\n\nOnly Spanner, Cloud SQL scale only storage automatically.","timestamp":"1641726420.0","comment_id":"520144","poster":"medeis_jar","upvote_count":"5"},{"content":"Correct: A","timestamp":"1637814540.0","upvote_count":"3","comment_id":"486439","poster":"JG123"},{"content":"Cloud SQL can easily scale up by adding up to 64 processor cores and 400+ GB of RAM to support up to 30TB of storage. What’s more, Cloud SQL can automatically scale up your storage capacity when you get close to your limit.","timestamp":"1637708340.0","comment_id":"485477","upvote_count":"1","comments":[{"timestamp":"1658412240.0","content":"What you are indicating is MANUAL scaling, the service has to be turn off for the scaling process.","poster":"alecuba16","upvote_count":"1","comment_id":"634647"},{"comment_id":"634654","comments":[{"upvote_count":"2","timestamp":"1666899660.0","poster":"MisuLava","content":"you don't have to change the machine type. it simply scales up by itself increasing the storage. A is the correct answer","comment_id":"705881"}],"timestamp":"1658412360.0","content":"\"In order to change the machine type, you have to stop the Compute Engine instance and, after the change, restart it.\"\n\nhttps://cloud.google.com/architecture/elastically-scaling-your-mysql-environment\n\nSo it is MANUAL scaling with the drawback of the service being interrupted.\n\nSpanner doesn't require to turn off the service for scaling.","upvote_count":"2","poster":"alecuba16"}],"poster":"alvinlxw"},{"poster":"gcp_k","content":"Answer could be : A\n\nBoth Cloud Spanner and Cloud SQL does not have auto scaling feature natively. You need to build automation around it based on metrics. \n\nBoth Cloud SQL and Cloud Spanner supports SQL.\n\nWith Cloud SQL, you can go up to 10 TB of storage which also satisfies the other requirement. \n\nConsistency - Of course, with Cloud SQL, you have single master and read replicas. So technically the data will be consistent across all instances so to speak.\n\nThe reason why I didn't choose Spanner is - There is no requirement for HA DR, multi region, secondary indexes, etc.. So I choose \"A\"","timestamp":"1634828520.0","upvote_count":"9","comment_id":"465745"},{"comment_id":"463672","poster":"dotchi","content":"Cloud SQL\nCloud SQL is a good option when you need relational database capabilities but don’t need storage capacity over 10TB or more than 4000 concurrent connections. You also need to be skilled at on-premise management. Cloud Spanner\nCloud Spanner is a good option when you plan to use large amounts of data (more than 10TB) and need transactional consistency. It is also good if you want to use sharding for higher throughput and accessibility.If you know or think that you might eventually need to be able to horizontally scale your Google Cloud database, Cloud Scanner is a better option than Cloud SQL. If you start with Cloud SQL and need to eventually move to Cloud Spanner, be prepared to re-write your application in addition to migrating your database.","upvote_count":"2","timestamp":"1634501040.0"},{"comments":[{"content":"now, Cloud SQL can manage upto 64 TB.","comment_id":"705883","poster":"MisuLava","upvote_count":"2","timestamp":"1666899720.0"}],"poster":"triipinbee","content":"A: Cloud SQL can manage upto 10TB of data, the storage is automatically increase if you have selected the option and is fully managed.","timestamp":"1630078560.0","comment_id":"433193","upvote_count":"3"},{"upvote_count":"4","timestamp":"1627803780.0","poster":"DeepakS227","content":"Able to automatically scale up so A can be eliminated. right answer is C","comment_id":"418160"},{"upvote_count":"2","comment_id":"313368","timestamp":"1615992720.0","poster":"Shammy45","content":"Per documentation, Cloud spanner is Transactionally consistent, so the answer is C"},{"poster":"karthik89","content":"Spanner does not support SQL, it has spanner specific query language. Correct me if i am wrong.","upvote_count":"2","timestamp":"1613899260.0","comments":[{"content":"you are wrong :)","upvote_count":"5","comment_id":"506066","poster":"baubaumiaomiao","timestamp":"1640086500.0"}],"comment_id":"295660"},{"content":"Ans - C seems more accurate as per google docs where they have clearly mentioned about \"Transactional consistency\" for spanner but i couldn't find this proof for Cloud SQL.. but i agree that Cloud SQL is also transactional consistent.","upvote_count":"3","timestamp":"1613591280.0","poster":"ArunSingh1028","comment_id":"292834"},{"poster":"elenamatay","upvote_count":"1","comment_id":"288784","content":"Datastore isn't able to be queried using SQL but projection queries (source: https://cloud.google.com/datastore/docs/concepts/queries). Thus, I think the best option is A.","timestamp":"1613122320.0"},{"content":"Correct C. Cloud SQL has limitation, it cant scale automatically. Scale your instances effortlessly with a single API call whether you start with simple testing or you need a highly available database in production.\nhttps://cloud.google.com/sql","poster":"someshsehgal","comment_id":"286635","timestamp":"1612857960.0","upvote_count":"3"},{"poster":"StelSen","content":"Answer: C\nLook at key requirement: Automatic Scale Up (An ability to increase compute/memory capacity, Not storage. For storage, there is another requirement which is increase upto 6TB). So, Option-C is more appropriate, although the Cost is high. This is just an exam. In real world, I will convince customer to use Cloud SQL and will be able to use some Cloud Function to achieve near real time scaling even if I use Cloud SQL)","upvote_count":"7","timestamp":"1610593620.0","comment_id":"266690"},{"poster":"SPutri","timestamp":"1605963240.0","upvote_count":"4","content":"the keyword is in \"transactional consistency\" which lead me to choose Cloud Spanner :)\nhttps://cloud.google.com/spanner","comment_id":"224339"},{"upvote_count":"2","comment_id":"195716","content":"The correct answer is A (Cloud SQL)\nReasons:-\nFully managed - Cloud SQL is fully managed \nAble to automatically scale up - Cloud SQL can automatically scale vertically, the question does not specifically points anything about horizontal scaling, so technically this meets the requirement \nTransactional consistent - Yes, Cloud SQL is ACID compliant \nAble to scale up to 6 TB - Here is the catch, Cloud SQL is recommended where storage is up to 10TB, so this meets the requirement of \"upto 6 TB. Typically we should consider Spanner, when the DB size is estimated to be more than 1TB and beyond / across multi region / horizontal scaling / very expensive solution\nAble to be queried using SQL - It is a fully managed MySQL / Postgress SQL solution from Google.","poster":"Alasmindas","timestamp":"1602129540.0"},{"comments":[{"upvote_count":"1","timestamp":"1601010000.0","poster":"SureshKotla","content":"By branding, I mean Google advertised widely","comment_id":"186751"}],"content":"Only Spanner and Datastore are branded as ACID compliant while BigQuery and SQL do support them. So, if to choose an option, it will be Spanner","comment_id":"186749","upvote_count":"2","timestamp":"1601009940.0","poster":"SureshKotla"},{"content":"Should be A. While A & C both serves the purpose, Spanner would be an overkill for storing for just max 6 TB of data and will not be cost effective when compared with Cloud SQL.","comment_id":"186250","timestamp":"1600960740.0","upvote_count":"1","poster":"SteelWarrior"},{"timestamp":"1600663020.0","comments":[{"poster":"H_S","comment_id":"364330","content":"GOOD JOB","timestamp":"1621761660.0","upvote_count":"1"}],"poster":"Diqtator","comment_id":"183431","upvote_count":"2","content":"Question is about scaling up, not out. Therefore it should be A."},{"timestamp":"1598635620.0","upvote_count":"5","poster":"atnafu2020","content":"A\nCloud Spanner-Scales out/horizontally by increasing its storage capacity.\nCloud SQL scales up/vertically by increasing its storage capacity.\nCloud SQL instances support MySQL 8.0, 5.7 (default), and 5.6, and provide up to 416 GB of RAM and 30 TB of data storage, with the option to automatically increase the storage size, as needed.","comment_id":"168643"},{"comment_id":"146637","content":"Both A and C meet most the requirements. The difference is in automatically scaling. Spanner does horizontally scale. Cloud SQL has API you can use and scale. It is not automated. It does increase storage automatically but not CPU, RAM etc. So I vote for Spanner","poster":"sh2020","upvote_count":"6","timestamp":"1596038640.0"},{"upvote_count":"3","content":"\"A\" Cloud SQL meets the requirements.\nhttps://cloud.google.com/sql\nEasily scale up as your data grows—add up to 64 processor cores and more than 400 GB of RAM and 30 TB of storage and add read replicas to handle increasing read traffic.\nCloud SQL can automatically scale up storage capacity when you are near your limit. This way you don’t have to spend time estimating future storage needs or spend money on capacity until you need it.","comment_id":"129769","poster":"dg63","timestamp":"1594213380.0"},{"content":"Option [A] - Cloud SQL looks to be the correct answer.\nCloud SQL meets all the criterion- https://cloud.google.com/sql/docs/mysql/instance-settings","poster":"dambilwa","timestamp":"1593515280.0","upvote_count":"3","comment_id":"123438"},{"content":"I don't know if Cloud SQL provide transactionally consistency. \nI would say C is more appropriate.","upvote_count":"3","timestamp":"1593421080.0","poster":"gvaf4","comment_id":"122578"},{"content":"its A Cloud SQL meets all these requirements","poster":"Callumr","upvote_count":"5","timestamp":"1592746740.0","comment_id":"115607"},{"upvote_count":"2","content":"Between A and C. Cloud SQL tolerance is 10TB, while Spanner is >10TB. C is correct answer.","comments":[{"comment_id":"126402","poster":"droogie","upvote_count":"8","content":"The number is 6TB, so the answer should be A (according to your logic)","timestamp":"1593898260.0","comments":[{"content":"It needs to scale up to 6TB only. No need of Cloud Spanner for this use case","upvote_count":"2","timestamp":"1614852720.0","poster":"Raangs","comment_id":"303275"}]}],"timestamp":"1589482140.0","poster":"AJKumar","comment_id":"89096"},{"comments":[{"content":"Should be A. Cloud SQL supports scale up up to 30TB. And its fully managed too.","timestamp":"1594474380.0","comment_id":"132114","poster":"Rajokkiyam","upvote_count":"3"}],"timestamp":"1585887600.0","upvote_count":"2","content":"Should be C\nhttps://cloud.google.com/products/databases","comment_id":"70616","poster":"Rajokkiyam"},{"poster":"digvijay","upvote_count":"4","content":"Should be C. As Cloud SQL is not suitable for scaling for huge data.","timestamp":"1585016280.0","comment_id":"67423"}],"unix_timestamp":1584859320,"timestamp":"2020-03-22 07:42:00","exam_id":10,"answers_community":["A (52%)","C (48%)"],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/17213-exam-professional-data-engineer-topic-1-question-159/","question_id":67,"choices":{"A":"Cloud SQL","B":"Cloud Bigtable","C":"Cloud Spanner","D":"Cloud Datastore"},"answer_ET":"A","topic":"1","answer":"A"},{"id":"WaCsUqKbC2l8PN5OCRny","answer_images":[],"answer_ET":"A","question_images":[],"choices":{"A":"Use Google Stackdriver Audit Logs to review data access.","B":"Get the identity and access management IIAM) policy of each table","D":"Use the Google Cloud Billing API to see what account the warehouse is being billed to.","C":"Use Stackdriver Monitoring to see the usage of BigQuery query slots."},"answer_description":"","discussion":[{"content":"Table access control is now possible in big query. However, before even checking table access control permission which is not set by the company as a formal security policy yet, we need to first understand by looking at the big query immutable audit logs as who is accessing what DAT sets and tables. Based on the information, access control policy at dataset and table level can be set. \n\nSo the correct answer is A","timestamp":"1604691420.0","poster":"Radhika7983","upvote_count":"6","comment_id":"214267"},{"upvote_count":"5","poster":"Cloud_Student","comment_id":"136442","timestamp":"1594901160.0","content":"A - need to check first who is accessing which table"},{"comment_id":"1342434","timestamp":"1737181560.0","poster":"cqrm3n","upvote_count":"1","content":"Selected Answer: A\nStackdriver Audit Logs is now called Cloud Audit logs. To secure a data warehouse, the first step is to understand how the datasets are being accessed and used. Cloud Audit logs can track data access as it provides a detailed log of all data access operations."},{"poster":"MaxNRG","comment_id":"475019","content":"A is correct because this is the best way to get granular access to data showing which users are accessing which data. \nB is not correct because we already know that all users already have access to all data, so this information is unlikely to be useful. It will also not show what users have done, just what they can do.\nC is not correct because slot usage will not inform security policy.\nD is not correct because a billing account is typically shared among many people and will only show the amount of data queried and stored\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs/#mapping-audit-entries-to-log-streams\nhttps://cloud.google.com/bigquery/docs/monitoring#slots-available","timestamp":"1727155680.0","upvote_count":"2"},{"poster":"rocky48","content":"Selected Answer: A\nA. Use Google Stackdriver Audit Logs to review data access.\n\nReviewing the audit logs provides visibility into who is accessing your data, when they are doing so, and what actions they are taking within BigQuery. This is crucial for understanding current data usage and potential security risks.\n\nOption B (getting the IAM policy of each table) is important but more focused on controlling access rather than discovering what everyone is currently doing.\n\nOption C (using Stackdriver Monitoring to see query slots usage) can help with monitoring and optimizing your BigQuery usage but doesn't provide a comprehensive view of what users are doing with the data.\n\nOption D (using the Google Cloud Billing API) is more related to tracking billing information rather than understanding what users are doing with the data.","upvote_count":"2","timestamp":"1727155620.0","comment_id":"1063243"},{"timestamp":"1727155620.0","upvote_count":"4","comment_id":"1050494","content":"Selected Answer: A\nTo begin securing your data warehouse in Google BigQuery and gain insights into what everyone is doing with the datasets, the first step you should take is:\n\nA. Use Google Stackdriver Audit Logs to review data access.\n\nReviewing the audit logs provides visibility into who is accessing your data, when they are doing so, and what actions they are taking within BigQuery. This is crucial for understanding current data usage and potential security risks.\n\nOption B (getting the IAM policy of each table) is important but more focused on controlling access rather than discovering what everyone is currently doing.\n\nOption C (using Stackdriver Monitoring to see query slots usage) can help with monitoring and optimizing your BigQuery usage but doesn't provide a comprehensive view of what users are doing with the data.\n\nOption D (using the Google Cloud Billing API) is more related to tracking billing information rather than understanding what users are doing with the data.","poster":"rtcpost"},{"content":"A. Use Google Stackdriver Audit Logs to review data access.\n\nIn this scenario, you have been asked to secure the data warehouse in Google BigQuery. To do that, you first need to understand what everyone is doing with the data, i.e., who is accessing it and what actions they are performing. Google Stackdriver Audit Logs can provide you with a detailed record of all the data access and actions taken by users in Google BigQuery. It's like having a logbook that keeps track of who enters the library, which books they read, and what they do with the books.\n\nC just give how many people accessing the same dataset at given time \nC. Another tool you have is called \"Stackdriver Monitoring.\" It helps you see how many people are using the library at the same time. It's like knowing how many readers are in the library at any given moment.","poster":"NeoNitin","upvote_count":"1","timestamp":"1727155620.0","comment_id":"966793"},{"timestamp":"1727155620.0","poster":"RT_G","upvote_count":"1","comment_id":"1065123","content":"Selected Answer: A\nA - Since the question is to discover what everyone is doing. Also the question has indicated that no security policies have been implemented."},{"content":"A is the answer.\nBut recently, I think Dataplex is used for data governance .","timestamp":"1706187840.0","poster":"philli1011","upvote_count":"1","comment_id":"1131681"},{"content":"A. Use Google Stackdriver Audit Logs to review data access.\n\nStackdriver Audit Logs provide detailed logs on who accessed what resources and when, including data in BigQuery. Reviewing these logs will give you insight into which users and service accounts are accessing datasets, what operations they are performing, and when these accesses occur. This would be a crucial first step in understanding current usage and subsequently in crafting a security policy.","poster":"imran79","upvote_count":"1","comment_id":"1027035","timestamp":"1696647360.0"},{"timestamp":"1694794860.0","comment_id":"1008579","poster":"suku2","upvote_count":"1","content":"Selected Answer: A\nStackdriver audit logs is where we will view which datasets are being accessed by whom"},{"content":"Selected Answer: A\nIn order to take a decision you need to analyze the access lofs","poster":"bha11111","timestamp":"1678510080.0","upvote_count":"1","comment_id":"835664"},{"comment_id":"807484","timestamp":"1676298600.0","upvote_count":"1","poster":"niketd","content":"\"Discover what everyone is doing\" will happen through Audit logs, hence correct answer is A"},{"poster":"Nirca","upvote_count":"1","comment_id":"741688","content":"Selected Answer: A\n\"...to secure the data warehouse\" is to list all tables/views/Mviews VS. who is accessing these objects. Slot info is not relevant.","timestamp":"1670762220.0"},{"upvote_count":"1","poster":"fedebos8","timestamp":"1668357960.0","comment_id":"717423","content":"Selected Answer: A\nA is correct."},{"upvote_count":"1","poster":"nkunwar","comment_id":"681454","content":"Selected Answer: A\nAudit log ..logs activities against resources , is the best place to discover about activities against BQ","timestamp":"1664346480.0"},{"content":"Selected Answer: A\nA for sure","comment_id":"672377","timestamp":"1663506840.0","upvote_count":"1","poster":"John_Pongthorn"},{"timestamp":"1663007940.0","comment_id":"667359","upvote_count":"1","poster":"lalli117","content":"need to discuss permissions on the dataset level for each user , so i think A is appropriate answer."},{"poster":"ducc","upvote_count":"1","content":"Selected Answer: A\nA is correct","timestamp":"1661722740.0","comment_id":"653167"},{"timestamp":"1661276460.0","content":"Selected Answer: A\nA - check audit logs, to see what everyone is doing.","upvote_count":"1","comment_id":"650933","poster":"PhuocT"},{"content":"A. Use audit logs. You can access this from Google Admin console.","timestamp":"1660617720.0","comment_id":"647423","poster":"rowan_","upvote_count":"1"},{"poster":"anand81","timestamp":"1658897580.0","comment_id":"637802","content":"Correct A\nAs question related to bigquery and what everyone is doing. this will we could get it from stackdriver logs","upvote_count":"1"},{"comment_id":"618304","upvote_count":"1","poster":"SnowLearner","content":"Selected Answer: A\nAnswer is A","timestamp":"1655564700.0"},{"timestamp":"1655439480.0","content":"Selected Answer: A\nmost voted for A","poster":"som_420","upvote_count":"1","comment_id":"617516"},{"comment_id":"616876","timestamp":"1655313780.0","content":"Selected Answer: A\nAnswer: A\n\nYou need to understand who is using and what they are using in BigQuery before elaborate any kind of policy. Stackdriver audit is good for it","poster":"noob_master","upvote_count":"1"},{"comment_id":"613575","upvote_count":"1","poster":"chester_hello","content":"https://cloud.google.com/logging/docs/audit","timestamp":"1654738920.0"},{"timestamp":"1654556820.0","poster":"Ishiske","upvote_count":"2","comments":[{"upvote_count":"1","timestamp":"1654789920.0","comment_id":"614081","content":"Can you elaborate on why you are choosing c over a","poster":"Dip1994"},{"comment_id":"620460","timestamp":"1655907720.0","poster":"paauul","upvote_count":"2","content":"\"You have been asked to secure the data warehouse.\"\nLooking at slot usage will not tell you anything required to complete the task of securing the warehouse. If the question were about optimizing performance then C would be correct, but not in this case.\nAlso, 4 years in GCP time is like 20 years in real time. A lot will change in those 4 years. This is why they make you renew your certs every 2 years."}],"comment_id":"612506","content":"Selected Answer: C\nC is the answer.\nThe question is that you want to know what are they doing, not who. You don't need to know the event, u need to know on what they are spending the BQ resources (slots). \n\nAnd, I have the official guide for the exam from google 2018 (the book), this question is on the free practice exam. On the book the answer is C."},{"timestamp":"1649145720.0","upvote_count":"1","poster":"jagan79","content":"Data Access audit logs-- except for BigQuery Data Access audit logs-- are disabled by default because audit logs can be quite large. If you want Data Access audit logs to be written for Google Cloud services other than BigQuery, you must explicitly enable them. Enabling the logs might result in your Cloud project being charged for the additional logs usage. For instructions on enabling and configuring Data Access audit logs\n\nhttps://cloud.google.com/logging/docs/audit#data-access\n\nOnly for BigQuery the access logs are enabled by default","comment_id":"581102"},{"content":"Selected Answer: A\nA is correct since we need to check who is accessing what and that information is available in the logs.\nB is not correct since BigQuery does not have table level access control\nC is giving information on slot usage and D is about billing. Neither provide anything on the access.","poster":"GautamP","upvote_count":"1","timestamp":"1644623220.0","comments":[{"comment_id":"547092","timestamp":"1644839940.0","poster":"ML_Novice","content":"Are you sure \nin the documentation it says: BigQuery table ACL lets you set table-level permissions on resources like tables and views. Table-level permissions determine the users, groups, and service accounts that can access a table or view\nsource:\nhttps://cloud.google.com/bigquery/docs/table-access-controls-intro","upvote_count":"3"}],"comment_id":"545570"},{"timestamp":"1642882380.0","upvote_count":"2","comment_id":"530061","content":"correct answer -> Use Google Stackdriver Audit Logs to review data access.\n\n*** Key point of the question is \"You need to discover what everyone is doing\"?***\n----> Basically find out  \"who did what, where, and when?\"\n\nOnce you understand that visit reference page for confirmation of the answer.\n\nReference: https://cloud.google.com/logging/docs/audit","poster":"samdhimal"},{"comment_id":"504093","poster":"Anjit","upvote_count":"1","content":"Answer Is : A","timestamp":"1639815660.0"},{"upvote_count":"2","poster":"anji007","comment_id":"462665","content":"Ans: A\nTo understand access patterns or at least to know who/ which team are using which tables more need to look at history, which is nothing but Logs.\n\nOnce understood the pattern, IAM policy can be set (Option B). Usage of slots or billing account will not tell anything about access patterns of tables.","timestamp":"1634310000.0"},{"comment_id":"441123","timestamp":"1631052480.0","poster":"Nittin","content":"A is correct . To Track its easier in Operations","upvote_count":"2"},{"comment_id":"390690","poster":"sumanshu","content":"vote for 'A'","timestamp":"1624644180.0","upvote_count":"2"},{"poster":"naga","upvote_count":"3","comment_id":"284971","timestamp":"1612630320.0","comments":[{"comment_id":"390693","upvote_count":"2","timestamp":"1624644360.0","content":"Yes, because 'SLOTS' are just Virtual CPU used by BigQuery to process a query. https://cloud.google.com/bigquery/docs/slots","poster":"sumanshu"}],"content":"Sorry not C , correct answer A"},{"upvote_count":"1","content":"Correct C","poster":"naga","comment_id":"284968","timestamp":"1612630020.0"},{"poster":"arghya13","comment_id":"204489","timestamp":"1603431060.0","content":"Correct answer is A","upvote_count":"3"},{"poster":"cookiethecat","timestamp":"1597993920.0","upvote_count":"3","comment_id":"162768","content":"I think A is the best answer."}],"question_id":68,"timestamp":"2020-03-16 11:25:00","url":"https://www.examtopics.com/discussions/google/view/16729-exam-professional-data-engineer-topic-1-question-16/","topic":"1","isMC":true,"unix_timestamp":1584354300,"exam_id":10,"answer":"A","question_text":"Your startup has never implemented a formal security policy. Currently, everyone in the company has access to the datasets stored in Google BigQuery. Teams have freedom to use the service as they see fit, and they have not documented their use cases. You have been asked to secure the data warehouse. You need to discover what everyone is doing. What should you do first?","answers_community":["A (91%)","9%"]},{"id":"MJvpxj3BRaJbHtUCDy92","exam_id":10,"answer":"A","isMC":true,"discussion":[{"upvote_count":"33","comments":[{"timestamp":"1608564180.0","poster":"Gcpyspark","comments":[{"content":"you can always call GCP to add quota.. .Spanner is for global reach, ideally...","upvote_count":"3","comment_id":"789216","poster":"desertlotus1211","timestamp":"1674781320.0"}],"content":"Sure, however in future if the capacity grows beyond 30 TB then Cloud SQL won't work right then Spanner would be the option?","comment_id":"249441","upvote_count":"2"},{"upvote_count":"7","comment_id":"88006","content":"Up to 30,720 GB, depending on the machine type. This looks like correct choice.\nhttps://cloud.google.com/sql/docs/quotas#fixed-limits","timestamp":"1589327580.0","comments":[{"comment_id":"1267864","upvote_count":"1","content":"65 TB now in Aug 2024","comments":[{"timestamp":"1723935600.0","poster":"Satishjuly18","comment_id":"1267865","upvote_count":"1","content":"*64 TB"}],"poster":"Satishjuly18","timestamp":"1723935540.0"},{"poster":"odacir","comment_id":"739185","upvote_count":"3","timestamp":"1670511240.0","content":"https://cloud.google.com/sql/docs/quotas#storage_limits\n64TB"}],"poster":"vindahake"},{"comments":[{"content":"https://cloud.google.com/sql/docs/quotas#storage_limits","poster":"[Removed]","comment_id":"518271","upvote_count":"1","timestamp":"1641477660.0"}],"comment_id":"452808","poster":"dagoat","timestamp":"1632780900.0","upvote_count":"14","content":"65 TB now in Sept 2021"}],"content":"A. Cloud SQL (30TB)","comment_id":"68975","poster":"jvg637","timestamp":"1585426680.0"},{"timestamp":"1594124400.0","comment_id":"128920","upvote_count":"6","poster":"Rajuuu","content":"A as limit is now 30 TB for Cloud SQL"},{"poster":"SamuelTsch","comment_id":"1303871","upvote_count":"1","content":"Selected Answer: A\nCloud SQL storage limit: dedicated core up to 64 TB.","timestamp":"1730102100.0"},{"timestamp":"1698240720.0","content":"Selected Answer: C\ntwo keywords: Transactional data, 20 TB","poster":"drpay","comment_id":"1053758","upvote_count":"2"},{"upvote_count":"2","timestamp":"1695565560.0","comment_id":"1015892","poster":"barnac1es","content":"Selected Answer: C\nScalability: Cloud Spanner is designed to handle large volumes of data, making it suitable for a 20 TB database. It can scale horizontally and vertically to accommodate growing data needs.\n\nGlobal Distribution: Cloud Spanner allows you to distribute data globally for low-latency access across regions, which can be advantageous for operational systems.\n\nStrong Consistency: It provides strong transactional consistency, which is important for operational systems that require ACID compliance.\n\nSQL Support: Cloud Spanner supports SQL, which is a familiar query language for developers.\n\nWhile Cloud SQL, Cloud Bigtable, and Cloud Datastore have their use cases, Cloud Spanner is better suited for larger databases with strong consistency requirements, making it a suitable choice for migrating a 20 TB operational system database to GCP."},{"poster":"ashu381","upvote_count":"1","comment_id":"1007822","content":"Cloud SQL, upto 64 TB now, you can always call GCP for increasing the quota though !!","timestamp":"1694711220.0"},{"poster":"vaga1","upvote_count":"2","content":"Selected Answer: A\nCloud SQL is generally better for OLTP, and Cloud SQL is up to 64 TB now.\nhttps://cloud.google.com/sql/docs/quotas#storage_limits","comment_id":"928383","timestamp":"1687259760.0"},{"upvote_count":"2","poster":"vaga1","timestamp":"1683888180.0","comment_id":"895840","content":"\"move its operational system transaction data from an on-premises database to GCP\". Cloud SQL may be plug-and-play"},{"comment_id":"812980","upvote_count":"1","timestamp":"1676725680.0","poster":"musumusu","content":"Not 100% in favour of A, Should i recommend my client Cloud SQL, when they are coming to me with 20TB already 30TB is limit, its transactional data, which i can't compromise. I will propose cloud spanner. There is nothing mentioned that they want to save cost."},{"timestamp":"1672511400.0","upvote_count":"1","comment_id":"762826","content":"A. Cloud SQL","poster":"AzureDP900"},{"content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/sql/docs/features#features\nUp to 64 TB of storage available, with the ability to automatically increase storage size as needed.","timestamp":"1669816020.0","comment_id":"731541","upvote_count":"5","poster":"zellck"},{"content":"Selected Answer: A\nWith the given requirements A. Cloud SQL is more than sufficient. Don't try to overthink scenarios like what if it grows.. what if there's additional requirement in future.. what if this what if that.. just look at the question and see the stated requirement. If there are more than one answer try to see which is simple and doesn't come with extra frills.","timestamp":"1668944280.0","poster":"Jay_Krish","comment_id":"722573","upvote_count":"3"},{"content":"A\n65 TB now in Nov 2022","comment_id":"717775","timestamp":"1668412320.0","poster":"Atnafu","upvote_count":"2"},{"timestamp":"1658944140.0","poster":"WZH","upvote_count":"1","content":"it is already 20 TB at the moment, and you probably want to change the database because the capacity of your current storage solution is not enough. Then you decide to change it to Cloud SQL(up to 30 TB) which may not increase much capacity? I am not sure about the answer but A looks weird imho.","comment_id":"638261"},{"poster":"Dan226","upvote_count":"1","content":"Cloud SQL can store 64 Tb, but in the intial set up the operation are 20tb. It will reach the limitation soon if you choose Cloud SQL","comment_id":"633513","timestamp":"1658229840.0"},{"comment_id":"467075","content":"Depends.. I mean, C is correct if the exam is not updated. A is correct if the exam is updated. So ... kinda in catch 22 situation ...","poster":"gcp_k","upvote_count":"4","timestamp":"1635098400.0"},{"upvote_count":"2","poster":"KokkiKumar","comment_id":"464334","content":"Hi everyone, Can i purchase this exam? is it worthable?","timestamp":"1634599260.0"},{"poster":"Alasmindas","timestamp":"1605328980.0","content":"Option A - Cloud SQL is the correct answer. Cloud SQL can store upto 30 TB.\nhttps://cloud.google.com/sql/docs/quotas#:~:text=Cloud%20SQL%20storage%20limits&text=Up%20to%2030%2C720%20GB%2C%20depending,for%20PostgreSQL%20or%20SQL%20Server.","upvote_count":"4","comment_id":"218920"},{"poster":"SureshKotla","content":"SQL is sufficient","timestamp":"1601010120.0","comment_id":"186754","upvote_count":"2"},{"comment_id":"161227","poster":"Ravivarma4786","content":"Ans: A Cloud SQL limit increased to 30 TB","upvote_count":"3","timestamp":"1597810260.0"},{"content":"A Cloud SQL is correct as the requirement is to kinda lift n shift the db from on prem to cloud","timestamp":"1596382620.0","comment_id":"149223","upvote_count":"4","poster":"Prakzz"},{"poster":"arnabbis4u","comments":[{"timestamp":"1589766540.0","upvote_count":"8","poster":"arnabbis4u","content":"A - After Further Study. Cloud SQL can now Scale upto 30 TB.","comment_id":"90936"}],"timestamp":"1588981740.0","content":"Answer D : Datastore is a transactional database and can easily handle 20 tb. Supports ACID transactions.","comment_id":"85868","upvote_count":"3"},{"timestamp":"1585887780.0","upvote_count":"2","comments":[{"content":"Limit for Cloud SQL changed from 10 to 30 TB. So answer A","comment_id":"132117","timestamp":"1594474440.0","upvote_count":"9","poster":"Rajokkiyam"}],"content":"Should be C.","comment_id":"70617","poster":"Rajokkiyam"},{"content":"should be C.\nCloud SQL — Structured data, less than 10TB, less than 4000 concurrent connections, vertical scalability.\nCloud Spanner — Structured data, greater than 10TB, global horizontal scalability.\nCloud Firestore — Severless, NoSQL, less than 1TB, Real time analysis, zero down scaling.\nCloud Bigtable — NoSQL, Big data analytics, Petabytes upon petabytes, HBase integration, up scaling.","comments":[{"poster":"elenamatay","comment_id":"288785","timestamp":"1613122500.0","upvote_count":"3","content":"Cloud SQL less than 10 TB? It's for up to 30 TB of data storage (https://cloud.google.com/sql/docs/features).","comments":[{"poster":"MarcoDipa","content":"and the limit now are 64TB (now is Sept'21)","comment_id":"500551","upvote_count":"1","timestamp":"1639394160.0"}]}],"upvote_count":"5","poster":"digvijay","timestamp":"1585016580.0","comment_id":"67424"},{"timestamp":"1584859380.0","comment_id":"66833","upvote_count":"1","poster":"[Removed]","content":"Should be C"}],"unix_timestamp":1584859380,"question_images":[],"answer_description":"","answer_images":[],"question_id":69,"answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/17214-exam-professional-data-engineer-topic-1-question-160/","question_text":"You work for a mid-sized enterprise that needs to move its operational system transaction data from an on-premises database to GCP. The database is about 20\nTB in size. Which database should you choose?","timestamp":"2020-03-22 07:43:00","answers_community":["A (73%)","C (27%)"],"choices":{"A":"Cloud SQL","C":"Cloud Spanner","D":"Cloud Datastore","B":"Cloud Bigtable"},"topic":"1"},{"id":"Ezxd3ljyYuOKDALIVzVV","answer_ET":"C","question_text":"You need to choose a database to store time series CPU and memory usage for millions of computers. You need to store this data in one-second interval samples. Analysts will be performing real-time, ad hoc analytics against the database. You want to avoid being charged for every query executed and ensure that the schema design will allow for future growth of the dataset. Which database and data model should you choose?","exam_id":10,"timestamp":"2020-03-15 19:41:00","question_id":70,"url":"https://www.examtopics.com/discussions/google/view/16689-exam-professional-data-engineer-topic-1-question-161/","answers_community":["C (77%)","D (23%)"],"answer_images":[],"isMC":true,"answer":"C","topic":"1","discussion":[{"content":"Answer C\n\nA tall and narrow table has a small number of events per row, which could be just one event, whereas a short and wide table has a large number of events per row. As explained in a moment, tall and narrow tables are best suited for time-series data.\n\nFor time series, you should generally use tall and narrow tables. This is for two reasons: Storing one event per row makes it easier to run queries against your data. Storing many events per row makes it more likely that the total row size will exceed the recommended maximum (see Rows can be big but are not infinite).\n\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series#patterns_for_row_key_design","upvote_count":"35","timestamp":"1588258680.0","comments":[{"comment_id":"762827","content":"C. Create a narrow table in Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second","timestamp":"1672511700.0","poster":"AzureDP900","upvote_count":"1"},{"comment_id":"612213","upvote_count":"1","timestamp":"1654497120.0","poster":"nadavw","content":"there is a limit of 60 columns per row according to question. in addition in D the cost will be a lower which is a requirement. so D seems more suitable."}],"comment_id":"81750","poster":"psu"},{"timestamp":"1584297660.0","poster":"madhu1171","upvote_count":"19","comment_id":"64416","content":"C correct answer"},{"timestamp":"1734601260.0","upvote_count":"1","comment_id":"1328923","content":"Selected Answer: C\nEven though I am sure the answer is C, but I am not sure why it will help avoid being charged for every query?","poster":"shangning007"},{"content":"Selected Answer: C\njust like psu said. Additionally for a time-series data, it it usually the best practice to combine the identifier + time, not the values.","comment_id":"1303873","timestamp":"1730102520.0","poster":"SamuelTsch","upvote_count":"1"},{"upvote_count":"1","timestamp":"1709570400.0","content":"Selected Answer: C\nOption C is correct answer. Narrow table is good for time series data.","comment_id":"1165807","poster":"mothkuri"},{"timestamp":"1695603060.0","poster":"barnac1es","content":"Selected Answer: C\nScalability: Bigtable can handle large-scale data efficiently, making it suitable for storing time series data for millions of computers.\n\nLow Latency: Bigtable provides low-latency access to data, which is crucial for real-time analytics.\n\nFlexible Schema: The narrow table design allows you to efficiently store and query time series data without specifying all possible columns in advance, providing flexibility for future growth.\n\nColumn Families: Bigtable supports column families, allowing you to organize data logically.\n\nRow Key Design: Combining the computer identifier with the sample time at each second in the row key allows for efficient retrieval of data for specific computers and time intervals.\n\nAnalytics: While Bigtable does not support SQL directly, it allows for efficient data retrieval and can be integrated with other tools for analytics.","upvote_count":"1","comment_id":"1016260"},{"content":"Selected Answer: D\n\"..and ensure that the schema design will allow for future growth of the dataset\":\n\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series#time-buckets\n\n\"Data stored in this way is compressed more efficiently than data in tall, narrow tables.\"\n\nI read the \"future growth\" as a sign to be effective in storage, and go for the Time-Buckets.","timestamp":"1686201780.0","poster":"WillemHendr","upvote_count":"3","comment_id":"917830"},{"poster":"zellck","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series","comment_id":"731538","upvote_count":"2","timestamp":"1669815780.0"},{"comment_id":"664918","timestamp":"1662752940.0","content":"Selected Answer: C\ntime series = narrow table","poster":"Remi2021","upvote_count":"2"},{"timestamp":"1650431640.0","comment_id":"588446","poster":"_8008_","content":"What about \"avoid being charged for every query executed\"? Nothing on this topic in here https://cloud.google.com/bigtable/docs/schema-design-time-series can anyone comment?","upvote_count":"3"},{"comment_id":"520146","timestamp":"1641726900.0","content":"Selected Answer: C\nNarrow and tall table for a single event and good for time-series data\nShort and Wide table for data over a month, multiple events","upvote_count":"2","poster":"medeis_jar"},{"upvote_count":"2","comment_id":"486450","timestamp":"1637816940.0","poster":"JG123","content":"Correct: C"},{"poster":"squishy_fishy","comment_id":"462828","upvote_count":"3","content":"Answer is C.\nBigtable is best suited to the following scenarios: time-series data (e.g. CPU and memory usage over time for multiple servers), financial data (e.g. transaction histories, stock prices, and currency exchange rates), and IoT (Internet of Things) use cases.\nhttps://www.xplenty.com/blog/bigtable-vs-bigquery/","timestamp":"1634338440.0"},{"upvote_count":"5","comment_id":"423557","timestamp":"1628765820.0","poster":"safiyu","content":"C is the correct answer. If you consider wide table, then 60 columns for cpu usage and 60 columns for memory usage. in future, if you need to add a new kpi to the table, then the schema changes. you will have to add 60 more columns for the new feature. this is not so future proof.. so D is out of the picture."},{"comment_id":"418172","content":"BQ is optimized for large-scale, ad-hoc SQL-based analysis. i Think it should be A","poster":"DeepakS227","upvote_count":"2","timestamp":"1627806240.0"},{"upvote_count":"1","content":"First C & D won't cause hotspoting as computer_identifier is first part of row key\nI prefer D because \"ensure that the schema design will allow for future growth of the dataset.\"\nC is too tall and narrow I cannot see schema design grow in the future","poster":"koupayio","timestamp":"1623195840.0","comment_id":"377857"},{"poster":"crslake","comment_id":"364303","upvote_count":"1","timestamp":"1621758960.0","content":"D, Better overall, harder to implement (but that is not stated as a constraint)\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series#time-buckets","comments":[{"content":"How do you store both CPU and memory usage though? Two sets of 60 columns per row? I am wondering if that goes along with \"the schema design will allow for future growth\"...what if by future growth they mean monitoring N more metrics. That would imply N*60 columns, right?","poster":"lollo1234","upvote_count":"2","comment_id":"375375","timestamp":"1622916360.0"}]},{"comment_id":"325313","poster":"Sumanth09","timestamp":"1617218100.0","content":"Should be A\nquestion did not talk about latency\nwithout query cost -- BigQuery Cache \nflexible schema - BigQuery (nested and repeated)","upvote_count":"8"},{"comment_id":"239558","comments":[{"content":"I will go for C","comment_id":"239559","poster":"VM_GCP","timestamp":"1607553840.0","upvote_count":"2"}],"content":"BigQuery cannot be the answer as\n[1] This will not save the price by caching as query will not always be same\nhttps://cloud.google.com/bigquery/docs/cached-results#limitations\n[2] And the BigTable is always the correct solution for this use case as given in \nhttps://cloud.google.com/bigtable/docs/overview#what-its-good-for\n\nBigTable has native time series support and is preferred for large analytical and operational work\nAnswer can be C or D.\nBut problem is both the options will cause hot spotting. So not sure which to select","poster":"VM_GCP","timestamp":"1607553780.0","upvote_count":"2"},{"comment_id":"149227","poster":"Prakzz","comments":[{"upvote_count":"1","content":"No, it won't, because preceding timestamp with identifier values helps in a tradeoff between randomised row keys and manifesting grouping of similar entities.","timestamp":"1702537980.0","comment_id":"1096135","poster":"Aman47"}],"content":"both C and D row key will cause hotspotting","upvote_count":"3","timestamp":"1596382800.0"},{"comment_id":"129777","upvote_count":"4","timestamp":"1594214040.0","content":"I believe A. Use case requires support for Adhoc queries, and avoiding duplicate query costs.","poster":"dg63"},{"poster":"Callumr","content":"A - BigQuery caching will reduce query costs, and low latency of big table is not needed","comment_id":"115609","upvote_count":"3","timestamp":"1592746980.0"},{"upvote_count":"4","poster":"arnabbis4u","comment_id":"90940","content":"C look Fine.. Only doubt I have is it says they don't want to pay for every query execution. That requires the Bigquery caching feature. Confused. Anyone wants to comment on this?","timestamp":"1589766960.0","comments":[{"poster":"arnabbis4u","timestamp":"1589767800.0","content":"I think A.","comment_id":"90947","comments":[{"poster":"FARR","timestamp":"1597382100.0","upvote_count":"4","content":"bigquery pricing is query based and query-caching is helpful when queries exactly the same\nwhereas bigtable prcing is mostly infra-based\nhence, answer should be C","comment_id":"157817"}],"upvote_count":"4"}]},{"timestamp":"1585155360.0","comment_id":"68177","upvote_count":"7","poster":"[Removed]","content":"Correct: C\n\nNarrow and tall table for single event and good for timeseries data\nShort and Wide table for data over a month, multiple events"},{"upvote_count":"3","content":"Should be C","poster":"[Removed]","comment_id":"66824","timestamp":"1584858060.0"},{"timestamp":"1584493380.0","upvote_count":"6","content":"should be C:\n\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series","poster":"rickywck","comment_id":"65409"}],"question_images":[],"answer_description":"","choices":{"D":"Create a wide table in Bigtable with a row key that combines the computer identifier with the sample time at each minute, and combine the values for each second as column data.","A":"Create a table in BigQuery, and append the new samples for CPU and memory to the table","B":"Create a wide table in BigQuery, create a column for the sample value at each second, and update the row with the interval for each second","C":"Create a narrow table in Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second"},"unix_timestamp":1584297660}],"exam":{"isBeta":false,"numberOfQuestions":319,"lastUpdated":"15 Feb 2025","id":10,"name":"Professional Data Engineer","isImplemented":true,"provider":"Google","isMCOnly":true},"currentPage":14},"__N_SSP":true}