{"pageProps":{"questions":[{"id":"wGcQizPXghNGAjXnpOMO","answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/79775-exam-professional-data-engineer-topic-1-question-103/","isMC":true,"discussion":[{"content":"Answer is B. Timetravel only covers for 7 days and a scheduled query is needed for creating Table snapshots for 30 days. Also table snapshot must remain in the same region as base table (please refer to limitation of table snapshot from below link) https://cloud.google.com/bigquery/docs/table-snapshots-intro","upvote_count":"9","poster":"DeepakVenkatachalam","comment_id":"1008711","timestamp":"1694813460.0"},{"content":"Answer is C: https://cloud.google.com/bigquery/docs/table-snapshots-intro\n\"Benefits of using table snapshots include the following:\n\nKeep a record for longer than seven days. With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\n\nMinimize storage cost. BigQuery only stores bytes that are different between a snapshot and its base table, so a table snapshot typically uses less storage than a full copy of the table.\" \nBut the wording is foolish... It's table snapshot, NOT point in time snapshot!\n\nhttps://cloud.google.com/bigquery/docs/time-travel#restore-a-table\nthis is point in time using time travel window - max is 7 days...","comment_id":"781578","poster":"desertlotus1211","timestamp":"1674159960.0","upvote_count":"5"},{"timestamp":"1737726060.0","upvote_count":"1","poster":"loki82","comment_id":"1346139","content":"Selected Answer: A\nRPO is NOT the same as a PITR window. Point-in-time recovery (PITR) is a process that allows users to restore data or settings from a previous point in time. A recovery point objective (RPO) is the maximum amount of data loss that an organization can tolerate after a data loss event. So a PITR snapshot easily meets an RPO of 30 days. A regional bucket minimizes cost."},{"timestamp":"1735206060.0","poster":"hussain.sain","upvote_count":"1","content":"Selected Answer: C\nAnswer is C. As question is related to highly available so this rules out A and B.","comment_id":"1331855"},{"comment_id":"1327034","poster":"clouditis","content":"Selected Answer: C\nBecause this option uses Multiregional & BQ Snapshot, others are not right/cumbersome","timestamp":"1734292560.0","upvote_count":"1"},{"timestamp":"1733039280.0","content":"Selected Answer: A\nA is the right Answer","upvote_count":"1","comment_id":"1320481","poster":"cloud_rider"},{"comment_id":"1307060","timestamp":"1730745060.0","poster":"Erg_de","upvote_count":"1","content":"Selected Answer: B\nBest choice, minimized cost"},{"comment_id":"1303454","upvote_count":"2","content":"Selected Answer: B\nMinimized Cost with Regional Storage: Regional datasets are less costly than multi-regional datasets in BigQuery. Since there is no requirement here for multi-regional availability, regional storage meets the high availability need while keeping costs lower.\n\nRPO Compliance with Scheduled Backups: A scheduled query that periodically creates copies of the data (e.g., daily or weekly, depending on the requirements) allows for recovery within the 30-day RPO, meeting the requirement for data retention and recovery.\n\nPoint-in-Time Recovery Not Native in BigQuery: Although BigQuery provides a limited \"table snapshot\" feature, it’s not a true point-in-time recovery option for the last 30 days. Creating periodic backups through scheduled queries gives you control over retention, enabling you to keep backups for 30 days and reducing dependency on more costly or limited snapshot capabilities.","poster":"Gcpteamprep","timestamp":"1729986900.0"},{"poster":"Vogangster","timestamp":"1728580560.0","content":"D.Create monthly snapshots of a table by using a service account that runs a scheduled query. Link: https://cloud.google.com/bigquery/docs/table-snapshots-scheduled","upvote_count":"1","comment_id":"1295681"},{"poster":"AlizCert","timestamp":"1717536180.0","content":"Selected Answer: D\nHA => multi-region\n30-days RPO => manual backups as max time-travel is 7 days","comment_id":"1224371","upvote_count":"3"},{"content":"This is in one of google's training practice questions and the answer for it is C.","upvote_count":"3","timestamp":"1716293040.0","comments":[{"upvote_count":"1","content":"Agreed. Multi-regional datasets offer higher availability by replicating data across multiple regions","poster":"NickNtaken","comment_id":"1218341","timestamp":"1716643080.0"}],"comment_id":"1214914","poster":"Lestrang"},{"poster":"rocky48","comment_id":"1089029","content":"Selected Answer: A\nou should consider option A.\n\nSetting the BigQuery dataset to be regional and using a point-in-time snapshot to recover the data in the event of an emergency can help you achieve the desired level of availability and minimize cost. This approach can help you avoid the additional cost of creating and maintaining backup copies of the data, which can be expensive.\n\nSetting the BigQuery dataset to be multi-regional (options C and D) can provide additional redundancy and availability. However, this approach can be more expensive than setting the dataset to be regional, especially if you do not require the additional level of redundancy.","timestamp":"1701842220.0","upvote_count":"4"},{"upvote_count":"4","comment_id":"1026372","comments":[{"poster":"ffggrre","timestamp":"1698166800.0","content":"typically Multi-region cost is equal or less than a region. https://cloud.google.com/bigquery/pricing#storage","comment_id":"1053052","upvote_count":"1"}],"poster":"Nirca","content":"Selected Answer: A\nI'm going for A: \n1. Set the BigQuery dataset to be regional. This will reduce the cost of storage compared to a multi-regional dataset.\n2. building Snapshot: bq snapshot --dataset <dataset_id> --table <table_id> <snapshot_id>","timestamp":"1696581780.0"},{"upvote_count":"4","comment_id":"1011986","poster":"ckanaar","content":"I think the answer is A: \n\nThis option meets the 30-day RPO requirement, assuming that the snapshot is maintained for that long. It offers high availability as data is written synchronously to 2 zones within a region: https://cloud.google.com/blog/topics/developers-practitioners/backup-disaster-recovery-strategies-bigquery/. The cost would be lower than maintaining a multi-regional dataset, but you'll incur the cost of the snapshot.","timestamp":"1695192240.0"},{"content":"Why not B? Setting dataset regional or multi does not affect the backup and recovery strategy.","upvote_count":"3","timestamp":"1679017920.0","comment_id":"841503","poster":"lucaluca1982"},{"content":"Selected Answer: C\n1. HA -> Multi-region\n2. DR -> Snapshot","upvote_count":"4","poster":"midgoo","comment_id":"833692","timestamp":"1678348680.0"},{"timestamp":"1677291420.0","upvote_count":"2","poster":"kostol","comment_id":"821112","content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/table-snapshots-scheduled"},{"comments":[{"content":"How is using snapshots more expensive, snapshots store the delta of the state of the table during time of taking snapshot and the current state of the table (https://cloud.google.com/bigquery/docs/table-snapshots-intro#storage_costs)\n\nWhen a snapshot is created, there is literally zero cost... until changes are made to the base table, only then will the delta storage cost be charged.. This certainly seems cheaper than creating a new table","timestamp":"1671709500.0","comment_id":"753244","poster":"jkhong","upvote_count":"2"}],"comment_id":"752027","content":"One of the criteria is to minimize cost. While C is an ideal solution but more expensive compared to D. In my opinion the correct answer should be D.","upvote_count":"2","poster":"YatMoh","timestamp":"1671612660.0"},{"upvote_count":"3","timestamp":"1670978580.0","poster":"raghu06raj","comment_id":"744592","content":"Even when the dataset/table is regional, google provides high availability with data available in 2 zones. Isn't A correct answer?"},{"content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots\nA BigQuery table snapshot preserves the contents of a table (called the base table) at a particular time. You can save a snapshot of a current table, or create a snapshot of a table as it was at any time in the past seven days. A table snapshot can have an expiration; when the configured amount of time has passed since the table snapshot was created, BigQuery deletes the table snapshot. You can query a table snapshot as you would a standard table. Table snapshots are read-only, but you can create (restore) a standard table from a table snapshot, and then you can modify the restored table.","comment_id":"735915","poster":"zellck","upvote_count":"4","timestamp":"1670242980.0"},{"poster":"hauhau","comments":[{"comment_id":"747775","timestamp":"1671253980.0","upvote_count":"1","poster":"jkhong","content":"We can keep PIT snapshots for as long as we want. I think you confused snapshots with time travel... \n\n\"With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\"\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots"},{"content":"https://cloud.google.com/bigquery/docs/time-travel#configure_the_time_travel_window\n'You can set the duration of the time travel window, from a minimum of two days to a maximum of seven days'\n\nNot the same as snapshot","comment_id":"781574","poster":"desertlotus1211","timestamp":"1674159480.0","upvote_count":"1"}],"upvote_count":"3","comment_id":"731604","content":"Selected Answer: D\npoint in time snapshots only have data from past 7 days","timestamp":"1669819080.0"},{"poster":"Atnafu","timestamp":"1668211920.0","upvote_count":"2","comment_id":"716378","content":"C\nBenefits of using table snapshots include the following:\n\nKeep a record for longer than seven days. With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\n\nMinimize storage cost. BigQuery only stores bytes that are different between a snapshot and its base table, so a table snapshot typically uses less storage than a full copy of the table."},{"content":"Selected Answer: C\nC\nMulti-region for HA\nSnapshot for recovery","comment_id":"712612","upvote_count":"2","timestamp":"1667764080.0","poster":"cloudmon"},{"timestamp":"1663592040.0","poster":"max_c","content":"Selected Answer: C\nC for high availability and use of built-in feature\n\nPoint-in-time snapshots only have data from the past 7 days at their creation. They can be stored for as long as desired. https://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots","comment_id":"673301","upvote_count":"4"},{"content":"Selected Answer: C\nC and D are high avaiability\nBut C use buid-in feature in BQ \nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots\n\nWhile D it is quite put too much effeot and not minimize cost","comment_id":"669133","poster":"John_Pongthorn","timestamp":"1663168740.0","upvote_count":"1"},{"poster":"MounicaN","comment_id":"666245","upvote_count":"2","content":"Selected Answer: D\npoint in time snapshots only have data from past 7 days","comments":[{"timestamp":"1664840400.0","poster":"devaid","upvote_count":"2","comment_id":"685828","content":"Incorrect, that's for Big Query's time travel. You can create a snapshot of a current table, or of a table like it was at 7 days ago. With a snapshot, you can preserve a table's data from a specified point in time for as long as you want. Source: https://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots"},{"timestamp":"1667223960.0","content":"30 days RPO means 30 days of data is good for recovery. but 7 days of data is even better.\nRPO means the maximum age of an acceptable backup in case of a disaster recovery.\nAm I wrong ?","upvote_count":"2","poster":"MisuLava","comment_id":"708447"}],"timestamp":"1662910920.0"},{"upvote_count":"3","poster":"bigquery1102","comments":[{"upvote_count":"1","poster":"jkhong","comment_id":"747776","content":"We can keep PIT snapshots for as long as we want. I think you confused snapshots with time travel...\n\n\"With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\"\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots","timestamp":"1671254040.0"}],"timestamp":"1662509400.0","comment_id":"661770","content":"Should be D while point-in-time query only access data fron recent 7 days"},{"comment_id":"658411","upvote_count":"2","poster":"AWSandeep","timestamp":"1662206640.0","content":"Selected Answer: C\nC. Set the BigQuery dataset to be multi-regional. In the event of an emergency, use a point-in-time snapshot to recover the data."}],"exam_id":10,"question_text":"You have a data stored in BigQuery. The data in the BigQuery dataset must be highly available. You need to define a storage, backup, and recovery strategy of this data that minimizes cost. How should you configure the BigQuery table that have a recovery point objective (RPO) of 30 days?","answers_community":["C (45%)","A (24%)","D (24%)","7%"],"question_images":[],"answer_description":"","choices":{"C":"Set the BigQuery dataset to be multi-regional. In the event of an emergency, use a point-in-time snapshot to recover the data.","A":"Set the BigQuery dataset to be regional. In the event of an emergency, use a point-in-time snapshot to recover the data.","D":"Set the BigQuery dataset to be multi-regional. Create a scheduled query to make copies of the data to tables suffixed with the time of the backup. In the event of an emergency, use the backup copy of the table.","B":"Set the BigQuery dataset to be regional. Create a scheduled query to make copies of the data to tables suffixed with the time of the backup. In the event of an emergency, use the backup copy of the table."},"unix_timestamp":1662206640,"answer_images":[],"answer":"C","timestamp":"2022-09-03 14:04:00","question_id":6,"topic":"1"},{"id":"EksDpLIQcpmxDNekCXDI","discussion":[{"timestamp":"1687144440.0","content":"I'd pick D because it's the only option which allows variable execution (since we need to execute the dataprep job only after the prior load job). Although D suggests the export of Dataflow templates, this discussion suggests that the export option is no longer available (https://stackoverflow.com/questions/72544839/how-to-get-the-dataflow-template-of-a-dataprep-job), there are already Airflow Operators for Dataprep which we should be using instead - https://airflow.apache.org/docs/apache-airflow-providers-google/stable/operators/cloud/dataprep.html","comment_id":"749481","upvote_count":"12","poster":"jkhong"},{"upvote_count":"9","content":"Selected Answer: D\nSince the load job execution time is unexpected, schedule the Dataprep based on a fixed time window may not work.\nWhen the Dataprep job run the first time, we can find the Dataflow job for that in the console. We can use that to create the Template --> With the help of the Composer to determine if the load job is completed, we can then trigger the Dataflow job","timestamp":"1694240400.0","poster":"midgoo","comment_id":"833707"},{"content":"Selected Answer: A\nDataprep by Trifacta allows you to schedule the execution of recipes. You can set up a cron schedule directly within Dataprep to automatically run your recipe at specified intervals, such as daily.\nWHY NOT D ? : This option involves significant additional complexity. Exporting the Dataprep job as a Dataflow template and then incorporating it into a Composer (Apache Airflow) job is a more complicated process and is typically used for more complex orchestration needs that go beyond simple scheduling.","upvote_count":"2","timestamp":"1718971620.0","poster":"TVH_Data_Engineer","comment_id":"1102590"},{"poster":"MaxNRG","content":"Selected Answer: D\nWe have external dependency \"after the load job with variable execution time completes\"\nwhich requires DAG -> Airflow (Cloud Composer)\n\nThe reasons:\n\nA scheduler like Cloud Scheduler won't handle the dependency on the BigQuery load completion time\nUsing Composer allows creating a DAG workflow that can:\nTrigger the BigQuery load\nWait for BigQuery load to complete\nTrigger the Dataprep Dataflow job\nDataflow template allows easy reuse of the Dataprep transformation logic\nComposer coordinates everything based on the dependencies in an automated workflow","comment_id":"1098818","upvote_count":"1","timestamp":"1718614620.0"},{"upvote_count":"1","comment_id":"1090732","poster":"rocky48","content":"Selected Answer: D\nI'd pick D because it's the only option which allows variable execution","timestamp":"1717808040.0"},{"poster":"gaurav0480","comment_id":"994724","upvote_count":"3","timestamp":"1709185920.0","content":"The key here is \"after the load job with variable execution time completes\" which means the execution of this job depends on the completion of another job which has a variable execution time. Hence D"},{"timestamp":"1708690020.0","comment_id":"988172","content":"This approach ensures the dynamic triggering of the Dataprep job based on the completion of the preceding load job, ensuring data is processed accurately and in sequen","upvote_count":"1","poster":"god_brainer"},{"content":"Selected Answer: A\nA is correct. D is too complicated. \n\nA is correct, because you can schedule a job right from Dataprep UI.\n\nhttps://cloud.google.com/blog/products/gcp/scheduling-and-sampling-arrive-for-google-cloud-dataprep\nScheduling and sampling arrive for Google Cloud Dataprep\nThroughout our early releases, users’ most common request has been Flow scheduling. As of Thursday’s release, Flows can be scheduled with minute granularity at any frequency.","timestamp":"1697415480.0","poster":"Adswerve","upvote_count":"2","comment_id":"871387"},{"timestamp":"1695712380.0","poster":"lucaluca1982","comment_id":"850793","content":"Selected Answer: C\nI think C it is more straighforward","upvote_count":"4"},{"content":"Answer C: Use Recipe Template feature of dataprep. Don't need to change the service.","upvote_count":"3","poster":"musumusu","timestamp":"1692881580.0","comment_id":"820630"},{"upvote_count":"1","comment_id":"772438","content":"Selected Answer: C\nWhy not C?","timestamp":"1689068100.0","poster":"jroig_"},{"upvote_count":"1","poster":"zellck","timestamp":"1686129420.0","comment_id":"737768","content":"Selected Answer: D\nD is the answer."},{"upvote_count":"4","content":"Selected Answer: A\nIt's A. You can set it directly in Dataprep a job and it will use Dataflow under the hood.","poster":"anicloudgirl","comment_id":"736914","timestamp":"1686055860.0"},{"upvote_count":"2","content":"It's A. You can set it directly in Dataprep a job and it will use Dataflow under the hood. No need to export nor incorporate into a Composer job.\nDataprep by trifacta - https://docs.trifacta.com/display/DP/cron+Schedule+Syntax+Reference\nDataprep job uses dataflow - https://cloud.google.com/dataprep","comments":[{"upvote_count":"4","poster":"jkhong","comment_id":"749473","timestamp":"1687143600.0","content":"The question mentions after a load job with variable time, i dont think setting a dataprep cron job can address the issue of variable load times"}],"comment_id":"736912","poster":"anicloudgirl","timestamp":"1686055800.0"},{"comment_id":"712613","timestamp":"1683395400.0","upvote_count":"2","poster":"cloudmon","content":"Selected Answer: D\nIt's D"},{"content":"Selected Answer: D\nDataprep and Dataflow are same famitly","upvote_count":"2","poster":"John_Pongthorn","comment_id":"669134","timestamp":"1678814460.0"},{"poster":"AWSandeep","upvote_count":"4","comment_id":"658412","timestamp":"1677852300.0","content":"Selected Answer: D\nD. Export the Dataprep job as a Dataflow template, and incorporate it into a Composer job.\nReveal Solution"},{"poster":"damaldon","timestamp":"1677750000.0","upvote_count":"2","content":"It’s D, use composer to schedule tasks","comment_id":"657128"}],"choices":{"C":"Export the recipe as a Dataprep template, and create a job in Cloud Scheduler.","B":"Create an App Engine cron job to schedule the execution of the Dataprep job.","D":"Export the Dataprep job as a Dataflow template, and incorporate it into a Composer job.","A":"Create a cron schedule in Dataprep."},"answer_ET":"D","timestamp":"2022-09-02 09:40:00","topic":"1","unix_timestamp":1662104400,"exam_id":10,"answers_community":["D (61%)","A (24%)","C (15%)"],"answer":"D","question_images":[],"answer_description":"","question_id":7,"answer_images":[],"isMC":true,"question_text":"You used Dataprep to create a recipe on a sample of data in a BigQuery table. You want to reuse this recipe on a daily upload of data with the same schema, after the load job with variable execution time completes. What should you do?","url":"https://www.examtopics.com/discussions/google/view/79318-exam-professional-data-engineer-topic-1-question-104/"},{"id":"dWa02NeZ9Ykdm3KB54E3","discussion":[{"poster":"Sofiia98","timestamp":"1720354200.0","upvote_count":"1","comment_id":"1115892","content":"Selected Answer: B\nOf course, it is Cloud Composer!"},{"content":"Selected Answer: B\nB. Cloud Composer is the right answer !","comment_id":"1026373","timestamp":"1712393220.0","poster":"Nirca","upvote_count":"1"},{"timestamp":"1706351760.0","upvote_count":"1","content":"Selected Answer: B\nCloud Composer is a managed service that allows you to create and run Apache Airflow workflows. Airflow is a workflow management platform that can be used to automate complex data pipelines. It is a good choice for this use case because it is a managed service, which means that Google will take care of the underlying infrastructure. It also supports multiple dependencies, so you can easily schedule a multi-step pipeline","comment_id":"964504","poster":"vamgcp"},{"comment_id":"880272","timestamp":"1698232560.0","poster":"vaga1","upvote_count":"2","content":"Selected Answer: B\nAirflow is the only choiche to handle dependencies and being able to call all of the services included in the question"},{"upvote_count":"2","poster":"niketd","timestamp":"1692687720.0","content":"Selected Answer: B\nMulti-step sequential pipelines -> Cloud Composer","comment_id":"817638"},{"poster":"AzureDP900","content":"Cloud composer B is right","timestamp":"1688146500.0","upvote_count":"2","comment_id":"762264"},{"poster":"odacir","timestamp":"1686151740.0","comment_id":"738170","content":"Selected Answer: B\nCloud Composer (Airflow) is the answer to chain different steps from different apps...","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: B\n\" multiple dependencies on each other. You want to use managed service\"\n = Cloud Composer","comment_id":"704290","poster":"MisuLava","timestamp":"1682470980.0"},{"timestamp":"1678814940.0","comment_id":"669141","upvote_count":"2","content":"Selected Answer: B\nif you want your wf to schedule there are 3 ways to perform it, it of them is composer\nhttps://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions","poster":"John_Pongthorn"},{"comment_id":"662837","poster":"Remi2021","content":"Selected Answer: B\ncomposer :)","upvote_count":"1","timestamp":"1678225260.0"},{"comment_id":"659814","timestamp":"1678006080.0","poster":"YorelNation","content":"Selected Answer: B\nComposer","upvote_count":"1"},{"content":"Selected Answer: B\nB. Cloud Composer","timestamp":"1677852300.0","upvote_count":"1","poster":"AWSandeep","comment_id":"658414"},{"timestamp":"1677750060.0","content":"Use composer to schedule tasks","poster":"damaldon","upvote_count":"1","comment_id":"657129"}],"answer_images":[],"question_text":"You want to automate execution of a multi-step data pipeline running on Google Cloud. The pipeline includes Dataproc and Dataflow jobs that have multiple dependencies on each other. You want to use managed services where possible, and the pipeline will run every day. Which tool should you use?","choices":{"C":"Cloud Scheduler","A":"cron","B":"Cloud Composer","D":"Workflow Templates on Dataproc"},"answer_description":"","exam_id":10,"timestamp":"2022-09-02 09:41:00","answer_ET":"B","isMC":true,"question_id":8,"url":"https://www.examtopics.com/discussions/google/view/79319-exam-professional-data-engineer-topic-1-question-105/","topic":"1","unix_timestamp":1662104460,"question_images":[],"answers_community":["B (100%)"],"answer":"B"},{"id":"LZmhpOwTLw4vnE0g8jpl","discussion":[{"comment_id":"762267","upvote_count":"6","poster":"AzureDP900","content":"D is right\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters#using_graceful_decommissioning","timestamp":"1703965080.0"},{"timestamp":"1733626920.0","upvote_count":"1","poster":"rocky48","content":"Selected Answer: D\nD is right\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters#using_graceful_decommissioning","comment_id":"1090735"},{"upvote_count":"1","content":"Selected Answer: A\nShould be A. You can configure the preemptible worker to gracefull decommission, its for non preemptible worker nodes.","timestamp":"1703235960.0","comment_id":"753112","comments":[{"comment_id":"763869","poster":"wan2three","content":"nope, they are not only for non-preeemtible workers","upvote_count":"1","timestamp":"1704215400.0"}],"poster":"Prakzz"},{"upvote_count":"2","poster":"yafsong","comment_id":"747003","content":"graceful decommissioning: to finish work in progress on a worker before it is removed from the Cloud Dataproc cluster.\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters","timestamp":"1702719000.0"},{"poster":"odacir","comment_id":"738169","content":"Selected Answer: D\nAll your workers need to be the same kind. Use Graceful Decommissioning for don't lose any data and add more(increase the cluster) preemptible workers because there are more cost-effective .","timestamp":"1701970020.0","upvote_count":"1"},{"comment_id":"716804","timestamp":"1699806780.0","poster":"skp57","upvote_count":"2","content":"A. \"graceful decommissioning\" is not a configuration value but a parameter passed with scale down action - to decrease the number of workers to save money (see Graceful Decommissioning as an option to use when downsizing a cluster to avoid losing work in progress)"},{"timestamp":"1694697360.0","poster":"John_Pongthorn","upvote_count":"3","comments":[{"timestamp":"1698656700.0","comments":[],"content":"This weird.\nThe question mentions that increase cluster, but Graceful Decommissioning use in downscale the cluster","upvote_count":"2","comment_id":"707719","poster":"hauhau"}],"comment_id":"669044","content":"Selected Answer: D\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters\nWhy scale a Dataproc cluster?\nto increase the number of workers to make a job run faster\nto decrease the number of workers to save money (see Graceful Decommissioning as an option to use when downsizing a cluster to avoid losing work in progress).\nto increase the number of nodes to expand available Hadoop Distributed Filesystem (HDFS) storage"},{"content":"Selected Answer: D\nD. Increase the cluster size with preemptible worker nodes, and configure them to use graceful decommissioning.","poster":"AWSandeep","timestamp":"1693742760.0","upvote_count":"1","comment_id":"658416"}],"answer_images":[],"question_text":"You are managing a Cloud Dataproc cluster. You need to make a job run faster while minimizing costs, without losing work in progress on your clusters. What should you do?","choices":{"A":"Increase the cluster size with more non-preemptible workers.","C":"Increase the cluster size with preemptible worker nodes, and use Cloud Stackdriver to trigger a script to preserve work.","B":"Increase the cluster size with preemptible worker nodes, and configure them to forcefully decommission.","D":"Increase the cluster size with preemptible worker nodes, and configure them to use graceful decommissioning."},"answer_description":"","exam_id":10,"answer_ET":"D","timestamp":"2022-09-03 14:06:00","isMC":true,"question_id":9,"topic":"1","unix_timestamp":1662206760,"url":"https://www.examtopics.com/discussions/google/view/79777-exam-professional-data-engineer-topic-1-question-106/","question_images":[],"answers_community":["D (86%)","14%"],"answer":"D"},{"id":"RQiPOdgUa5CA6aEKhxSS","exam_id":10,"discussion":[{"upvote_count":"5","content":"Selected Answer: D\nThe cloud function with DLP seems the best option","comment_id":"758867","timestamp":"1703699280.0","poster":"dconesoko"},{"comment_id":"962368","poster":"PhilipKoku","upvote_count":"1","content":"Selected Answer: D\nDLP is required","timestamp":"1721885280.0"},{"timestamp":"1694195100.0","content":"Selected Answer: D\nD option","comment_id":"663864","poster":"HarshKothari21","upvote_count":"1"},{"comment_id":"658417","poster":"AWSandeep","upvote_count":"2","comments":[{"comment_id":"762272","poster":"AzureDP900","content":"Agreed","timestamp":"1703965500.0","upvote_count":"1"}],"timestamp":"1693742820.0","content":"Selected Answer: D\nD. Build a Cloud Function that reads the topics and makes a call to the Cloud Data Loss Prevention (Cloud DLP) API. Use the tagging and confidence levels to either pass or quarantine the data in a bucket for review."}],"choices":{"B":"Install a third-party data validation tool on Compute Engine virtual machines to check the incoming data for sensitive information.","D":"Build a Cloud Function that reads the topics and makes a call to the Cloud Data Loss Prevention (Cloud DLP) API. Use the tagging and confidence levels to either pass or quarantine the data in a bucket for review.","A":"Create an authorized view in BigQuery to restrict access to tables with sensitive data.","C":"Use Cloud Logging to analyze the data passed through the total pipeline to identify transactions that may contain sensitive information."},"url":"https://www.examtopics.com/discussions/google/view/79778-exam-professional-data-engineer-topic-1-question-107/","answers_community":["D (100%)"],"answer_ET":"D","question_id":10,"answer_description":"","topic":"1","question_images":[],"timestamp":"2022-09-03 14:07:00","answer":"D","unix_timestamp":1662206820,"question_text":"You work for a shipping company that uses handheld scanners to read shipping labels. Your company has strict data privacy standards that require scanners to only transmit tracking numbers when events are sent to Kafka topics. A recent software update caused the scanners to accidentally transmit recipients' personally identifiable information (PII) to analytics systems, which violates user privacy rules. You want to quickly build a scalable solution using cloud-native managed services to prevent exposure of PII to the analytics systems. What should you do?","isMC":true,"answer_images":[]}],"exam":{"provider":"Google","id":10,"isMCOnly":true,"isBeta":false,"lastUpdated":"15 Feb 2025","isImplemented":true,"numberOfQuestions":319,"name":"Professional Data Engineer"},"currentPage":2},"__N_SSP":true}