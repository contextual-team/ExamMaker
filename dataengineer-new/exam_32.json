{"pageProps":{"questions":[{"id":"CDtEEmPNYL4VmCvq84fD","isMC":true,"answer_ET":"A","choices":{"A":"","C":"","D":"","B":""},"question_id":161,"exam_id":10,"question_text":"You are preparing data that your machine learning team will use to train a model using BigQueryML. They want to predict the price per square foot of real estate. The training data has a column for the price and a column for the number of square feet. Another feature column called ‘feature1’ contains null values due to missing data. You want to replace the nulls with zeros to keep more data points. Which query should you use?","answer_description":"","discussion":[{"upvote_count":"10","timestamp":"1710161400.0","comment_id":"1171020","poster":"52ed0e5","content":"Selected Answer: A\nOption A is the correct choice because it retains all the original columns and specifically addresses the issue of null values in ‘feature1’ by replacing them with zeros, without altering any other columns or performing unnecessary calculations. This makes the data ready for use in BigQueryML without losing any important information. \n\nOption C is not the best choice because it includes the EXCEPT clause for the price and square_feet columns, which would exclude these columns from the results. This is not desirable since you need these columns for the machine learning model to predict the price per square foot"},{"comment_id":"1124089","content":"Selected Answer: C\nCorrect answer is C. \nIt both replace NULL with 0 and pass price per square foot of real estate.","poster":"datapassionate","timestamp":"1705400940.0","comments":[{"content":"Option C isn't a good practice. What if any 0 value is contained in the column of squre_feet, then price / 0 will throw an exception. IF(IFNULL(squre_feet, 0) = 0, 0, price/squre_feet).","upvote_count":"6","comments":[{"comment_id":"1294127","upvote_count":"2","poster":"baimus","content":"I think the assumption here is that no houses are zero feet in size. If they are, that should be caught in preprocessing, which is outside the short scope of this question. If the answer isn't C, then it's A, which would mean the question is suggesting you need an ML model to calculate price per square for data where you already have both price and square feet as features. In that instance you clearly need to only divide one by the other. Those columns must be intended to be the target, or the whole question is nonsense.","timestamp":"1728287700.0"}],"comment_id":"1149009","timestamp":"1707813720.0","poster":"George_Zhu"}],"upvote_count":"6"},{"timestamp":"1737920940.0","comment_id":"1347084","upvote_count":"1","content":"Selected Answer: C\nNot worded well but the best answer I would think would be C since it has price per square foot but I understand the argument for A.","poster":"LP_PDE"},{"poster":"AWSandeep","upvote_count":"1","comment_id":"1330337","timestamp":"1734861480.0","content":"Selected Answer: C\nLet's step away from GCP for a minute. If price & square feet are already features, then there is no need for BigQuery ML to do any prediction. You'd want to predict price per square foot based on other fields like location, weather, etc. The first sentence in the question indicates that a machine learning team is preparing the data for model training. Therefore, C's query a fantastic preparation step. If this query for any other use case, then A would've been the answer."},{"poster":"Robbing_the_hood","comment_id":"1323931","content":"Selected Answer: C\nTo people saying you need price and square footage to predict price/sq. feet: you do not need an ML model then, you need a calculator. C is the correct answer because you want to predict price/sq. feet from the features EXCLUDING price and sq. footage.","upvote_count":"1","timestamp":"1733734440.0"},{"poster":"ToiToi","timestamp":"1730404500.0","comment_id":"1305573","content":"Selected Answer: C\nGemini told me C\n\nHere's why it's the best of the limited choices:\n\nCalculates price_per_sqft: It includes the calculation for the target variable your model needs.\nHandles Nulls: It uses IFNULL(feature1, 0) to replace nulls in feature1 with 0, similar to COALESCE.\nMost Comprehensive: While it excludes the original price, square_feet, and feature1 columns, it still retains any other columns that might be present in the training_data table.","upvote_count":"2","comments":[{"comment_id":"1320036","content":"C is wrong as it excludes price and square feet values, what will model use to train model?","poster":"cloud_rider","upvote_count":"1","timestamp":"1732937100.0","comments":[{"comment_id":"1323932","content":"The features? Why would you train an ML model if you have price and sq. feet available?","poster":"Robbing_the_hood","timestamp":"1733734500.0","upvote_count":"1"}]}]},{"content":"Selected Answer: C\nit should be C.","upvote_count":"1","poster":"SamuelTsch","comment_id":"1305110","timestamp":"1730312280.0"},{"poster":"baimus","comment_id":"1294124","upvote_count":"1","content":"Selected Answer: C\nThis must be C, though the wording isn't great. If price and square foot are included in the data, they are either intended to be the target, in which case you need to create that target as per C, or if they are genuinely features, you DO NOT need a machine learning model. If you already know price and square feet, price per square foot is just price/ft2. You don't need ML to predict that, it's just a division. The only context this makes sense in is if they mean \"price and square foot are the target, and feature1 is the predictive feature\", which means C is correct. The removing nulls from feature1 and the creation of price per square foot is C.","timestamp":"1728287520.0"},{"content":"Selected Answer: C\nFont Cloude 3.5 and GPT 4o, in theoy is better to keep the less amount of features, then price_per_sqft and feature1 cleaned is the best option","timestamp":"1720020000.0","poster":"47767f9","comment_id":"1241485","upvote_count":"1"},{"comment_id":"1204166","poster":"srinidutt","upvote_count":"1","content":"EXCEPT means it won't select that column.","timestamp":"1714415160.0"},{"comment_id":"1161813","poster":"demoro86","timestamp":"1709139060.0","content":"Selected Answer: A\nC is not a valid answer. You are introducing a redundant variable, that could be valid, but removing from the dataset 2 variables that exactly influence in the predictions you are trying to make.","upvote_count":"4"},{"poster":"baimus","upvote_count":"2","content":"Just to clarify, they don't \"influence\" the prediction, they are in fact the target. The model needs to predict price per square foot. If you have price, and square foot, they are either 1) the prediction target price/squarefoot, or if not then you absolutely do not need a machine learning model, you just device price by square foot.","comment_id":"1294123","timestamp":"1728287400.0"},{"timestamp":"1708870140.0","comment_id":"1158849","upvote_count":"1","content":"Selected Answer: C\nOption C not only handles the null values in feature1 by replacing them with zeros (using IFNULL(feature1, 0) as feature1_cleaned), but it also creates a new feature price_per_sqft by dividing the price by the number of square feet (price/square_feet as price_per_sqft). This new feature directly corresponds to what your team wants to predict (the price per square foot of real estate), and could therefore be very useful for the machine learning model.","poster":"PetrSz"},{"poster":"cuadradobertolinisebastiancami","comment_id":"1157480","upvote_count":"2","content":"Selected Answer: C\nIt should be C.\n\n\"They want to predict the price per square foot of real estate. The training data has a column for the price and a column for the number of square feet.\"\n\nYou need to create the column the model is going to predict.","timestamp":"1708720200.0"},{"comment_id":"1154464","content":"Selected Answer: A\nOption A","upvote_count":"3","poster":"JyoGCP","timestamp":"1708401060.0"},{"poster":"oleg25","comment_id":"1152477","timestamp":"1708164600.0","content":"I didn't get why they mentioned in the task price and square feet columns. Just to irritate us? Do we need to do something with these columns or just with column feature1?","comments":[{"comment_id":"1181586","comments":[{"comment_id":"1181605","timestamp":"1711287060.0","upvote_count":"1","poster":"d11379b","content":"But I still prefer to choose A since the square_feet column itself may have influence on price, which shouldn’t be removed"}],"content":"I think they just want us to build a “label” (target) column ourselves since there’s no direct value in the training set","poster":"d11379b","timestamp":"1711286640.0","upvote_count":"1"}],"upvote_count":"5"},{"upvote_count":"2","poster":"Matt_108","timestamp":"1705152780.0","content":"Selected Answer: A\noption A clearly","comment_id":"1121684"},{"timestamp":"1704404700.0","content":"Selected Answer: A\nStraight forward","comment_id":"1114091","upvote_count":"6","poster":"raaad"}],"unix_timestamp":1704404700,"answers_community":["A (60%)","C (40%)"],"answer_images":[],"question_images":[],"answer":"A","url":"https://www.examtopics.com/discussions/google/view/130349-exam-professional-data-engineer-topic-1-question-243/","topic":"1","timestamp":"2024-01-04 22:45:00"},{"id":"h0GRIEH1gmfJ4noSMFur","unix_timestamp":1703957040,"answers_community":["C (100%)"],"answer_description":"","discussion":[{"poster":"raaad","content":"Selected Answer: C\n- Analytics Hub allows organizations to create and manage exchanges where producers can publish their data and consumers can discover and subscribe to data products. \n- Asking each team to publish their data in Analytics Hub and having other teams subscribe to them is a scalable and controlled way of sharing data. \n- It minimizes operational tasks because data doesn't need to be duplicated or manually managed after setup, and teams can maintain full control over their datasets.","timestamp":"1720122540.0","upvote_count":"5","comment_id":"1114092"},{"upvote_count":"1","poster":"CGS22","comment_id":"1191263","timestamp":"1728345420.0","content":"Selected Answer: C\nWhy C is the best choice:\n\nCentralized Data Exchange: Analytics Hub provides a unified platform for data sharing across teams and organizations. It simplifies the process of publishing, discovering, and subscribing to datasets, reducing operational overhead.\nData Ownership and Control: Each team retains full control over their data, deciding which datasets to publish and who can access them. This ensures data governance and security.\nCross-Project Querying: Once a team subscribes to a dataset in Analytics Hub, they can query it directly from their own BigQuery project, enabling seamless data access without data replication.\nCost Efficiency: Analytics Hub eliminates the need for data duplication or complex ETL processes, reducing storage and processing costs."},{"upvote_count":"1","poster":"hanoverquay","comment_id":"1179045","content":"Selected Answer: C\nvote C","timestamp":"1726897980.0"},{"poster":"JyoGCP","content":"Selected Answer: C\nC. Analytics Hub","upvote_count":"1","comment_id":"1154465","timestamp":"1724118720.0"},{"poster":"Matt_108","timestamp":"1720870440.0","comment_id":"1121685","content":"Selected Answer: C\nthat's what analytics hub is designed for","upvote_count":"2"},{"comment_id":"1109857","timestamp":"1719761040.0","poster":"rahulvin","upvote_count":"3","content":"Selected Answer: C\nAnalytics hub to reduce operational overhead of creating/maintaining views permissions etc"}],"answer":"C","exam_id":10,"isMC":true,"question_images":[],"timestamp":"2023-12-30 18:24:00","question_id":162,"url":"https://www.examtopics.com/discussions/google/view/129902-exam-professional-data-engineer-topic-1-question-244/","answer_ET":"C","topic":"1","question_text":"Different teams in your organization store customer and performance data in BigQuery. Each team needs to keep full control of their collected data, be able to query data within their projects, and be able to exchange their data with other teams. You need to implement an organization-wide solution, while minimizing operational tasks and costs. What should you do?","answer_images":[],"choices":{"C":"Ask each team to publish their data in Analytics Hub. Direct the other teams to subscribe to them.","B":"Create a BigQuery scheduled query to replicate all customer data into team projects.","A":"Ask each team to create authorized views of their data. Grant the biquery.jobUser role to each team.","D":"Enable each team to create materialized views of the data they need to access in their projects."}},{"id":"I7aGkeq9eke1BSD4tOAr","topic":"1","answers_community":["C (100%)"],"choices":{"D":"Test and evaluate your model on your curated data to determine how well the model performs.","C":"Delineate what data will be used for testing and what will be used for training the model.","A":"Use your model to run predictions on fresh customer input data.","B":"Monitor your model performance, and make any adjustments needed."},"url":"https://www.examtopics.com/discussions/google/view/129901-exam-professional-data-engineer-topic-1-question-245/","answer_images":[],"answer":"C","timestamp":"2023-12-30 18:23:00","isMC":true,"answer_ET":"C","question_images":[],"discussion":[{"comment_id":"1114097","poster":"raaad","content":"Selected Answer: C\n- Before you can train a model, you need to decide how to split your dataset.","timestamp":"1704405180.0","upvote_count":"7"},{"timestamp":"1705153020.0","comment_id":"1121688","content":"Selected Answer: C\nOption C - you've just concluded processing data, ending up with clean and prepared data for the model. Now you need to decide how to split the data for testing and for training. Only afterwards, you can train the model, evaluate it, fine tune it and, eventually, predict with it","upvote_count":"6","poster":"Matt_108"},{"comment_id":"1263446","upvote_count":"3","poster":"meh_33","content":"Selected Answer: C\nFirst ever time Exam Topic answers matching with users answer yoooo hoooooo.\n\nC","timestamp":"1723289700.0","comments":[{"content":"Yoooo hoooooo.Yep !","timestamp":"1726329900.0","poster":"1919730","upvote_count":"1","comment_id":"1283705"}]},{"upvote_count":"1","timestamp":"1708401120.0","poster":"JyoGCP","comment_id":"1154466","content":"Selected Answer: C\nOption C"},{"upvote_count":"6","content":"Selected Answer: C\nModel doesn't seem to be trained yet","comment_id":"1109856","poster":"rahulvin","timestamp":"1703956980.0"}],"unix_timestamp":1703956980,"exam_id":10,"question_text":"You are developing a model to identify the factors that lead to sales conversions for your customers. You have completed processing your data. You want to continue through the model development lifecycle. What should you do next?","answer_description":"","question_id":163},{"id":"u3S8HNid8K9BMLTexYdR","question_id":164,"question_text":"You have one BigQuery dataset which includes customers’ street addresses. You want to retrieve all occurrences of street addresses from the dataset. What should you do?","answer_description":"","discussion":[{"comment_id":"1114098","poster":"raaad","content":"Selected Answer: B\n- Cloud Data Loss Prevention (Cloud DLP) provides powerful inspection capabilities for sensitive data, including predefined detectors for infoTypes such as STREET_ADDRESS. \n- By creating a deep inspection job for each table with the STREET_ADDRESS infoType, you can accurately identify and retrieve rows that contain street addresses.","timestamp":"1720123020.0","upvote_count":"6"},{"timestamp":"1731980880.0","comment_id":"1213520","content":"Selected Answer: B\nhttps://cloud.google.com/sensitive-data-protection/docs/learn-about-your-data#inspection","poster":"josech","upvote_count":"2"},{"upvote_count":"1","timestamp":"1724118780.0","poster":"JyoGCP","comment_id":"1154467","content":"Selected Answer: B\nOption B"},{"content":"Why not C? Discovery scan configuration can also help to identify risk/sensitivity fields.","comment_id":"1123920","poster":"AllenChen123","upvote_count":"1","timestamp":"1721104800.0","comments":[{"comment_id":"1124269","poster":"datapassionate","content":"In the question we need to retrieve all occurances of street adresses from the dataset. In C you create discovery confiuration plan on whole organization. Its not needed.","comments":[{"poster":"mdell","timestamp":"1734534540.0","upvote_count":"1","comment_id":"1328579","content":"This. C scans EVERYTHING in the org when all we want is the dataset"}],"upvote_count":"4","timestamp":"1721131260.0"}]},{"upvote_count":"3","comment_id":"1121691","content":"Selected Answer: B\nOption B - you want to retrieve ALL occurrences within the dataset","poster":"Matt_108","timestamp":"1720870860.0"},{"poster":"scaenruy","timestamp":"1720005960.0","upvote_count":"2","comment_id":"1112786","content":"Selected Answer: B\nB. Create a deep inspection job on each table in your dataset with Cloud Data Loss Prevention and create an inspection template that includes the STREET_ADDRESS infoType."}],"answer":"B","topic":"1","timestamp":"2024-01-03 14:26:00","answer_images":[],"answers_community":["B (100%)"],"exam_id":10,"question_images":[],"choices":{"B":"Create a deep inspection job on each table in your dataset with Cloud Data Loss Prevention and create an inspection template that includes the STREET_ADDRESS infoType.","A":"Write a SQL query in BigQuery by using REGEXP_CONTAINS on all tables in your dataset to find rows where the word “street” appears.","D":"Create a de-identification job in Cloud Data Loss Prevention and use the masking transformation.","C":"Create a discovery scan configuration on your organization with Cloud Data Loss Prevention and create an inspection template that includes the STREET_ADDRESS infoType."},"isMC":true,"answer_ET":"B","unix_timestamp":1704288360,"url":"https://www.examtopics.com/discussions/google/view/130188-exam-professional-data-engineer-topic-1-question-246/"},{"id":"3F6q0GywJflwVuDY1DXA","answer":"C","timestamp":"2024-01-03 14:29:00","choices":{"C":"1. Create one lake for each domain. Inside each lake, create one zone for each team.\n2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.\n3. Direct each domain to manage their own lake’s data assets.","A":"1. Create one lake for each team. Inside each lake, create one zone for each domain.\n2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.\n3. Have the central data platform team manage all zones’ data assets.","B":"1. Create one lake for each team. Inside each lake, create one zone for each domain.\n2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.\n3. Direct each domain to manage their own zone’s data assets.","D":"1. Create one lake for each domain. Inside each lake, create one zone for each team.\n2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.\n3. Have the central data platform team manage all lakes’ data assets."},"discussion":[{"comment_id":"1114117","poster":"raaad","timestamp":"1720127700.0","comments":[{"comment_id":"1123942","content":"Agree. https://cloud.google.com/dataplex/docs/introduction#a_domain-centric_data_mesh","poster":"AllenChen123","timestamp":"1721107320.0","upvote_count":"4"}],"content":"Selected Answer: C\n- each domain should manage their own lake’s data assets","upvote_count":"7"},{"content":"Selected Answer: C\nvote C","poster":"hanoverquay","upvote_count":"1","comment_id":"1178675","timestamp":"1726856700.0"},{"comment_id":"1154469","content":"Selected Answer: C\nOption C","poster":"JyoGCP","timestamp":"1724118840.0","upvote_count":"1"},{"poster":"Matt_108","upvote_count":"4","content":"Selected Answer: C\nOption C - create a lake for each domain, each team manages its own assets","comment_id":"1121693","timestamp":"1720871040.0"},{"poster":"task_7","timestamp":"1720668180.0","upvote_count":"2","comment_id":"1119377","content":"Selected Answer: B\nSeparate lakes for each team\nZones within each lake dedicated to different domains"},{"poster":"scaenruy","upvote_count":"1","timestamp":"1720006140.0","comment_id":"1112787","content":"Selected Answer: C\nC. \n1. Create one lake for each domain. Inside each lake, create one zone for each team.\n2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.\n3. Direct each domain to manage their own lake’s data assets."}],"exam_id":10,"answer_images":[],"answers_community":["C (88%)","13%"],"topic":"1","isMC":true,"answer_ET":"C","question_id":165,"answer_description":"","question_text":"Your company operates in three domains: airlines, hotels, and ride-hailing services. Each domain has two teams: analytics and data science, which create data assets in BigQuery with the help of a central data platform team. However, as each domain is evolving rapidly, the central data platform team is becoming a bottleneck. This is causing delays in deriving insights from data, and resulting in stale data when pipelines are not kept up to date. You need to design a data mesh architecture by using Dataplex to eliminate the bottleneck. What should you do?","url":"https://www.examtopics.com/discussions/google/view/130189-exam-professional-data-engineer-topic-1-question-247/","question_images":[],"unix_timestamp":1704288540}],"exam":{"isImplemented":true,"isMCOnly":true,"id":10,"numberOfQuestions":319,"isBeta":false,"lastUpdated":"15 Feb 2025","name":"Professional Data Engineer","provider":"Google"},"currentPage":33},"__N_SSP":true}