{"pageProps":{"questions":[{"id":"oAqs0jMUTQz8a2wUEkg5","exam_id":10,"question_id":216,"topic":"1","answers_community":["D (84%)","B (16%)"],"discussion":[{"timestamp":"1704989400.0","poster":"raaad","upvote_count":"11","comment_id":"1119941","content":"Selected Answer: D\n- Decentralized ownership: Each department controls its data lake, aligning with the core principle of data ownership in a data mesh.\n- Self-service data access: Departments can create and manage their own Cloud Storage buckets and BigQuery datasets within their data lakes, enabling self-service data access.\n- Interdepartmental sharing: Dataplex facilitates data sharing by enabling departments to publish their data products from their data lakes, making it easily discoverable and usable by other departments."},{"upvote_count":"1","timestamp":"1738845780.0","comment_id":"1352379","poster":"plum21","content":"Selected Answer: D\nD because it looks like B is impossible due to the lack of GCS support in Analytics Hub"},{"content":"Selected Answer: B\nIn \"a data mesh approach to share the data between sales, product design, and marketing departments\", Analytics Hub is the solution.","upvote_count":"1","poster":"marlon.andrei","comment_id":"1337366","timestamp":"1736198580.0"},{"upvote_count":"2","comment_id":"1259973","content":"B is better option as organization is migrating to google cloud, that means teams doesnt have much hands on, analytical hub is more ease to use and solved the purpose as compared to dataplex were setup itself if very complex.","timestamp":"1722620220.0","poster":"Nandababy"},{"comment_id":"1252622","poster":"987af6b","timestamp":"1721582280.0","content":"Selected Answer: B\nFor a straightforward data mesh approach where the focus is on decentralizing data management while enabling easy data sharing and discovery, Analytics Hub is often the more appropriate choice due to its simplicity and directness. It facilitates the core objectives of a data mesh—decentralized data ownership and accessible data sharing—without the added complexity of managing data lakes and advanced governance features.","upvote_count":"2"},{"timestamp":"1712820840.0","poster":"joao_01","comments":[{"comment_id":"1193536","upvote_count":"2","timestamp":"1712821140.0","content":"I was wrong in my explanation guys. Look at this link:\nhttps://cloud.google.com/dataplex/docs/add-zone\n\n\"A lake can include one or more zones. While a zone can only be part of one lake, it may contain assets that point to resources that are part of projects outside of its parent project.\"\n\nSo, option D seems good.","poster":"joao_01"}],"comment_id":"1193533","content":"I think its B. I know since we are talking about Datamesh we want to go to the Dataplex service suddenly. However, in Dataplex a Lake can only have assets (bq tables etc) that are in the same project as the Dataplex service. \n\nExample: There is bq table in project A and B. I want to to create a Lake in Dataplex in Project A that contains tables of project B. I can´t do that, i can only host tables of the Project A, since the Lake is in project A.\n\nWith this said, I think the best option is B, because the datamesh approach is related to \"to share the data between sales, product design, and marketing departments\". So the question is focusing only in the sharing part of the datamesh. Option B fits just fine.","upvote_count":"2"},{"upvote_count":"1","timestamp":"1708535520.0","comment_id":"1155733","poster":"JyoGCP","content":"Selected Answer: D\nOption D"},{"content":"Selected Answer: D\nthat's pure data mesh, which is what dataplex has been built for","upvote_count":"2","poster":"Matt_108","comment_id":"1120862","timestamp":"1705071300.0"},{"poster":"Sofiia98","content":"Selected Answer: D\nFor me, Dataplex looks more logical","comment_id":"1118791","upvote_count":"1","timestamp":"1704902520.0"},{"comment_id":"1115745","content":"D. Dataplex looks more suitable for data mesh approach, Check the ref - https://cloud.google.com/dataplex/docs/introduction","upvote_count":"1","timestamp":"1704625140.0","poster":"GCP001"}],"choices":{"D":"1. Create multiple projects for storage of the data for each of your departments’ applications.\n2. Enable each department to create Cloud Storage buckets and BigQuery datasets.\n3. In Dataplex, map each department to a data lake and the Cloud Storage buckets, and map the BigQuery datasets to zones.\n4. Enable each department to own and share the data of their data lakes.","A":"1. Create a project for storage of the data for each of your departments.\n2. Enable each department to create Cloud Storage buckets and BigQuery datasets.\n3. Create user groups for authorized readers for each bucket and dataset.\n4. Enable the IT team to administer the user groups to add or remove users as the departments’ request.","B":"1. Create multiple projects for storage of the data for each of your departments’ applications.\n2. Enable each department to create Cloud Storage buckets and BigQuery datasets.\n3. Publish the data that each department shared in Analytics Hub.\n4. Enable all departments to discover and subscribe to the data they need in Analytics Hub.","C":"1. Create a project for storage of the data for your organization.\n2. Create a central Cloud Storage bucket with three folders to store the files for each department.\n3. Create a central BigQuery dataset with tables prefixed with the department name.\n4. Give viewer rights for the storage project for the users of your departments."},"answer_images":[],"answer":"D","answer_ET":"D","unix_timestamp":1704364620,"question_text":"Your organization is modernizing their IT services and migrating to Google Cloud. You need to organize the data that will be stored in Cloud Storage and BigQuery. You need to enable a data mesh approach to share the data between sales, product design, and marketing departments. What should you do?","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/130301-exam-professional-data-engineer-topic-1-question-293/","timestamp":"2024-01-04 11:37:00","answer_description":"","question_images":[]},{"id":"FldAE5EEP2Lk9a78gDNe","topic":"1","answer_ET":"B","question_id":217,"timestamp":"2024-01-04 11:45:00","answer":"B","question_text":"You work for a large ecommerce company. You are using Pub/Sub to ingest the clickstream data to Google Cloud for analytics. You observe that when a new subscriber connects to an existing topic to analyze data, they are unable to subscribe to older data. For an upcoming yearly sale event in two months, you need a solution that, once implemented, will enable any new subscriber to read the last 30 days of data. What should you do?","url":"https://www.examtopics.com/discussions/google/view/130307-exam-professional-data-engineer-topic-1-question-294/","choices":{"B":"Set the topic retention policy to 30 days.","A":"Create a new topic, and publish the last 30 days of data each time a new subscriber connects to an existing topic.","D":"Ask the source system to re-push the data to Pub/Sub, and subscribe to it.","C":"Set the subscriber retention policy to 30 days."},"unix_timestamp":1704365100,"discussion":[{"comment_id":"1119948","upvote_count":"11","content":"Selected Answer: B\n- Topic Retention Policy: This policy determines how long messages are retained by Pub/Sub after they are published, even if they have not been acknowledged (consumed) by any subscriber.\n- 30 Days Retention: By setting the retention policy of the topic to 30 days, all messages published to this topic will be available for consumption for 30 days. This means any new subscriber connecting to the topic can access and analyze data from the past 30 days.","poster":"raaad","timestamp":"1704989580.0"},{"timestamp":"1735303860.0","content":"Selected Answer: B\nB is correct.\nBy setting the topic retention policy to 30 days, any new subscriber will be able to access the data for the past 30 days, regardless of when they connect. This solution is both cost-effective and efficient for your use case.","upvote_count":"1","comment_id":"1332405","poster":"hussain.sain"},{"comment_id":"1304931","content":"Option B is wrong i think (topic retention) because it only makes unconsumed messages available for 30 days. I propose option A\n\nOption A (creating a new topic and republishing the last 30 days of data for each new subscriber) is actually a better solution to ensure that new subscribers have access to the full 30-day history.","poster":"romain773","upvote_count":"1","timestamp":"1730282460.0"},{"poster":"romain773","comment_id":"1302831","upvote_count":"1","timestamp":"1729853460.0","content":"Option B is wrong (topic retention) because it only makes unconsumed messages available for 30 days.\n\nOption A (creating a new topic and republishing the last 30 days of data for each new subscriber) is actually a better solution to ensure that new subscribers have access to the full 30-day history."},{"poster":"joao_01","comment_id":"1193554","upvote_count":"1","timestamp":"1712822040.0","content":"Its B. It could be C as well because subscription has message retention. However, in the subscription there is a maximum value for it: 7 days.\n\nLink:https://cloud.google.com/pubsub/docs/subscription-properties","comments":[{"poster":"joao_01","upvote_count":"1","comment_id":"1193556","content":"In a topic the maximum value is 31 days.\n\nLink: https://cloud.google.com/pubsub/docs/topic-properties","timestamp":"1712822100.0"}]},{"poster":"Matt_108","upvote_count":"2","comment_id":"1121912","content":"Selected Answer: B\nDefinitely B","timestamp":"1705167120.0"},{"comment_id":"1118805","content":"Selected Answer: B\nhttps://cloud.google.com/blog/products/data-analytics/pubsub-gains-topic-retention-feature","timestamp":"1704903180.0","poster":"Sofiia98","upvote_count":"4"},{"timestamp":"1704365100.0","upvote_count":"1","content":"Selected Answer: B\nB. Set the topic retention policy to 30 days.","poster":"scaenruy","comment_id":"1113573"}],"exam_id":10,"isMC":true,"answers_community":["B (100%)"],"question_images":[],"answer_images":[],"answer_description":""},{"id":"IPct2zNclbNfwFjR0lJP","isMC":true,"unix_timestamp":1704365400,"question_id":218,"answer_images":[],"answer":"B","url":"https://www.examtopics.com/discussions/google/view/130309-exam-professional-data-engineer-topic-1-question-295/","question_images":[],"discussion":[{"upvote_count":"10","poster":"raaad","content":"Selected Answer: A\n- Dataflow service agent is the one responsible for setting up and managing the network resources that Dataflow requires. \n- By granting the compute.networkUser role to this service agent, we are enabling it to provision the necessary network resources within the Shared VPC for your Dataflow job.","comment_id":"1119982","timestamp":"1704992100.0"},{"comment_id":"1145314","poster":"saschak94","timestamp":"1707465120.0","upvote_count":"5","content":"Selected Answer: A\nAll projects that have used the resource Dataflow Job have a Dataflow Service Account, also known as the Dataflow service agent.\n\nMake sure the Shared VPC subnetwork is shared with the Dataflow service account and has the Compute Network User role assigned on the specified subnet."},{"comment_id":"1347889","timestamp":"1738067040.0","content":"Selected Answer: A\nhttps://cloud.google.com/dataflow/docs/concepts/security-and-permissions#df-service-account","upvote_count":"1","poster":"loki82"},{"comment_id":"1338233","poster":"Pime13","timestamp":"1736410500.0","upvote_count":"1","content":"Selected Answer: B\nhttps://cloud.google.com/dataflow/docs/guides/specifying-networks\nhttps://cloud.google.com/dataflow/docs/guides/specifying-networks#shared\nMake sure the Shared VPC subnetwork is shared with the Dataflow service account and has the Compute Network User role assigned on the specified subnet. The Compute Network User role must be assigned to the Dataflow service account in the host project."},{"comment_id":"1305969","poster":"SamuelTsch","timestamp":"1730488980.0","comments":[{"timestamp":"1730793660.0","poster":"ach5","content":"service account - it's B","upvote_count":"3","comment_id":"1307244"}],"content":"Selected Answer: A\nFrom https://cloud.google.com/dataflow/docs/guides/specifying-networks, it says \"Make sure the Shared VPC subnetwork is shared with the Dataflow service account and has the Compute Network User role assigned on the specified subnet. The Compute Network User role must be assigned to the Dataflow service account in the host project.\"","upvote_count":"2"},{"upvote_count":"4","content":"Selected Answer: B\nIf you see in the comments, A was answer by people around 8 months ago but recent ones have answered B with the documentation. The GCP documentation evolves with time","comment_id":"1289232","poster":"Preetmehta1234","timestamp":"1727311140.0"},{"timestamp":"1727311020.0","upvote_count":"2","poster":"Preetmehta1234","content":"Selected Answer: B\nservice account that executes the Dataflow pipeline\nIt's straight forward","comment_id":"1289231"},{"poster":"Preetmehta1234","comment_id":"1288812","upvote_count":"2","timestamp":"1727218320.0","content":"Selected Answer: B\nAssign the compute.networkUser role to the service account that executes the Dataflow pipeline"},{"timestamp":"1721797020.0","comment_id":"1254125","upvote_count":"3","poster":"Jeyaraj","content":"The correct answer is B. Assign the compute.networkUser role to the service account that executes the Dataflow pipeline.\n\nHere's why:\n\n Shared VPC and Network Access: When using a Shared VPC, you need to grant specific permissions to service accounts in the service project (where your Dataflow pipeline runs) to access resources in the host project's network.\n compute.networkUser Role: This role grants the necessary permissions for a service account to use the network resources in the Shared VPC. This includes accessing subnets, creating instances, and communicating with other services within the network.\n Service Account for Pipeline Execution: The service account that executes your Dataflow pipeline is the one that needs these network permissions. This is because the Dataflow service uses this account to create and manage worker instances within the Shared VPC network."},{"poster":"extraego","timestamp":"1718052780.0","comment_id":"1228103","content":"Selected Answer: B\nDataflow service agent is a role that is assigned to a service account. So is compute.networkUser.\nhttps://cloud.google.com/dataflow/docs/concepts/access-control#example","upvote_count":"4"},{"poster":"josech","content":"Selected Answer: B\nOption B https://cloud.google.com/knowledge/kb/dataflow-job-in-shared-vpc-xpn-permissions-000004261","upvote_count":"4","timestamp":"1716156840.0","comment_id":"1214007"},{"timestamp":"1714561320.0","content":"Selected Answer: B\nI believe the answer is B. All authentication documentation points to Service Accounts. https://cloud.google.com/dataflow/docs/concepts/authentication#on-gcp\n\nDataflow service agent typically manages general interactions with the Dataflow service but does not execute the actual jobs.","poster":"chrissamharris","upvote_count":"3","comment_id":"1205027"},{"upvote_count":"3","timestamp":"1705167420.0","comment_id":"1121919","poster":"Matt_108","comments":[{"timestamp":"1707387420.0","content":"But your link it's explain that \"Network User role must be assigned to the Dataflow service account\" \n\nMake sure the Shared VPC subnetwork is shared with the Dataflow service account and has the Compute Network User role assigned on the specified subnet. The Compute Network User role must be assigned to the Dataflow service account in the host project.","comment_id":"1144346","poster":"tibuenoc","comments":[{"timestamp":"1708282020.0","comment_id":"1153496","poster":"ML6","upvote_count":"2","content":"All projects that have used the resource Dataflow Job have a Dataflow Service Account, also known as the Dataflow service agent.\nSource: https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#df-service-account"}],"upvote_count":"1"}],"content":"Selected Answer: A\nOption A, I do agree with Raaad, it's the dataflow service agent that needs the networkUser role, because it's the one that provisions the network resources https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared"},{"poster":"task_7","comment_id":"1120590","upvote_count":"4","content":"Selected Answer: B\ncompute.networkUser to the service account that executes the Dataflow pipeline.","timestamp":"1705049640.0"},{"content":"Selected Answer: B\nOption B is Correct. \n\nExplanation:\nYou need to give compute networkuser role to service account that is processing the pipeline as it will need to deploy nessesary worker nodes on the shared vpc project. \n\nOption A is incorrect as Dataflow Service Agent is Google MGS service account that will not responsible for running or deoplying workers in shared vpc.\n\nOption C and D is incorrect as dataflow.admin is elevated privlages to create and manage all of dataflow components not deploying resources in shared vpc.","timestamp":"1704987480.0","poster":"BIGQUERY_ALT_ALT","upvote_count":"2","comment_id":"1119907"},{"comments":[{"content":"Option A makes more sense:\n- Assigning the compute.networkUser role to the pipeline's service account grants it unnecessary and possibly excessive permissions outside its core responsibility of data processing. \n\nThe question focused specifically on the deployment aspect (i.e., provisioning of network resources like VMs) rather than what the pipeline accesses or processes once it's running.","poster":"raaad","comment_id":"1119985","timestamp":"1704992220.0","comments":[{"upvote_count":"1","timestamp":"1705518660.0","content":"Yes , I agree, it should be A. Dataflow service account should be the one having this permission instaed of worker","comment_id":"1125246","comments":[{"comment_id":"1306256","poster":"8284a4c","timestamp":"1730567280.0","upvote_count":"2","content":"The compute.networkUser role needs to be assigned to the specific service account running the Dataflow job, not the Dataflow service agent, as the service agent does not execute the pipeline."}],"poster":"GCP001"}],"upvote_count":"1"}],"content":"B. Assign the compute.networkUser role to the service account that executes the Dataflow pipeline. See the ref - https://cloud.google.com/dataflow/docs/guides/specifying-networks","comment_id":"1115725","timestamp":"1704623520.0","upvote_count":"2","poster":"GCP001"}],"answer_description":"","choices":{"A":"Assign the compute.networkUser role to the Dataflow service agent.","B":"Assign the compute.networkUser role to the service account that executes the Dataflow pipeline.","C":"Assign the dataflow.admin role to the Dataflow service agent.","D":"Assign the dataflow.admin role to the service account that executes the Dataflow pipeline."},"question_text":"You are designing the architecture to process your data from Cloud Storage to BigQuery by using Dataflow. The network team provided you with the Shared VPC network and subnetwork to be used by your pipelines. You need to enable the deployment of the pipeline on the Shared VPC network. What should you do?","topic":"1","answer_ET":"B","timestamp":"2024-01-04 11:50:00","exam_id":10,"answers_community":["B (55%)","A (45%)"]},{"id":"f08uInOXB6USC4S323iK","isMC":true,"answer_images":[],"answer_ET":"C","answer_description":"","timestamp":"2023-12-30 21:07:00","discussion":[{"comment_id":"1337082","upvote_count":"1","content":"Selected Answer: C\nC. Use Dataflow, write a pipeline that reads the data from Kafka, and writes the data to BigQuery.\n\nThis approach allows you to directly connect Dataflow to your Kafka cluster, ensuring minimal latency by avoiding additional intermediaries like Pub/Sub. Dataflow is designed to handle high-throughput data processing and can efficiently ingest and process streaming data from Kafka, then write it to BigQuery. This setup leverages the interconnect link for a direct and efficient data flow","poster":"Pime13","comments":[{"poster":"Pime13","timestamp":"1736410620.0","upvote_count":"1","content":"https://cloud.google.com/dataflow/docs/kafka-dataflow#deploy_kafka\nAlternatively, you might have an existing Kafka cluster that resides outside of Google Cloud. For example, you might have an existing workload that is deployed on-premises or in another public cloud.","comment_id":"1338234"}],"timestamp":"1736160840.0"},{"poster":"meh_33","upvote_count":"1","comment_id":"1263326","timestamp":"1723268340.0","content":"Going with C"},{"content":"Selected Answer: C\nLatency: Option C, with direct integration between Kafka and Dataflow, offers lower latency by eliminating intermediate steps.\nFlexibility: Custom Dataflow pipelines (Option C) provide more control over data processing and optimization compared to using a pre-built template.","timestamp":"1720234440.0","upvote_count":"2","poster":"Anudeep58","comment_id":"1243137"},{"poster":"anushree09","content":"per the text below at https://cloud.google.com/dataflow/docs/kafka-dataflow -\n\n\"Alternatively, you might have an existing Kafka cluster that resides outside of Google Cloud. For example, you might have an existing workload that is deployed on-premises or in another public cloud.\"","timestamp":"1712866740.0","comment_id":"1193996","upvote_count":"1"},{"comment_id":"1163135","upvote_count":"2","poster":"Moss2011","timestamp":"1709293620.0","content":"Selected Answer: C\nFrom my point of view, the best option is C taking into account this doc: https://cloud.google.com/dataflow/docs/kafka-dataflow"},{"poster":"MaxNRG","timestamp":"1708976700.0","upvote_count":"2","comment_id":"1160025","comments":[{"comments":[{"poster":"MaxNRG","timestamp":"1708976820.0","upvote_count":"1","content":"The Google template may be easier to set up initially, but a custom pipeline provides more control over optimizations specifically for low latency requirements stated in the question.\nThat being said, option A would still work reasonably well - but option D allows squeezing out that extra bit of performance if low millisecond latency is absolutely critical in the pipeline through precise tuning.\nSo in summary, option A is easier to implement but option D provides more optimization flexibility for ultra low latency streaming requirements.","comment_id":"1160030"}],"content":"Why choose option D over A?\nThe key advantage with option D is that by writing a custom Dataflow pipeline rather than using a Google provided template, there is more flexibility to customize performance tuning and optimization for lowest latency.\n• Some potential optimizations:\n• Fine tuning number of workers, machine types to meet specific throughput targets\n• Custom data parsing/processing logic if applicable\n• Experimenting with autoscaling parameters or triggers","timestamp":"1708976820.0","comment_id":"1160029","poster":"MaxNRG","upvote_count":"1"},{"upvote_count":"3","content":"You are adding an intermediate hop in between on prem kafka and Dataflow ( pubsub ). Why won't this add additional latency.","comment_id":"1183491","poster":"SanjeevRoy91","timestamp":"1711474680.0"}],"content":"Selected Answer: D\nBased on the key requirements highlighted:\n• Interconnect link between GCP and on-prem Kafka\n• High throughput streaming pipeline\n• Minimal latency\n• Data to be stored in BigQuery\nD - The key reasons this meets the requirements:\n• Kafka connect provides a reliable bridge to Pub/Sub over the interconnect\n• Reading from Pub/Sub minimizes latency vs reading directly from Kafka\n• Dataflow provides a high throughput streaming engine\n• Writing to BigQuery gives scalable data storage\nBy leveraging these fully managed GCP services over the dedicated interconnect, a low latency streaming pipeline from on-prem Kafka into BigQuery can be implemented rapidly.\nOptions A/B/C have higher latencies or custom code requirements, so do not meet the minimal latency criteria as well as option D."},{"upvote_count":"2","comment_id":"1155765","comments":[{"comments":[{"upvote_count":"2","comment_id":"1156493","timestamp":"1708616520.0","content":"Final???","poster":"DarkLord2104"}],"timestamp":"1708570740.0","poster":"JyoGCP","comment_id":"1156049","upvote_count":"2","content":"Going with C"}],"poster":"JyoGCP","timestamp":"1708538880.0","content":"A Vs C -- Not sure which one would have low latency.\n\nPoints related to option C:\n\"Yes, Dataflow can read events from Kafka. Dataflow is a fully-managed, serverless streaming analytics service that supports both batch and stream processing. It can read events from Kafka, process them, and write the results to a BigQuery table for further analysis. \"\n\n\"Dataflow supports Kafka support, which was added to Apache Beam in 2016. Google provides a Dataflow template that configures a Kafka-to-BigQuery pipeline. The template uses the BigQueryIO connector provided in the Apache Beam SDK. \""},{"comment_id":"1138703","timestamp":"1706890320.0","content":"Selected Answer: C\nOption C makes more sense to me because of the \"minimal latency as possible\".\nI would have chosen option A if it were \"less CODING as possible\".","poster":"T2Clubber","upvote_count":"3"},{"content":"Selected Answer: A\nOption A, leverage dataflow template for Kafka https://cloud.google.com/dataflow/docs/kafka-dataflow","timestamp":"1705167540.0","poster":"Matt_108","upvote_count":"4","comment_id":"1121922","comments":[{"upvote_count":"1","poster":"AllenChen123","content":"Agree. \"Google provides a Dataflow template that configures a Kafka-to-BigQuery pipeline. The template uses the BigQueryIO connector provided in the Apache Beam SDK.\"","comment_id":"1127700","timestamp":"1705824120.0"},{"content":"But it includes setting up a Kafka Connect bridge while an interconnect link has already been set up. https://cloud.google.com/dataflow/docs/kafka-dataflow#connect_to_an_external_cluster","comment_id":"1153507","upvote_count":"1","poster":"ML6","timestamp":"1708282920.0"}]},{"content":"Selected Answer: C\nC. Use Dataflow, write a pipeline that reads the data from Kafka, and writes the data to BigQuery.","poster":"scaenruy","timestamp":"1704366600.0","comment_id":"1113611","upvote_count":"4"},{"content":"Selected Answer: C\nDataflow has templates to read from Kafka. Other options are too complicated\nhttps://cloud.google.com/dataflow/docs/kafka-dataflow","upvote_count":"3","comment_id":"1109949","comments":[{"upvote_count":"2","content":"so, this is the answer A, whe C?","poster":"Sofiia98","timestamp":"1704903360.0","comment_id":"1118808","comments":[{"timestamp":"1706540100.0","content":"but Option A introduces additional replication into Pub/Sub and the question states with minimal latency. In my opinion subscribing to Kafka via Dataflow has a lower latency than replicating the messages first to Pub/Sub and subscribing with Dataflow to it.","upvote_count":"7","comment_id":"1135060","poster":"saschak94"}]}],"poster":"rahulvin","timestamp":"1703966820.0"}],"question_id":219,"question_text":"Your infrastructure team has set up an interconnect link between Google Cloud and the on-premises network. You are designing a high-throughput streaming pipeline to ingest data in streaming from an Apache Kafka cluster hosted on- premises. You want to store the data in BigQuery, with as minimal latency as possible. What should you do?","choices":{"A":"Setup a Kafka Connect bridge between Kafka and Pub/Sub. Use a Google-provided Dataflow template to read the data from Pub/Sub, and write the data to BigQuery.","B":"Use a proxy host in the VPC in Google Cloud connecting to Kafka. Write a Dataflow pipeline, read data from the proxy host, and write the data to BigQuery.","C":"Use Dataflow, write a pipeline that reads the data from Kafka, and writes the data to BigQuery.","D":"Setup a Kafka Connect bridge between Kafka and Pub/Sub. Write a Dataflow pipeline, read the data from Pub/Sub, and write the data to BigQuery."},"url":"https://www.examtopics.com/discussions/google/view/129909-exam-professional-data-engineer-topic-1-question-296/","topic":"1","question_images":[],"answer":"C","answers_community":["C (71%)","A (19%)","10%"],"unix_timestamp":1703966820,"exam_id":10},{"id":"rR39W4DtcVfixx0Alnmi","question_images":[],"choices":{"A":"1. Deploy a long-living Dataproc cluster with Apache Hive and Ranger enabled.\n2. Configure Ranger for column level security.\n3. Process with Dataproc Spark or Hive SQL.","C":"1. Load the data to BigQuery tables.\n2. Create a taxonomy of policy tags in Data Catalog.\n3. Add policy tags to columns.\n4. Process with the Spark-BigQuery connector or BigQuery SQL.","D":"1. Apply an Identity and Access Management (IAM) policy at the file level in Cloud Storage.\n2. Define a BigQuery external table for SQL processing.\n3. Use Dataproc Spark to process the Cloud Storage files.","B":"1. Define a BigLake table.\n2. Create a taxonomy of policy tags in Data Catalog.\n3. Add policy tags to columns.\n4. Process with the Spark-BigQuery connector or BigQuery SQL."},"question_id":220,"answer_description":"","unix_timestamp":1704366900,"discussion":[{"content":"Selected Answer: B\n- BigLake Integration: BigLake allows you to define tables on top of data in Cloud Storage, providing a bridge between data lake storage and BigQuery's powerful analytics capabilities. This approach is cost-effective and scalable.\n- Data Catalog for Governance: Creating a taxonomy of policy tags in Google Cloud's Data Catalog and applying these tags to specific columns in your BigLake tables enables fine-grained, column-level access control. \n- Processing with Spark and SQL: The Spark-BigQuery connector allows data scientists to process data using Apache Spark directly against BigQuery (and BigLake tables). This supports both Spark and SQL processing needs.\n- Scalability into a Data Mesh: BigLake and Data Catalog are designed to scale and support the data mesh architecture, which involves decentralized data ownership and governance.","timestamp":"1720710180.0","comment_id":"1119989","upvote_count":"15","poster":"raaad"},{"timestamp":"1724257140.0","poster":"JyoGCP","comment_id":"1155776","upvote_count":"1","content":"Selected Answer: B\nGoing with 'B' based on the comments"},{"timestamp":"1720885260.0","content":"Selected Answer: B\nOption B, agree with comments explanation","upvote_count":"1","poster":"Matt_108","comment_id":"1121926"},{"comment_id":"1116291","poster":"Jordan18","upvote_count":"4","timestamp":"1720393020.0","content":"Selected Answer: B\nBigLake leverages existing Cloud Storage infrastructure, eliminating the need for a dedicated Dataproc cluster, reducing costs significantly."},{"comments":[],"comment_id":"1113616","upvote_count":"1","timestamp":"1720084500.0","content":"Selected Answer: C\nC. \n1. Load the data to BigQuery tables.\n2. Create a taxonomy of policy tags in Data Catalog.\n3. Add policy tags to columns.\n4. Process with the Spark-BigQuery connector or BigQuery SQL.","poster":"scaenruy"}],"timestamp":"2024-01-04 12:15:00","answer_ET":"B","answers_community":["B (95%)","5%"],"question_text":"You migrated your on-premises Apache Hadoop Distributed File System (HDFS) data lake to Cloud Storage. The data scientist team needs to process the data by using Apache Spark and SQL. Security policies need to be enforced at the column level. You need a cost-effective solution that can scale into a data mesh. What should you do?","isMC":true,"url":"https://www.examtopics.com/discussions/google/view/130313-exam-professional-data-engineer-topic-1-question-297/","topic":"1","exam_id":10,"answer":"B","answer_images":[]}],"exam":{"name":"Professional Data Engineer","id":10,"numberOfQuestions":319,"isBeta":false,"provider":"Google","isMCOnly":true,"lastUpdated":"15 Feb 2025","isImplemented":true},"currentPage":44},"__N_SSP":true}