{"pageProps":{"questions":[{"id":"JAqGYq8gr9RV6MWawYVz","answer_images":[],"choices":{"A":"Use Dataflow and the Cloud Data Loss Prevention API to mask sensitive data. Write the processed data in BigQuery.","C":"Use the Cloud Data Loss Prevention API and Dataflow to detect and remove sensitive fields from the data in Cloud Storage. Write the filtered data in BigQuery.","B":"Use customer-managed encryption keys (CMEK) to directly encrypt the data in Cloud Storage. Use federated queries from BigQuery. Share the encryption key by following the principle of least privilege.","D":"Use Dataflow and Cloud KMS to encrypt sensitive fields and write the encrypted data in BigQuery. Share the encryption key by following the principle of least privilege."},"unix_timestamp":1704370860,"answer_description":"","question_id":231,"answers_community":["A (100%)"],"question_images":[],"discussion":[{"comment_id":"1115169","poster":"raaad","upvote_count":"13","timestamp":"1720261860.0","content":"Selected Answer: A\n- Prioritizes Data Privacy: It protects sensitive information by masking it, reducing the risk of exposure in case of unauthorized access or accidental leaks.\n- Reduces Data Sensitivity: Masking renders sensitive data unusable for attackers, even if they gain access to it.\n- Preserves Data Utility: Masked data can still be used for consumer analyses, as patterns and relationships are often preserved, allowing meaningful insights to be derived.","comments":[{"timestamp":"1723420800.0","content":"why not d ?","comment_id":"1147786","comments":[{"upvote_count":"2","content":"Data in Cloud Storage is encrypted by default.","timestamp":"1724056620.0","comment_id":"1153841","poster":"ML6"}],"upvote_count":"2","poster":"dungct"}]},{"upvote_count":"3","comment_id":"1224788","poster":"AlizCert","content":"Selected Answer: A\nWhat made me decide on A instead of C was the \"The data will be used to create consumer analyses\" sentence. Having all the PIIs completely redacted from the records, we were unable to distinguish between the individual customers.","timestamp":"1733418240.0"},{"poster":"Matt_108","timestamp":"1720882440.0","upvote_count":"1","content":"Selected Answer: A\nOption A, agree with raaad explanation","comment_id":"1121877"},{"content":"Selected Answer: A\nA. Use Dataflow and the Cloud Data Loss Prevention API to mask sensitive data. Write the processed data in BigQuery.","upvote_count":"2","comment_id":"1113664","poster":"scaenruy","timestamp":"1720088460.0"}],"url":"https://www.examtopics.com/discussions/google/view/130321-exam-professional-data-engineer-topic-1-question-306/","exam_id":10,"answer_ET":"A","answer":"A","timestamp":"2024-01-04 13:21:00","question_text":"You are preparing an organization-wide dataset. You need to preprocess customer data stored in a restricted bucket in Cloud Storage. The data will be used to create consumer analyses. You need to comply with data privacy requirements.\n\nWhat should you do?","topic":"1","isMC":true},{"id":"mk2jptx23ZClzTUDLUQU","timestamp":"2024-01-04 13:13:00","topic":"1","exam_id":10,"answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/130320-exam-professional-data-engineer-topic-1-question-307/","isMC":true,"question_images":[],"discussion":[{"upvote_count":"10","timestamp":"1720711260.0","poster":"raaad","comment_id":"1120004","content":"Selected Answer: C\n- Using the Cloud SQL Auth proxy is a recommended method for secure connections, especially when dealing with dynamic IP addresses. \n- The Auth proxy provides secure access to your Cloud SQL instance without the need for Authorized Networks or managing IP addresses. \n- It works by encapsulating database traffic and forwarding it through a secure tunnel, using Google's IAM for authentication.\n- Leaving the Authorized Networks empty means you're not allowing any direct connections based on IP addresses, relying entirely on the Auth proxy for secure connectivity. This is a secure and flexible solution, especially for applications with dynamic IPs."},{"content":"Selected Answer: C\nOption C","timestamp":"1724316780.0","upvote_count":"1","comment_id":"1156312","poster":"JyoGCP"},{"upvote_count":"3","poster":"Pukapuiz","timestamp":"1721309460.0","comment_id":"1126002","content":"Selected Answer: C\nThe Cloud SQL Auth Proxy is a Cloud SQL connector that provides secure access to your instances without a need for Authorized networks or for configuring SSL.\nhttps://cloud.google.com/sql/docs/mysql/sql-proxy"},{"timestamp":"1720789020.0","content":"Selected Answer: C\nalways use Cloud SQL Auth proxy if possible","comment_id":"1120864","upvote_count":"1","poster":"Matt_108"},{"timestamp":"1720694280.0","content":"https://stackoverflow.com/questions/27759356/how-to-authorize-my-dynamic-ip-network-address-in-google-cloud-sql\nhttps://stackoverflow.com/questions/24749810/how-to-make-a-google-cloud-sql-instance-accessible-for-any-ip-address","upvote_count":"4","comments":[{"upvote_count":"4","timestamp":"1724316720.0","poster":"JyoGCP","comment_id":"1156311","content":"Links also say not to go with option D.\n0.0.0.0/0 which includes all possible IP Addresses is not recommended for security reasons. You have to keep access as restricted as possible."}],"comment_id":"1119723","poster":"Sofiia98"},{"comments":[{"poster":"BennyXu","comment_id":"1194601","timestamp":"1728782820.0","upvote_count":"1","content":"Save your shxx answer in your dxxb head."}],"comment_id":"1119721","upvote_count":"1","timestamp":"1720694220.0","content":"Selected Answer: D\nAs for me, after reading documentation, option D looks appropriate","poster":"Sofiia98"},{"poster":"scaenruy","timestamp":"1720087980.0","content":"Selected Answer: C\nC. Leave the Authorized Network empty. Use Cloud SQL Auth proxy on all applications.","comment_id":"1113655","upvote_count":"1"}],"question_id":232,"answer_description":"","question_text":"You need to connect multiple applications with dynamic public IP addresses to a Cloud SQL instance. You configured users with strong passwords and enforced the SSL connection to your Cloud SQL instance. You want to use Cloud SQL public IP and ensure that you have secured connections. What should you do?","answers_community":["C (94%)","6%"],"answer_images":[],"answer":"C","choices":{"A":"Add CIDR 0.0.0.0/0 network to Authorized Network. Use Identity and Access Management (IAM) to add users.","D":"Add CIDR 0.0.0.0/0 network to Authorized Network. Use Cloud SQL Auth proxy on all applications.","B":"Add all application networks to Authorized Network and regularly update them.","C":"Leave the Authorized Network empty. Use Cloud SQL Auth proxy on all applications."},"unix_timestamp":1704370380},{"id":"1r6oO03rTTswzqXg3j5c","answer_images":[],"isMC":true,"discussion":[{"timestamp":"1704476100.0","poster":"raaad","upvote_count":"7","content":"Selected Answer: C\n- It addresses the likely issue: that the signed URLs have expired or are otherwise invalid. By creating a new TSV file with freshly generated signed URLs (with a longer validity period), you're ensuring that the Storage Transfer Service has valid authorization to access the files.\n- Splitting the TSV file and running parallel jobs might help in managing the workload more efficiently and overcoming any limitations related to the number of files or transfer speed.","comment_id":"1114705"},{"comment_id":"1146367","upvote_count":"6","timestamp":"1707576120.0","content":"C. Create a new TSV file for the remaining files by generating signed URLs with a longer validity period. Split the TSV file into multiple smaller files and submit them as separate Storage Transfer Service jobs in parallel.\n\nHere's why:\n\nHTTP 403 errors: These errors indicate unauthorized access, but since you verified the source system and signed URLs, the issue likely lies with expired signed URLs. Renewing the URLs with a longer validity period prevents this issue for the remaining files.\nSeparate jobs: Splitting the file into smaller chunks and submitting them as separate jobs improves parallelism and potentially speeds up the transfer process.\nAvoid manual intervention: Options A and D require manual intervention and complex setups, which are less efficient and might introduce risks.\nLonger validity: While option B addresses expired URLs, splitting the file offers additional benefits for faster migration.","poster":"srivastavas08"},{"content":"Selected Answer: C\nC. Create a new TSV file for the remaining files by generating signed URLs with a longer validity period. Split the TSV file into multiple smaller files and submit them as separate Storage Transfer Service jobs in parallel.\n\nThe HTTP 403 errors likely occurred because the signed URLs expired before the transfer could complete. By generating new signed URLs with a longer validity period, you can ensure that the URLs remain valid for the duration of the transfer. Splitting the TSV file into smaller files and submitting them as separate jobs can help manage the load and reduce the risk of timeouts or other issues","poster":"Pime13","upvote_count":"1","comment_id":"1337076","timestamp":"1736159880.0"},{"comment_id":"1260547","poster":"iooj","content":"Selected Answer: C\ngot this one on the exam, aug 2024, passed","upvote_count":"2","timestamp":"1722755580.0"},{"content":"Selected Answer: C\nOption C - agree with Raaad","comment_id":"1121871","timestamp":"1705164600.0","upvote_count":"3","poster":"Matt_108"},{"content":"Selected Answer: C\nC. Create a new TSV file for the remaining files by generating signed URLs with a longer validity period. Split the TSV file into multiple smaller files and submit them as separate Storage Transfer Service jobs in parallel.","poster":"scaenruy","comment_id":"1113646","timestamp":"1704369900.0","upvote_count":"2"}],"question_id":233,"unix_timestamp":1704369900,"timestamp":"2024-01-04 13:05:00","exam_id":10,"question_text":"You are migrating a large number of files from a public HTTPS endpoint to Cloud Storage. The files are protected from unauthorized access using signed URLs. You created a TSV file that contains the list of object URLs and started a transfer job by using Storage Transfer Service. You notice that the job has run for a long time and eventually failed. Checking the logs of the transfer job reveals that the job was running fine until one point, and then it failed due to HTTP 403 errors on the remaining files. You verified that there were no changes to the source system. You need to fix the problem to resume the migration process. What should you do?","question_images":[],"answer_description":"","topic":"1","answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/130319-exam-professional-data-engineer-topic-1-question-308/","answers_community":["C (100%)"],"choices":{"D":"Update the file checksums in the TSV file from using MD5 to SHA256. Remove the completed files from the TSV file and rerun the Storage Transfer Service job.","A":"Set up Cloud Storage FUSE, and mount the Cloud Storage bucket on a Compute Engine instance. Remove the completed files from the TSV file. Use a shell script to iterate through the TSV file and download the remaining URLs to the FUSE mount point.","C":"Create a new TSV file for the remaining files by generating signed URLs with a longer validity period. Split the TSV file into multiple smaller files and submit them as separate Storage Transfer Service jobs in parallel.","B":"Renew the TLS certificate of the HTTPS endpoint. Remove the completed files from the TSV file and rerun the Storage Transfer Service job."},"answer":"C"},{"id":"SN0uTInCdL4XfwFfOla4","question_text":"You work for an airline and you need to store weather data in a BigQuery table. Weather data will be used as input to a machine learning model. The model only uses the last 30 days of weather data. You want to avoid storing unnecessary data and minimize costs. What should you do?","answers_community":["B (95%)","5%"],"answer_ET":"B","isMC":true,"discussion":[{"upvote_count":"6","content":"Selected Answer: B\nPartitioned based on weather date, with partition expiration set","timestamp":"1706225400.0","poster":"AllenChen123","comment_id":"1132144"},{"comment_id":"1260548","poster":"iooj","upvote_count":"6","content":"Selected Answer: B\ngot this one on the exam, aug 2024, passed","timestamp":"1722755640.0"},{"poster":"juliorevk","timestamp":"1737818460.0","comment_id":"1346522","content":"Selected Answer: B\nB\n\nBQ partitioning with partition expiration of 30 days allows you to only filter for the last 30 days and delete days that are beyond 30 days.","upvote_count":"1"},{"poster":"d11379b","comment_id":"1182766","timestamp":"1711398180.0","upvote_count":"3","content":"https://cloud.google.com/bigquery/docs/partitioned-tables\nHere it mentions “ For TIMESTAMP and DATETIME columns, the partitions can have either hourly, daily, monthly, or yearly granularity.l\nSo you should not calculate the amount of partitions on second granularity"},{"comments":[{"content":"This is a good point","timestamp":"1710384720.0","comments":[{"upvote_count":"6","content":"It's not a good point. The granularity goes to DAYs, not SECONDs. So, the right answer is B.","timestamp":"1713167340.0","comment_id":"1195885","poster":"joao_01"}],"poster":"ce9e395","comment_id":"1173049","upvote_count":"1"}],"upvote_count":"1","content":"Selected Answer: D\nSkeptical about Option B as maximum partitions in a BQ table is 4000.Since Datetime value is a timestamp it will have more than 4000 values in a duration for 30 days (30*24*60*60 = 259,200 ). So Option D is right imo","timestamp":"1709883360.0","poster":"chambg","comment_id":"1168621"},{"poster":"JyoGCP","upvote_count":"1","comment_id":"1156320","content":"Selected Answer: B\nOption B","timestamp":"1708600440.0"},{"upvote_count":"4","content":"Selected Answer: B\nWe need the last 30 days, we don't care about ingestion time","timestamp":"1706876580.0","comment_id":"1138512","poster":"Sofiia98"}],"answer_images":[],"timestamp":"2024-01-26 00:30:00","question_id":234,"answer_description":"","choices":{"A":"Create a BigQuery table where each record has an ingestion timestamp. Run a scheduled query to delete all the rows with an ingestion timestamp older than 30 days.","B":"Create a BigQuery table partitioned by datetime value of the weather date. Set up partition expiration to 30 days.","C":"Create a BigQuery table partitioned by ingestion time. Set up partition expiration to 30 days.","D":"Create a BigQuery table with a datetime column for the day the weather data refers to. Run a scheduled query to delete rows with a datetime value older than 30 days."},"question_images":[],"unix_timestamp":1706225400,"url":"https://www.examtopics.com/discussions/google/view/132182-exam-professional-data-engineer-topic-1-question-309/","topic":"1","exam_id":10,"answer":"B"},{"id":"62PLkmIEiBjxpOg14641","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/17051-exam-professional-data-engineer-topic-1-question-31/","choices":{"C":"Create a new pipeline that has the same Cloud Pub/Sub subscription and cancel the old pipeline.","B":"Update the current pipeline and provide the transform mapping JSON object.","D":"Create a new pipeline that has a new Cloud Pub/Sub subscription and cancel the old pipeline.","A":"Update the current pipeline and use the drain flag."},"answer":"A","answers_community":["A (48%)","B (29%)","D (17%)","7%"],"question_id":235,"topic":"1","discussion":[{"content":"Correct Option : A\nExplanation:-This option is correct as the key requirement is not to lose\nthe data, the Dataflow pipeline can be stopped using the Drain option.\nDrain options would cause Dataflow to stop any new processing, but would\nalso allow the existing processing to complete","comments":[{"content":"To all the New Guys Here. Please don't get confused with all the people's fight over here. Just google the question and you will get the correct ans in many website. Still I recommend to refer this website for question. for this Particular problem ans is A. Reason is here --> https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python\nhave time to read the full page when to use Update using Json mapping and when to use Drain. (you will have question following for Drain option though).\nThumb rule is this,\n# If any major change to windowing transformation (like completely changing window fn from fixed to sliding) in Beam/Dataflow/you want to stop pipeline but want inflight data --> use Drain option.\n# For all other use cases and Minor changing to windowing fn (like just changing window time of sliding window) --> Use Update with Json mapping.\n\nIn this case it is Code change to new version. so, Update with Json mapping. Simple as that.\n\nAll the Best Guys.","upvote_count":"32","timestamp":"1638714480.0","poster":"BigQuery","comments":[{"content":"SORRY I MEANT TO SAY ANS IS 'B'. In this case it is Code change to new version. so, Update with Json mapping.","timestamp":"1638768180.0","upvote_count":"6","comments":[{"poster":"anji007","timestamp":"1647884640.0","comments":[{"timestamp":"1665297300.0","content":"JSON Mapping is a way to solve compatibility issues when updating","comment_id":"689944","upvote_count":"1","poster":"maxdataengineer"}],"upvote_count":"7","content":"Its clearly mentioned in the question that pipeline in compatible, if it is so you can not update with JSON mapping. Only way is to stop the pipeline with Drain and replace it with a new one. So the closest answer is A only.","comment_id":"572405"}],"poster":"BigQuery","comment_id":"494916"}],"comment_id":"494419"},{"content":"C and D are incorrect because canceling the old pipeline can cause data loss \nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline \nA is incorrect because updating pipeline does not include any drain flag\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline","comment_id":"437906","upvote_count":"7","poster":"sergio6","timestamp":"1630589400.0","comments":[{"content":"drain is in the guide ...stopping-a-pipeline. Just ...updating-a-pipeline is not enough to evaluate this question.\n\nthat's why drainnin is not a flag in a pipeline update. it is a process about how to stop a pipeline w/o data loss !\n\ndata in dataflow is in 3 stages. ingestion data, buffered data and in-flight data which is processing by old pipeline.","comment_id":"532205","poster":"Tanzu","upvote_count":"2","timestamp":"1643122080.0"},{"comments":[{"upvote_count":"2","content":"new pipeline is incompatible means, compatibility check will fail. so you wil not be able to update as new pipeline.\n\nthat's why B cannot be valid answer here in this context.","timestamp":"1643122200.0","comment_id":"532208","comments":[{"upvote_count":"2","timestamp":"1665297240.0","content":"B is a way to solve compatibility issues","comment_id":"689943","poster":"maxdataengineer"}],"poster":"Tanzu"}],"comment_id":"470255","timestamp":"1635601620.0","poster":"sergio6","content":"B is correct: Update the current pipeline and provide the transform mapping JSON object.\n Dataflow always performs a compatibility check between the old and new job and without the mapping (necessary as old and new are incompatible) it would give an error and the old job would continue to be executed\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Mapping\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#CCheck","upvote_count":"5"}]},{"comment_id":"143865","timestamp":"1595745840.0","poster":"VishalB","content":"Option C & D are incorrect as Cancel Option will lead to loose the data\nOption B is very Close, since the new Code make pipeline incompatible by providing transform mapping JSON file you can handle this","comments":[{"poster":"sergio6","content":"A is incorrect because updating pipeline does not include any drain flag","comments":[{"comment_id":"532211","comments":[{"content":"Yes but the compatibility problem will still be there, stopping the pipeline does not solve that","upvote_count":"1","comment_id":"689941","poster":"maxdataengineer","timestamp":"1665297180.0"}],"content":"two steps.. 1st drain the job w/ sdk or console. then, update the pipeline. cause it is OK to update a job while in draining","upvote_count":"2","timestamp":"1643122440.0","poster":"Tanzu"}],"comment_id":"470257","timestamp":"1635601740.0","upvote_count":"2"},{"content":"There are 5 update scenarios in a job in update-a-pipeline context. \na- changing transform name (requires mapping) , adding a new step (no need for mapping)\nb- windowing or triggering (only for minor changes, otherwise don't do that)\nc- coders (don't do that\nd- schema (adding or required to nullable is possible) other scenarios not possible\ne- stateful operations\n\nnone of them are relevant, here. cause there is no specific detail, secondly incompatible w. new pipeline. \n\nand mostly if in compatible only a has a solve. but not for all cases.\nso, drain == no data loss (ingesting, buffered and in-flight data) is the only scenario.","upvote_count":"6","poster":"Tanzu","comment_id":"532217","timestamp":"1643122860.0"}],"upvote_count":"3"},{"content":"As you said, Drain stops the pipeline but it does not solve the compatibility issue. The pipeline will not be able to be updated which is the core problem of the question.","comment_id":"689940","poster":"maxdataengineer","upvote_count":"3","comments":[{"poster":"assU2","upvote_count":"2","timestamp":"1668867780.0","content":"You do not want to lose any data when making this update - is the core problem. You are doing it ANYWAY.","comment_id":"722048"}],"timestamp":"1665297120.0"}],"timestamp":"1595745480.0","upvote_count":"80","comment_id":"143863","poster":"VishalB"},{"comments":[{"poster":"[Removed]","comments":[{"comment_id":"75087","poster":"arnabbis4u","timestamp":"1586990460.0","comments":[{"upvote_count":"3","timestamp":"1635601860.0","content":"Canceling the job will cause data loss, against the requirement","comment_id":"470259","poster":"sergio6"}],"content":"The job can be incompatible for reasons other than transformation changes. Since it is clearly mentioned that the change job is incompatible, I think we have to create a new job and D should be correct.","upvote_count":"9"}],"comment_id":"66268","timestamp":"1584709980.0","content":"Changing the pipeline graph without providing a mapping. When you update a job, the Dataflow service attempts to match the transforms in your prior job to the transforms in the replacement job in order to transfer intermediate state data for each step. If you've renamed or removed any steps, you'll need to provide a transform mapping so that Dataflow can match state data accordingly.","upvote_count":"4"}],"upvote_count":"24","content":"Correct B - https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#preventing_compatibility_breaks","timestamp":"1584709980.0","poster":"[Removed]","comment_id":"66267"},{"timestamp":"1734102180.0","poster":"dans_puts","content":"Selected Answer: D\nD. Create a new pipeline that has a new Cloud Pub/Sub subscription and cancel the old pipeline:\n\nBy creating a new subscription, the new pipeline will consume messages independently of the old pipeline. This ensures no data is lost as messages published to Pub/Sub are delivered to all subscriptions.\nOnce the new pipeline is verified to be running as expected, the old pipeline can be safely canceled.","comment_id":"1326167","upvote_count":"1"},{"poster":"Smakyel79","timestamp":"1732120380.0","content":"Why option D is better for this case - In this scenario: the pipeline is incompatible with the old one; running the two pipelines concurrently ensures no data loss and allows for easier debugging of the new pipeline; a new subscription ensures the old pipeline can finish processing its messages while the new pipeline starts fresh","upvote_count":"1","comment_id":"1315389"},{"upvote_count":"1","timestamp":"1732119900.0","content":"Draining a pipeline stops it in an orderly manner but does not address incompatibility issues. Once the pipeline is drained, no more data is processed, and the new pipeline starts fresh. This can lead to data loss if there are messages in Pub/Sub that the drained pipeline didn't process.","comment_id":"1315379","poster":"Smakyel79"},{"upvote_count":"2","timestamp":"1706268480.0","comment_id":"1132471","content":"Option: C\nusing draining will stop the subscription totally while allowing the existing data to complete processing. While the pipeline is stopped, will lose streaming data. The best option is to create a new pipeline that is connected to the same subscription, then we can apply drain to the old pipeline and end it. That way we will capture all the streaming data.","poster":"philli1011"},{"comment_id":"1098051","timestamp":"1702721160.0","comments":[{"upvote_count":"1","content":"While other options might work in some cases, they have drawbacks:\n\nB. Transform mapping JSON: This option is mainly for schema changes and doesn't guarantee data completion before shutdown.\nC. New pipeline, same subscription: This risks duplicate processing of data if both pipelines run concurrently.\nD. New pipeline, new subscription: This loses the current pipeline's state and potentially data, making it impractical for incompatible changes.\nTherefore, the most reliable and data-safe approach is to update the current pipeline with the drain flag for seamless transition and data integrity.\n\nRemember, always test updates in a staging environment before deploying to production.","timestamp":"1702721160.0","poster":"MaxNRG","comment_id":"1098052"}],"content":"Selected Answer: A\nDrain flag: This flag allows the pipeline to finish processing all existing data in the Pub/Sub subscription before shutting down. This ensures no data is lost during the update.\nCurrent pipeline: Updating the current pipeline minimizes disruption and avoids setting up entirely new infrastructure.\nIncompatible changes: Even with incompatible changes, the drain flag ensures existing data is processed correctly.","upvote_count":"1","poster":"MaxNRG"},{"comment_id":"1096406","timestamp":"1702553760.0","content":"Selected Answer: C\nSame Cloud Pub/Sub Subscription: By using the same Cloud Pub/Sub subscription for the new pipeline, you ensure that no messages are lost during the transition. Pub/Sub manages message delivery, ensuring that unacknowledged messages (those that haven't been processed by your old pipeline) will be available for the new pipeline to process.\n\nCreating a New Pipeline: Since the update makes the new pipeline incompatible with the current version, it's necessary to create a new pipeline. Attempting to update the current pipeline in place (options A and B) would not be feasible due to compatibility issues.\n\nCancel the Old Pipeline: Once the new pipeline is up and running and processing messages, you can safely cancel the old pipeline. This ensures a smooth transition with no data loss.","upvote_count":"2","poster":"TVH_Data_Engineer"},{"upvote_count":"1","content":"In order to make an update to a Google Cloud Dataflow streaming pipeline without losing any data, the recommended approach is:\n\nA. Update the current pipeline and use the drain flag.\n\nExplanation:\n\nThe drain flag is designed to allow the current pipeline to finish processing any remaining data before shutting down. This helps ensure that no data is lost during the update process.\nBy updating the current pipeline and using the drain flag, you allow the pipeline to complete its current processing before the update takes effect, minimizing the risk of data loss.\nThis approach is a safe way to transition from the old version to the new version without interrupting data processing.","timestamp":"1701701460.0","poster":"JOKKUNO","comment_id":"1087725"},{"comment_id":"1076383","upvote_count":"1","poster":"axantroff","content":"I would vote for A because of the structure of the exam, but there are other options worth considering as well","timestamp":"1700577960.0"},{"content":"Selected Answer: C\nMy answer is C. Chatted with ChatGPT and narrowed down on this option. Let me know your thoughts on this perspective.\nOption C - By using the existing subscription, you can ensure that the data flow remains uninterrupted, and there is no loss of data during the transition from the old pipeline to the new one.\n\nCreating a new pipeline that uses the same Cloud Pub/Sub subscription allows for a seamless transition without any interruptions to the data flow. This approach ensures that the new pipeline can continue to consume data from the same subscription as the old pipeline, thereby maintaining data continuity throughout the update process.","poster":"RT_G","timestamp":"1699396920.0","upvote_count":"1","comment_id":"1065203"},{"comment_id":"1064932","content":"Selected Answer: A\nCorrect Option : A\nExplanation:-This option is correct as the key requirement is not to lose\nthe data, the Dataflow pipeline can be stopped using the Drain option.","poster":"rocky48","timestamp":"1699370640.0","upvote_count":"1"},{"comment_id":"1053171","upvote_count":"1","content":"It should be B\nDrain will stop the existing job only and it does not suffice the updated schema.\nIn order to bring updated schema into effect, updated JSON mapping need to be applied.","poster":"mk_choudhary","timestamp":"1698181800.0"},{"content":"The two cores of this question are: 1- Don't lose data ← Drain, is perfect for this because you process all buffer data and stop reviving messages; normally this message is alive for 7 days of retry, so when you start a new job you will receive all without lose any data. 2- Incompatible new code ← mapping solve some incompatibilities like change name of a ParDO but no a version issue. So launch a new job with the new code, it's the only option.\nSo, option is A.","comment_id":"1042002","poster":"Simhamed2015","upvote_count":"1","timestamp":"1697134980.0"},{"content":"The best choice is D. Create a new pipeline that has a new Cloud Pub/Sub subscription and cancel the old pipeline.","poster":"imran79","comment_id":"1027200","upvote_count":"1","timestamp":"1696667400.0"},{"comment_id":"955299","content":"Selected Answer: A\nAnswer is A \n\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain","upvote_count":"1","poster":"Mathew106","timestamp":"1689678960.0"},{"comment_id":"948017","content":"You don't want to lose data, so drain dude, drain!","timestamp":"1688987220.0","upvote_count":"1","poster":"itsmynickname"},{"comment_id":"879203","timestamp":"1682330280.0","poster":"Oleksandr0501","content":"Option C is the correct answer because creating a new pipeline with the same Cloud Pub/Sub subscription and canceling the old pipeline allows for a seamless transition without any data loss. The new pipeline can consume from the same subscription as the old pipeline and continue processing data, while the old pipeline can be safely stopped without impacting data ingestion. This approach ensures continuity of data processing while updating the pipeline code.","upvote_count":"1"},{"poster":"izekc","comment_id":"867217","upvote_count":"1","timestamp":"1681210500.0","content":"Selected Answer: A\nA is correct since it is the first step to prevent data loss"},{"upvote_count":"1","timestamp":"1678449660.0","comment_id":"834932","content":"https://cloud.google.com/blog/products/gcp/managing-streaming-pipelines-in-google-cloud-dataflow-just-got-better","poster":"vivek123etl"},{"timestamp":"1677653280.0","poster":"mahdiiii","comment_id":"825585","upvote_count":"1","content":"C: \nCreate a new pipeline that has the same Cloud Pub/Sub subscription and cancel the old pipeline.\n\nIf the update to the Cloud Dataflow pipeline will make it incompatible with the current version, it is best to create a new pipeline that has the same Cloud Pub/Sub subscription and then cancel the old pipeline. This approach ensures that you do not lose any data while still being able to perform the necessary update."},{"poster":"musumusu","comment_id":"819498","comments":[{"poster":"musumusu","content":"You can't *update * the pipeline. Please check the docs","comment_id":"819500","upvote_count":"1","timestamp":"1677174000.0"}],"upvote_count":"1","content":"Answer C:\nAccording to latest docs you can update an existing batch pipleine, \nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python_1\n\ncreate new one with same subscription. And start getting the data as before and once its ready, remove earlier one. \nI will go with C","timestamp":"1677173940.0"},{"content":"Selected Answer: D\nExplained below.","upvote_count":"2","comment_id":"816750","comments":[{"content":"This is a tricky question.\n1. Because of incompatibility, you just cannot \"UPDATE\", plus, for update method, there is never a \"DRAIN\" flag. \n2. When you talk about a flag, it is like a parameter attached to the \"UPDATE\" method, it is not the same as stopping the pipeline with Drain method. \n3. Option D is not the best because you will need to reprocess duplicated old data, HOWEVER, you will not LOSE any data. It says must not lose any data, it did not say must not lose \"in-flight\" data.\n4. I see many comments jumped on the \"CANCEL\" word in C&D, but don't forget, although you did not \"DRAIN\", the data will not be lost with new subscription.\n5. In this particular question, I believe general consensus might not be a wise one.","upvote_count":"2","poster":"[Removed]","timestamp":"1676993760.0","comment_id":"816751"}],"poster":"[Removed]","timestamp":"1676993760.0"},{"upvote_count":"1","poster":"midgoo","content":"Selected Answer: B\nThere is no such thing as 'drain flag' for update. It is for stopping the pipeline. It could work too but the answer has to be rephrased","comment_id":"816337","timestamp":"1676966520.0"},{"timestamp":"1676289600.0","upvote_count":"1","comment_id":"807352","poster":"hellobot","content":"Correct Option: A\nBut practically, D can be correct too we will not lose any data here as per requirement, only problem here is to update all connected services to use the new subscription that were getting messages from pub/sub topic. \nOption B: is not good approach as json transform mapping is used for specifying the mapping between input data and output data, which does not address our issue because we an have version issue and it does not ensure that data will not be lost during update."},{"content":"Selected Answer: A\nDraining is correct","comment_id":"787468","upvote_count":"1","poster":"waiebdi","timestamp":"1674640380.0"},{"content":"Selected Answer: A\nYou do not want to lose any data, so you should drain the pipeline instead of just cancelling it. Since the only option that includes draining the pipeline, the answer should be A.","upvote_count":"1","comment_id":"771077","timestamp":"1673328120.0","poster":"ler_mp"},{"upvote_count":"1","comment_id":"769001","content":"Selected Answer: B\nUpdate command can only be done if the replacement pipeline is compatible with existing pipeline. the problem state very clear that the replacement pipeline is incompatible, thus Option A is out of choice. \n\nThe problem does say that they dont want to lose any of the processing data, thus provide JSON mapping object to deal with incompatible is correct choice here.","poster":"korntewin","timestamp":"1673139660.0"},{"upvote_count":"1","content":"Selected Answer: A\nA - Drain a job. This method applies only to streaming pipelines. Draining a job enables the Dataflow service to finish processing the buffered data while simultaneously ceasing the ingestion of new data. For more information, see Draining a job.","comment_id":"755895","timestamp":"1671991800.0","poster":"Brillianttyagi"},{"content":"B is the answer\nWhen transfer forming mapping, the data will be buffered temporarily. it will be transferred to the new job after the update is completed.","poster":"adelynllllllllll","timestamp":"1671630540.0","upvote_count":"1","comment_id":"752384"},{"comment_id":"750083","timestamp":"1671471180.0","poster":"Krish6488","content":"Selected Answer: B\nThe answer seems to be B to me. Dataflow automatically performs any compatibility checks when the code is updated. Inflight data processing is also taken care. There could be a small amount of downtime while the new jobs start post the compatibility checks. However this process means that new incoming data may not be applied with the revised transformations. But since the question is about not losing the data, the answer is B","upvote_count":"2"},{"content":"Selected Answer: B\nB is right.","comment_id":"749947","upvote_count":"1","timestamp":"1671462060.0","poster":"Prakzz"},{"poster":"Nirca","content":"Selected Answer: A\nCorrect Option : A","upvote_count":"1","comment_id":"744063","timestamp":"1670937780.0"},{"upvote_count":"2","poster":"DGames","comment_id":"743497","timestamp":"1670897040.0","content":"Selected Answer: D\nAnswer - D Why ? after reading all discussion point and then re-read the question again , might I am thinking incorrect, Here is below point are mention in question. \n1 . New Pipeline - means this is not current pipeline where we need to apply drain option. Yes in question it misleading statement mention about data lost.\n2. Option B - Why you need to update current pipeline with JSON, No sense to update.\n3. Option C - Old subscription already in use with current Pipeline.\nSO I will go Option D."},{"upvote_count":"1","comment_id":"736915","poster":"odacir","timestamp":"1670338320.0","content":"Selected Answer: A\nThere are two cores in this question.\n1- Don't lose data ← Drain, is perfect for this because you process all buffer data and stop reviving messages; normally this message is alive for 7 days of retry, so when you start a new job you will receive all without lose any data.\n2- Incompatible new code ← mapping solve some incompatibilities like change name of a ParDO but no a version issue. So launch a new job with the new code, it's the only option. \nSo, option is A."},{"content":"The below two links shows the answer as A. https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline and https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline","upvote_count":"2","poster":"Thasni","comment_id":"723333","timestamp":"1669022160.0"},{"timestamp":"1668957840.0","content":"A\n\nhttps://stackoverflow.com/questions/74012067/how-to-update-a-dataflow-incompatible-pipeline-without-loosing-in-data","upvote_count":"2","poster":"Fabius","comment_id":"722726"},{"upvote_count":"1","content":"Selected Answer: A\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline","timestamp":"1667937240.0","comment_id":"714090","poster":"RMK2000"},{"poster":"wan2three","upvote_count":"1","comment_id":"713918","content":"B should be the answer\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Mapping:~:text=templates%20for%20instructions.-,The%20update%20process%20and%20its%20effects,-When%20you%20update","timestamp":"1667920200.0"},{"content":"Selected Answer: B\nAnswer is B: see links and text below from GCP website.\n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#java_3\nPreventing compatibility breaks:\nChanging the pipeline graph without providing a mapping. When you update a job, the Dataflow service attempts to match the transforms in your prior job to the transforms in the replacement job in order to transfer intermediate state data for each step. If you rename or remove any steps, you need to provide a transform mapping so that Dataflow can match state data accordingly.","poster":"RMK2000","timestamp":"1667827200.0","comments":[{"upvote_count":"2","comment_id":"714089","poster":"RMK2000","timestamp":"1667937180.0","content":"Apologies, I take back my vote and comment. The previous answer applies not to this scenraio where Pub/Sub is a source. Answer A makes sense. https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline"}],"comment_id":"713061","upvote_count":"1"},{"comment_id":"712165","upvote_count":"1","timestamp":"1667713680.0","poster":"Troezon","content":"Correct Option - A\n\nAs per documentation at https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Mapping\n\nyou can update existing streaming pipeline by passing the --update flag and mapping options."},{"upvote_count":"1","timestamp":"1666301520.0","comment_id":"700312","content":"A drained flag is the right option for this case","poster":"kennyloo"},{"content":"Selected Answer: A\nA. For streaming pipelines and to prevent data loss per doc: https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain:~:text=If%20you%20want%20to%20prevent%20data%20loss","upvote_count":"1","timestamp":"1666121940.0","poster":"Hoham","comment_id":"698482"},{"timestamp":"1665445260.0","comment_id":"691585","poster":"devaid","upvote_count":"1","content":"Selected Answer: B\nCan't be C or D (data lost). A can't be because don't exist a drain flag for updating a pipeline."},{"comment_id":"688955","upvote_count":"1","timestamp":"1665187620.0","poster":"devaid","content":"Selected Answer: B\nAnswer: B. \nOptions C & D are discarded because if you cancel, you losse data. Option A can't be because the update itself won't stop/drain your job, it will save the in-flight data states. According to google, refering to the update function: \"The replacement job will preserve any intermediate state data from the prior job. Note, however, that changing the windowing or triggering strategies will not affect data that's already buffered or already being processed by the pipeline. In-flight data will still be processed by the transforms in your new pipeline. Additional transforms that you add in your replacement pipeline code may or may not take effect, depending on where the records are buffered.\""},{"upvote_count":"1","poster":"Remi2021","content":"Selected Answer: B\nbe is right answer. According to documentation:\nThe replacement job preserves any intermediate state data from the prior job, as well as any buffered data records or metadata currently \"in-flight\" from the prior job. For example, some records in your pipeline might be buffered while waiting for a window to resolve. So since we are not loosing data option A lost in value and therefore B is best!","comment_id":"667237","timestamp":"1663000920.0"},{"comment_id":"653257","content":"Selected Answer: A\nAccording to the Cloud SDK: https://cloud.google.com/sdk/gcloud/reference/dataflow/jobs/drain\n\nOnce Drain is triggered, the pipeline will stop accepting new inputs. The input watermark will be advanced to infinity. Elements already in the pipeline will continue to be processed. Drained jobs can safely be cancelled.","timestamp":"1661742060.0","poster":"Dan137","upvote_count":"2"},{"timestamp":"1661723640.0","content":"Selected Answer: A\nVote for A\nAny question with Dataflow that avoiding missing in-flight data => Drain","upvote_count":"1","poster":"ducc","comment_id":"653173"},{"comment_id":"619241","poster":"noob_master","content":"Selected Answer: D\nD is the option: the creation of the new subscription for the new pipeline is the best option. The data will be processed in the new and old pipeline in the same time - Yes, there is a risk of duplication of data - but 0% will be lost in the process.\n\n(because there is no limitation for duplicate data processing, D is best than A in my point of view)","timestamp":"1655726700.0","upvote_count":"1"},{"poster":"FrankT2L","upvote_count":"1","content":"Selected Answer: D\nWe have to stop the pipeline in DRAIN mode and make the changes but there is no option in the responses.\n\nSo the only possibility is to create the new pipeline attached to a new Pub / Sub subscription and delete the old pipeline.","comment_id":"613269","timestamp":"1654693140.0"},{"poster":"alecuba16","content":"Selected Answer: A\nIncompatible -> B is not a viable option, then A.","upvote_count":"1","timestamp":"1652185200.0","comment_id":"599581"},{"poster":"alecuba16","timestamp":"1650456300.0","comment_id":"588636","content":"Selected Answer: A\nAs google states into their best practises, the best way to go is B, but unfortunatelly it is only possible when the upgrade is compatible. As the question indicates that it is incompatible, we have to drain the pipeline and stop reading messages from pub/sub (stop the subscription) , save the current in flight processes and then update the pipeline, not CANCEL. so C and D are not correct because you will loose the data that is being processed right now.","comments":[{"timestamp":"1667967420.0","comment_id":"714281","poster":"vetaal","upvote_count":"1","content":"But the source is pub/sub. You won't lose data once you create a subscription to the topic, and then cancel, as the new pipeline should pick it up.\n\nAlso, updating the pipeline won't be possible as it would fail the compatibility check."}],"upvote_count":"1"},{"upvote_count":"2","comment_id":"587696","timestamp":"1650288120.0","poster":"tavva_prudhvi","content":"Updating Dataflow Pipelines\n- Scenario: Update streaming Dataflow pipeline with new code:\n- New code = new pipeline not compatible with current version\n- Need data to switch over to new job/pipeline without losing\nanything in the process\n- Solution: Update job:\n- Creates new job with same name/new jobID\n- Compatibility between old/new jobs:\n- Map old to new job transforms with transform mapping\n- \"Bridge\" between old and new code base\n- After compatibility check:\n- Buffered data transferred to new job, using transform\nmapping to translate changes"},{"comment_id":"581671","upvote_count":"1","poster":"tavva_prudhvi","timestamp":"1649229960.0","content":"Guys, drain option doesn't update the pipeline it helps us finishing processing buffered jobs before shutting down. Please refer to this link, https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain\n\nComing to, C & D https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#important_information_on_canceling_a_job this look tells us there might be a data loss! But, as the data jobs are compatible as stated in the question the pipeline might not get updated too, so I was stuck here.","comments":[{"timestamp":"1672200960.0","content":"yes but option says update the current pipeline along with drain flag.","upvote_count":"1","poster":"Wonka87","comment_id":"759346"}]},{"content":"Selected Answer: B\nB\nPlease read:\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline\n\n1) not A as Drain it is not updating it is stopping option\n2) not C and D because canceling the old pipeline can cause data loss\n3) B is correct as we have next info in https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline :\n\nIf your replacement pipeline has changed any transform names from those in your prior pipeline, the Dataflow service requires a transform mapping. The transform mapping maps the named transforms in your prior pipeline code to names in your replacement pipeline code. You can pass the mapping by using the --transform_name_mapping command-line option, using the following general format:\n--transform_name_mapping= .\n{\"oldTransform1\":\"newTransform1\",\"oldTransform2\":\"newTransform2\",...} (it is a json object)","poster":"Arkon88","comment_id":"560144","timestamp":"1646319900.0","upvote_count":"3"},{"comment_id":"559546","poster":"rafaelgildin","timestamp":"1646236740.0","content":"Correct Option : B\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python_2","upvote_count":"1"},{"poster":"RRK2021","timestamp":"1645066860.0","content":"Correct Option : B","upvote_count":"1","comment_id":"549110"},{"poster":"Novice1308","upvote_count":"1","content":"Important keyword here is \"Incompatible\". According to docs \"The compatibility check ensures that things like intermediate state information and buffered data can be transferred from your prior job to your replacement job.\" Incompatible word clearly states that the current buffer data is not compatible with your new job. In this case, draining the current data is the only option left.\nI would surely go for \"A\"","timestamp":"1644393900.0","comment_id":"543607"},{"comment_id":"523137","timestamp":"1642108800.0","poster":"sraakesh95","content":"Selected Answer: A\n@VishalB","upvote_count":"1"},{"upvote_count":"1","poster":"sandbar_dorados_09","content":"Selected Answer: A\nSince the question states the changes to the pipeline are \"incompatible with the current version\", the right approach would be to stop the existing pipeline using the drain flag to minimize data loss, then launch a replacement job with the same job name using the new code. If the new code was compatible with the existing pipeline, B would be correct (a transform mapping is provided when updating an ongoing pipeline). Since the new code is incompatible with the current version, A is the answer.","timestamp":"1640830440.0","comment_id":"512941"},{"content":"Option B looks correct,it talks about updating the pipeline and also other part is it is indirectly posing how you can arrest incompatibility","poster":"prasanna77","timestamp":"1640676420.0","comment_id":"510879","upvote_count":"1"},{"content":"Correct Option : B \n100% \nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#:~:text=If%20any%20transform%20names%20in%20your%20pipeline%20have%20changed%2C%20you%20must%20supply%20a%20transform%20mapping%20and%20pass%20it%20using%20the%20--transform_name_mapping%20option","upvote_count":"2","timestamp":"1639663440.0","poster":"freeman1715","comment_id":"502968"},{"upvote_count":"2","comment_id":"494420","poster":"BigQuery","timestamp":"1638714540.0","content":"Selected Answer: A\nTo all the New Guys Here. Please don't get confused with all the people's fight over here. Just google the question and you will get the correct ans in many website. Still I recommend to refer this website for question. for this Particular problem ans is A. Reason is here --> https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python\nhave time to read the full page when to use Update using Json mapping and when to use Drain. (you will have question following for Drain option though).\nThumb rule is this,\n# If any major change to windowing transformation (like completely changing window fn from fixed to sliding) in Beam/Dataflow/you want to stop pipeline but want inflight data --> use Drain option.\n# For all other use cases and Minor changing to windowing fn (like just changing window time of sliding window) --> Use Update with Json mapping.\n\nIn this case it is Code change to new version. so, Update with Json mapping. Simple as that.","comments":[{"content":"SORRY I MEANT TO SAY ANS IS 'B'. In this case it is Code change to new version. so, Update with Json mapping.","comment_id":"494917","timestamp":"1638768240.0","upvote_count":"2","poster":"BigQuery"}]},{"content":"ANSWER IS B. BECAUSE WHILE UPDATING A JOB, DATAFLOW NEVER LOOSES INTERMEDIATE DATA. 100%","timestamp":"1637460180.0","comment_id":"482979","poster":"Abhi16820","upvote_count":"2"},{"poster":"MaxNRG","comment_id":"478163","comments":[{"content":"Using the Drain option to stop your job tells the Cloud Dataflow service to finish your job in its current state. Your job will immediately stop ingesting new data from input sources. However, the Cloud Dataflow service will preserve any existing resources, such as worker instances, to finish processing and writing any buffered data in your pipeline. When all pending processing and write operations are complete, the Cloud Dataflow service will clean up the GCP resources associated with your job.\nNote: Your pipeline will continue to incur the cost of maintaining any associated GCP resources until all processing and writing has completed.\nUse the Drain option to stop your job if you want to prevent data loss as you bring down your pipeline.","comments":[{"content":"Effects of draining a job: When you issue the Drain command, Cloud Dataflow immediately closes any in-process windows and fires all triggers. The system does not wait for any outstanding time-based windows to finish. For example, if your pipeline is ten minutes into a two-hour window when you issue the Drain command, Cloud Dataflow won't wait for the remainder of the window to finish. It will close the window immediately with partial results.","comment_id":"478165","timestamp":"1636900200.0","upvote_count":"3","poster":"MaxNRG"}],"comment_id":"478164","timestamp":"1636900200.0","poster":"MaxNRG","upvote_count":"2"}],"content":"A - the key requirement is not to lose the data, the Dataflow pipeline can be stopped using the Drain option. This option would cause Dataflow to stop any new processing, but would also allow the existing processing to complete:\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline","timestamp":"1636900140.0","upvote_count":"1"},{"timestamp":"1634037420.0","content":"I would go with option A\n\nCompatible means, job that will pass compatibility check eventually. If it is the case as per https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Launching, --update flag & mapping etc are enough.\n\nIf it is not compatible, then new job may fail to pass Compatibility check and hence only way would be to drain and restart new job see: https://stackoverflow.com/questions/66475443/google-cloud-dataflow-update-pipeline\n\nC & D is not talking about inflight data within the existing pipeline...or about transformations within the pipeline (idempotent or not etc). If it is idempotent then may be we can allow more than once processing and cancel the old pipeline.","comment_id":"461044","poster":"anji007","upvote_count":"2"},{"content":"A is correct:\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline\nYou can update a pipeline that is being drained, without data loss.","timestamp":"1632015540.0","poster":"hadoopdk","comment_id":"447340","upvote_count":"4"},{"comments":[{"comment_id":"442790","upvote_count":"1","timestamp":"1631329920.0","poster":"kubosuke","content":"'A' looks like correct, but I don't understand the meaning 'drain flag'. what's this?"},{"timestamp":"1632839340.0","content":"The inflight messages are truly lost as their are acknowledge at the input of dataflow job and as such they won't be retried by pub/sub","upvote_count":"2","poster":"retep007","comment_id":"453468"},{"content":"True but you must be sure that older subscriptions data has been already process by the original pipeline.","timestamp":"1631459340.0","upvote_count":"2","poster":"yoshik","comment_id":"443532"}],"upvote_count":"4","timestamp":"1631329800.0","comment_id":"442789","content":"vote for D.\nsome people say that \"it might lose in-flight data\", but don't worry, new subscription has same data. if your app is idempotent, it's okay.\n\nTopic\n└─ Old Subscription <-- cancel here it might lose in-flight data\n└─ New Subscription <-- but the new subscription has the same data","poster":"kubosuke"},{"content":"i think its D. And not B bcaz - If the compatibility check fails, your prior job will continue running on the Dataflow service and your replacement job will return an error.","upvote_count":"4","comment_id":"424835","timestamp":"1628954160.0","poster":"ron123_aa"},{"poster":"safiyu","upvote_count":"2","timestamp":"1628585520.0","content":"Correct Option is A. Question mentions not to lose any data. All other options may lead to some data loss. If you want to prevent data loss as you bring down your streaming pipelines, the best option is to drain your job.","comment_id":"422578"},{"timestamp":"1625394240.0","upvote_count":"1","content":"Correct is A : https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain","comments":[{"upvote_count":"1","content":"precisely, stopping the pipeline, not updating","comment_id":"470264","poster":"sergio6","timestamp":"1635602160.0","comments":[{"timestamp":"1635602220.0","comment_id":"470265","upvote_count":"1","poster":"sergio6","content":"So A can't be correct"}]}],"poster":"timolo","comment_id":"398214"},{"poster":"sumanshu","timestamp":"1624742640.0","content":"Vote for 'B'\n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python","upvote_count":"2","comments":[{"upvote_count":"1","content":"I guess, 'A' is correct....","comment_id":"401875","poster":"sumanshu","timestamp":"1625745060.0"}],"comment_id":"391574"},{"poster":"moonlightbeamer","timestamp":"1624242120.0","comments":[{"content":"No, by using cancel option, you will loose the data which already read by pub/sub and which was in processing state...So C & D both wrong","upvote_count":"1","comment_id":"401877","poster":"sumanshu","timestamp":"1625745120.0"}],"upvote_count":"1","comment_id":"386698","content":"Answer: D\n\nwhen the question clearly state new pipeline is incompatible, updating existing pipeline will FAIL. and in D, a different subscription will not cause data loss when you cancel the existing pipeline/subscription. they are independently subscribing to the same topic. there is no reason Choice D won't work."},{"poster":"Ette","timestamp":"1620311940.0","upvote_count":"4","comment_id":"351092","content":"Correct C: \nA and B are not correct: the pipeline cannot be updated since it is incompatible.\nThe old pipeline should be canceled with the drain flag so all messages already read from pubsub will be processed til the end. If you use the same subscription then all messages that were not acked by the old pipeline will be held until the new pipeline reads it."},{"comments":[{"content":"The new Pipeline is incompatible so \"compatibility check \" will fail","comment_id":"406938","upvote_count":"1","poster":"EricM22","timestamp":"1626339540.0"}],"upvote_count":"2","poster":"gcper","content":"B. Update the current pipeline and provide the transform mapping JSON object.\n\n\"When you update your job, the Dataflow service performs a compatibility check between your currently-running job and your potential replacement job. The compatibility check ensures that things like intermediate state information and buffered data can be transferred from your prior job to your replacement job.\"\n\n\"When you update a job on the Dataflow service, you replace the existing job with a new job that runs your updated pipeline code.\"\n\nsource: https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline","comment_id":"321048","timestamp":"1616756580.0"},{"upvote_count":"1","timestamp":"1613031000.0","content":"With the small experience I have, looking at the alternatives I would go for D. But investigating the B option closer it seems that this link is supporting B as the answer: https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline","comment_id":"288121","poster":"kdiab"},{"timestamp":"1612712820.0","poster":"naga","comment_id":"285604","upvote_count":"3","content":"Correct D"},{"content":"Option A: Drain is the feature for avoiding any loss of data. Option C & D are incorrect as Cancel Option will lead to loose the data","poster":"someshsehgal","comment_id":"282636","timestamp":"1612347000.0","upvote_count":"3","comments":[{"upvote_count":"2","timestamp":"1615625100.0","comment_id":"309553","poster":"razerlg","content":"You should prepare better this exam."}]},{"timestamp":"1607989200.0","comment_id":"244133","poster":"NamitSehgal","upvote_count":"3","content":"After reading comments, my answer is more inclined to D instead B."},{"timestamp":"1607029440.0","upvote_count":"8","comment_id":"234368","content":"Answer is D.\n* A and B are not possible, since the new job is not compatible.\n* C might lead to lost data.\n* D might lead to data being processed twice, but no data will be lost.\nBetter would usually be to drain and start a new pipeline.","comments":[{"upvote_count":"1","comments":[{"timestamp":"1676993220.0","comment_id":"816737","content":"This is a tricky question, because of incompatibility, you just cannot \"UPDATE\", plus, for update method, there is never a \"DRAIN\" flag. When you talk about a flag, it is like a parameter attached to the \"UPDATE\", it is not the same of stop the pipeline with Drain method. Option D is not the best because you will need to reprocess duplicated old data, HOWEVER, you will not LOSE any data.","poster":"[Removed]","upvote_count":"1"}],"content":"Truth sometimes is in the hand of minority.","timestamp":"1676992980.0","comment_id":"816728","poster":"[Removed]"}],"poster":"IKGx1iGetOWGSjAQDD2x3"},{"upvote_count":"2","content":"To me C is correct. Using same subscribtion means that new pipeline will get nect message from pubsub. When we Cancel old pipeline Dataflow will not sent ASK message to pub/sub and message will be resend to new pipeline when it's free. Option D can lead to data duplication.","poster":"sfsdeniso","comment_id":"229547","timestamp":"1606558560.0"},{"comment_id":"222134","poster":"ppsev","timestamp":"1605724320.0","content":"I think option D is the correct one, since it is using pub/sub and this could be usefull to perform a new subscription for our new pipeline. I think it has to be a *new* pipeline and not an update of the old one, because it's stated that the code update makes the new pipeline incompatible with the old one. The only thing I would change is that it should perform a drain operation instead of cancellation.","upvote_count":"4"},{"content":"At first glance, I thought the answer was A but after after reading more of google cloud documentation, I go with answer B. \nSee below documentation, \n\n\"When you launch your replacement job, the Dataflow service performs a compatibility check between your replacement job and your prior job. If the compatibility check passes, your prior job will be stopped. Your replacement job will then launch on the Dataflow service while retaining the same job name. If the compatibility check fails, your prior job will continue running on the Dataflow service and your replacement job will return an error.\"\n\nIn this case, our prior job is still running and we are not losing any data. we need to fix the replacement job for which we have compatibility issue. Though there can be may reasons for compatibility issue, google docs talks about transform mapping so that Dataflow can match state data accordingly.\nRemember, Drain and cancel option will cease all the data ingestion services and we dont want that to happen. If the replacement job is fixed, data flow will take care of the replacement as mentioned https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline","comment_id":"220836","poster":"Radhika7983","timestamp":"1605590880.0","upvote_count":"8","comments":[{"comment_id":"397726","timestamp":"1625328120.0","poster":"awssp12345","content":"Appreciate the detailed explanation. your answer makes sense.","upvote_count":"1"}]},{"content":"B is the best practice , Similar question is there in Linux Academy","poster":"Tanmoyk","timestamp":"1602046920.0","upvote_count":"3","comment_id":"194863"},{"upvote_count":"5","timestamp":"1600736760.0","comment_id":"184088","content":"Correcta A. with de drain option to stop your job but all pending processing and write operation are complete","poster":"yurstev"},{"content":"When you update a job on the Dataflow service, you replace the existing job with a new job that runs your updated pipeline code. The Dataflow service retains the job name, but runs the replacement job with an updated Job ID.\n\nThe replacement job preserves any intermediate state data from the prior job, as well as any buffered data records or metadata currently \"in-flight\" from the prior job. For example, some records in your pipeline might be buffered while waiting for a window to resolve.\n\nso B , seems right","comment_id":"175188","upvote_count":"2","timestamp":"1599479040.0","poster":"nehaxlpb"},{"timestamp":"1599053100.0","content":"Prefer B.\nBut I think the confusion is the term \"imcompatible\"... My understanding is it is indicating that some steps are added/removed, so the mapping is required.","poster":"KennneK","comment_id":"172073","upvote_count":"2"},{"upvote_count":"2","comment_id":"161945","timestamp":"1597897440.0","poster":"atnafu2020","content":"D\nPass the --update option.\nSet the --jobName option in PipelineOptions to the same name as the job you want to update.\nIf any transform names in your pipeline have changed, you must supply a transform mapping and pass it using the --transformNameMapping option."},{"poster":"lgdantas","comment_id":"161817","content":"Correct: B.\nCannot be C or D. \"Because Cancel immediately halts processing, you may lose any \"in-flight\" data. \"In-flight\" data refers to data that has been read but is still being processed by your pipeline\" https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#cancel","timestamp":"1597875840.0","upvote_count":"1"},{"comments":[{"upvote_count":"1","content":"on further checks for me B is correct answer.","comment_id":"161130","poster":"saurabh1805","timestamp":"1597789560.0"}],"poster":"saurabh1805","content":"important point is not to loose any data, so the only way to achieve this is either your pipeline is complete or you drian you existing pipeline. Refer below link. \n\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain\nhence for me A is correct answer.","upvote_count":"4","comment_id":"160284","timestamp":"1597689120.0"},{"timestamp":"1596435000.0","poster":"Prakzz","content":"B is correct. Its mentioned in Linux Academy tutorials","comment_id":"149560","upvote_count":"1"},{"upvote_count":"6","comment_id":"126760","timestamp":"1593945120.0","content":"Answer is A, as the key requirement is not to lose the data","poster":"dmalviya"},{"comment_id":"115914","poster":"nasirdec","upvote_count":"2","timestamp":"1592783280.0","content":"Correct answer is D with the 'Drian' on old pipeline."},{"comment_id":"76106","poster":"itche_scratche","content":"D is correct. Need to create new job with same name; and with new subscription, but i would drain instead of cancel.","upvote_count":"1","timestamp":"1587222540.0"},{"poster":"arnabbis4u","upvote_count":"2","comment_id":"75085","content":"I think it should be D. B would have worked if the new job would have been compatible. But since it is clearly mentioned that the new job is not compatible , we have to create a new job with a new subscription.","timestamp":"1586990160.0"},{"comment_id":"68561","timestamp":"1585303500.0","upvote_count":"2","poster":"[Removed]","content":"Answer: B \nDescription: If any transform names in your pipeline have changed, you must supply a transform mapping and pass it using the --transformNameMapping option.","comments":[{"upvote_count":"1","content":"Not B b/c this is not talking about the transform name change. that is only true when transform mapping NAME is changed","poster":"atnafu2020","comment_id":"161944","timestamp":"1597897260.0"}]},{"timestamp":"1584911940.0","comment_id":"67092","upvote_count":"1","content":"Should be D: transform mapping is only used for transforms names changing. So A and B is not possible because the pipeline es incompatible (check validation will fail when updating the pipeline). So with D, we don't lose date.","comments":[{"upvote_count":"3","poster":"[Removed]","timestamp":"1585015200.0","comment_id":"67417","content":"Correct: B .. so first we need to handle dataflow version issue that only can be handled by transform mapping and then we need to think of pubsub...as we are not deleting the subscription the data will be there for atleast 7 days so we dnt need to look from data loosing perspective."}],"poster":"jvg637"}],"answer_description":"","answer_images":[],"timestamp":"2020-03-20 14:13:00","exam_id":10,"unix_timestamp":1584709980,"isMC":true,"answer_ET":"A","question_text":"You have Google Cloud Dataflow streaming pipeline running with a Google Cloud Pub/Sub subscription as the source. You need to make an update to the code that will make the new Cloud Dataflow pipeline incompatible with the current version. You do not want to lose any data when making this update. What should you do?"}],"exam":{"numberOfQuestions":319,"name":"Professional Data Engineer","isMCOnly":true,"lastUpdated":"15 Feb 2025","isImplemented":true,"isBeta":false,"id":10,"provider":"Google"},"currentPage":47},"__N_SSP":true}