{"pageProps":{"questions":[{"id":"RYolqYwId3KyU2CXYhQs","choices":{"D":"Create a script to export the historical data, and upload in batches to Cloud Storage. Set up a BigQuery Data Transfer Service instance from Cloud Storage to BigQuery.","B":"Create a Teradata Parallel Transporter (TPT) export script to export the historical data, and import to BigQuery by using the bq command-line tool.","A":"Use BigQuery Data Transfer Service by using the Java Database Connectivity (JDBC) driver with FastExport connection.","C":"Use BigQuery Data Transfer Service with the Teradata Parallel Transporter (TPT) tbuild utility."},"timestamp":"2023-12-30 19:32:00","answer":"A","answer_ET":"A","exam_id":10,"topic":"1","question_id":181,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/129906-exam-professional-data-engineer-topic-1-question-261/","discussion":[{"content":"Selected Answer: A\n- Reduced Local Storage: By using FastExport, data is directly streamed from Teradata to BigQuery without the need for local storage, addressing your storage limitations.\n- Minimal Programming: BigQuery Data Transfer Service offers a user-friendly interface, eliminating the need for extensive scripting or coding.","comment_id":"1114571","comments":[{"poster":"AllenChen123","comment_id":"1127622","content":"Agree. https://cloud.google.com/bigquery/docs/migration/teradata-overview#extraction_method\nExtraction using a JDBC driver with FastExport connection. If there are constraints on the local storage space available for extracted files, or if there is some reason you can't use TPT, then use this extraction method.","timestamp":"1705810860.0","upvote_count":"7"}],"upvote_count":"11","poster":"raaad","timestamp":"1704465720.0"},{"content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/migration/teradata-overview#extraction_method\n\nLack of local storage pushes this to JDBC driver","comment_id":"1109895","timestamp":"1703961120.0","poster":"rahulvin","upvote_count":"6"},{"timestamp":"1736171940.0","comment_id":"1337171","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/migration/teradata-overview#extraction_method\nExtraction using a JDBC driver with FastExport connection. If there are constraints on the local storage space available for extracted files, or if there is some reason you can't use TPT, then use this extraction method.\n\nIn this mode, the migration agent extracts tables into a collection of AVRO files on the local file system. It then uploads these files to a Cloud Storage bucket, where they are used by the transfer job. Once the files are uploaded to Cloud Storage, the migration agent deletes them from the local file system.\n\nIn this mode, you can limit the amount of space used by the AVRO files on the local file system. If this limit is exceeded, extraction is paused until space is freed up by the migration agent uploading and deleting existing AVRO files.","upvote_count":"1","poster":"Pime13"},{"poster":"ToiToi","comment_id":"1305553","upvote_count":"2","content":"Selected Answer: C\nBigQuery Data Transfer Service (DTS): DTS automates data movement from various sources (including Teradata) to BigQuery. It handles schema conversion, data transfer, and scheduling, minimizing manual effort and programming.\nTeradata Parallel Transporter (TPT) tbuild: TPT is a powerful utility for high-performance data extraction from Teradata. The tbuild operator specifically creates optimized external data files.\nEfficiency: Combining DTS with TPT tbuild allows you to efficiently extract large volumes of data from Teradata and load it into BigQuery with minimal coding.\nLimited Local Storage: This approach streams data directly from Teradata to Cloud Storage, minimizing the need for temporary storage on your Teradata system.","timestamp":"1730400000.0"},{"content":"Selected Answer: C\nUsing TPT with the tbuild utility ensures that you can efficiently move large volumes of data directly from Teradata to BigQuery without requiring significant local storage space or extensive custom programming. This method leverages Teradataâ€™s optimized export capabilities and integrates with Google Cloud's tools for seamless data transfer.\n\nJDBC driver with FastExport can be used, it typically requires more programming and manual setup compared to the TPT solution, and may not be as optimized for large-scale data transfers","comment_id":"1301742","poster":"kurayish","timestamp":"1729641840.0","upvote_count":"2"},{"content":"Selected Answer: A\nExtraction using a JDBC driver with FastExport connection. If there are constraints on the local storage space available for extracted files, or if there is some reason you can't use TPT, then use this extraction method.\nhttps://cloud.google.com/bigquery/docs/migration/teradata-overview#extraction_method","poster":"CGS22","timestamp":"1712603760.0","upvote_count":"1","comment_id":"1191790"},{"content":"Selected Answer: A\nOption A, the JDBC driver is the key to solve the limited local storage","poster":"Matt_108","timestamp":"1705156860.0","upvote_count":"3","comment_id":"1121750"}],"question_text":"You want to migrate your existing Teradata data warehouse to BigQuery. You want to move the historical data to BigQuery by using the most efficient method that requires the least amount of programming, but local storage space on your existing data warehouse is limited. What should you do?","isMC":true,"answer_images":[],"answers_community":["A (85%)","C (15%)"],"unix_timestamp":1703961120},{"id":"q5srZVgERdP6gvsFir1E","choices":{"A":"Create the encryption key in the on-premises HSM, and import it into a Cloud Key Management Service (Cloud KMS) key. Associate the created Cloud KMS key while creating the BigQuery resources.","C":"Create the encryption key in the on-premises HSM, and import it into Cloud Key Management Service (Cloud HSM) key. Associate the created Cloud HSM key while creating the BigQuery resources.","D":"Create the encryption key in the on-premises HSM. Create BigQuery resources and encrypt data while ingesting them into BigQuery.","B":"Create the encryption key in the on-premises HSM and link it to a Cloud External Key Manager (Cloud EKM) key. Associate the created Cloud KMS key while creating the BigQuery resources."},"timestamp":"2024-01-03 17:44:00","answer":"B","answer_ET":"B","exam_id":10,"topic":"1","question_id":182,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130214-exam-professional-data-engineer-topic-1-question-262/","discussion":[{"timestamp":"1704467820.0","poster":"raaad","upvote_count":"18","comment_id":"1114592","content":"Selected Answer: B\n- Cloud EKM allows you to use encryption keys managed in external key management systems, including on-premises HSMs, while using Google Cloud services. \n- This means that the key material remains in your control and environment, and Google Cloud services use it via the Cloud EKM integration. \n- This approach aligns with the need to generate and store encryption material only on your on-premises HSM and is the correct way to integrate such keys with BigQuery.\n\n======\nWhy not Option C\n- Cloud HSM is a fully managed service by Google Cloud that provides HSMs for your cryptographic needs. However, it's a cloud-based solution, and the keys generated or managed in Cloud HSM are not stored on-premises. This option doesn't align with the requirement to use only on-premises HSM for key storage."},{"timestamp":"1736172000.0","poster":"Pime13","upvote_count":"1","comment_id":"1337173","content":"Selected Answer: B\ncheck raaad's comment."},{"content":"Selected Answer: B\nOption B, I agree with Raaad on the approach","poster":"meh_33","comment_id":"1263425","upvote_count":"1","timestamp":"1723284600.0"},{"upvote_count":"3","content":"Selected Answer: B\nhttps://cloud.google.com/kms/docs/ekm#ekm-management-mode","poster":"f74ca0c","comment_id":"1212793","timestamp":"1715935560.0"},{"content":"B- https://cloud.google.com/kms/docs/ekm#ekm-management-mode\nCoordinated external keys are made possible by EKM via VPC connections that use EKM key management from Cloud KMS. If your EKM supports the Cloud EKM control plane, then you can enable EKM key management from Cloud KMS for your EKM via VPC connections to create coordinated external keys. With EKM key management from Cloud KMS enabled, Cloud EKM can request the following changes in your EKM:","comment_id":"1212791","poster":"f74ca0c","upvote_count":"1","timestamp":"1715935500.0"},{"timestamp":"1705157040.0","content":"Selected Answer: B\nOption B, I agree with Raaad on the approach","upvote_count":"2","poster":"Matt_108","comment_id":"1121754"},{"comment_id":"1112941","upvote_count":"3","poster":"scaenruy","content":"Selected Answer: C\nC. Create the encryption key in the on-premises HSM, and import it into Cloud Key Management Service (Cloud HSM) key. Associate the created Cloud HSM key while creating the BigQuery resources.","timestamp":"1704300240.0"}],"question_text":"You are on the data governance team and are implementing security requirements. You need to encrypt all your data in BigQuery by using an encryption key managed by your team. You must implement a mechanism to generate and store encryption material only on your on-premises hardware security module (HSM). You want to rely on Google managed solutions. What should you do?","isMC":true,"answer_images":[],"unix_timestamp":1704300240,"answers_community":["B (89%)","11%"]},{"id":"5ycGPC0Eywxm2VUklKaZ","answer_ET":"A","question_images":[],"question_id":183,"unix_timestamp":1704301740,"answer_description":"","topic":"1","answer_images":[],"isMC":true,"answer":"A","answers_community":["A (89%)","11%"],"exam_id":10,"question_text":"You maintain ETL pipelines. You notice that a streaming pipeline running on Dataflow is taking a long time to process incoming data, which causes output delays. You also noticed that the pipeline graph was automatically optimized by Dataflow and merged into one step. You want to identify where the potential bottleneck is occurring. What should you do?","timestamp":"2024-01-03 18:09:00","discussion":[{"comment_id":"1114596","upvote_count":"9","poster":"raaad","timestamp":"1704468060.0","content":"Selected Answer: A\n- The Reshuffle operation is used in Dataflow pipelines to break fusion and redistribute elements, which can sometimes help improve parallelization and identify bottlenecks. \n- By inserting Reshuffle after each processing step and observing the pipeline's performance in the Dataflow console, you can potentially identify stages that are disproportionately slow or stalled. \n- This can help in pinpointing the step where the bottleneck might be occurring.","comments":[{"content":"ince we don't know for sure if fusion is the culprit, detailed debug logging is still the top choice to find the precise slow operation(s).","comment_id":"1147095","upvote_count":"1","poster":"srivastavas08","timestamp":"1707642780.0"}]},{"upvote_count":"1","content":"Selected Answer: A\nLooks like A. However, this option does not provide any option of identifying the underlying cause. https://cloud.google.com/dataflow/docs/pipeline-lifecycle#prevent_fusion","poster":"m_a_p_s","comment_id":"1325893","timestamp":"1734042780.0"},{"poster":"f6bc4a0","timestamp":"1728195240.0","comment_id":"1293757","content":"Selected Answer: B\nB identifies where the problem lies.","upvote_count":"1"},{"content":"Selected Answer: A\nOption A","comment_id":"1154760","upvote_count":"1","poster":"JyoGCP","timestamp":"1708437180.0"},{"timestamp":"1707585780.0","content":"It should be C","comment_id":"1146450","poster":"srivastavas08","upvote_count":"2"},{"comment_id":"1135906","upvote_count":"1","poster":"tibuenoc","content":"Selected Answer: B\nThe best option is B\nBecause create additional output to capturing and processing error data, will get error each step that allows you to observe the writing throughput of each block, which can help identify specific processing steps causing bottlenecks.\n\nOption A also is valid but can not directly address all bottlenecks, especially if the graph was merged.","timestamp":"1706627160.0"},{"timestamp":"1704803820.0","comment_id":"1117475","content":"Selected Answer: A\nFrom the Dataflow documentation: \"There are a few cases in your pipeline where you may want to prevent the Dataflow service from performing fusion optimizations. These are cases in which the Dataflow service might incorrectly guess the optimal way to fuse operations in the pipeline, which could limit the Dataflow service's ability to make use of all available workers.\nYou can insert a Reshuffle step. Reshuffle prevents fusion, checkpoints the data, and performs deduplication of records. Reshuffle is supported by Dataflow even though it is marked deprecated in the Apache Beam documentation.\"","poster":"Sofiia98","upvote_count":"4"},{"timestamp":"1704301740.0","comment_id":"1112953","upvote_count":"2","poster":"scaenruy","content":"Selected Answer: A\nA. Insert a Reshuffle operation after each processing step, and monitor the execution details in the Dataflow console."}],"choices":{"B":"Insert output sinks after each key processing step, and observe the writing throughput of each block.","D":"Verify that the Dataflow service accounts have appropriate permissions to write the processed data to the output sinks.","C":"Log debug information in each ParDo function, and analyze the logs at execution time.","A":"Insert a Reshuffle operation after each processing step, and monitor the execution details in the Dataflow console."},"url":"https://www.examtopics.com/discussions/google/view/130215-exam-professional-data-engineer-topic-1-question-263/"},{"id":"GQFBBeuMfFQUq36PsBXQ","choices":{"D":"Create a BigQuery reservation for the project.","B":"Create a BigQuery reservation for the job.","C":"Create a BigQuery reservation for the service account running the job.","A":"Create a BigQuery reservation for the dataset."},"timestamp":"2024-01-05 02:59:00","answer":"D","answer_ET":"D","topic":"1","exam_id":10,"question_id":184,"question_images":[],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130360-exam-professional-data-engineer-topic-1-question-264/","discussion":[{"poster":"Matt_108","content":"Selected Answer: D\nOption D, reservation can't be applied to resources lower than projects (only to Org, folders or projects)","upvote_count":"10","timestamp":"1705157460.0","comment_id":"1121760","comments":[{"comment_id":"1127629","poster":"AllenChen123","timestamp":"1705812240.0","content":"Seems correct. https://cloud.google.com/bigquery/docs/reservations-intro#understand_workload_management","upvote_count":"6"}]},{"timestamp":"1705040580.0","upvote_count":"7","poster":"task_7","content":"Selected Answer: B\nReserve assignments\nTo use the slot capacity you purchased, assign projects, folders, or organizations to a reservation. When a job in a project runs, it uses slots from the assigned reservation. Resources can inherit roles from their parents in the resource hierarchy. Even if a project is not assigned to a reservation, it inherits the assignment from the parent folder or organization, if any. If a project does not have an assigned or inherited reservation, the job uses on-demand pricing. For more information about the resource hierarchy, see Organizing BigQuery Resources .","comments":[{"comment_id":"1314490","timestamp":"1732001400.0","content":"The first sentence of that text already points towards D, not B. You can't assign jobs to a reservation, only projects, folders or organizations.","comments":[{"comment_id":"1332520","timestamp":"1735316940.0","upvote_count":"1","poster":"apoio.certificacoes.closer","content":"Best that could be done is a specify this a batch job, but the slot capacity is still at the project level."}],"poster":"Positron75","upvote_count":"2"}],"comment_id":"1120483"},{"comment_id":"1325892","upvote_count":"1","poster":"m_a_p_s","content":"Selected Answer: D\n\"When you create commitments and reservations, they are associated with a Google Cloud project.\" - https://cloud.google.com/bigquery/docs/reservations-intro#admin-project","timestamp":"1734042240.0"},{"timestamp":"1732002000.0","content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/reservations-intro#assignments","upvote_count":"3","poster":"Positron75","comment_id":"1314496"},{"content":"Selected Answer: B\nOption B provides a more granularity solution","comment_id":"1305164","timestamp":"1730320800.0","upvote_count":"1","poster":"SamuelTsch"},{"poster":"viciousjpjp","comments":[{"timestamp":"1732001700.0","upvote_count":"1","content":"Jobs cannot be assigned to a reservation. You should read the documentation instead of relying on AI answers which are wrong half the time.\n\nhttps://cloud.google.com/bigquery/docs/reservations-intro#understand_workload_management","poster":"Positron75","comments":[{"content":"Sorry, this is the actual link: https://cloud.google.com/bigquery/docs/reservations-intro#assignments","timestamp":"1732001760.0","upvote_count":"1","comment_id":"1314493","poster":"Positron75"}],"comment_id":"1314492"}],"upvote_count":"2","timestamp":"1723847700.0","comment_id":"1267355","content":"Selected Answer: B\nThe correct answer is B. Create a BigQuery reservation at the job level.\n\nCreate a BigQuery reservation at the job level: This is the most suitable option. By creating a job-level reservation, you can allocate resources specifically to the CDC process and improve the accuracy of cost forecasting.\n\nSteps to create and apply a BigQuery reservation:\nIdentify the job: Clearly identify the job that executes the CDC process.\nCreate a reservation: Use the BigQuery console or API to create a reservation, specifying the job's label, query text, and other details.\nApply the reservation: Assign the created reservation to the job that executes the CDC process."},{"upvote_count":"1","timestamp":"1723283940.0","content":"Are these questions really useful andd coming in GDE Exam? Anyone appeared recently and passed ?","comment_id":"1263418","poster":"meh_33"},{"timestamp":"1716097860.0","upvote_count":"3","poster":"Anudeep58","comment_id":"1213621","content":"Selected Answer: D\nhttps://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas.\nQuotas can be applied on Project or User Level"},{"poster":"f74ca0c","timestamp":"1715938020.0","upvote_count":"3","comment_id":"1212820","content":"Selected Answer: D\nD- choose the correct project and apply the task type background:https://cloud.google.com/bigquery/docs/reservations-intro?hl=fr#assignments"},{"content":"Selected Answer: D\nOption D","comment_id":"1155158","poster":"JyoGCP","upvote_count":"1","timestamp":"1708481580.0"},{"comment_id":"1116923","upvote_count":"3","poster":"GCP001","timestamp":"1704741120.0","content":"D.\nReservation is on project, folder or organisation level."},{"timestamp":"1704469020.0","upvote_count":"3","content":"Selected Answer: D\nC or D ?? \n\nOption C (service account) allows you to target the reservation specifically to the CDC process or any other jobs run by that service account. This is particularly useful if you have multiple processes running in the project with different performance or cost requirements.\n\nOption D (project) applies the reservation across all jobs in the project, which is a broader approach. If the CDC process is the primary or sole job running in the project and you want all jobs to share the same reservation, then this option might be more straightforward.","comment_id":"1114608","poster":"raaad"},{"timestamp":"1704419940.0","content":"Selected Answer: D\nD. Create a BigQuery reservation for the project.","comment_id":"1114187","upvote_count":"1","poster":"scaenruy"}],"question_text":"You are running your BigQuery project in the on-demand billing model and are executing a change data capture (CDC) process that ingests data. The CDC process loads 1 GB of data every 10 minutes into a temporary table, and then performs a merge into a 10 TB target table. This process is very scan intensive and you want to explore options to enable a predictable cost model. You need to create a BigQuery reservation based on utilization information gathered from BigQuery Monitoring and apply the reservation to the CDC process. What should you do?","isMC":true,"answer_images":[],"answers_community":["D (71%)","B (29%)"],"unix_timestamp":1704419940},{"id":"D3lmS0yGNYp2coQoFNb8","discussion":[{"comments":[{"upvote_count":"1","comment_id":"1147098","content":"BigQuery's time travel feature typically retains history up to 7 days. However, if the corruption affects the underlying data for an extended period, the 7-day window might not be long enough.","poster":"srivastavas08","timestamp":"1723360560.0"}],"timestamp":"1720186740.0","poster":"raaad","upvote_count":"15","content":"Selected Answer: A\n- Lowest RPO: Time travel offers point-in-time recovery for the past seven days by default, providing the shortest possible recovery point objective (RPO) among the given options. You can recover data to any state within that window.\n- No Additional Costs: Time travel is a built-in feature of BigQuery, incurring no extra storage or operational costs.\n- Managed Service: BigQuery handles time travel automatically, eliminating manual backup and restore processes.","comment_id":"1114611"},{"timestamp":"1728417540.0","content":"Selected Answer: C\nMeets Recovery Needs: Table snapshots provide point-in-time copies of your data, allowing you to restore data from any point within the last seven days, effectively addressing the corruption event recovery requirement.\nLow RPO: With daily snapshots, your Recovery Point Objective (RPO) is at most 24 hours, satisfying the need for a low RPO.\nManaged Service: Table snapshots are a fully managed service within BigQuery, aligning with your preference.\nCost-Effective: Snapshots only store the changes from the base table, minimizing storage costs compared to full table copies.","poster":"CGS22","upvote_count":"1","comment_id":"1191807"},{"upvote_count":"1","content":"Selected Answer: A\nvote for A","timestamp":"1726544700.0","comment_id":"1175606","poster":"hanoverquay"},{"comment_id":"1155159","upvote_count":"1","content":"Selected Answer: A\nOption A","poster":"JyoGCP","timestamp":"1724199240.0"},{"content":"Selected Answer: A\nOption A, raaad explanation is perfect","comment_id":"1121766","timestamp":"1720875240.0","poster":"Matt_108","upvote_count":"3"},{"upvote_count":"1","comment_id":"1114188","timestamp":"1720137600.0","poster":"scaenruy","content":"Selected Answer: A\nA. Access historical data by using time travel in BigQuery."}],"question_id":185,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/130361-exam-professional-data-engineer-topic-1-question-265/","question_text":"You are designing a fault-tolerant architecture to store data in a regional BigQuery dataset. You need to ensure that your application is able to recover from a corruption event in your tables that occurred within the past seven days. You want to adopt managed services with the lowest RPO and most cost-effective solution. What should you do?","answer_images":[],"topic":"1","answer":"A","unix_timestamp":1704420000,"timestamp":"2024-01-05 03:00:00","isMC":true,"exam_id":10,"answer_description":"","choices":{"A":"Access historical data by using time travel in BigQuery.","D":"Migrate your data to multi-region BigQuery buckets.","C":"Create a BigQuery table snapshot on a daily basis.","B":"Export the data from BigQuery into a new table that excludes the corrupted data"},"answers_community":["A (95%)","5%"],"answer_ET":"A"}],"exam":{"name":"Professional Data Engineer","isMCOnly":true,"provider":"Google","isBeta":false,"id":10,"lastUpdated":"15 Feb 2025","isImplemented":true,"numberOfQuestions":319},"currentPage":37},"__N_SSP":true}