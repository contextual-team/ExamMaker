{"pageProps":{"questions":[{"id":"gdlrl8yZeRSJwoenli7s","exam_id":10,"answer_description":"","answer":"B","answers_community":["B (76%)","D (24%)"],"discussion":[{"comment_id":"335227","comments":[{"poster":"sfsdeniso","comment_id":"730040","timestamp":"1701241380.0","upvote_count":"2","content":"google send via pub sub web indexes\ntwice a day a whole internet is being sent via pub sub"},{"poster":"ralf_cc","comment_id":"393797","timestamp":"1656508020.0","upvote_count":"8","content":"lol this is a vendor exam..."}],"poster":"Manue","content":"\"However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume.\" \n\nSure man, Kafka is not performing, let's use PubSub instead hahaha...","timestamp":"1649917080.0","upvote_count":"37"},{"upvote_count":"23","comment_id":"66294","poster":"[Removed]","timestamp":"1616254920.0","content":"Answer: B"},{"timestamp":"1729610640.0","poster":"rtcpost","content":"Selected Answer: B\nB. Attach the timestamp and Package ID on the outbound message from each publisher device as they are sent to Cloud Pub/Sub.\n\nHere's why this approach is the most suitable:\n\nBy attaching a timestamp and Package ID at the point of origin (publisher device), you ensure that each message has a clear and consistent timestamp associated with it from the moment it is generated. This provides a reliable and accurate record of when each package-tracking message was created, which is crucial for analyzing the data over time.\n\nThis approach allows you to maintain the chronological order of events as they occurred at the source, which is important for real-time reporting and historical analysis.\n\nOption A suggests attaching the timestamp in the Cloud Pub/Sub subscriber application. While this can work, it introduces a potential delay and the risk of timestamps not being accurate if there are issues with message processing.\n\nOption C, using the NOW() function in BigQuery, records the time when the data is ingested into BigQuery, which may not reflect the actual time of the event.","upvote_count":"5","comment_id":"1050794"},{"timestamp":"1707406260.0","poster":"JJJJim","content":"Selected Answer: B\nAnswer is B, attach the timestamp and ID is necessary to analyze data easily.","comment_id":"802189","upvote_count":"1"},{"upvote_count":"4","content":"Selected Answer: B\nAnswer: B","timestamp":"1682487660.0","poster":"nidmed","comment_id":"592114"},{"upvote_count":"1","poster":"Arkon88","comment_id":"560228","content":"Selected Answer: B\nwe need package ID + Timestamp so B","timestamp":"1677862080.0"},{"poster":"davidqianwen","timestamp":"1674484440.0","upvote_count":"1","content":"Selected Answer: B\nAnswer: B","comment_id":"530591"},{"poster":"exnaniantwort","content":"Selected Answer: B\nagree with humza","comment_id":"530197","timestamp":"1674438660.0","upvote_count":"1"},{"upvote_count":"4","poster":"sraakesh95","content":"Selected Answer: D\nhttps://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage","timestamp":"1673656080.0","comment_id":"523214"},{"timestamp":"1665775740.0","poster":"[Removed]","comment_id":"462227","comments":[{"poster":"Tanzu","upvote_count":"3","comment_id":"532808","comments":[{"comment_id":"584078","content":"Also, since this is a International company, adding timestamp on message receiving would help catch local time.","upvote_count":"1","poster":"Pinko1497","timestamp":"1681195620.0"}],"content":"there are 2 requirements\n1- is about ordering due to historical data analysis \n2- what it means to write a single topic and its impact.. why some sentence added here.\n\n1st is primary, 2nd is secondary req. in this context.\n\nSo, \n- in pub/sub, processTime is filled by server, not publisher. but that does not guarantee the ordering due to latency, pub/sub handling, sensors or any other reasons..\n- you need to populate orderingKey field too, so that subscribers can get in ordered.","timestamp":"1674730500.0"}],"upvote_count":"1","content":"D is enough.. we have publish timestamp which is enough for this requirement"},{"comments":[{"poster":"Tanzu","comment_id":"532803","upvote_count":"1","timestamp":"1674729900.0","content":"not just timing, but also package-id .. cause they are sending 1 topic in gcp instead of to many in kafka. that means there must be added some additional critical data too."}],"timestamp":"1665624300.0","upvote_count":"1","content":"It is about processing time and event time.. Answer is B.","poster":"Chelseajcole","comment_id":"461323"},{"content":"Ans: B\nA: Adding timestamp as they received is not a better option, messages may not arrive in order at the receiver/ subscriber, could be due to connectivity or network.\nB: Timestamp should be added here.\nC: Doesn't make sense at all.\nD: Ordering should be based on the order how messages are generated at the publisher but not as per order they reach the pub/sub.","timestamp":"1665591960.0","comment_id":"461176","upvote_count":"4","poster":"anji007"},{"upvote_count":"7","timestamp":"1657456860.0","comment_id":"403340","poster":"humza","content":"Answer: B\nA. There is no indication that the application can do this. Moreover, due to networking problems, it is possible that Pub/Sub doesn't receive messages in order. This will analysis difficult.\nB. This makes sure that you have access to publishing timestamp which provides you with the correct ordering of messages.\nC. If timestamps are already messed up, BigQuery will get wrong results anyways.\nD. The timestamp we are interested in is when the data was produced by the publisher, not when it was received by Pub/Sub."},{"comment_id":"401951","poster":"sumanshu","content":"Vote for B","upvote_count":"2","timestamp":"1657286520.0"},{"content":"Better if the publisher attached the package ID and Timestamp as packages can come in an Asynchronous fashion.","poster":"funtoosh","timestamp":"1645289460.0","upvote_count":"3","comment_id":"294426"},{"timestamp":"1644251760.0","content":"Correct B","upvote_count":"3","poster":"naga","comment_id":"285626"},{"upvote_count":"6","poster":"Radhika7983","comment_id":"221067","timestamp":"1637153760.0","content":"The answer is B. \n\nJSON representation\n{\n \"data\": string,\n \"attributes\": {\n string: string,\n ...\n },\n \"messageId\": string,\n \"publishTime\": string,\n \"orderingKey\": string\n}\n\nIn the attribute, we can have package id and timestamp."},{"content":"D. \"PublishTime- It must not be populated by the publisher in a topics.publish call.\" https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage","comment_id":"220590","poster":"snamburi3","timestamp":"1637091840.0","upvote_count":"1","comments":[{"timestamp":"1674730680.0","comment_id":"532812","poster":"Tanzu","content":"you can add timestamp as data , not as processTime.","upvote_count":"1"},{"comments":[{"content":"but no one can guarantee that finally the message will receive in order, because during the transmission of message the order of them will change. then using an identifier and timestamp would be a must.","timestamp":"1646864760.0","upvote_count":"1","poster":"daghayeghi","comment_id":"306718"}],"timestamp":"1637092020.0","comment_id":"220593","upvote_count":"1","poster":"snamburi3","content":"We can order the messages: https://cloud.google.com/pubsub/docs/ordering"}]},{"timestamp":"1629327480.0","comment_id":"161137","content":"B Correct","upvote_count":"5","poster":"haroldbenites"},{"comment_id":"138482","content":"Answer: B eiei","poster":"bbozz_","timestamp":"1626678000.0","upvote_count":"4"}],"timestamp":"2020-03-20 16:42:00","answer_images":[],"answer_ET":"B","topic":"1","unix_timestamp":1584718920,"question_text":"Flowlogistic Case Study -\n\nCompany Overview -\nFlowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.\n\nCompany Background -\nThe company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.\n\nSolution Concept -\nFlowlogistic wants to implement two concepts using the cloud:\n✑ Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads\n✑ Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.\n\nExisting Technical Environment -\nFlowlogistic architecture resides in a single data center:\n✑ Databases\n8 physical servers in 2 clusters\n- SQL Server `\" user data, inventory, static data\n3 physical servers\n- Cassandra `\" metadata, tracking messages\n10 Kafka servers `\" tracking message aggregation and batch insert\n✑ Application servers `\" customer front end, middleware for order/customs\n60 virtual machines across 20 physical servers\n- Tomcat `\" Java services\n- Nginx `\" static content\n- Batch servers\n✑ Storage appliances\n- iSCSI for virtual machine (VM) hosts\n- Fibre Channel storage area network (FC SAN) `\" SQL server storage\n- Network-attached storage (NAS) image storage, logs, backups\n✑ 10 Apache Hadoop /Spark servers\n- Core Data Lake\n- Data analysis workloads\n✑ 20 miscellaneous servers\n- Jenkins, monitoring, bastion hosts,\n\nBusiness Requirements -\n✑ Build a reliable and reproducible environment with scaled panty of production.\n✑ Aggregate data in a centralized Data Lake for analysis\n✑ Use historical data to perform predictive analytics on future shipments\n✑ Accurately track every shipment worldwide using proprietary technology\n✑ Improve business agility and speed of innovation through rapid provisioning of new resources\n✑ Analyze and optimize architecture for performance in the cloud\n✑ Migrate fully to the cloud if all other requirements are met\n\nTechnical Requirements -\n✑ Handle both streaming and batch data\n✑ Migrate existing Hadoop workloads\n✑ Ensure architecture is scalable and elastic to meet the changing demands of the company.\n✑ Use managed services whenever possible\n✑ Encrypt data flight and at rest\n✑ Connect a VPN between the production data center and cloud environment\n\nSEO Statement -\nWe have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.\nWe need to organize our information so we can more easily understand where our customers are and what they are shipping.\n\nCTO Statement -\nIT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.\n\nCFO Statement -\nPart of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.\nFlowlogistic is rolling out their real-time inventory tracking system. The tracking devices will all send package-tracking messages, which will now go to a single\nGoogle Cloud Pub/Sub topic instead of the Apache Kafka cluster. A subscriber application will then process the messages for real-time reporting and store them in\nGoogle BigQuery for historical analysis. You want to ensure the package data can be analyzed over time.\nWhich approach should you take?","question_id":251,"choices":{"A":"Attach the timestamp on each message in the Cloud Pub/Sub subscriber application as they are received.","D":"Use the automatically generated timestamp from Cloud Pub/Sub to order the data.","B":"Attach the timestamp and Package ID on the outbound message from each publisher device as they are sent to Clod Pub/Sub.","C":"Use the NOW () function in BigQuery to record the event's time."},"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/17059-exam-professional-data-engineer-topic-1-question-37/","question_images":[]},{"id":"QbRM1RwkkOYmFO30Xs4W","answer_ET":"D","question_images":[],"discussion":[{"timestamp":"1621311900.0","content":"The correct answer is D. Please look for the details in below\nhttps://cloud.google.com/dataflow/docs/guides/specifying-exec-params\nWe need to specify and set execution parameters for cloud data flow .\n\nAlso, to enable autoscaling, set the following execution parameters when you start your pipeline:\n\n--autoscaling_algorithm=THROUGHPUT_BASED\n--max_num_workers=N\nThe objective of autoscaling streaming pipelines is to minimize backlog while maximizing worker utilization and throughput, and quickly react to spikes in load. By enabling autoscaling, you don't have to choose between provisioning for peak load and fresh results. Workers are added as CPU utilization and backlog increase and are removed as these metrics come down. This way, you’re paying only for what you need, and the job is processed as efficiently as possible.","poster":"Radhika7983","comment_id":"221620","upvote_count":"27"},{"upvote_count":"26","poster":"jvg637","content":"D. The maximum number of workers answers to the scale question","comment_id":"64252","timestamp":"1600164960.0"},{"upvote_count":"1","content":"Selected Answer: D\nThe answer is D because Google Dataflow is serverless and auto scales based on demand. To allow it to scale up compute power dynamically, we need to set the maximum number of workers.","poster":"cqrm3n","comment_id":"1349801","timestamp":"1738401720.0"},{"timestamp":"1726285800.0","upvote_count":"2","poster":"I__SHA1234567","comment_id":"1173148","content":"Selected Answer: D\nCloud Dataflow dynamically scales the number of workers based on the amount of data being processed and the processing requirements. By updating the maximum number of workers, you allow Dataflow to scale up the compute power as needed to handle the workload efficiently. This ensures that the pipeline can adapt to changes in data volume and processing demands."},{"comment_id":"1050798","upvote_count":"2","content":"Selected Answer: D\nD. The maximum number of workers\n\nBy increasing the maximum number of workers, you ensure that Cloud Dataflow can scale its compute power to handle the increased data processing load efficiently.","timestamp":"1713799560.0","poster":"rtcpost"},{"content":"Selected Answer: D\ndataflow auto-scales, then if it is not scaling is because it has reached the maximum number of workers that have been set.","upvote_count":"2","timestamp":"1700578260.0","comment_id":"903228","poster":"vaga1"},{"comment_id":"876123","content":"A is the correct answer. Dataflow is Serverless. Specify your Region, autoscaling and other 'knobing' activities that are 'under the hood' will be taken care for you. Remember the company cannot afford to staff an Operations team to monitor data feeds so rely on ...","poster":"abi01a","upvote_count":"2","timestamp":"1697852880.0"},{"content":"Selected Answer: D\nthis is correct","comment_id":"835705","upvote_count":"2","poster":"bha11111","timestamp":"1694407020.0"},{"upvote_count":"1","comment_id":"779638","timestamp":"1689651480.0","poster":"GCPpro","content":"D . is the correct answer"},{"timestamp":"1689580920.0","poster":"jkh_goh","content":"Answer A provided is definitely wrong. Who comes up with these answers?","upvote_count":"1","comment_id":"778771"},{"poster":"Ender_H","timestamp":"1679940540.0","comment_id":"681047","content":"Selected Answer: D\nCorrect Answer: D\n\n❌ A: The zone has nothing to do with scaling computer power.\n\n❌ B: The key word here is, \"Scale its compute power up AS REQUIRED\", with this answer, the number of workers would immediately scale the computer power.\n\n❌ C: we need to scale compute power, not storage\n\n✅ D: is the correct answer, changing the Number of Maximum workers will allow Dataflow to add up to that number of workers if required.\n\nhttps://cloud.google.com/dataflow/docs/reference/pipeline-options#resource_utilization","upvote_count":"2"},{"upvote_count":"2","poster":"sraakesh95","timestamp":"1657751760.0","comment_id":"523218","content":"Selected Answer: D\n@Radhika7983"},{"poster":"medeis_jar","comment_id":"516558","content":"Selected Answer: D\nThe correct answer is D. \nhttps://cloud.google.com/dataflow/docs/guides/specifying-exec-params\nWe need to specify and set execution parameters for cloud data flow .\n\nAlso, to enable autoscaling, set the following execution parameters when you start your pipeline:\n\n--autoscaling_algorithm=THROUGHPUT_BASED\n--max_num_workers=N","upvote_count":"2","timestamp":"1656929640.0"},{"comments":[{"timestamp":"1698955320.0","comment_id":"887775","poster":"Jarek7","upvote_count":"1","content":"Have you ever use it? You pay for workers processing, so you specify max number of workers. Here is the doc: https://cloud.google.com/sdk/gcloud/reference/dataflow/jobs/run"}],"timestamp":"1653825480.0","poster":"maurodipa","comment_id":"489933","upvote_count":"5","content":"Answer is A: Dataflow is serverless, so no need to specify neither the number of workers, nor the max number of workers. https://cloud.google.com/dataflow"},{"comment_id":"461177","timestamp":"1649780940.0","content":"Ans: D","upvote_count":"1","poster":"anji007"},{"poster":"sumanshu","comment_id":"401952","content":"Vote for D","timestamp":"1641655620.0","upvote_count":"3"},{"comment_id":"313486","content":"D, thats because scalability is directly corerlated to max number of workers, size determines the speed of functioning","timestamp":"1631893680.0","upvote_count":"3","poster":"Lodu_Lalit"},{"comment_id":"285629","timestamp":"1628347680.0","content":"Correct D","upvote_count":"4","poster":"naga"},{"timestamp":"1627401960.0","comment_id":"277986","content":"A exactly is correct:\nBecause Dataflow is scalable and don't need to repeated like cluster in Spark. then the only problem that remain is the zone of data that is important.","poster":"daghayeghi","upvote_count":"3","comments":[{"upvote_count":"2","poster":"daghayeghi","content":"D is correct,\nit was my mistake, because it said: \"as required\". then we can increase maxNumWorkers parameter to be used in required situation.","comment_id":"306721","timestamp":"1631219940.0"}]},{"comments":[{"timestamp":"1640601120.0","poster":"sumanshu","comment_id":"391898","upvote_count":"1","content":"Dataflow is autoscaled, So we can't mention exact number of workers. But we can cap it, even though we have default value of CAP.\n\nWith autoscaling enabled, the Dataflow service does not allow user control of the exact number of worker instances allocated to your job. You might still cap the number of workers by specifying the --max_num_workers option when you run your pipeline.\n\nhttps://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline"}],"upvote_count":"1","content":"Shouldn't the answer be B. This will create the workers with the minimum number of workers required to support the streaming data and scale up as required.","timestamp":"1616063520.0","comment_id":"181486","poster":"SteelWarrior"},{"timestamp":"1613846400.0","comment_id":"162357","content":"D\nnumWorkers- int- The initial number of Google Compute Engine instances to use when executing your pipeline. This option determines how many workers the Dataflow service starts up when your job begins. If unspecified, the Dataflow service determines an appropriate number of workers.\nmaxNumWorkers-int-The maximum number of Compute Engine instances to be made available to your pipeline during execution. Note that this can be higher than the initial number of workers (specified by numWorkers to allow your job to scale up, automatically or otherwise.","poster":"atnafu2020","upvote_count":"4"},{"upvote_count":"3","poster":"haroldbenites","comment_id":"161138","timestamp":"1613696400.0","content":"D Obviously"},{"timestamp":"1612014660.0","comment_id":"147376","content":"D. Clearly","upvote_count":"4","poster":"yoRob"},{"upvote_count":"4","timestamp":"1610503920.0","poster":"tprashanth","content":"D, Max Workers can be changed as needed","comment_id":"133344"},{"content":"Correct D","upvote_count":"4","comment_id":"90169","poster":"arnabbis4u","timestamp":"1605572160.0"},{"timestamp":"1600650840.0","poster":"[Removed]","content":"Answer D","upvote_count":"4","comment_id":"66397"}],"answers_community":["D (100%)"],"answer_images":[],"choices":{"B":"The number of workers","C":"The disk size per worker","D":"The maximum number of workers","A":"The zone"},"exam_id":10,"answer_description":"","unix_timestamp":1584274560,"question_id":252,"url":"https://www.examtopics.com/discussions/google/view/16657-exam-professional-data-engineer-topic-1-question-38/","topic":"1","isMC":true,"timestamp":"2020-03-15 13:16:00","question_text":"MJTelco Case Study -\n\nCompany Overview -\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\n\nCompany Background -\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\n\nSolution Concept -\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:\n✑ Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\n✑ Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\nMJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.\n\nBusiness Requirements -\n✑ Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\n✑ Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.\n✑ Provide reliable and timely access to data for analysis from distributed research workers\n✑ Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\n\nTechnical Requirements -\n✑ Ensure secure and efficient transport and storage of telemetry data\n✑ Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.\n✑ Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\n✑ Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\n\nCEO Statement -\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\n\nCTO Statement -\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\n\nCFO Statement -\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.\nMJTelco's Google Cloud Dataflow pipeline is now ready to start receiving data from the 50,000 installations. You want to allow Cloud Dataflow to scale its compute power up as required. Which Cloud Dataflow pipeline configuration setting should you update?","answer":"D"},{"id":"rYaSatO1T557hojNc1wk","question_text":"MJTelco Case Study -\n\nCompany Overview -\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\n\nCompany Background -\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\n\nSolution Concept -\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:\n✑ Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\n✑ Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\nMJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.\n\nBusiness Requirements -\n✑ Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\n✑ Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.\n✑ Provide reliable and timely access to data for analysis from distributed research workers\nMaintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\n//IMG//\n\n\nTechnical Requirements -\n✑ Ensure secure and efficient transport and storage of telemetry data\n✑ Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.\n✑ Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\n✑ Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\n\nCEO Statement -\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\n\nCTO Statement -\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\n\nCFO Statement -\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.\nYou need to compose visualizations for operations teams with the following requirements:\n✑ The report must include telemetry data from all 50,000 installations for the most resent 6 weeks (sampling once every minute).\n✑ The report must not be more than 3 hours delayed from live data.\n✑ The actionable report should only show suboptimal links.\n✑ Most suboptimal links should be sorted to the top.\n✑ Suboptimal links can be grouped and filtered by regional geography.\n✑ User response time to load the report must be <5 seconds.\nWhich approach meets the requirements?","answer":"D","timestamp":"2020-03-21 04:22:00","exam_id":10,"topic":"1","unix_timestamp":1584760920,"isMC":true,"question_id":253,"question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0003000006.png"],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/17075-exam-professional-data-engineer-topic-1-question-39/","answers_community":["D (83%)","C (17%)"],"choices":{"C":"Load the data into Google Cloud Datastore tables, write a Google App Engine Application that queries all rows, applies a function to derive the metric, and then renders results in a table using the Google charts and visualization API.","D":"Load the data into Google BigQuery tables, write a Google Data Studio 360 report that connects to your data, calculates a metric, and then uses a filter expression to show only suboptimal rows in a table.","B":"Load the data into Google BigQuery tables, write Google Apps Script that queries the data, calculates the metric, and shows only suboptimal rows in a table in Google Sheets.","A":"Load the data into Google Sheets, use formulas to calculate a metric, and use filters/sorting to show only suboptimal links in a table."},"answer_description":"","discussion":[{"poster":"[Removed]","content":"Answer: D","upvote_count":"28","comment_id":"66398","timestamp":"1600651320.0"},{"timestamp":"1603036260.0","comment_id":"76115","poster":"itche_scratche","comments":[{"comment_id":"1011289","timestamp":"1710858540.0","upvote_count":"1","content":"Dataflow does connect to Datastore, D is still the right answer though.","poster":"ckanaar"}],"content":"D; dataflow doesn't connect to datastore, and not really for reporting. BQ, and data studio is a better choice.","upvote_count":"13"},{"content":"Selected Answer: D\nD. Load the data into Google BigQuery tables, write a Google Data Studio 360 report that connects to your data, calculates a metric, and then uses a filter expression to show only suboptimal rows in a table.\n\nHere's why this option is the most suitable:\n\nGoogle BigQuery is a powerful data warehouse for processing and analyzing large datasets. It can efficiently handle the telemetry data from all 50,000 installations.\nGoogle Data Studio 360 is designed for creating interactive and visually appealing reports and dashboards.\nUsing Google Data Studio allows you to connect to BigQuery, calculate the required metrics, and apply filters to show only suboptimal links.\nIt can provide real-time or near-real-time data updates, ensuring that the report is not more than 3 hours delayed from live data.\nGoogle Data Studio can also be used to sort and group suboptimal links and display them based on regional geography.\nWith the right design, you can ensure that user response time to load the report is less than 5 seconds.\nThis approach leverages Google's cloud services effectively to meet the specified requirements.","comments":[{"content":"Is Google Data studio 360 a product now?","timestamp":"1731835860.0","upvote_count":"1","poster":"mark1223jkh","comment_id":"1212743"}],"timestamp":"1713799620.0","poster":"rtcpost","upvote_count":"2","comment_id":"1050800"},{"poster":"theseawillclaim","upvote_count":"3","timestamp":"1705521780.0","content":"Selected Answer: D\nWhy bother with a custom GAE app when you have Data Studio?","comment_id":"954495"},{"comment_id":"744480","upvote_count":"2","content":"Selected Answer: C\nIts think answer would be C because of telemetry data and response time is <5 second that force me to think about datastore,","poster":"DGames","timestamp":"1686685680.0"},{"timestamp":"1670650080.0","comment_id":"614309","comments":[{"timestamp":"1709669340.0","upvote_count":"2","comment_id":"999783","content":"Ummm, sorry @willymac2, but you have to account for size and growth which datastore cannot scale to. \nThen, you have to worry about sub-second response time and datastore cannot do that as well as BigQuery...","poster":"gudguy1a"}],"poster":"willymac2","content":"I believe the answer is C.\nFirst requirement is that it must be a visualisation with, so A and B do not work (create a table and a spreadsheet).\nNow the second constraint which I believe is important is that the report MUST load in less than 5 seconds. But we do not know how complex the metric computation is, thus I cannot assume that we can compute it when we want to load the report, making me think that it be must be pre-computed. Thus option D cannot work as it create the metric AFTER querying the data (we are also not sure if we can really compute it in a query).","upvote_count":"4"},{"content":"Answer D","comment_id":"598082","poster":"Raj0123","timestamp":"1667826300.0","upvote_count":"1"},{"upvote_count":"1","comment_id":"585160","content":"Selected Answer: D\nDataStudio and BQ are the simpliest way to do it","timestamp":"1665659700.0","poster":"CedricLP"},{"content":"Selected Answer: D\nThey also can activate BI Engine feature to improve the response time.","poster":"devric","timestamp":"1664927160.0","comment_id":"580958","upvote_count":"1"},{"timestamp":"1657751940.0","content":"Selected Answer: D\nD: Usually when a reporting tool is involved for GCP, DataStudio mostly goes by default due to it's no cost analytics and BigQuery joins it due to it's OLAP nature and the wonderful integration provided by GCP for these 2","poster":"sraakesh95","upvote_count":"2","comment_id":"523219"},{"comment_id":"516560","poster":"medeis_jar","timestamp":"1656929760.0","upvote_count":"1","content":"Selected Answer: D\nas explained by JayZeeLee"},{"content":"D. \nA and B are incorrect, because Google Sheets are not the best fit to handle large amount of data. \nC may work, but it requires building an application which equates to more work. \nD is more efficient, therefore a better option.","comments":[{"upvote_count":"1","content":"I can't think of a single compelling reason to go with anything but D, given the scope definition in the question brief.","poster":"wubston","comment_id":"717926","timestamp":"1684059000.0"}],"comment_id":"471866","poster":"JayZeeLee","timestamp":"1651530840.0","upvote_count":"4"},{"poster":"Chelseajcole","upvote_count":"1","comment_id":"461324","content":"Visualization = Data Studio 360","comments":[{"poster":"Chelseajcole","timestamp":"1649813340.0","comment_id":"461325","content":"Next question give you the answer: Question #40. They using Data Studio 360 and Bigquery as source","upvote_count":"1"}],"timestamp":"1649813280.0"},{"timestamp":"1649781180.0","content":"Ans: D","upvote_count":"1","poster":"anji007","comment_id":"461181"},{"upvote_count":"3","comment_id":"401963","content":"Vote for D","timestamp":"1641656520.0","poster":"sumanshu"},{"content":"just check the next question (#40) to get an idea about correct answer","timestamp":"1637247360.0","upvote_count":"3","comment_id":"360519","poster":"zosoabi"},{"upvote_count":"2","timestamp":"1631480760.0","comment_id":"309262","poster":"BhupiSG","content":"D\nCorrect"},{"poster":"naga","comment_id":"285633","upvote_count":"1","timestamp":"1628347860.0","content":"Correct C"},{"timestamp":"1625415780.0","comment_id":"259611","poster":"RogerLoSa","content":"After reading it twice, it's C. It's telemetry data, unstructured data, can't use BigQuery.","upvote_count":"2","comments":[{"timestamp":"1629623280.0","upvote_count":"3","poster":"Jay3244","comment_id":"296575","content":"Agreed... however they are using Biguery for analysis right for the company... so using Data Flow for the telemetry data and then loading to BQ... wont help ?? This way D is most suitable right ??"},{"poster":"daghayeghi","content":"Analyzing telemetry data won't make us to use NoSQL database and it depend to goal of the project. when the main goal is analytical job, then BigQuery is the best choice.","upvote_count":"2","timestamp":"1631276220.0","comment_id":"307224"}]},{"content":"The correct answer is D. Data studio is best to accelerate data exploration and analysis.\nAlso once the data is loaded in big query the data can be easily visualized in data studio.","upvote_count":"4","poster":"Radhika7983","timestamp":"1621312260.0","comment_id":"221622"},{"timestamp":"1616063700.0","comment_id":"181487","upvote_count":"3","content":"Answer should be D.","poster":"SteelWarrior"},{"poster":"atnafu2020","comment_id":"162364","content":"D\ncorrect","upvote_count":"4","timestamp":"1613846880.0"},{"comment_id":"161139","upvote_count":"4","content":"D Correct","poster":"haroldbenites","timestamp":"1613696520.0"},{"upvote_count":"4","poster":"yoRob","content":"D. .. App Engine is too much work","comment_id":"147381","timestamp":"1612014780.0"},{"poster":"arnabbis4u","comment_id":"90170","content":"Correct D","timestamp":"1605572340.0","upvote_count":"5"}],"answer_ET":"D"},{"id":"DLwyVZhF0A5ExlelcXcx","topic":"1","answer_description":"","exam_id":10,"question_text":"You create an important report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. You notice that visualizations are not showing data that is less than 1 hour old. What should you do?","isMC":true,"question_images":[],"answers_community":["A (76%)","C (18%)","6%"],"timestamp":"2022-09-03 06:43:00","unix_timestamp":1662180180,"url":"https://www.examtopics.com/discussions/google/view/79677-exam-professional-data-engineer-topic-1-question-4/","answer":"A","choices":{"D":"Clear your browser history for the past hour then reload the tab showing the virtualizations.","B":"Disable caching in BigQuery by editing table details.","C":"Refresh your browser tab showing the visualizations.","A":"Disable caching by editing the report settings."},"answer_ET":"A","question_id":254,"discussion":[{"poster":"Khaled_Rashwan","timestamp":"1677332580.0","comment_id":"821524","content":"A. Disable caching by editing the report settings.\n\nBy default, Google Data Studio 360 caches data to improve performance and reduce the amount of queries made to the data source. However, this can cause visualizations to not show data that is less than 1 hour old, as the cached data is not up-to-date.\n\nTo resolve this, you should disable caching by editing the report settings. This can be done by following these steps:\n\nOpen the report in Google Data Studio 360.\nClick on the \"File\" menu in the top left corner of the screen.\nSelect \"Report settings\" from the dropdown menu.\nIn the \"Report settings\" window, scroll down to the \"Data\" section.\nToggle off the \"Enable cache\" option.\nClick the \"Save\" button to apply the changes.\nDisabling caching ensures that the data shown in the visualizations is always up-to-date, but it may increase the query load on the data source and affect the report's performance. Therefore, it's important to consider the trade-off between performance and data accuracy when making this change.","upvote_count":"12"},{"content":"Selected Answer: A\nTo ensure that Google Data Studio visualizations show the most recent data, you should disable caching within Data Studio's report settings.","poster":"SamuelTsch","upvote_count":"1","timestamp":"1729486440.0","comment_id":"1300758"},{"comment_id":"1060869","upvote_count":"3","content":"Selected Answer: A\nA. Disable caching by editing the report settings.\n\nBy default, Google Data Studio 360 caches data to improve performance and reduce the amount of queries made to the data source. However, this can cause visualizations to not show data that is less than 1 hour old, as the cached data is not up-to-date.","timestamp":"1698956880.0","poster":"rocky48"},{"comment_id":"1050465","timestamp":"1697971860.0","content":"Selected Answer: A\nDisabling caching in the report settings will ensure that the visualizations are not using cached data and will reflect the most up-to-date information from your Google BigQuery data source. This will allow your report to show data that is less than 1 hour old. Caching is often used for performance optimization, but it can result in delays in displaying real-time or near-real-time data, so disabling it is the appropriate action in this case.","poster":"rtcpost","upvote_count":"1"},{"upvote_count":"1","comment_id":"975506","poster":"Websurfer","timestamp":"1691491920.0","content":"Selected Answer: A\ndisable caching in report setting will get the issue resolved"},{"poster":"Morock","content":"Selected Answer: D\nThe solution from the site is perfect.","comment_id":"810165","upvote_count":"1","timestamp":"1676511300.0"},{"comment_id":"786605","timestamp":"1674570720.0","content":"Selected Answer: A\nwhat is relevant here is to uncache Data Studio","poster":"PolyMoe","upvote_count":"1"},{"comment_id":"784790","timestamp":"1674431100.0","content":"A. Disable caching by editing the report settings.\n\nData Studio 360 uses caching to speed up report loading times. When caching is enabled, Data Studio 360 will only show the data that was present in the data source at the time the report was loaded. To ensure that the visualizations in your report are always up-to-date, you should disable caching by editing the report settings. This will force Data Studio 360 to retrieve the latest data from the data source (in this case BigQuery) every time the report is loaded.\n\nOption B is incorrect as it would only disable caching in BigQuery, but it wouldn't affect the caching in Data Studio 360, so the visualizations would still not show the latest data.\n\nOption C and D will not help as the data is not being updated in Data Studio 360, it's just the cache that needs to be updated.","upvote_count":"4","poster":"samdhimal"},{"poster":"korntewin","timestamp":"1673065440.0","content":"I'm confused, is it possible that the cache is in BigQuery level? and the looker just get the cache from bigquery","comment_id":"768231","upvote_count":"1"},{"timestamp":"1669078320.0","content":"Selected Answer: A\nBased on the doc, you can refresh the report using refresh button on the report, not the browser's refresh button. So the answer is A.","poster":"ejlp","comment_id":"724010","upvote_count":"1"},{"comment_id":"714647","content":"On my opinion it's C, because Data Studio doesn't support the Real-Time dashboard updates, so it means that if caching will be disabled, user will be forced to update the dashboard manually, otherwise report will be stuck on the data from the last update. According to documentation https://support.google.com/looker-studio/answer/7020039?hl=en#zippy=%2Cin-this-article, if we want to keep the data fresh we need to setup caching with minimum value of 15 minites - it means that data in the report will be updated automatically wvery 15 minutes, if cache will be disabled completely then the report will be stuck until we will manually update it. So, tbh for me it doesn't make sense to disable cache.","upvote_count":"1","poster":"maksi","timestamp":"1668003720.0"},{"timestamp":"1667276640.0","content":"C. \nSince the data is not always stale. When it is, click on refresh button. Document also says the same\nRefresh report data manually\nReport editors can refresh the cache at any time:\n\nView or edit the report.\nIn the top right, click More options. and then click RefreshRefresh data .\nThis refreshes the cache for every data source added to the report.","upvote_count":"1","comment_id":"708834","poster":"viks1122","comments":[{"poster":"beowulf_kat","comment_id":"709216","upvote_count":"3","timestamp":"1667312880.0","content":"Refreshing the web browser does not refresh the data behind the viz's in Data Studio. You have to click the 'refresh data source' button."}]},{"timestamp":"1666782840.0","comment_id":"704610","content":"Selected Answer: A\nhttps://support.google.com/looker-studio/answer/7020039?hl=en#zippy=%2Cin-this-article","upvote_count":"1","poster":"nicholascz"},{"timestamp":"1666250280.0","content":"A. should be correct. after disabled the cache, it will retrieve data every time.","comment_id":"699630","upvote_count":"1","poster":"kennyloo"},{"comment_id":"686048","timestamp":"1664873760.0","upvote_count":"3","poster":"max_c","content":"Selected Answer: C\nSame question from a Cloud guru and answer was C. The wording is slightly different in the documentation but still, the idea is that you can trigger a manual refresh","comments":[{"upvote_count":"1","poster":"Lestrang","content":"The option is to refresh the browser tab not inside data studio itself. incorrect.","timestamp":"1674892560.0","comment_id":"790387"}]},{"upvote_count":"4","timestamp":"1662180180.0","comment_id":"658072","poster":"AWSandeep","content":"Selected Answer: A\nA. Disable caching by editing the report settings."}],"answer_images":[]},{"id":"sLDAoxpSv7MgLe5Rjee4","question_text":"MJTelco Case Study -\n\nCompany Overview -\nMJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.\n\nCompany Background -\nFounded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.\nTheir management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.\n\nSolution Concept -\nMJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:\n✑ Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.\n✑ Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.\nMJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.\n\nBusiness Requirements -\n✑ Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.\n✑ Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.\nProvide reliable and timely access to data for analysis from distributed research workers\n//IMG//\n\n✑ Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.\n\nTechnical Requirements -\n✑ Ensure secure and efficient transport and storage of telemetry data\n✑ Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.\n✑ Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day\n✑ Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.\n\nCEO Statement -\nOur business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.\n\nCTO Statement -\nOur public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.\n\nCFO Statement -\nThe project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.\nYou create a new report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. It is company policy to ensure employees can view only the data associated with their region, so you create and populate a table for each region. You need to enforce the regional access policy to the data.\nWhich two actions should you take? (Choose two.)","answer":"BE","timestamp":"2020-03-21 04:25:00","exam_id":10,"topic":"1","unix_timestamp":1584761100,"isMC":true,"question_id":255,"question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0003200005.png"],"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/17076-exam-professional-data-engineer-topic-1-question-40/","answers_community":["BE (56%)","BC (28%)","Other"],"answer_description":"","choices":{"D":"Adjust the settings for each view to allow a related region-based security group view access.","E":"Adjust the settings for each dataset to allow a related region-based security group view access.","A":"Ensure all the tables are included in global dataset.","C":"Adjust the settings for each table to allow a related region-based security group view access.","B":"Ensure each table is included in a dataset for a region."},"discussion":[{"timestamp":"1584761100.0","poster":"[Removed]","comment_id":"66399","content":"Answer: B E","upvote_count":"48"},{"upvote_count":"36","comments":[{"comment_id":"428781","poster":"samstar4180","timestamp":"1629559020.0","content":"Yes, the correct answer should be BC - since we can have table-level access and each region represents a table.","upvote_count":"11"}],"comment_id":"363543","content":"C is correct starting 2020, as BigQuery come with table level access control\nhttps://cloud.google.com/blog/products/data-analytics/introducing-table-level-access-controls-in-bigquery","poster":"shanjin14","timestamp":"1621675080.0"},{"upvote_count":"1","comment_id":"1259059","poster":"iooj","content":"Selected Answer: BE\nC doesn't make sense, if you have already selected B","timestamp":"1722452940.0"},{"comment_id":"1214108","poster":"suwalsageen12","timestamp":"1716178380.0","upvote_count":"3","content":"Selected Answer: BE\nIf I choose Option E, then Option C and D are eliminated because, Once you provide the access level in dataset for the use group, it applies for both Table and view.\n\nNow for remaining options A and B.\nThe Question itself has stated to include the table region wise in dataset, So Option A is eliminated.\nSo, B and E are Correct answers."},{"poster":"niru12376","upvote_count":"6","comment_id":"1162341","content":"BE... If we've already split the tables into regions via datasets then why give regional access through tables again.. If not, then AC but definitely not BC.. Please help","timestamp":"1709195280.0"},{"comment_id":"1065315","upvote_count":"3","poster":"rocky48","timestamp":"1699418100.0","content":"Answer : BE\n\nB. Ensure each table is included in a dataset for a region.\nThis means that you should organize your data in BigQuery into separate datasets, one for each region. Each dataset contains the tables specific to that region. This ensures that data is segregated by region.\nE. Adjust the settings for each dataset to allow a related region-based security group view access."},{"content":"Selected Answer: BE\nB. Ensure each table is included in a dataset for a region.\n\nThis means that you should organize your data in BigQuery into separate datasets, one for each region. Each dataset contains the tables specific to that region. This ensures that data is segregated by region.\nE. Adjust the settings for each dataset to allow a related region-based security group view access.\n\nYou should set the access controls at the dataset level in BigQuery. This means configuring access permissions for each dataset based on regional security groups. This way, you can enforce the regional access policy to the data, ensuring that users from different regions can only access the data associated with their region.\nOption A is not necessary because you don't need to include all the tables in a global dataset. Segregating data into region-specific datasets is a better approach for enforcing access controls.\n\nOptions C and D are not typical actions in BigQuery. Access control and permissions are usually managed at the dataset level, and you can grant access to specific groups at that level.","timestamp":"1697988480.0","comment_id":"1050802","poster":"rtcpost","upvote_count":"1"},{"poster":"Mathew106","comment_id":"961337","upvote_count":"4","timestamp":"1690187760.0","content":"It's B and E or B and C. However, B and E makes some more sense because if you have one dataset for each region and they need to access the data for each region then why not allow them access to the whole dataset? What if you want to add other supplementary tables later? If you did that on a table level you would have to add access to every table separately. \n\nStill, I think both are valid because we don't have any extra requirement, but B E makes more sense."},{"timestamp":"1683051360.0","comment_id":"887781","upvote_count":"4","poster":"Jarek7","content":"Selected Answer: C\nThe intended answer was for sure BE. If C or D would be the right answers there is absolutely no reason to do B, right? Why should you put each table into separate dataset if you then set the accesss on table/view level? What is more the question is about tables not views, so I have no idea why would anybody take D. \nThe issue is that this question is out of date and now the right answer would be sole C."},{"upvote_count":"2","poster":"Oleksandr0501","timestamp":"1682336280.0","comment_id":"879268","content":"The two actions that should be taken are B and E.\nB. Ensure each table is included in a dataset for a region: By creating separate datasets for each region and including only the tables associated with that region, you can enforce the regional access policy.\n\nE. Adjust the settings for each dataset to allow a related region-based security group view access: By adjusting the settings for each dataset to allow only the related region-based security group view access, you can ensure that employees can only view data associated with their region.\n\nA is incorrect because including all tables in a global dataset would not enforce the regional access policy.\n\nC is incorrect because adjusting the settings for each table is not a scalable solution, especially as the number of tables grows.\n\nD is incorrect because adjusting the settings for each view does not ensure that employees can only view data associated with their region."},{"content":"Selected Answer: BC\nB: Location is on dataset level: https://cloud.google.com/bigquery/docs/datasets#dataset_limitations\nC: IAM can be set on table level","poster":"sjtesla","timestamp":"1681557420.0","upvote_count":"2","comment_id":"870865"},{"timestamp":"1675003980.0","comment_id":"791742","poster":"Lestrang","content":"Guys, \nthere are 2 possible combinations\nIf you think that each table represents a region, then they should all be in a global dataset and you should apply table access control to them.\nSo A+C\n\nOtherwise you would put each table in a regional dataset, and apply access control to the dataset. Why would you create a dataset for the purpose of controlling regional access, and then only apply the controls to a table inside it? that is not extensible in the future.\n Anyway create dataset+access control for dataset (B+E) is also valid.\n\nWhich to choose? I dont know.","upvote_count":"3"},{"upvote_count":"2","content":"Selected Answer: BE\nFirst put tables in region-dedicated dataset (B)\nThen, ensure access control at dataset level (by creating region-based security groups) (E)","comment_id":"788665","timestamp":"1674733260.0","poster":"PolyMoe"},{"comments":[{"poster":"korntewin","timestamp":"1673142600.0","content":"Oh, the location should be specified in the dataset level! Then, the dataset should be splitted by region, my bad!","upvote_count":"1","comment_id":"769021"}],"upvote_count":"3","timestamp":"1673142300.0","poster":"korntewin","content":"Selected Answer: AC\nI would vote for AC. As we already split the table for each region, why do we need to split the dataset per region? Furthermore, the access control will be provided to the users based on table level anyway.","comment_id":"769018"},{"timestamp":"1667219820.0","comment_id":"708406","upvote_count":"7","content":"Selected Answer: BE\nif you create table-level access control and grant it to different groups for different tables, what is the point of putting tables in different datasets and different regions?\nSo i choose BE","poster":"MisuLava"},{"comment_id":"678794","content":"Selected Answer: BC\nBigQuery come with table level access control. Since we can have table-level access and each region represents a table, B & C is correct answer.","poster":"svkds","timestamp":"1664110920.0","upvote_count":"2"},{"timestamp":"1661724180.0","upvote_count":"1","content":"Selected Answer: BC\nI voted for BC","poster":"ducc","comment_id":"653176"},{"poster":"NR22","timestamp":"1650629640.0","content":"Selected Answer: BC\ncan apply IAM policies at table level","upvote_count":"1","comment_id":"589971"},{"upvote_count":"1","poster":"devric","content":"Selected Answer: BE\nB & E. If you want to apply access controls over tables and views, you should apply them over datasets.","timestamp":"1649116560.0","comment_id":"580964"},{"comment_id":"580317","upvote_count":"1","poster":"szl0144","timestamp":"1648995840.0","content":"Selected Answer: BC\nBC are correct"},{"upvote_count":"1","timestamp":"1642903380.0","comment_id":"530203","poster":"exnaniantwort","content":"Selected Answer: BC\n\"ensure employees can view only the data associated with their region, so you create and populate a table for each region\"\ndefinitely C instead of E"},{"comment_id":"523223","timestamp":"1642121100.0","upvote_count":"2","poster":"sraakesh95","content":"Selected Answer: BC\nAgree with B as the table split is region / location based\nC: table level access is possible in Cloud IAM, which can be used to our advatnage for individual regions.\nhttps://cloud.google.com/bigquery/docs/table-access-controls-intro"},{"timestamp":"1641298800.0","comment_id":"516564","poster":"medeis_jar","content":"Selected Answer: BE\nas explained by hendrixlives","upvote_count":"1"},{"timestamp":"1640685780.0","content":"B,E because Table takes region of dataset","comment_id":"510981","upvote_count":"1","poster":"prasanna77"},{"timestamp":"1639723080.0","poster":"hendrixlives","upvote_count":"8","comment_id":"503396","comments":[{"poster":"exnaniantwort","comments":[{"upvote_count":"1","timestamp":"1649116500.0","poster":"devric","content":"But the option B means that you will generate a data set per region also. So the permissions can be assigned at the dataset level.\n\nIf you pick the option A also, then you should assign the permissions over tables but... what about views? You should assign permissions to them also.\n\nThat's why I pick B and E.","comment_id":"580963"}],"content":"1 table for each region\nyou can just expect there is a limited number of regions\nthis number should be manageable\nnot E","comment_id":"530201","upvote_count":"1","timestamp":"1642903260.0"}],"content":"Selected Answer: BE\nB/E: Even if now BQ offers table level access control, since the number of tables can be expected to be high, controlling access at the dataset level would ease operations. That is why I would still go for E instead of C."},{"poster":"nellyoaid","comment_id":"495194","content":"Selected Answer: BC\nAnswer","comments":[{"upvote_count":"2","timestamp":"1639073220.0","comment_id":"497950","content":"Now, Bigquery allows table level access control, but if you have 1.000 tables in a regional dataset, focusing on dataset is the easy and quick way to manage it.","poster":"Jlozano"}],"upvote_count":"2","timestamp":"1638800040.0"},{"content":"BD\nA is incorrect, since regional access control is required, global won't work. \nCE are incorrect, because employees are not required to access the actual data, they just need to view it. So D is a better option in this case.","timestamp":"1635899880.0","upvote_count":"2","comment_id":"471869","poster":"JayZeeLee"},{"content":"D is wrong as it's a limitation with views. From doc.\nThe dataset that contains your view and the dataset that contains the tables referenced by the view must be in the same location.","comment_id":"468916","timestamp":"1635379920.0","upvote_count":"1","poster":"gcp_k"},{"comment_id":"465165","content":"BC\nhttps://cloud.google.com/blog/products/data-analytics/introducing-table-level-access-controls-in-bigquery","timestamp":"1634734380.0","poster":"littlewat","upvote_count":"1"},{"timestamp":"1634088780.0","upvote_count":"7","comment_id":"461326","poster":"Chelseajcole","content":"Some newer answers support Table level control as Google launched new table level control last year. But do we need table-level control? We only need dataset-level control. So create different region data as different dataset, make sure all table belong to that region included in the regional dataset, grant access on dataset. Answer is BE."},{"timestamp":"1634056680.0","poster":"anji007","upvote_count":"1","content":"Ans: B & C\nActually B, C and E are correct, but since need to select only TWO so choose B & C as table level access is possible.","comment_id":"461188"},{"content":"B, C and E.\nC is correct. \nhttps://cloud.google.com/blog/products/data-analytics/introducing-table-level-access-controls-in-bigquery","poster":"squishy_fishy","timestamp":"1633265160.0","upvote_count":"2","comment_id":"456578"},{"upvote_count":"1","timestamp":"1625751840.0","content":"Vote for BE","poster":"sumanshu","comment_id":"401964"},{"comment_id":"309268","content":"Correct: CD\nThis is my understanding. The dataset is same globally. We create separate tables and views for each region in the same dataset. Access can now be controlled at the table and view level in BigQuery.","poster":"BhupiSG","timestamp":"1615590840.0","upvote_count":"2"},{"upvote_count":"4","poster":"daghayeghi","content":"B, E:\nAccess is at the dataset level, not each object like table, view.","comment_id":"307232","timestamp":"1615386180.0"},{"timestamp":"1612716660.0","content":"Correct BD","upvote_count":"2","poster":"naga","comment_id":"285632"},{"content":"Why not BC. We can assign table level access based on region . It serves the same purpose that of E.","timestamp":"1609710480.0","comment_id":"258895","upvote_count":"2","poster":"muraric"},{"content":"B is definitely correct. But now that we have table level access control in big query, will C also be a right answer? But because question says table is populated for each region, dataset is also created region specific considering which E is correct.","comments":[{"content":"If its single option, I think we can choose C... as access at the granular level can be given... and as we are creating a data set as part of option B, should go with E I believe.","timestamp":"1613993040.0","comment_id":"296590","upvote_count":"2","poster":"Jay3244"}],"comment_id":"221908","timestamp":"1605706080.0","upvote_count":"4","poster":"Radhika7983"},{"upvote_count":"2","timestamp":"1600418160.0","content":"Answer: B, E","poster":"SteelWarrior","comment_id":"181488"},{"poster":"haroldbenites","comment_id":"161140","timestamp":"1597791780.0","upvote_count":"3","content":"B E Correct"},{"poster":"yoRob","timestamp":"1596110100.0","comment_id":"147382","upvote_count":"4","content":"B and E.\nB and D dont make sense as a combination. Why create different Datasets and then access control Views."},{"content":"BD\nAccess can be granted \nAt Project level, dataset level and view level","comments":[{"upvote_count":"5","content":"sorry its BE not BD","timestamp":"1595269440.0","comment_id":"139769","poster":"atnafu2020"}],"comment_id":"139766","poster":"atnafu2020","upvote_count":"2","timestamp":"1595269380.0"},{"timestamp":"1594599420.0","upvote_count":"4","comment_id":"133347","content":"B, E. Access is at the dataset level, not each object like table, view.","poster":"tprashanth"},{"content":"Correct B and E","timestamp":"1594356780.0","poster":"Rajuuu","upvote_count":"2","comment_id":"131151"},{"upvote_count":"4","timestamp":"1589667660.0","comment_id":"90173","content":"Correct BE","poster":"arnabbis4u"}],"answer_ET":"BE"}],"exam":{"isBeta":false,"numberOfQuestions":319,"id":10,"name":"Professional Data Engineer","provider":"Google","isMCOnly":true,"lastUpdated":"15 Feb 2025","isImplemented":true},"currentPage":51},"__N_SSP":true}