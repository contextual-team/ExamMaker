{"pageProps":{"questions":[{"id":"Ixs9C4hKihXMZmBm6XHj","timestamp":"2022-09-02 13:45:00","exam_id":10,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/79388-exam-professional-data-engineer-topic-1-question-28/","choices":{"B":"Use .fromQuery operation to read specific fields from the table.","C":"Use of both the Google BigQuery TableSchema and TableFieldSchema classes.","A":"Specify the TableReference object in the code.","D":"Call a transform that returns TableRow objects, where each element in the PCollection represents a single row in the table."},"answer_description":"","unix_timestamp":1662119100,"discussion":[{"comment_id":"657369","upvote_count":"12","timestamp":"1677764700.0","content":"B BigQueryIO.read.fromQuery() executes a query and then reads the results received after the query execution. Therefore, this function is more time-consuming, given that it requires that a query is first executed (which will incur in the corresponding economic and computational costs).","poster":"arthur2385"},{"upvote_count":"7","comments":[{"timestamp":"1680951660.0","comment_id":"689208","content":"The answer is B","poster":"maxdataengineer","upvote_count":"2","comments":[{"timestamp":"1700232840.0","content":"According to Chat GPT, it is also B\nIn general, if your \"primary goal is to reduce the amount of data read and transferred\", and the downstream processing mainly focuses on a subset of fields, using .fromQuery to select specific fields would be a good choice. \n\nOn the other hand, if you need to simplify downstream processing and optimize resource utilization, transforming data into TableRow objects might be more suitable.","poster":"cetanx","upvote_count":"3","comment_id":"900155"}]}],"content":"Since we want to be able to analyze data from a new ML feature (column) we only need to check values from that column. By doing a fromQuery(SELECT featueColum FROM table)\nwe are optimizing costs and performance since we are not checking all columns.\n\nhttps://cloud.google.com/bigquery/docs/best-practices-costs#avoid_select_","timestamp":"1680951600.0","comment_id":"689207","poster":"maxdataengineer"},{"poster":"MaxNRG","content":"Selected Answer: B\nB as BigQueryIO.read.from() directly reads the whole table from BigQuery. \nThis function exports the whole table to temporary files in Google Cloud Storage, where it will later be read from. \nThis requires almost no computation, as it only performs an export job, and later Dataflow reads from GCS (not from BigQuery).\nBigQueryIO.read.fromQuery() executes a query and then reads the results received after the query execution. Therefore, this function is more time-consuming, given that it requires that a query is first executed (which will incur in the corresponding economic and computational costs).\nhttps://stackoverflow.com/questions/54413681/bigqueryio-read-vs-fromquery","upvote_count":"1","timestamp":"1718523960.0","comment_id":"1098035"},{"comment_id":"1076373","upvote_count":"1","content":"Selected Answer: B\nB works for me","timestamp":"1716294660.0","poster":"axantroff"},{"upvote_count":"1","poster":"pue_dev_anon","comment_id":"1075430","timestamp":"1716204780.0","content":"Selected Answer: B\nWe are trying to optimize reading each row is not optimal, we want columns"},{"upvote_count":"5","content":"Selected Answer: B\nB. Use the .fromQuery operation to read specific fields from the table.\n\nUsing the .fromQuery operation allows you to specify the exact fields you need to read from the table, which can significantly improve performance by reducing the amount of data that needs to be processed. This is particularly important when dealing with large and growing datasets.\n\nOption A (specifying the TableReference object) provides information about the table but doesn't inherently improve the performance of reading specific fields.\n\nOption C (using Google BigQuery TableSchema and TableFieldSchema classes) is related to specifying the schema of the data but doesn't directly address improving the performance of reading specific fields.\n\nOption D (calling a transform that returns TableRow objects) is more about how the data is processed after it's read, not how it's initially read from BigQuery.","timestamp":"1713787980.0","comment_id":"1050537","poster":"rtcpost"},{"content":"When I have a different answer then the \"Correct Answer\", I run it through AI and it keeps saying ExamTopics is wrong. Is there any way to know if I am going to pass or fail this exam?","timestamp":"1711049220.0","comments":[{"comment_id":"1076368","content":"AI is just a LLM model, not a silver bullet at all","upvote_count":"1","poster":"axantroff","timestamp":"1716294420.0"}],"comment_id":"1013306","poster":"emmylou","upvote_count":"2"},{"comment_id":"1008770","timestamp":"1710555780.0","upvote_count":"1","poster":"suku2","content":"Selected Answer: B\nSince the requirement is to read the data for a *new* key features in the logs, it makes sense to select limited columns, which are required rather than using .from() method which exports the entire BigQuery table.\nB makes sense here."},{"upvote_count":"1","timestamp":"1709667960.0","content":"Selected Answer: B\nSHOULD be B.\nNot quite sure how D is the correct answer (Red herring....?) when you want to improve the query, which is .fromQuery and NOT transform and PCollection....","poster":"gudguy1a","comment_id":"999763"},{"poster":"odiez3","comment_id":"966774","upvote_count":"1","content":"Answer Is D, imagine that you dont have permission on BQ AND you cant see the table info or anything else about the table you only aré working whit dataflow the only way Is transform the data using apache beam","timestamp":"1706585160.0"},{"poster":"Mathew106","content":"Selected Answer: B\nI have seen people explain why B is not right because it doesn't optimize performance but only cost, which is not true, or because fromQuery is still not performant.\n\nI think it's B because no other option is more performant, even if you claim it's not good. \n\nAs for option D, the transform given by the description is already a transform that provides as output a PCollection of TableRow objects. So how would that be any different?\n\nhttps://beam.apache.org/releases/javadoc/2.1.0/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html","comment_id":"961301","timestamp":"1706090220.0","upvote_count":"1"},{"comment_id":"954256","content":"Why should it be D? \n\"fromQuery()\" allows us to read only the columns we want, I see no point in using a Transform for each row of a \"SELECT *\", which, moreover, is a bad BQ Practice.","timestamp":"1705505640.0","upvote_count":"1","poster":"theseawillclaim"},{"comment_id":"904957","poster":"avg_uchiha","content":"Correct answer should be B. BigQuery is a columnar storage, so reducing the number of fields being selected should improve performance.","timestamp":"1700754120.0","upvote_count":"1"},{"content":"Selected Answer: B\nDoes BigQuery have a pCollections? I thought it's unique to Apache Beam i.e. Cloud Dataflow","comment_id":"778732","timestamp":"1689578340.0","upvote_count":"1","poster":"jkh_goh"},{"content":"Guys, how is B the answer? Like all the justifications given here, BigQueryIO.read.fromQuery() is time consuming and the question asked for a better performance solution.","upvote_count":"4","comment_id":"731587","poster":"kelvintoys93","timestamp":"1685448900.0","comments":[{"comment_id":"791671","timestamp":"1690630560.0","content":"That part is the docs trying to explain the side effects of using it, however, the part that is important to us is the fact that it reads from a query. \"Read\" reads the whole table. If we specify a query we can say select col1 only, which makes it all more efficient.","poster":"Lestrang","upvote_count":"2"}]},{"timestamp":"1681302060.0","comment_id":"693068","content":"Selected Answer: B\nreading only relevant cols","upvote_count":"6","poster":"gcm7"},{"comment_id":"688938","poster":"devaid","upvote_count":"1","timestamp":"1680910020.0","content":"Selected Answer: D\nAnswer is D, apparently."},{"upvote_count":"3","content":"Answer is Use .fromQuery operation to read specific fields from the table.\n\nBigQueryIO.read.from() directly reads the whole table from BigQuery. This function exports the whole table to temporary files in Google Cloud Storage, where it will later be read from. This requires almost no computation, as it only performs an export job, and later Dataflow reads from GCS (not from BigQuery).\n\nBigQueryIO.read.fromQuery() executes a query and then reads the results received after the query execution. Therefore, this function is more time-consuming, given that it requires that a query is first executed (which will incur in the corresponding economic and computational costs).\n\nReference:\nhttps://cloud.google.com/bigquery/docs/best-practices-costs#avoid_select_","timestamp":"1679739840.0","comment_id":"678576","poster":"Kowalski"},{"content":"Selected Answer: D\nD is correct, you want to increase the performance, not the cost","poster":"crismo04","comment_id":"665289","timestamp":"1678449480.0","upvote_count":"2"},{"timestamp":"1677849240.0","poster":"AWSandeep","comment_id":"658371","content":"Selected Answer: B\nB. Use .fromQuery operation to read specific fields from the table.","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: B\nB is correct","comment_id":"658043","timestamp":"1677822900.0","poster":"ducc"}],"isMC":true,"question_text":"Your company is performing data preprocessing for a learning algorithm in Google Cloud Dataflow. Numerous data logs are being are being generated during this step, and the team wants to analyze them. Due to the dynamic nature of the campaign, the data is growing exponentially every hour.\nThe data scientists have written the following code to read the data for a new key features in the logs.\n//IMG//\n\nYou want to improve the performance of this data read. What should you do?","topic":"1","answers_community":["B (88%)","12%"],"answer_ET":"B","question_id":201,"question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0001600001.png"],"answer":"B"},{"id":"MbSYWKXEoGHesIiehFZc","url":"https://www.examtopics.com/discussions/google/view/130265-exam-professional-data-engineer-topic-1-question-280/","question_text":"You are running a streaming pipeline with Dataflow and are using hopping windows to group the data as the data arrives. You noticed that some data is arriving late but is not being marked as late data, which is resulting in inaccurate aggregations downstream. You need to find a solution that allows you to capture the late data in the appropriate window. What should you do?","question_images":[],"answer_ET":"A","answer_description":"","topic":"1","answers_community":["A (100%)"],"timestamp":"2024-01-04 05:32:00","discussion":[{"poster":"raaad","content":"Selected Answer: A\n- Watermarks: Watermarks in a streaming pipeline are used to specify the point in time when Dataflow expects all data up to that point to have arrived. \n- Allow Late Data: configure the pipeline to accept and correctly process data that arrives after the watermark, ensuring it's captured in the appropriate window.","comment_id":"1117884","upvote_count":"7","timestamp":"1720555860.0"},{"poster":"Pime13","comment_id":"1337169","content":"Selected Answer: A\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#watermarks\nA watermark is a threshold that indicates when Dataflow expects all of the data in a window to have arrived. If the watermark has progressed past the end of the window and new data arrives with a timestamp within the window, the data is considered late data. For more information, see Watermarks and late data in the Apache Beam documentation.\n\nDataflow tracks watermarks because of the following reasons:\n\nData is not guaranteed to arrive in time order or at predictable intervals.\nData events are not guaranteed to appear in pipelines in the same order that they were generated.","timestamp":"1736171760.0","upvote_count":"1"},{"poster":"m_a_p_s","upvote_count":"1","comment_id":"1325827","timestamp":"1734032340.0","content":"Selected Answer: A\nA - https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#watermarks"},{"poster":"JyoGCP","upvote_count":"1","content":"Selected Answer: A\nOption A","comment_id":"1155359","timestamp":"1724220660.0"},{"timestamp":"1720881300.0","comment_id":"1121855","poster":"Matt_108","upvote_count":"3","content":"Selected Answer: A\nOption A - https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#watermarks"},{"timestamp":"1720532220.0","content":"Selected Answer: A\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#watermarks","comment_id":"1117607","poster":"Sofiia98","upvote_count":"2"},{"comment_id":"1113337","upvote_count":"1","timestamp":"1720060320.0","content":"Selected Answer: A\nA. Use watermarks to define the expected data arrival window. Allow late data as it arrives.","poster":"scaenruy"}],"answer_images":[],"answer":"A","unix_timestamp":1704342720,"exam_id":10,"choices":{"D":"Expand your hopping window so that the late data has more time to arrive within the grouping.","B":"Change your windowing function to tumbling windows to avoid overlapping window periods.","A":"Use watermarks to define the expected data arrival window. Allow late data as it arrives.","C":"Change your windowing function to session windows to define your windows based on certain activity."},"question_id":202,"isMC":true},{"id":"1t4NBapK9iWrFAqbgVPH","discussion":[{"poster":"Matt_108","timestamp":"1720881480.0","comments":[{"timestamp":"1721879880.0","upvote_count":"10","content":"Agree. https://cloud.google.com/bigtable/docs/garbage-collection#data-removed\n\"Because it can take up to a week for expired data to be deleted, you should never rely solely on garbage collection policies to ensure that read requests return the desired data. Always apply a filter to your read requests that excludes the same values as your garbage collection rules. You can filter by limiting the number of cells per column or by specifying a timestamp range.\"","comment_id":"1131343","poster":"AllenChen123"}],"content":"Selected Answer: B\nAgree with others https://cloud.google.com/bigtable/docs/garbage-collection","comment_id":"1121856","upvote_count":"8"},{"content":"Selected Answer: B\nAgree with MAtt_108 and AllenChen 123.\n\"Garbage collection is a continuous process in which Bigtable checks the rules for each column family and deletes expired and obsolete data accordingly. In general, it can take up to a week from the time that data matches the criteria in the rules for the data to actually be deleted. You are not able to change the timing of garbage collection.\"\n\n\"Always apply a filter to your read requests that exclude the same values as your garbage collection rules. \"\n\nRef: https://cloud.google.com/bigtable/docs/garbage-collection#data-removed","timestamp":"1724622660.0","comment_id":"1159253","poster":"cuadradobertolinisebastiancami","upvote_count":"5"},{"content":"Selected Answer: B\nBecause it can take up to a week for expired data to be deleted, you should never rely solely on garbage collection policies to ensure that read requests return the desired data. Always apply a filter to your read requests that excludes the same values as your garbage collection rules. You can filter by limiting the number of cells per column or by specifying a timestamp range.","timestamp":"1736167320.0","upvote_count":"1","comment_id":"1337140","poster":"Pime13"},{"poster":"m_a_p_s","timestamp":"1734031740.0","content":"Selected Answer: B\n\"Because it can take up to a week for expired data to be deleted, you should never rely solely on garbage collection policies to ensure that read requests return the desired data. Always apply a filter to your read requests that excludes the same values as your garbage collection rules. You can filter by limiting the number of cells per column or by specifying a timestamp range.\"\n\nhttps://cloud.google.com/bigtable/docs/garbage-collection#data-removed","upvote_count":"1","comment_id":"1325825"},{"content":"Selected Answer: B\nI will go for B too","poster":"Sofiia98","upvote_count":"1","timestamp":"1720591080.0","comment_id":"1118322"},{"upvote_count":"3","timestamp":"1720343700.0","poster":"GCP001","content":"B. Use a timestamp range filter in the query to fetch the customer's data for a specific range.\n\nAlways use query filter as garbage collectore runs on it's way - https://cloud.google.com/bigtable/docs/garbage-collection","comment_id":"1115752"},{"upvote_count":"1","poster":"scaenruy","timestamp":"1720063680.0","comment_id":"1113352","content":"Selected Answer: B\nB. Use a timestamp range filter in the query to fetch the customer's data for a specific range."}],"url":"https://www.examtopics.com/discussions/google/view/130269-exam-professional-data-engineer-topic-1-question-281/","question_id":203,"answer_ET":"B","exam_id":10,"timestamp":"2024-01-04 06:28:00","choices":{"A":"Set the expiring values of the column families to 29 days and keep the number of versions to 1.","C":"Schedule a job daily to scan the data in the table and delete data older than 30 days.","B":"Use a timestamp range filter in the query to fetch the customer's data for a specific range.","D":"Set the expiring values of the column families to 30 days and set the number of versions to 2."},"answer_images":[],"question_images":[],"answers_community":["B (100%)"],"answer":"B","question_text":"You work for a large ecommerce company. You store your customer's order data in Bigtable. You have a garbage collection policy set to delete the data after 30 days and the number of versions is set to 1. When the data analysts run a query to report total customer spending, the analysts sometimes see customer data that is older than 30 days. You need to ensure that the analysts do not see customer data older than 30 days while minimizing cost and overhead. What should you do?","answer_description":"","isMC":true,"unix_timestamp":1704346080,"topic":"1"},{"id":"IyspWcJCLafk4fWV6rHT","answer_images":[],"isMC":true,"answer":"B","choices":{"C":"Use the BigQuery Streaming API and ensure that your target BigQuery table is regional.","D":"Use the BigQuery Streaming API and ensure that your target BigQuery table is multiregional.","A":"Use the BigQuery Storage Write API and ensure that your target BigQuery table is regional.","B":"Use the BigQuery Storage Write API and ensure that your target BigQuery table is multiregional."},"unix_timestamp":1704247560,"url":"https://www.examtopics.com/discussions/google/view/130157-exam-professional-data-engineer-topic-1-question-282/","topic":"1","question_id":204,"answer_ET":"B","timestamp":"2024-01-03 03:06:00","question_images":[],"discussion":[{"timestamp":"1717596720.0","comment_id":"1224767","poster":"AlizCert","content":"Selected Answer: B\nIt should B, Storage Write API has \"3 GB per second throughput in multi-regions; 300 MB per second in regions\"","upvote_count":"15"},{"content":"Selected Answer: A\n- BigQuery Storage Write API: This API is designed for high-throughput, low-latency writing of data into BigQuery. It also provides tools to prevent data duplication, which is essential for exactly-once delivery semantics.\n- Regional Table: Choosing a regional location for the BigQuery table could potentially provide better performance and lower latency, as it would be closer to the Dataflow job if they are in the same region.","poster":"raaad","upvote_count":"13","comment_id":"1117901","comments":[{"comment_id":"1131348","timestamp":"1706162640.0","upvote_count":"4","content":"Agree.\nhttps://cloud.google.com/bigquery/docs/write-api#advantages","poster":"AllenChen123"}],"timestamp":"1704840960.0"},{"poster":"Siahara","content":"Selected Answer: A\nA. Implement the BigQuery Storage Write API and guarantee that the target BigQuery table is regional.\n\nHere's the breakdown:\n\nWhy Option A is Superior\n\nExactly-Once Delivery: The BigQuery Storage Write API intrinsically supports exactly-once delivery using stream offsets. This guarantees that each message is written to BigQuery exactly one time, even in the case of retries due to the lack of native exactly-once support in your message bus.\n\nHigh Throughput: The Storage Write API is optimized for high-throughput scenarios. It can handle the expected ingestion throughput of 1.5 GB per second.\n\nRegional Tables: Using a regional BigQuery table aligns with best practices when utilizing the Storage Write API, as it helps to minimize latency and reduce potential cross-region communication costs.","upvote_count":"1","timestamp":"1738862460.0","comment_id":"1352537"},{"comment_id":"1349571","poster":"juliorevk","content":"Selected Answer: B\n- BigQuery Storage Write API: This API is designed for high-throughput, low-latency writing of data into BigQuery. It also provides tools to prevent data duplication, which is essential for exactly-once delivery semantics.\n- The multiregional table ensures that your data is highly available and can be streamed into BigQuery across multiple regions. It is better suited for high-throughput and low-latency workloads, as it provides distributed write capabilities that can handle large data volumes, such as the 1.5 GB per second you expect to stream.","timestamp":"1738336260.0","upvote_count":"1"},{"timestamp":"1736167620.0","upvote_count":"1","poster":"Pime13","content":"Selected Answer: A\nhttps://cloud.google.com/bigquery/docs/streaming-data-into-bigquery\nFor new projects, we recommend using the BigQuery Storage Write API instead of the tabledata.insertAll method. The Storage Write API has lower pricing and more robust features, including exactly-once delivery semantics\nhttps://cloud.google.com/bigquery/docs/write-api#advantages","comment_id":"1337142"},{"content":"Selected Answer: B\nB is correct.\nWhen aiming for exactly-once delivery in a Dataflow streaming job, the key is to use the BigQuery Storage Write API, as it provides the capability to handle large-scale data ingestion with the correct semantics, including exactly-once delivery.","upvote_count":"1","comment_id":"1332385","timestamp":"1735301460.0","poster":"hussain.sain"},{"timestamp":"1734202020.0","poster":"himadri1983","content":"Selected Answer: B\n3 GB per second throughput in multi-regions; 300 MB per second in regions\nhttps://cloud.google.com/bigquery/quotas#write-api-limits","upvote_count":"1","comment_id":"1326591"},{"content":"Selected Answer: B\nstreamed into BigQuery with exactly-once delivery semantics >>> Storage Write API\n\ningestion throughput into BigQuery to be about 1.5 GB per second >>> multiregional (check throughput rate here >>> https://cloud.google.com/bigquery/quotas#write-api-limits)","upvote_count":"2","comment_id":"1325823","timestamp":"1734031560.0","poster":"m_a_p_s"},{"upvote_count":"1","poster":"NatyNogas","comment_id":"1320572","timestamp":"1733061060.0","content":"Selected Answer: A\n- Choosing a regional target BigQuery table ensures that data is stored redundantly in a single region, providing high availability and durability."},{"timestamp":"1732750080.0","upvote_count":"2","comment_id":"1318955","content":"Selected Answer: B\nAccording to this documentation, its B\nhttps://cloud.google.com/bigquery/quotas#write-api-limits","poster":"CloudAdrMX"},{"upvote_count":"1","content":"Selected Answer: A\nWrite API support 2.5 GB / sec speed and support exactly-once delivery semantics\nhttps://cloud.google.com/bigquery/docs/write-api#connections\n\nwhereas in streaming duplicates can come and needed to remove them manually \nhttps://cloud.google.com/bigquery/docs/streaming-data-into-bigquery#dataavailability","poster":"imazy","timestamp":"1731236220.0","comment_id":"1309403"},{"comment_id":"1305832","upvote_count":"4","poster":"SamuelTsch","timestamp":"1730468040.0","content":"Selected Answer: B\nlooking for this documentation https://cloud.google.com/bigquery/quotas#write-api-limits. 3 GB/s in multi-regions; 300MB/s in regions"},{"upvote_count":"2","timestamp":"1727669340.0","content":"To ensure that analysts do not see customer data older than 30 days while minimizing cost and overhead, the best option is:\nB. Use a timestamp range filter in the query to fetch the customer’s data for a specific range.\n\nThis approach directly addresses the issue by filtering out data older than 30 days at query time, ensuring that only the relevant data is retrieved. It avoids the overhead and potential delays associated with garbage collection and manual deletion processes","comment_id":"1291384","poster":"HermanTan"},{"upvote_count":"1","comment_id":"1174376","content":"Selected Answer: D\noption D","timestamp":"1710519360.0","comments":[{"comment_id":"1190772","poster":"BennyXu","upvote_count":"1","timestamp":"1712467320.0","content":"you are wrong!!!!!!!!!!!!"}],"poster":"hanoverquay"},{"upvote_count":"1","poster":"Matt_108","timestamp":"1705164060.0","content":"Selected Answer: A\nOption A","comment_id":"1121860"},{"comments":[{"upvote_count":"4","timestamp":"1704645120.0","poster":"Smakyel79","content":"This option leverages the BigQuery Storage Write API's capability for exactly-once delivery semantics and a regional table setting that can meet compliance and data locality needs without impacting the delivery semantics. The BigQuery Storage Write API is more suitable for your high-throughput requirements compared to the BigQuery Streaming API.","comment_id":"1116006"}],"comment_id":"1112419","timestamp":"1704247560.0","content":"Selected Answer: A\nVoting on A","poster":"Ed_Kim","upvote_count":"2"}],"answer_description":"","answers_community":["B (55%)","A (43%)","2%"],"question_text":"You are using a Dataflow streaming job to read messages from a message bus that does not support exactly-once delivery. Your job then applies some transformations, and loads the result into BigQuery. You want to ensure that your data is being streamed into BigQuery with exactly-once delivery semantics. You expect your ingestion throughput into BigQuery to be about 1.5 GB per second. What should you do?","exam_id":10},{"id":"rl2Tv3438KcelLgbStbQ","timestamp":"2024-01-07 16:17:00","discussion":[{"comments":[{"poster":"AllenChen123","comments":[{"poster":"AllenChen123","content":"And https://cloud.google.com/bigquery/docs/external-data-cloud-storage#upgrade-external-tables-to-biglake-tables","timestamp":"1721880480.0","upvote_count":"5","comment_id":"1131351"}],"upvote_count":"4","content":"Agree. https://cloud.google.com/bigquery/docs/biglake-intro#metadata_caching_for_performance","timestamp":"1721536380.0","comment_id":"1127666"}],"timestamp":"1720558860.0","comment_id":"1117905","poster":"raaad","content":"Selected Answer: C\n- BigLake Table: BigLake allows for more efficient querying of data lakes stored in Cloud Storage. It can handle large datasets more effectively than standard external tables.\n- Metadata Caching: Enabling metadata caching can significantly improve query performance by reducing the time taken to read and process metadata from a large number of files.","upvote_count":"8"},{"content":"Selected Answer: C\nvote C","upvote_count":"1","comment_id":"1174374","timestamp":"1726409700.0","poster":"hanoverquay"},{"poster":"JyoGCP","content":"Selected Answer: C\nOption C","upvote_count":"1","timestamp":"1724227980.0","comment_id":"1155425"},{"comment_id":"1121863","content":"Selected Answer: C\nOption C","timestamp":"1720881780.0","poster":"Matt_108","upvote_count":"1"},{"poster":"Sofiia98","content":"Selected Answer: C\nagree with C","comment_id":"1118362","timestamp":"1720592220.0","upvote_count":"1"},{"content":"C. Upgrade the external table to a BigLake table. Enable metadata caching for the table. \nCheck ref - https://cloud.google.com/bigquery/docs/biglake-intro","timestamp":"1720358220.0","upvote_count":"2","poster":"GCP001","comment_id":"1115952"}],"question_images":[],"unix_timestamp":1704640620,"question_text":"You have created an external table for Apache Hive partitioned data that resides in a Cloud Storage bucket, which contains a large number of files. You notice that queries against this table are slow. You want to improve the performance of these queries. What should you do?","isMC":true,"question_id":205,"exam_id":10,"answer_description":"","topic":"1","answers_community":["C (100%)"],"answer":"C","answer_ET":"C","url":"https://www.examtopics.com/discussions/google/view/130510-exam-professional-data-engineer-topic-1-question-283/","answer_images":[],"choices":{"B":"Create an individual external table for each Hive partition by using a common table name prefix. Use wildcard table queries to reference the partitioned data.","A":"Change the storage class of the Hive partitioned data objects from Coldline to Standard.","C":"Upgrade the external table to a BigLake table. Enable metadata caching for the table.","D":"Migrate the Hive partitioned data objects to a multi-region Cloud Storage bucket."}}],"exam":{"lastUpdated":"15 Feb 2025","isMCOnly":true,"numberOfQuestions":319,"id":10,"name":"Professional Data Engineer","provider":"Google","isImplemented":true,"isBeta":false},"currentPage":41},"__N_SSP":true}