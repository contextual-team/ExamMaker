{"pageProps":{"questions":[{"id":"dDNBtUe6Frc0TpdEBl8s","topic":"1","isMC":true,"question_images":[],"question_text":"You need to create a SQL pipeline. The pipeline runs an aggregate SQL transformation on a BigQuery table every two hours and appends the result to another existing BigQuery table. You need to configure the pipeline to retry if errors occur. You want the pipeline to send an email notification after three consecutive failures. What should you do?","answer":"B","choices":{"C":"Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable email notifications.","A":"Use the BigQueryUpsertTableOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.","B":"Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.","D":"Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable notification to Pub/Sub topic. Use Pub/Sub and Cloud Functions to send an email after three failed executions."},"question_id":191,"discussion":[{"comments":[{"timestamp":"1724167620.0","poster":"SuperVan","comment_id":"1154870","upvote_count":"6","content":"The prompt wants an email notification sent after three failed attempts. Is there any concern that the retry parameter is set to 3, wouldn't this mean that the email is sent after 4 failed attempts (1 original + 3 retries)?"}],"upvote_count":"6","poster":"raaad","content":"Selected Answer: B\n- It provides a direct and controlled way to manage the SQL pipeline using Cloud Composer (Apache Airflow). \n- The BigQueryInsertJobOperator is well-suited for running SQL jobs in BigQuery, including aggregate transformations and handling of results. \n- The retry and email_on_failure parameters align with the requirements for error handling and notifications. \n- Cloud Composer requires more setup than using BigQuery's scheduled queries directly, but it offers robust workflow management, retry logic, and notification capabilities, making it suitable for more complex and controlled data pipeline requirements.","timestamp":"1720192680.0","comment_id":"1114688"},{"content":"Selected Answer: D\nThe retry times in B and clearly mentioned 2 hours in D make me think D is the better option..","poster":"Augustax","comment_id":"1349556","upvote_count":"1","timestamp":"1738333980.0"},{"comment_id":"1348970","timestamp":"1738229220.0","content":"Selected Answer: D\n\"You want the pipeline to send an email notification after three consecutive failures\" - it is not about retries which are configurable via Composer operator - it is about 3 consecutive executions which could be for different hours.","upvote_count":"1","poster":"plum21"},{"content":"Selected Answer: D\nTerrible options as usual. Whilst B is the most elegant, it doesn't explicitly address the 2 hour scheduling (you can schedule within Composer, but the answer doesn't mention it).\n\nIf we take these answers on the surface level, D is the only option that actually achieves our goal.","timestamp":"1736265900.0","poster":"b3e59c2","upvote_count":"1","comment_id":"1337620"},{"timestamp":"1736174280.0","upvote_count":"1","content":"Selected Answer: B\nB. Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.\n\nThis approach leverages Cloud Composer (Apache Airflow) to manage the SQL pipeline, providing robust workflow management, retry logic, and notification capabilities. By setting the retry parameter to three and enabling email notifications, you ensure that the pipeline will retry on errors and notify you after three consecutive failures.","comment_id":"1337184","poster":"Pime13"},{"poster":"e593506","comment_id":"1330200","timestamp":"1734823800.0","content":"Selected Answer: D\nThe prompt wants an email notification sent after three failed attempts\nOption B does not meet that condition","upvote_count":"1"},{"content":"Selected Answer: B\nhttps://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/bigquery/index.html#airflow.providers.google.cloud.operators.bigquery.BigQueryInsertJobOperator\nhttps://cloud.google.com/composer/docs/composer-2/write-dags#notifications_on_operator_failure","upvote_count":"1","comment_id":"1213945","timestamp":"1732049880.0","poster":"josech"},{"poster":"joao_01","timestamp":"1728291420.0","comment_id":"1190866","upvote_count":"1","content":"It s B (however for me its a incomplete answers cause it does not address the schedule of every 2 hours).\n\nIts not C or D because BigQuery scheduled queries by default does not retries the queries when error occurs. Link: https://cloud.google.com/bigquery/docs/scheduling-queries"},{"upvote_count":"1","timestamp":"1724199660.0","comment_id":"1155167","content":"Selected Answer: B\nOption B","poster":"JyoGCP"},{"content":"Selected Answer: D\nD. Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable notification to Pub/Sub topic. Use Pub/Sub and Cloud Functions to send an email after three failed executions\n\nThis method utilizes BigQuery's native scheduling capabilities for running the SQL job and leverages Pub/Sub and Cloud Functions for customized notification handling, including the specific requirement of sending an email after three consecutive failures.","poster":"datapassionate","comments":[{"timestamp":"1724355240.0","content":"Option D mentions nothing about how the job retrying is put in place, so for that reason I don't think this is the correct option.","upvote_count":"3","poster":"RenePetersen","comment_id":"1156755"}],"upvote_count":"2","timestamp":"1722235980.0","comment_id":"1134774"},{"timestamp":"1720021380.0","content":"Selected Answer: B\nB. Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.","comment_id":"1112985","upvote_count":"1","poster":"scaenruy"}],"answer_images":[],"timestamp":"2024-01-03 18:43:00","answer_ET":"B","unix_timestamp":1704303780,"exam_id":10,"answers_community":["B (63%)","D (38%)"],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130220-exam-professional-data-engineer-topic-1-question-270/"},{"id":"JvJVfT45aZEf4nz9IM2g","exam_id":10,"isMC":true,"topic":"1","question_images":[],"question_text":"You are monitoring your organization’s data lake hosted on BigQuery. The ingestion pipelines read data from Pub/Sub and write the data into tables on BigQuery. After a new version of the ingestion pipelines is deployed, the daily stored data increased by 50%. The volumes of data in Pub/Sub remained the same and only some tables had their daily partition data size doubled. You need to investigate and fix the cause of the data increase. What should you do?","discussion":[{"comment_id":"1114700","content":"Selected Answer: C\n- Detailed Investigation of Logs and Jobs Checking for duplicate rows targets the potential immediate cause of the issue.\n- Checking the BigQuery Audit logs helps identify which jobs might be contributing to the increased data volume.\n- Using Cloud Monitoring to correlate job starts with pipeline versions helps identify if a specific version of the pipeline is responsible.\n- Managing multiple versions of pipelines ensures that only the intended version is active, addressing any versioning errors that might have occurred during deployment.\n\n\n=======\nWhy not B\nWhile it addresses the symptom (excess data), it doesn't necessarily stop the problem from recurring. (The questions asked to investigate and fix)","upvote_count":"12","timestamp":"1704475740.0","poster":"raaad"},{"timestamp":"1730355000.0","content":"Why not D?","comment_id":"1305290","poster":"mi_yulai","upvote_count":"1"},{"upvote_count":"2","comment_id":"1305191","timestamp":"1730323800.0","content":"Selected Answer: B\nNo idea which one to choose. Option C miss a step - to restore the tables.","comments":[{"comment_id":"1345970","poster":"Ryannn23","content":"\" You need to investigate and fix the cause of the data increase. \" - fixing the target tables was not required.","upvote_count":"1","timestamp":"1737706440.0"}],"poster":"SamuelTsch"},{"upvote_count":"1","comment_id":"1121789","poster":"Matt_108","content":"Selected Answer: C\nOption C - agree with Raaad on the reasons","timestamp":"1705159320.0"},{"comments":[{"content":"This does not fix the error, it basically assumes that the error is not really there.","timestamp":"1707909240.0","comment_id":"1150129","upvote_count":"3","poster":"RenePetersen"}],"upvote_count":"2","comment_id":"1120504","poster":"task_7","timestamp":"1705043040.0","content":"Selected Answer: B\nB. Check for code errors in the deployed pipelines, multiple writing to pipeline BigQuery sink, errors in Cloud Logging, and if necessary, restore tables using time travel.\nCheck for code errors\nCheck for multiple writes\nCheck Cloud Logging\nRestore tables if necessary:"}],"choices":{"D":"1. Roll back the last deployment.\n2. Restore the BigQuery tables to their content before the last release by using time travel.\n3. Restart the Dataflow jobs and replay the messages by seeking the subscription to the timestamp of the release.","C":"1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled.\n2. Check the BigQuery Audit logs to find job IDs.\n3. Use Cloud Monitoring to determine when the identified Dataflow jobs started and the pipeline code version.\n4. When more than one pipeline ingests data into a table, stop all versions except the latest one.","B":"1. Check for code errors in the deployed pipelines.\n2. Check for multiple writing to pipeline BigQuery sink.\n3. Check for errors in Cloud Logging during the day of the release of the new pipelines.\n4. If no errors, restore the BigQuery tables to their content before the last release by using time travel.","A":"1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled.\n2. Schedule daily SQL jobs to deduplicate the affected tables.\n3. Share the deduplication script with the other operational teams to reuse if this occurs to other tables."},"answer":"C","question_id":192,"timestamp":"2024-01-05 18:29:00","unix_timestamp":1704475740,"answer_images":[],"answer_description":"","answers_community":["C (76%)","B (24%)"],"url":"https://www.examtopics.com/discussions/google/view/130427-exam-professional-data-engineer-topic-1-question-271/","answer_ET":"C"},{"id":"rYFReOWufQlyNX36rTm7","question_id":193,"answer_description":"","exam_id":10,"isMC":true,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/130221-exam-professional-data-engineer-topic-1-question-272/","question_text":"You have a BigQuery dataset named “customers”. All tables will be tagged by using a Data Catalog tag template named “gdpr”. The template contains one mandatory field, “has_sensitive_data”, with a boolean value. All employees must be able to do a simple search and find tables in the dataset that have either true or false in the “has_sensitive_data’ field. However, only the Human Resources (HR) group should be able to see the data inside the tables for which “has_sensitive data” is true. You give the all employees group the bigquery.metadataViewer and bigquery.connectionUser roles on the dataset. You want to minimize configuration overhead. What should you do next?","question_images":[],"answer":"C","choices":{"D":"Create the “gdpr” tag template with public visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.","A":"Create the “gdpr” tag template with private visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.","C":"Create the “gdpr” tag template with public visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.","B":"Create the “gdpr” tag template with private visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data."},"discussion":[{"timestamp":"1704803160.0","content":"Selected Answer: C\n- The most straightforward solution with minimal configuration overhead.\n- By creating the \"gdpr\" tag template with public visibility, you ensure that all employees can search and find tables based on the \"has_sensitive_data\" field. \n- Assigning the bigquery.dataViewer role to the HR group on tables with sensitive data ensures that only they can view the actual data in these tables.","comments":[{"poster":"ML6","content":"Wouldn't employees still need the roles/datacatalog.tagTemplateViewer role to view private AND public tags?\nTo get the permissions that you need to view public and private tags on Bigtable resources, ask your administrator to grant you the following IAM roles:\n- roles/datacatalog.tagTemplateViewer\n- roles/bigtable.viewer\nSource: https://cloud.google.com/bigtable/docs/manage-data-assets-using-data-catalog#permissions-view-tags","timestamp":"1708250880.0","upvote_count":"2","comment_id":"1153181"},{"timestamp":"1708251660.0","comment_id":"1153189","content":"Ignore the last reply. The correct answer would be C.\nTags = Custom metadata fields that you can attach to a data entry to provide context.\nTag templates = Reusable structures that you can use to rapidly create new tags.\nIn short, the employees do not need a tagTemplateViewer role because it pertains to the tag templates, not the tags themselves.","upvote_count":"1","poster":"ML6"}],"upvote_count":"18","comment_id":"1117465","poster":"raaad"},{"content":"Selected Answer: D\nOption C:\nPublic Visibility: Ensures that all employees can see the tags and their values.\nData Viewer Role for HR: Restricts access to the data inside the tables with sensitive data to the HR group.\nOption D:\nPublic Visibility: Ensures that all employees can see the tags and their values.\nTag Template Viewer Role: Explicitly grants the datacatalog.tagTemplateViewer role to the all employees group, ensuring they can view the tags.\nData Viewer Role for HR: Restricts access to the data inside the tables with sensitive data to the HR group.\nWhile both options provide public visibility for the tags and restrict data access to the HR group, Option D explicitly grants the datacatalog.tagTemplateViewer role to the all employees group, ensuring they can view the tags and perform searches based on the \"hs_sensitive_data\" field.","comment_id":"1337154","comments":[{"poster":"Pime13","content":"You can search for public tags using simple search. You can view a data entry, including its public tags, as long as you have the required permissions to view the data entry. No additional permissions on the tag template are required. For permissions required to view the data entry, see the table in this section.\nHowever, we recommend to also grant the datacatalog.tagTemplates.get permission to users who are expected to search for these public tags. This permission allows users to also use the search predicate tag: or use the tag template search facet in the Data Catalog search page.\nFor private tags, you need view permissions on both the tag template and the data entry to search for the tag and to see the tag in the entry detail page. Users must use the tag: search predicate or the tag template search facet to find the tags; simple search for private tags isn't supported.","comment_id":"1337156","timestamp":"1736169240.0","upvote_count":"1"}],"upvote_count":"1","poster":"Pime13","timestamp":"1736169180.0"},{"timestamp":"1723280340.0","poster":"meh_33","upvote_count":"2","content":"Selected Answer: C\nThis Guy Raasd is mostly correct with explanation thanks mate.","comment_id":"1263396"},{"timestamp":"1722631380.0","poster":"iooj","upvote_count":"1","comment_id":"1260029","content":"A - employees cannot use the tag\nB - increases the configuration overhead\nC - exactly what we need\nD - unnecessary role assignment, the tag template is already visibile"},{"comments":[{"content":"As all employees have the role “ bigquery.metadataViewer” they are already capable to see tags on BigQuery then","comment_id":"1181980","poster":"d11379b","upvote_count":"1","timestamp":"1711314900.0"}],"timestamp":"1711314720.0","upvote_count":"2","content":"Selected Answer: C\nWhile D works well, it is not obligated to give all employees the role of tagTemplateViewer, as it will give them the view permission for tag templates as well as the tags created by the template.\nHowever, Tags are a type of business metadata. Adding tags to a data entry helps provide meaningful context to anyone who needs to use the asset.And public tags provide less strict access control for searching and viewing the tag as compared to private tags. Any user who has the required view permissions for a data entry can view all the public tags associated with it. View permissions for public tags are only required when you perform a search in Data Catalog using the tag: syntax or when you view an unattached tag template.","poster":"d11379b","comment_id":"1181979"},{"upvote_count":"2","poster":"JyoGCP","timestamp":"1708491720.0","comment_id":"1155254","content":"Selected Answer: C\nI'll go with raaad's answer"},{"content":"Selected Answer: B\nIf you working with PII, We can't granted public access. So Private Visibility for the Tag Template its the best option.\n\nCheck it https://cloud.google.com/data-catalog/docs/tags-and-tag-templates","comment_id":"1137526","upvote_count":"4","timestamp":"1706784240.0","poster":"tibuenoc"},{"upvote_count":"4","poster":"scaenruy","content":"Selected Answer: D\nD. Create the “gdpr” tag template with public visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.","timestamp":"1704304500.0","comment_id":"1112996"}],"answer_ET":"C","answer_images":[],"timestamp":"2024-01-03 18:55:00","answers_community":["C (73%)","D (15%)","12%"],"unix_timestamp":1704304500},{"id":"EKCK9YrheBydyS26AjEZ","choices":{"C":"1. Use Cloud Build to build a container and the KubernetesPodOperator to deploy the code of the DAG to the Google Kubernetes Engine (GKE) cluster of the development instance for testing.\n2. If the tests pass, copy the code to the Cloud Storage bucket of the production instance.","B":"1. Use Cloud Build to build a container with the code of the DAG and the KubernetesPodOperator to deploy the code to the Google Kubernetes Engine (GKE) cluster of the development instance for testing.\n2. If the tests pass, use the KubernetesPodOperator to deploy the container to the GKE cluster of the production instance.","A":"1. Use Cloud Build to copy the code of the DAG to the Cloud Storage bucket of the development instance for DAG testing.\n2. If the tests pass, use Cloud Build to copy the code to the bucket of the production instance.","D":"1. Use Cloud Build to copy the code of the DAG to the Cloud Storage bucket of the development instance for DAG testing.\n2. If the tests pass, use Cloud Build to build a container with the code of the DAG and the KubernetesPodOperator to deploy the container to the Google Kubernetes Engine (GKE) cluster of the production instance."},"exam_id":10,"url":"https://www.examtopics.com/discussions/google/view/130514-exam-professional-data-engineer-topic-1-question-273/","question_id":194,"question_images":[],"answer_description":"","answer_images":[],"discussion":[{"timestamp":"1704972120.0","upvote_count":"11","content":"Selected Answer: A\nThe Answer is A. Given that there are two instances (development and production) already available, and the goal is to deploy DAGs to Cloud Composer not entire composer infra build. \n\nExplanation:\n- This approach leverages Cloud Build to manage the deployment process.\n- It first deploys the code to the Cloud Storage bucket of the development instance for testing purposes.\n- If the tests are successful in the development environment, the same Cloud Build process is used to copy the code to the Cloud Storage bucket of the production instance.\n \nB. GKE-based approach is not standard for Cloud Composer. C. GKE used for testing is unconventional for DAG deployments. D. Involves unnecessary GKE deployment for production. Testing DAGs should use Composer instances directly, not Kubernetes containers in GKE.","poster":"BIGQUERY_ALT_ALT","comment_id":"1119643"},{"timestamp":"1723279860.0","content":"Selected Answer: A\nMost confusing question to confuse us why GKE needed its already mentioned they have 2 composer environment","upvote_count":"2","poster":"meh_33","comment_id":"1263391"},{"comment_id":"1155256","upvote_count":"2","timestamp":"1708492200.0","poster":"JyoGCP","content":"Selected Answer: A\nOption A"},{"comment_id":"1121815","poster":"Matt_108","timestamp":"1705160820.0","content":"Selected Answer: A\nOption A, DAGs are routinely stored in cloud storage buckets, Cloud Build act as a trigger for both the deployment process to test env and the test itslef\nhttps://cloud.google.com/composer/docs/dag-cicd-integration-guide","upvote_count":"4"},{"comment_id":"1117562","content":"Selected Answer: A\nI vote fore A","timestamp":"1704810180.0","upvote_count":"1","poster":"Sofiia98"},{"timestamp":"1704645060.0","comments":[{"comment_id":"1117560","content":"But why do we need the Google Kubernetes Engine (GKE) cluster for this?","upvote_count":"1","poster":"Sofiia98","comments":[{"content":"Yea, it should be A","timestamp":"1705531920.0","comment_id":"1125401","poster":"GCP001","upvote_count":"1"}],"timestamp":"1704810120.0"}],"poster":"GCP001","content":"C. \nIt looks the correct choice, first build, test and verify everything on dev enviornment and then just copy the files on prod bucket.\nhttps://cloud.google.com/composer/docs/dag-cicd-integration-guide","comment_id":"1116005","upvote_count":"1"},{"poster":"Smakyel79","timestamp":"1704643860.0","comment_id":"1115997","upvote_count":"2","content":"Selected Answer: A\nThis approach is straightforward and leverages Cloud Build to automate the deployment process. It doesn't require containerization, making it simpler and possibly quicker."}],"unix_timestamp":1704643860,"topic":"1","answer_ET":"A","answers_community":["A (100%)"],"isMC":true,"question_text":"You are creating the CI/CD cycle for the code of the directed acyclic graphs (DAGs) running in Cloud Composer. Your team has two Cloud Composer instances: one instance for development and another instance for production. Your team is using a Git repository to maintain and develop the code of the DAGs. You want to deploy the DAGs automatically to Cloud Composer when a certain tag is pushed to the Git repository. What should you do?","timestamp":"2024-01-07 17:11:00","answer":"A"},{"id":"fvDDJoCnEVRTFeUILDLW","unix_timestamp":1704644040,"answer_description":"","topic":"1","timestamp":"2024-01-07 17:14:00","isMC":true,"answer_ET":"B","exam_id":10,"answer":"B","answers_community":["B (58%)","D (42%)"],"url":"https://www.examtopics.com/discussions/google/view/130515-exam-professional-data-engineer-topic-1-question-274/","question_images":[],"answer_images":[],"discussion":[{"poster":"raaad","comments":[{"comment_id":"1121820","upvote_count":"5","timestamp":"1705161240.0","poster":"Matt_108","comments":[{"comments":[{"content":"Correct, but the question states 'use keys from a centralized Cloud KMS project', so only D is correct.","comments":[{"upvote_count":"1","content":"PubSub is an application and holds data on the fly, this data does not mean data at rest. The data that is ingested in GSC only means data at rest so B is the right answer.","poster":"cloud_rider","comment_id":"1319681","timestamp":"1732878900.0"}],"upvote_count":"3","comment_id":"1153234","poster":"ML6","timestamp":"1708256700.0"}],"content":"No, \"The ingested data is encrypted with a Google-managed encryption key\", target is ingested data in BigQuery.","comment_id":"1127639","timestamp":"1705814820.0","poster":"AllenChen123","upvote_count":"2"}],"content":"But also pub/sub has some data at rest, e.g. messages with retention period. \nTo comply with the organisation policy, we need to adapt also pub/sub"}],"timestamp":"1704889800.0","content":"Selected Answer: B\n- New BigQuery Table with CMEK: This option involves creating a new BigQuery table configured to use a CMEK from Cloud KMS. It directly addresses the need to use a CMEK for data at rest in BigQuery.\n- Migrate Data: Migrating data from the old table (encrypted with a Google-managed key) to the new table (encrypted with CMEK) ensures that all existing data complies with the new policy.","upvote_count":"11","comment_id":"1117775"},{"comment_id":"1348923","poster":"plum21","upvote_count":"1","content":"Selected Answer: D\nThere is data at rest in Pub/Sub, which is stated here in the docs: https://cloud.google.com/pubsub/docs/encryption \nAt rest data -> Application layer -> CMEK encryption","timestamp":"1738220040.0"},{"timestamp":"1736169660.0","content":"Selected Answer: B\nB. \nThere is no need to create a new pubsub topic since it can be updated with the note that change is not retroactive. https://cloud.google.com/pubsub/docs/encryption#update_cmek_for_a_topic","comment_id":"1337161","poster":"Pime13","upvote_count":"1"},{"comment_id":"1325850","content":"Selected Answer: B\nB. You don't need to create a new topic in order to use the new CMEK. Existing topic can be updated to use the new key: https://cloud.google.com/pubsub/docs/encryption#update_cmek_for_a_topic","poster":"m_a_p_s","timestamp":"1734036120.0","upvote_count":"1"},{"upvote_count":"2","timestamp":"1730325120.0","comment_id":"1305202","content":"Selected Answer: B\nshould be B. Pub/Sub is not designed for storing data at rest.","poster":"SamuelTsch"},{"comment_id":"1302745","content":"Selected Answer: B\nAgree with raaad","poster":"gr3yWind","upvote_count":"1","timestamp":"1729826760.0"},{"comment_id":"1271755","poster":"shanks_t","timestamp":"1724518440.0","content":"Selected Answer: D\nRequirement for Cloud KMS keys: The new organization policy requires using keys from a centralized Cloud KMS project for encrypting data at rest. This necessitates the use of customer-managed encryption keys (CMEK).\nBigQuery table encryption: The existing BigQuery table is encrypted with a Google-managed key. To meet the new policy, a new table needs to be created with CMEK.\nPub/Sub topic encryption: Since the data is ingested directly from a Pub/Sub subscription, the Pub/Sub topic also needs to use CMEK to ensure end-to-end encryption with customer-managed keys.\nData migration: The existing data in the old BigQuery table needs to be migrated to the new CMEK-encrypted table to ensure all data complies with the new policy","upvote_count":"1"},{"content":"Selected Answer: B\n\"The best solution here is B. Create a new BigQuery table by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table.\n\nHere's why:\n\nCustomer-Managed Encryption Keys (CMEK): CMEKs allow you to have granular control over your encryption keys, complying with the organization's policy to use keys from a centralized Cloud KMS project.\nData Migration: Since the data in the existing table is already encrypted with a Google-managed key, you cannot retroactively change the encryption key for that table. Migrating the data to a new table with the correct encryption is the most efficient way to meet compliance.","comments":[{"timestamp":"1721012760.0","comment_id":"1248044","poster":"carmltekai","content":"Why other options aren't suitable:\n\nA: Dataflow can't retroactively change the encryption of data that's already in BigQuery.\nC: Creating a new Pub/Sub topic with CMEK wouldn't address the data that's already in BigQuery.\nD: While creating a new Pub/Sub topic might be useful in the long run, it's not necessary for solving the immediate compliance issue with the existing data.\"","comments":[{"comment_id":"1260032","content":"You have some data in Pub/Sub at rest as well which is immediate compliance issue.","timestamp":"1722631800.0","poster":"iooj","upvote_count":"1"}],"upvote_count":"1"}],"timestamp":"1721012700.0","comment_id":"1248043","poster":"carmltekai","upvote_count":"2"},{"upvote_count":"2","content":"Selected Answer: D\nD. Create a new BigQuery table and Pub/Sub topic by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table.\n\nThis approach comprehensively addresses the requirement to use CMEK from a centralized Cloud KMS project for encrypting data at rest:\n\nCreate a new Pub/Sub topic configured to use CMEK from the centralized Cloud KMS project.\nCreate a new BigQuery table with CMEK enabled, using the same centralized Cloud KMS project.\nUpdate the ingestion process to use the new Pub/Sub topic to feed data into the new BigQuery table.\nMigrate existing data from the old BigQuery table to the new BigQuery table to ensure all data complies with the new encryption policy.","comment_id":"1230884","poster":"Anudeep58","timestamp":"1718445960.0"},{"comments":[{"content":"sry, I mean D","comment_id":"1224739","poster":"AlizCert","timestamp":"1717593840.0","upvote_count":"2"}],"upvote_count":"2","poster":"AlizCert","timestamp":"1717593780.0","comment_id":"1224738","content":"Selected Answer: B\nB, been there, done that..."},{"comment_id":"1219040","timestamp":"1716739980.0","content":"Selected Answer: D\nBigQuery and Pub/Sub shall be encrypted using CMEK using new versions of each one.\n https://cloud.google.com/pubsub/docs/encryption#using-cmek","poster":"josech","upvote_count":"2"},{"content":"Selected Answer: B\nData at rest in requirement = Big Query ONLY. \n\nPub/Sub is data in movement - overkill for the solution","timestamp":"1716024120.0","upvote_count":"2","poster":"chrissamharris","comment_id":"1213240"},{"poster":"f74ca0c","upvote_count":"1","content":"Selected Answer: D\nD- BigQuery and Pub/sub are automatically encrypted but here we need to apply a more secured policy by using CMEK so we need to use it for bigquery and pub/sub to meet this policy","timestamp":"1716011940.0","comment_id":"1213151"},{"timestamp":"1714037160.0","comment_id":"1201879","content":"Selected Answer: B\nB. Create a new BigQuery table by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table. Most Voted","comments":[{"upvote_count":"1","poster":"LaxmanTiwari","timestamp":"1714037340.0","content":"it should be B as the data in pub sub is already encrypted , please read it carefully and use Copilot or chat gpt to have confirmation.","comment_id":"1201881"}],"upvote_count":"2","poster":"LaxmanTiwari"},{"poster":"amanbawa96","timestamp":"1712208600.0","comment_id":"1189107","upvote_count":"1","content":"Selected Answer: B\nBigQuery allows you to encrypt data at rest using either Google-managed encryption keys or customer-managed encryption keys (CMEK) from Cloud KMS.\nSince the new policy requires using keys from a centralized Cloud KMS project, you need to create a new BigQuery table that is configured to use CMEK for encryption.\nAfter creating the new table with CMEK, you can migrate the data from the old table (encrypted with Google-managed keys) to the new table (encrypted with CMEK).\nThis approach ensures that the data in the BigQuery table is encrypted using the required CMEK while preserving the existing data.\n\nCreating a new BigQuery table and Pub/Sub topic with CMEK is not necessary because the focus is on encrypting the data at rest in BigQuery. The existing Pub/Sub subscription can still be used to ingest data into the new BigQuery table."},{"poster":"Izzyt99","comment_id":"1180961","timestamp":"1711205880.0","content":"D - 'as new organization policy that requires you to use keys from a centralized Cloud Key Management Service (Cloud KMS) project to encrypt data at rest.' Therefore, the Pub/Sub default Google-managed encryption key is not sufficient as the organization requires it's own CMEK that is to be generated from a centralized Cloud KMS project.","upvote_count":"3"},{"poster":"cuadradobertolinisebastiancami","timestamp":"1709065800.0","content":"Selected Answer: D\nAgree with ML6 and Smakyel. To encrypt data at rest we should encrypt the data in PubSub and BigQuery","upvote_count":"1","comment_id":"1160962"},{"content":"Only option D complies with the organisation policy:\n- By creating a new Pub/Sub topic with customer-managed encryption keys (CMEK), any new data ingested into Pub/Sub will be encrypted with the (!) organization's desired encryption keys (!).\n- Creating a new BigQuery table with CMEK ensures that all data stored in BigQuery, both newly ingested and migrated historical data, is encrypted according to organizational policies.\n- Migrating the data from the old BigQuery table to the new one ensures that historical data is also encrypted with the new keys, thus meeting the organization's requirements for encryption at rest for both Pub/Sub and BigQuery.","comment_id":"1153236","upvote_count":"3","poster":"ML6","timestamp":"1708257060.0"},{"poster":"GCP001","content":"D.\n\nWe should use new CMSK for both pubsub topic and BQ tables along with migrating old data.","upvote_count":"3","comment_id":"1116009","timestamp":"1704645480.0"},{"content":"Selected Answer: D\nThis option ensures that both the ingestion mechanism (Pub/Sub) and the storage component (BigQuery) are aligned with the organization's policy of using CMEK, providing end-to-end encryption control.","upvote_count":"3","poster":"Smakyel79","timestamp":"1704644040.0","comments":[{"content":"Why not B??","upvote_count":"2","poster":"raaad","timestamp":"1704827760.0","comment_id":"1117773"},{"timestamp":"1704827880.0","comments":[{"content":"to me it's D because also pub/sub has some data at rest, e.g. messages with retention period. \nTo comply with the organisation policy, we need to adapt also pub/sub encryption","comment_id":"1121821","timestamp":"1705161300.0","upvote_count":"3","comments":[{"content":"But they mention that ingested data is already encrypted?","comment_id":"1153232","poster":"ML6","upvote_count":"2","timestamp":"1708256520.0"}],"poster":"Matt_108"}],"comment_id":"1117776","upvote_count":"2","content":"Configuring a Pub/Sub topic with a CMEK is not necessary for encrypting data at rest in BigQuery.","poster":"raaad"},{"timestamp":"1704973620.0","content":"Requirement is encrypt bq data - \" The ingested data is encrypted with a Google-managed encryption key\" so pubsub encryption from ingestion is not needed. Option B is correct.","poster":"BIGQUERY_ALT_ALT","upvote_count":"3","comment_id":"1119658"},{"timestamp":"1704728640.0","comments":[{"comment_id":"1117774","upvote_count":"2","poster":"raaad","content":"I think option A is a partial solution as using Cloud KMS key in Dataflow for ingestion does not change the encryption of the data at rest in the BigQuery table.","timestamp":"1704827820.0"}],"comment_id":"1116735","poster":"KirkD","upvote_count":"1","content":"I considered also A as they are asking about encryption at rest. The BigQuery is the one but Pub/Sub, not sure."}],"comment_id":"1115999"}],"question_id":195,"choices":{"A":"Use Cloud KMS encryption key with Dataflow to ingest the existing Pub/Sub subscription to the existing BigQuery table.","B":"Create a new BigQuery table by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table.","C":"Create a new Pub/Sub topic with CMEK and use the existing BigQuery table by using Google-managed encryption key.","D":"Create a new BigQuery table and Pub/Sub topic by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table."},"question_text":"You have a BigQuery table that ingests data directly from a Pub/Sub subscription. The ingested data is encrypted with a Google-managed encryption key. You need to meet a new organization policy that requires you to use keys from a centralized Cloud Key Management Service (Cloud KMS) project to encrypt data at rest. What should you do?"}],"exam":{"name":"Professional Data Engineer","provider":"Google","lastUpdated":"15 Feb 2025","id":10,"numberOfQuestions":319,"isImplemented":true,"isBeta":false,"isMCOnly":true},"currentPage":39},"__N_SSP":true}