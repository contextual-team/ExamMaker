{"pageProps":{"questions":[{"id":"i6mFOJioOk53peQPvrJw","exam_id":10,"choices":{"D":"Import the new records from the CSV file into a new BigQuery table. Create a BigQuery job that merges the new records with the existing records and writes the results to a new BigQuery table.","A":"Reduce the number of records updated each day to stay within the BigQuery UPDATE DML statement limit.","B":"Increase the BigQuery UPDATE DML statement limit in the Quota management section of the Google Cloud Platform Console.","C":"Split the source CSV file into smaller CSV files in Cloud Storage to reduce the number of BigQuery UPDATE DML statements per BigQuery job."},"isMC":true,"question_images":[],"timestamp":"2020-03-15 16:01:00","answer_images":[],"question_id":36,"answer_ET":"D","unix_timestamp":1584284460,"answers_community":["D (100%)"],"url":"https://www.examtopics.com/discussions/google/view/16667-exam-professional-data-engineer-topic-1-question-130/","answer_description":"","answer":"D","topic":"1","question_text":"The marketing team at your organization provides regular updates of a segment of your customer dataset. The marketing team has given you a CSV with 1 million records that must be updated in BigQuery. When you use the UPDATE statement in BigQuery, you receive a quotaExceeded error. What should you do?","discussion":[{"comments":[{"content":"There is no mention about merge or limit in the link provided.","comment_id":"130287","upvote_count":"3","timestamp":"1625798940.0","comments":[{"upvote_count":"2","timestamp":"1664560800.0","poster":"Chelseajcole","comment_id":"455071","content":"A common scenario within OLAP systems involves updating existing data based on new information arriving from source systems (such as OLTP databases) on a periodic basis. In the retail business, inventory updates are typically done in this fashion. The following query demonstrates how to perform batch updates to the Inventory table based on the contents of another table (where new arrivals are kept) using the MERGE statement in BigQuery:"}],"poster":"Rajuuu"}],"upvote_count":"30","timestamp":"1615983780.0","comment_id":"65179","poster":"rickywck","content":"Should be D.\n\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery"},{"comment_id":"66899","comments":[{"upvote_count":"1","comment_id":"762467","content":"D is right","timestamp":"1703995260.0","poster":"AzureDP900"},{"comments":[{"comment_id":"504475","content":"Please what was the answer? @AACHB","upvote_count":"2","timestamp":"1671398280.0","poster":"nellyoaid"},{"poster":"GCPLearning2021","timestamp":"1673060160.0","content":"Does examtopics questions help?","comment_id":"518728","upvote_count":"1"}],"upvote_count":"3","poster":"AACHB","comment_id":"501401","timestamp":"1671026460.0","content":"I had it in the exam (14/12/2021)"}],"content":"Should be D\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement\n\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery","upvote_count":"16","timestamp":"1616408400.0","poster":"[Removed]"},{"comment_id":"1026555","upvote_count":"1","timestamp":"1728214860.0","content":"Selected Answer: D\nShould be D.","poster":"Nirca"},{"comment_id":"917996","content":"Selected Answer: D\nimport all the data into a separate table and use that for updates is better than creating smaller csv which leads to more operational time to get it done and harder to manage it.","timestamp":"1717834380.0","poster":"vaga1","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nThis limit was removed a long time ago already.\nAnyway, bulk imports are better.","comment_id":"844906","poster":"juliobs","timestamp":"1710940500.0"},{"timestamp":"1699806060.0","comment_id":"716798","poster":"Atnafu","upvote_count":"2","content":"D\nBigQuery DML statements have no quota limits.\nhttps://cloud.google.com/bigquery/quotas#data-manipulation-language-statements\n\n\nHowever, DML statements are counted toward the maximum number of table operations per day and partition modifications per day. DML statements will not fail due to these limits.\n\nIn addition, DML statements are subject to the maximum rate of table metadata update operations. If you exceed this limit, retry the operation using exponential backoff between retries."},{"poster":"MisuLava","timestamp":"1698319740.0","upvote_count":"2","comment_id":"704625","content":"there is no update quota anymore.\nbut i would say D"},{"timestamp":"1683284160.0","content":"Option D is the right answer","upvote_count":"1","poster":"amitsingla012","comment_id":"597266"},{"timestamp":"1681230000.0","poster":"tavva_prudhvi","upvote_count":"1","comment_id":"584307","content":"No DML limits from 3rd march 2020 But if the questions is given in the exam, choose D asfor the options A, B,C as they are speaking about the limitations of the DML Limits. Atleast, D is giving an alternative!"},{"poster":"nidnid","upvote_count":"3","comment_id":"536819","content":"Is this question still valid? What about DML without limits? https://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","timestamp":"1675160340.0"},{"timestamp":"1673277540.0","content":"Selected Answer: D\nD:\nBigQuery is primarly designed and suit to append-only technology with some limited DML statements.\nIt's not a relational database where you constantly update your user records if they edit their profile. Instead you need to arhitect your code so each edit is a new row in Bigquery, and you always query the latest row.\nThe DML statement limitation is low, because it targets different scenarios and not yours, aka live update on rows. You could ingest your data into a separate table, and issue 1 update statement per day.\nhttps://stackoverflow.com/questions/45183082/can-we-increase-update-quota-in-bigquery\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery","poster":"MaxNRG","comment_id":"520285","upvote_count":"2"},{"timestamp":"1673181720.0","upvote_count":"2","poster":"medeis_jar","comment_id":"519517","content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement\n\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery"},{"comment_id":"483202","content":"old question I guess, should not be in the exam anymore (?)\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","timestamp":"1669030140.0","upvote_count":"3","poster":"mjb65"},{"upvote_count":"4","poster":"sumanshu","comment_id":"397575","timestamp":"1656851760.0","content":"Vote for D"},{"timestamp":"1647109440.0","upvote_count":"3","comment_id":"309066","poster":"daghayeghi","content":"D:\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery"},{"content":"D:\nhttps://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery","timestamp":"1645223580.0","poster":"daghayeghi","comment_id":"293775","upvote_count":"3"},{"comment_id":"185885","content":"D should be the answer. Avoid updates in Datawarehousing environment instead use merge to create a new table.","timestamp":"1632460920.0","poster":"SteelWarrior","upvote_count":"3"},{"upvote_count":"1","content":"Quota Exceed Error Solution from BQ Documentation - \nView the message property of the error object for more information about which quota was exceeded. To reset or raise a BigQuery quota, contact support. To modify a custom quota, submit a request from the Google Cloud Console page. If you receive this error using the BigQuery sandbox, you can upgrade.","poster":"VIncent9261111","comment_id":"180673","timestamp":"1631852220.0"},{"comment_id":"163148","comments":[{"poster":"Chelseajcole","upvote_count":"1","content":"What is number of statements mean?","timestamp":"1663438140.0","comment_id":"446751"}],"upvote_count":"5","content":"D is correct.\nhttps://cloud.google.com/bigquery/quotas#queries.\nThe error is because of the number of statements over the table , but not because of the number of records.","poster":"haroldbenites","timestamp":"1629574320.0"},{"comment_id":"163143","content":"D is correct. Obviously.","timestamp":"1629573180.0","upvote_count":"4","poster":"haroldbenites"},{"poster":"dambilwa","content":"Option [D] is the most appropriate answer. However DMLs now longer have any limits in BigQuery - https://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery","comment_id":"119000","upvote_count":"7","timestamp":"1624582980.0"},{"timestamp":"1619654040.0","comment_id":"81079","content":"Correct D","upvote_count":"5","poster":"arnabbis4u"},{"upvote_count":"5","poster":"Rajokkiyam","content":"Should be D","comment_id":"70300","timestamp":"1617325740.0"},{"content":"Answer: D\nDescription: Google says to merge data in this cases by creating two tables","timestamp":"1616922720.0","poster":"[Removed]","comment_id":"68842","upvote_count":"7"},{"upvote_count":"7","content":"No dml limits from 3rd march 2020","comment_id":"64336","poster":"madhu1171","timestamp":"1615820460.0"}]},{"id":"OEqgsSya0yi3DSLRLkmw","url":"https://www.examtopics.com/discussions/google/view/17231-exam-professional-data-engineer-topic-1-question-131/","question_id":37,"question_images":[],"isMC":true,"choices":{"B":"Introduce resource hierarchy to leverage access control policy inheritance.","D":"Only use service accounts when sharing data for Cloud Storage buckets and BigQuery datasets.","C":"Create distinct groups for various teams, and specify groups in Cloud IAM policies.","E":"For each Cloud Storage bucket or BigQuery dataset, decide which projects need access. Find all the active members who have access to these projects, and create a Cloud IAM policy to grant access to all these users.","A":"Use Cloud Deployment Manager to automate access provision."},"discussion":[{"content":"Answer: B, C\nDescription: Google suggests that we should provide access by following google hierarchy and groups for users with similar roles","comment_id":"68845","timestamp":"1616924400.0","comments":[{"timestamp":"1637001120.0","upvote_count":"11","poster":"sipsap","comment_id":"219866","content":"\"Each project requires unique access control configurations\" rules out hirearchy"}],"upvote_count":"31","poster":"[Removed]"},{"poster":"AJKumar","content":"C is one option for sure, also C eliminates B as C includes groups and teams hierarchy, A can be eliminated as A talks about only deployment. From Remaining D and E, i find E most relevant to question--as E matches users with teams/groups and projects. Answer C and E.","upvote_count":"20","comments":[{"comment_id":"431089","timestamp":"1661394720.0","upvote_count":"5","content":"Question mention minimize IAM policies,but E should create complex policies","poster":"hauhau"}],"timestamp":"1624347300.0","comment_id":"116135"},{"content":"Answer: A, C.\nThe Key question is \"You want to simplify access control management by minimizing the number of policies\". At the company where I work, we use Terraform to create infrastructure and assign needed roles for different environments.","timestamp":"1730137320.0","upvote_count":"2","comment_id":"1056298","poster":"squishy_fishy"},{"poster":"amittomar","comment_id":"951145","content":"It should be AC as it is mentioned in the question itself \"You want to simplify access control management by minimizing the number of policies\" which rules out B","timestamp":"1720921980.0","upvote_count":"2"},{"poster":"zellck","upvote_count":"2","timestamp":"1701601800.0","comment_id":"734371","content":"Selected Answer: BC\nBC is the answer."},{"upvote_count":"7","timestamp":"1686119760.0","content":"Selected Answer: BC\n1. Define your resource hierarchy: Google Cloud resources are organized hierarchically. This hierarchy allows you to map your enterprise's operational structure to Google Cloud, and to manage access control and permissions for groups of related resources.\n\n2. Delegate responsibility with groups and service accounts: we recommend collecting users with the same responsibilities into groups and assigning IAM roles to the groups rather than to individual users.\n\nhttps://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations","comment_id":"612606","poster":"FrankT2L"},{"timestamp":"1673660160.0","content":"A and C is correct, same question I encountered on Udemy.","poster":"annie1196","comment_id":"523252","comments":[{"timestamp":"1679908260.0","comments":[{"upvote_count":"3","timestamp":"1706059200.0","comment_id":"786029","poster":"desertlotus1211","content":"you explanation for D is incorrect...."}],"upvote_count":"1","comment_id":"576088","poster":"tavva_prudhvi","content":"It doesn't mean it's right, please mention the reasons here not only the references. \nNot A -> Every project has unique requirements, so \"A\" automation will not do much.\nNot D -> As, Service accounts for computer to computer interactions not applications! \nNot E -> E should create complex policies"}],"upvote_count":"2"},{"comment_id":"520315","timestamp":"1673280540.0","upvote_count":"8","poster":"MaxNRG","comments":[{"poster":"MaxNRG","upvote_count":"4","comment_id":"520316","content":"We recommend collecting users with the same responsibilities into groups and assigning Cloud IAM roles to the groups rather than to individual users. For example, you can create a \"data scientist\" group and assign appropriate roles to enable interaction with BigQuery and Cloud Storage.\nGrant roles to a Google group instead of to individual users when possible. It is easier to manage members in a Google group than to update a Cloud IAM policy.\nhttps://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations","timestamp":"1673280540.0"},{"timestamp":"1704040860.0","comment_id":"762706","content":"BC is the answer","upvote_count":"1","poster":"AzureDP900"}],"content":"Selected Answer: BC\nB & C\nGoogle Cloud resources are organized hierarchically, where the organization node is the root node in the hierarchy, the projects are the children of the organization, and the other resources are descendents of projects. \nYou can set Cloud Identity and Access Management (Cloud IAM) policies at different levels of the resource hierarchy. Resources inherit the policies of the parent resource. The effective policy for a resource is the union of the policy set at that resource and the policy inherited from its parent.\nhttps://cloud.google.com/iam/docs/resource-hierarchy-access-control"},{"upvote_count":"4","content":"Selected Answer: AC\nhttps://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations\n\"Each project requires unique access control configurations\" -> C eliminates B\n\nA -> \"Google Cloud Deployment Manager is an infrastructure deployment service that automates the creation and management of Google Cloud resources. Write flexible template and configuration files and use them to create deployments that have a variety of Google Cloud services\"\n\n\"..simply the process..\"","comments":[{"upvote_count":"1","timestamp":"1723822800.0","comment_id":"982728","content":"A makes no sense whatsoever...","poster":"FP77"},{"upvote_count":"1","comments":[{"comments":[{"timestamp":"1674574440.0","comment_id":"531413","content":"So, I stay with BC :)))","poster":"MaxNRG","upvote_count":"3"}],"content":"... in other hand - \"Define your resource hierarchy\"\nhttps://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#define-hierarchy","comment_id":"531411","timestamp":"1674574380.0","poster":"MaxNRG","upvote_count":"1"}],"timestamp":"1674574200.0","content":"good point, AC looks better, agreed","comment_id":"531408","poster":"MaxNRG"}],"timestamp":"1673182200.0","poster":"medeis_jar","comment_id":"519519"},{"content":"Answer:- C, D\nC is used as best practice to create group and assign IAM roles\nD \"data from Cloud Storage buckets and BigQuery datasets must be shared for use in other projects in an ad hoc way\" is mentioned in question. When 2 project communicate, service account should be used","timestamp":"1664686920.0","upvote_count":"9","poster":"hellofrnds","comment_id":"455859"},{"poster":"sumanshu","upvote_count":"4","comment_id":"397793","timestamp":"1656874080.0","content":"Vote for B & C"},{"upvote_count":"3","comment_id":"309074","timestamp":"1647110760.0","content":"the question says adding permissions ad-hoc way\n[c] is correct answer\n[d] is right, as the access to bigQuery and cloud storage can be managed automatically by Cloud deployment\n\"Deployment Manager can also set access control permissions through IAM such that your developers are granted appropriate access as part of the project creation process.\"\nref: https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations","poster":"daghayeghi"},{"comments":[{"upvote_count":"4","comments":[{"content":"Doubt, It could be 'D\" - because - it's said - data from Cloud Storage buckets and BigQuery datasets must be shared for use in other projects in an ad hoc way.","poster":"sumanshu","comments":[{"poster":"awssp12345","content":"Agree with Sumanshu","upvote_count":"1","timestamp":"1657110780.0","comment_id":"399956"}],"timestamp":"1656873300.0","upvote_count":"3","comment_id":"397790"}],"timestamp":"1656872880.0","comment_id":"397787","poster":"sumanshu","content":"D is a service account - it means we need to access via applications. So, D is ruled out"}],"content":"the question says adding permissions ad-hoc way\n[c] is correct answer\n[d] is right, as the access to bigQuery and cloud storage can be managed automatically by Cloud deployment \n\"Deployment Manager can also set access control permissions through IAM such that your developers are granted appropriate access as part of the project creation process.\"\nref: https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations","comment_id":"239263","poster":"VM_GCP","timestamp":"1639059960.0","upvote_count":"4"},{"upvote_count":"3","timestamp":"1638357300.0","comment_id":"231861","poster":"ceak","content":"C & E are the correct answers.","comments":[]},{"poster":"vito9630","upvote_count":"1","content":"Answer: B, C","comment_id":"223066","timestamp":"1637353560.0"},{"comment_id":"221972","timestamp":"1637246640.0","poster":"kavs","content":"BC A ruled out as deployment manager is for infra yaml based deployments D at resource level we can't check hierarchy at org or Proj Level","upvote_count":"1"},{"poster":"rgpalop","content":"C and E","upvote_count":"3","comment_id":"191963","timestamp":"1633232460.0"},{"timestamp":"1630037160.0","upvote_count":"1","content":"B & C\nthe correct answer, as google suggest","comment_id":"167267","poster":"atnafu2020"},{"timestamp":"1629577140.0","content":"C, E is correct.","comment_id":"163164","poster":"haroldbenites","upvote_count":"3"},{"poster":"Unmesh93","upvote_count":"1","content":"B and C : D is eliminated because If Parent nodes grant Access to Cloud Storage & Bigquery, the child policies will be overridden irrespective of child policies/project. Therefore we don't need service account to access Cloud storage of other projects.","comments":[{"poster":"bobby8521","timestamp":"1644505440.0","comment_id":"287612","content":"it should be C, D.\n\nWe need to have service accounts for the data that we are sharing to other projects instead of giving the entire project access or respective roles to that project","upvote_count":"2"}],"comment_id":"129350","timestamp":"1625704860.0"},{"comment_id":"129183","timestamp":"1625681880.0","poster":"dg63","content":"Best options are B & C. I believe that B is talking about organization, folder, project hierarchy. C is talking about users and groups. Both are independent actions. Every project has unique requirements, so \"A\" automation will not do much. You can put storage and bigquery under a common folder & project to allow access to all other teams.","upvote_count":"3"},{"comment_id":"123467","poster":"dambilwa","content":"Option [A,D] are ruled out.. Ideally [B+C] & E are the steps required to achieve the goal stated in the question","timestamp":"1625054040.0","upvote_count":"2"},{"poster":"norwayping","content":"agree A,C https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations","comment_id":"121984","timestamp":"1624894200.0","upvote_count":"5","comments":[{"comment_id":"399978","upvote_count":"2","poster":"awssp12345","timestamp":"1657111440.0","content":"You mean BC right?"}]},{"poster":"Callumr","content":"Its A & C - Service accounts should be used for computer to computer interactions. The unique access control requirements rule out the hierarchical approach. Google recommends automating access provision to keep things simple and consistant\n\nhttps://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations","timestamp":"1624193940.0","upvote_count":"5","comment_id":"114784"},{"comment_id":"114592","content":"B and C","timestamp":"1624176480.0","upvote_count":"4","poster":"shane87"},{"comments":[{"timestamp":"1626409080.0","upvote_count":"6","comment_id":"136176","poster":"Rajuuu","content":"D is incorrect as access is needed for a Central IT team and not Applications."}],"timestamp":"1617325920.0","poster":"Rajokkiyam","upvote_count":"4","comment_id":"70301","content":"C and D"},{"content":"confused - BC / CD","comment_id":"66885","upvote_count":"1","poster":"[Removed]","comments":[{"timestamp":"1616480640.0","comment_id":"67195","poster":"[Removed]","upvote_count":"4","content":"Selected - CD"}],"timestamp":"1616404320.0"}],"topic":"1","answers_community":["BC (81%)","AC (19%)"],"exam_id":10,"question_text":"As your organization expands its usage of GCP, many teams have started to create their own projects. Projects are further multiplied to accommodate different stages of deployments and target audiences. Each project requires unique access control configurations. The central IT team needs to have access to all projects.\nFurthermore, data from Cloud Storage buckets and BigQuery datasets must be shared for use in other projects in an ad hoc way. You want to simplify access control management by minimizing the number of policies. Which two steps should you take? (Choose two.)","answer_images":[],"answer":"BC","unix_timestamp":1584868320,"answer_description":"","timestamp":"2020-03-22 10:12:00","answer_ET":"BC"},{"id":"XhI488V6KIUAPsZYpYjT","question_text":"Your United States-based company has created an application for assessing and responding to user actions. The primary table's data volume grows by 250,000 records per second. Many third parties use your application's APIs to build the functionality into their own frontend applications. Your application's APIs should comply with the following requirements:\n✑ Single global endpoint\n✑ ANSI SQL support\n✑ Consistent access to the most up-to-date data\nWhat should you do?","answer":"B","timestamp":"2020-03-22 10:15:00","choices":{"A":"Implement BigQuery with no region selected for storage or processing.","D":"Implement Bigtable with the primary cluster in North America and secondary clusters in Asia and Europe.","B":"Implement Cloud Spanner with the leader in North America and read-only replicas in Asia and Europe.","C":"Implement Cloud SQL for PostgreSQL with the master in North America and read replicas in Asia and Europe."},"topic":"1","unix_timestamp":1584868500,"discussion":[{"upvote_count":"26","poster":"[Removed]","comment_id":"68849","timestamp":"1632823440.0","content":"Answer: B\nDescription: All the criteria meets for Spanner"},{"content":"A - BigQuery with NO Region ? (Looks wrong)\nB - Spanner (SQL support and Scalable and have replicas ) - Looks correct\nC - SQL (can't store so many records) (wrong)\nD - Bigtable - NO SQL (wrong)\n\nVote for B","comment_id":"397806","upvote_count":"24","poster":"sumanshu","timestamp":"1672780620.0"},{"upvote_count":"3","content":"Selected Answer: B\nA - NO - BigQuery with must have a selected regional or multi-regional file storage\nB - YES - Spanner is specifically designed for this high and consistent throughput\nC - NO - I am not sure about what many said in this discussion as Cloud SQL can store this amount of records if u have just a few columns. Anyway, for sure Spanner is better and it is a GCP product.\nD - Bigtable - it's a NoSQL solution, no ANSI","poster":"vaga1","timestamp":"1733654940.0","comment_id":"918036"},{"comment_id":"762707","content":"B is the answer","upvote_count":"1","timestamp":"1719758640.0","poster":"AzureDP900"},{"upvote_count":"1","comment_id":"734370","content":"Selected Answer: B\nB is the answer.","poster":"zellck","timestamp":"1717405680.0"},{"content":"Selected Answer: B\nGuys, read documentation well. A is wrong, BigQuery has Maximum rows per request (50,000).\nhttps://cloud.google.com/bigquery/quotas\n\nIt is B","timestamp":"1710343680.0","upvote_count":"4","poster":"Remi2021","comment_id":"668063"},{"upvote_count":"2","poster":"JamesKarianis","comment_id":"633696","content":"Selected Answer: B\nSpanner is globally available and meets all the requirements","timestamp":"1705694160.0"},{"poster":"devric","timestamp":"1696645980.0","upvote_count":"2","content":"Selected Answer: A\nCorrect is A. There's no sense to having read replicas outside of US considering than the company is US based.\n\nIf you generate a dataset without specifing the Data Location it's gonna be stored in \"US Multiregion\" by default","comment_id":"582122"},{"upvote_count":"6","content":"Selected Answer: B\nB: Cloud Spanner is the first scalable, enterprise-grade, globally-distributed, and strongly consistent database service built for the cloud specifically to combine the benefits of relational database structure with non-relational horizontal scale.\nhttps://cloud.google.com/spanner/\nCloud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, schemas, SQL (ANSI 2011 with extensions), and automatic, synchronous replication for high availability.\nhttps://cloud.google.com/spanner/docs/\nhttps://cloud.google.com/spanner/docs/instances#available-configurations-multi-region","poster":"MaxNRG","timestamp":"1688912040.0","comment_id":"520318"},{"comment_id":"180707","upvote_count":"8","timestamp":"1647502140.0","poster":"Tanmoyk","content":"B is correct, Bigquery cannot support 250K data ingestion/second , as ANSI SQL support is required , no other options left except Spanner."},{"poster":"haroldbenites","comment_id":"163167","timestamp":"1645482780.0","content":"B is correct","upvote_count":"2"},{"poster":"Archy","upvote_count":"6","content":"B, as Cloud Spanner has three types of replicas: read-write replicas, read-only replicas, and witness replicas.","comment_id":"148333","timestamp":"1643691540.0"},{"timestamp":"1642849800.0","comment_id":"141014","poster":"VishalB","comments":[{"content":"but bigquery does so why not A?","comments":[{"comment_id":"341658","poster":"[Removed]","content":"The requirement is for transactional application serving customers , not analytical so BQ is ruled out.","timestamp":"1666537200.0","upvote_count":"1"},{"poster":"WizzzardLlama","content":"You can't create a BigQuery instance without region selected.\nI'm wondering about these read replicas, why read only replicas? It seems arbitrary, as the question does not state that API should be read-only, so there's no reason why those should be read-only replicas...","comment_id":"283246","upvote_count":"3","timestamp":"1659590400.0"}],"poster":"saurabh1805","upvote_count":"1","comment_id":"160949","timestamp":"1645201680.0"},{"comment_id":"175415","timestamp":"1646695440.0","upvote_count":"3","content":"wrong, cloud spanner can have read-only replicas\nhttps://cloud.google.com/spanner/docs/replication?hl=pt-br","poster":"mAbreu"}],"content":"Correct Answer : C\nExplanation:-\nB -> This option is incorrect, as we do not have option to configure read-replica in Cloud Spanner, Multi-region instance configurations use a combination of all three types’ read-write replicas, read-only replicas, and witness replicas\nC -> This is correct option, In Cloud Sql we have option to create a master node for read-write replicas and read-only replicas in other regions\nD -> This option is incorrect, as Bigtable do not support ANSI SQL","upvote_count":"2"},{"content":"I was wrong Bigtable doesn not support ansi SQL. B instead","poster":"norwayping","upvote_count":"2","comment_id":"124334","timestamp":"1641058560.0"},{"comment_id":"121998","content":"I think it is D. THere is limitation of QPS of Cloud Spanner of 2000 qps,\nhttps://cloud.google.com/spanner/docs/instances#multi-region-performance","upvote_count":"2","poster":"norwayping","timestamp":"1640713800.0"},{"comment_id":"70302","content":"Answer B","upvote_count":"4","timestamp":"1633137240.0","poster":"Rajokkiyam"},{"timestamp":"1632294900.0","comment_id":"66886","poster":"[Removed]","content":"Answer : B","upvote_count":"7"}],"isMC":true,"exam_id":10,"answer_images":[],"answers_community":["B (89%)","11%"],"question_id":38,"answer_ET":"B","answer_description":"","url":"https://www.examtopics.com/discussions/google/view/17232-exam-professional-data-engineer-topic-1-question-132/","question_images":[]},{"id":"ZI6SQTSJUwppoUVmbrNa","isMC":true,"answers_community":["D (88%)","13%"],"question_text":"A data scientist has created a BigQuery ML model and asks you to create an ML pipeline to serve predictions. You have a REST API application with the requirement to serve predictions for an individual user ID with latency under 100 milliseconds. You use the following query to generate predictions: SELECT predicted_label, user_id FROM ML.PREDICT (MODEL 'dataset.model', table user_features). How should you create the ML pipeline?","topic":"1","unix_timestamp":1584672540,"answer_description":"","answer_images":[],"answer_ET":"D","question_id":39,"exam_id":10,"answer":"D","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/17019-exam-professional-data-engineer-topic-1-question-133/","timestamp":"2020-03-20 03:49:00","choices":{"C":"Create a Dataflow pipeline using BigQueryIO to read results from the query. Grant the Dataflow Worker role to the application service account.","B":"Create an Authorized View with the provided query. Share the dataset that contains the view with the application service account.","A":"Add a WHERE clause to the query, and grant the BigQuery Data Viewer role to the application service account.","D":"Create a Dataflow pipeline using BigQueryIO to read predictions for all users from the query. Write the results to Bigtable using BigtableIO. Grant the Bigtable Reader role to the application service account so that the application can read predictions for individual users from Bigtable."},"discussion":[{"comment_id":"66164","comments":[{"upvote_count":"2","poster":"AzureDP900","content":"D. Create a Dataflow pipeline using BigQueryIO to read predictions for all users from the query. Write the results to Bigtable using BigtableIO. Grant the Bigtable Reader role to the application service account so that the application can read predictions for individual users from Bigtable.","timestamp":"1688136420.0","comment_id":"762708"}],"poster":"rickywck","timestamp":"1600562940.0","upvote_count":"30","content":"I think the key reason for pick D is the 100ms requirement."},{"content":"Answer: D\nDescription: Bigtable provides lowest latency","comment_id":"68851","timestamp":"1601287620.0","upvote_count":"14","poster":"[Removed]"},{"timestamp":"1718690580.0","comment_id":"1099513","content":"Selected Answer: D\nThe key requirements are serving predictions for individual user IDs with low (sub-100ms) latency.\nOption D meets this by batch predicting for all users in BigQuery ML, writing predictions to Bigtable for fast reads, and allowing the application access to query Bigtable directly for low latency reads.\nSince the application needs to serve low-latency predictions for individual user IDs, using Dataflow to batch predict for all users and write to Bigtable allows low-latency reads. Granting the Bigtable Reader role allows the application to retrieve predictions for a specific user ID from Bigtable.","upvote_count":"6","poster":"MaxNRG","comments":[{"content":"The other options either require changing the query for each user ID (higher latency, option A), reading directly from higher latency services like BigQuery (option B), or writing predictions somewhere without fast single row access (options A, B, C).\nOption A would not work well because the WHERE clause would need to be changed for each user ID, increasing latency.\nOption B using an Authorized View would still read from BigQuery which has higher latency than Bigtable for individual rows.\nOption C writes predictions to BigQuery which has higher read latency compared to Bigtable for individual rows.\nSo option D provides the best pipeline by predicting for all users in BigQueryML, batch writing to Bigtable for low latency reads, and granting permissions for the application to retrieve predictions. This meets the requirements of sub-100ms latency for individual user predictions.\nhttps://cloud.google.com/dataflow/docs/concepts/access-control","upvote_count":"2","comment_id":"1099514","timestamp":"1718690580.0","poster":"MaxNRG"}]},{"upvote_count":"1","comment_id":"1038762","poster":"Nirca","timestamp":"1712674140.0","content":"Selected Answer: D\nI think the key reason for pick D is the 100ms requirement/ me too"},{"comment_id":"1015424","poster":"barnac1es","timestamp":"1711252980.0","upvote_count":"2","content":"Selected Answer: B\nTo create an ML pipeline for serving predictions to individual user IDs with latency under 100 milliseconds using the given BigQuery ML query, the most suitable approach is:\n\nB. Create an Authorized View with the provided query. Share the dataset that contains the view with the application service account."},{"content":"Selected Answer: D\nAlways use Bigtable as an endpoint for client-facing applications (Low latency - high throughput)","poster":"Lanro","timestamp":"1706686920.0","upvote_count":"2","comment_id":"967794"},{"poster":"midgoo","timestamp":"1694579460.0","content":"Selected Answer: D\nOne of the way to improve the efficient of ML pipeline is to generate cache (store predictions). In this question, only D is doing that.","upvote_count":"3","comment_id":"837683"},{"poster":"musumusu","comments":[{"poster":"cheos71","comment_id":"978904","upvote_count":"1","content":"I think if there is too many concurrent requests the 100ms latency will def not hold reading from bigquery","timestamp":"1707685980.0"},{"poster":"vaga1","content":"and the view has to make a query in real-time which adds potential latency","upvote_count":"1","comment_id":"918047","timestamp":"1702033080.0"},{"comment_id":"918045","poster":"vaga1","content":"I would say that Bigtable is simply more suited to serve applications","timestamp":"1702032960.0","upvote_count":"1"}],"upvote_count":"2","comment_id":"812263","content":"what is wrong with B ? View can be precomputed and cached and it can definitely satisfy the 100 miliseconds request. create a pipeline to send data to bigtable .. don't you think its too much to run a simple prediction query ?","timestamp":"1692292680.0"},{"poster":"hiromi","timestamp":"1684660620.0","comment_id":"723418","content":"Selected Answer: D\nVote for D","upvote_count":"1"},{"content":"Selected Answer: D\nOption D","poster":"HarshKothari21","upvote_count":"1","timestamp":"1678253880.0","comment_id":"663068"},{"upvote_count":"6","timestamp":"1641245040.0","poster":"sumanshu","comment_id":"397811","content":"Vote for D, requirement to serve predictions with in 100 ms"},{"timestamp":"1615966260.0","upvote_count":"6","content":"D is correct , 100ms is most critical factor here.","comment_id":"180708","poster":"Tanmoyk"},{"comment_id":"172626","poster":"sh2020","content":"writing it to BigTable and then allowing application access will introduce more delays. I think answer should be C","comments":[{"poster":"f839","content":"Predictions are computed in advance for all users and written to BigTable for low-latency serving.","upvote_count":"3","timestamp":"1626001140.0","comment_id":"264729"}],"upvote_count":"1","timestamp":"1614780300.0"},{"upvote_count":"4","comment_id":"163168","content":"D is correct","poster":"haroldbenites","timestamp":"1613946960.0"},{"poster":"Rajokkiyam","timestamp":"1601601300.0","upvote_count":"6","content":"Answer D.","comment_id":"70303"},{"poster":"[Removed]","comment_id":"66888","timestamp":"1600760220.0","upvote_count":"7","content":"Should be D"}]},{"id":"si1lPa3HYDCj1MvbWshp","choices":{"A":"Cloud Dataflow, Cloud SQL, Cloud Spanner","B":"Cloud Pub/Sub, Cloud Storage, BigQuery","C":"Cloud Dataproc, Cloud Dataflow, BigQuery","D":"Cloud Pub/Sub, Cloud Dataproc, Cloud SQL"},"unix_timestamp":1584870180,"answer":"B","topic":"1","exam_id":10,"answer_description":"","answer_images":[],"question_id":40,"discussion":[{"comment_id":"66889","upvote_count":"24","content":"should be B","timestamp":"1584870180.0","poster":"[Removed]"},{"comments":[{"poster":"seiyassa","content":"I think pubsub doesn't have good connection to dataproc, so D is not the answer","upvote_count":"3","comments":[{"upvote_count":"2","content":"As of Dec 2022,there is the PubSub Lite connector to Dataproc","timestamp":"1671031920.0","comment_id":"745216","poster":"jkhong"}],"comment_id":"251358","timestamp":"1608770940.0"},{"poster":"jkhong","upvote_count":"1","comments":[{"comment_id":"921244","poster":"cetanx","upvote_count":"2","content":"Here is the reference:\nhttps://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics","timestamp":"1686557940.0"}],"timestamp":"1671032040.0","comment_id":"745218","content":"We can have our pubsub topics to have BigQuery subscriptions, where data is automatically streamed into our BQ tables. Autoscaling is already handled automatically so this renders Dataflow and Dataproc pretty irrelevant for our usecase"}],"poster":"itche_scratche","comment_id":"194737","content":"D, not ideal but only option that work. You need pubsub, then a processing layer (dataflow or dataproc), then storage (some sql database).","upvote_count":"12","timestamp":"1602033240.0"},{"timestamp":"1729791960.0","upvote_count":"1","poster":"SamuelTsch","comment_id":"1302577","content":"Selected Answer: D\nwhy B? The main goal of the question is data storage. Thus BigQuery is not neccessary for this situation. Option D from my point of view cover the whole requirements. Pub/Sub for streaming data, dataproc for data processing, SQL for storage."},{"poster":"barnac1es","content":"B. Cloud Pub/Sub, Cloud Storage, BigQuery.\n\nHere's how this solution aligns with your requirements:\nReal-time Event Stream: Cloud Pub/Sub is a managed messaging service that can handle real-time event streams efficiently. You can use Pub/Sub to ingest and publish real-time market data to consumers.\nANSI SQL Access: BigQuery supports ANSI SQL queries, making it suitable for both real-time and historical data analysis. You can stream data into BigQuery tables from Pub/Sub and provide ANSI SQL access to consumers.\nBatch Historical Exports: Cloud Storage can be used for batch historical exports. You can export data from BigQuery to Cloud Storage in batch, making it available for consumers to download.","upvote_count":"4","timestamp":"1695521400.0","comment_id":"1015426"},{"poster":"vaga1","upvote_count":"2","timestamp":"1686233760.0","comment_id":"918053","content":"Selected Answer: B\nI was in doubt as I did not know that BQ handles real-time access to data without dataflow underneath. \n\nhttps://cloud.google.com/bigquery/docs/write-api#:~:text=You%20can%20use%20the%20Storage,in%20a%20single%20atomic%20operation."},{"timestamp":"1678692300.0","poster":"midgoo","upvote_count":"1","content":"Selected Answer: B\nEvent Stream -> PubSub\nPubSub has direct Write to BigQuery\nHistorical Exports to GCS","comment_id":"837715"},{"upvote_count":"3","comment_id":"762710","comments":[{"content":"https://cloud.google.com/solutions/stream-analytics/","timestamp":"1672505340.0","comment_id":"762713","upvote_count":"1","poster":"AzureDP900"}],"timestamp":"1672505280.0","poster":"AzureDP900","content":"B. Cloud Pub/Sub, Cloud Storage, BigQuery"},{"upvote_count":"3","comment_id":"733821","content":"Selected Answer: B\nB is the answer.","poster":"zellck","timestamp":"1669991340.0"},{"upvote_count":"3","comment_id":"676172","poster":"John_Pongthorn","content":"Selected Answer: B\nB: https://cloud.google.com/solutions/stream-analytics/\nReal-time made real easy\nAdopt simple ingestion for complex events\nIngest and analyze hundreds of millions of events per second from applications or devices virtually anywhere on the globe with Pub/Sub. Or directly stream millions of events per second into your data warehouse for SQL-based analysis with BigQuery's streaming API.","timestamp":"1663852740.0"},{"content":"Selected Answer: B\nNo matter what the last of it must end up with bigquery and the first service is pubsub I think intimidate service it should be dataflow","poster":"John_Pongthorn","upvote_count":"1","comment_id":"667704","timestamp":"1663049340.0"},{"upvote_count":"1","comment_id":"578180","comments":[{"content":"You gonna use batch historical export for Spanner? It's B!","comment_id":"584809","poster":"tavva_prudhvi","timestamp":"1649776500.0","upvote_count":"2"}],"content":"Selected Answer: A\nDataflow: Streaming data\nCLoud SQL: for ansi sql support\nSpanner: for batch historical data export","poster":"Motivated_Gamer","timestamp":"1648635600.0"},{"content":"Answer is B","poster":"Prasanna_kumar","upvote_count":"1","timestamp":"1645466700.0","comment_id":"553073"},{"timestamp":"1641745200.0","upvote_count":"4","poster":"MaxNRG","comments":[{"timestamp":"1702886820.0","comment_id":"1099517","poster":"MaxNRG","content":"B. Cloud Pub/Sub, Cloud Storage, BigQuery\nThe key requirements here are:\n1. Real-time event stream (Pub/Sub)\n2. ANSI SQL access to real-time and historical data (BigQuery)\n3. Batch historical exports (Cloud Storage)\nSo Cloud Pub/Sub provides the real-time stream, BigQuery provides ANSI SQL access to stream and historical data, and Cloud Storage enables batch historical exports.\nOption A is incorrect because Cloud Spanner does not offer batch exports and Dataflow is overkill for just SQL access.\nOption C is incorrect as Dataproc is for spark workloads, not serving consumer data.\nOption D is incorrect as Cloud SQL does not provide batch export capabilities.\nTherefore, option B with Pub/Sub, Storage, and BigQuery is the best solution given the stated requirements. Dataflow https://cloud.google.com/solutions/stream-analytics/","upvote_count":"1"}],"comment_id":"520320","content":"Selected Answer: B\nCloud Pub/Sub, Cloud Dataflow, BigQuery \nhttps://cloud.google.com/solutions/stream-analytics/"},{"timestamp":"1641646560.0","comment_id":"519522","upvote_count":"10","poster":"medeis_jar","content":"Selected Answer: B\n✑ Real-time event stream -> Pub/Sub\n✑ ANSI SQL access to real-time stream and historical data -> BigQuery\n✑ Batch historical exports -> Cloud Storage"},{"upvote_count":"1","comment_id":"487117","content":"Correct: B","timestamp":"1637908860.0","poster":"JG123"},{"content":"I think it must be D because you need Pub/Sub for streaming data, Dataflow or DataProc to get the data from Pub/Sub and store it in a database and finally the Cloud SQL database to store the data.\nA and C cannot be because it is missing something for streaming data\nB It can't be because you need something to pass the data from Pub/Sub to Cloud storage","comment_id":"486164","timestamp":"1637777760.0","upvote_count":"3","poster":"AdrianMonter26"},{"upvote_count":"3","content":"Vote for B","poster":"sumanshu","timestamp":"1625341500.0","comment_id":"397825"},{"upvote_count":"1","content":"Vote for B","poster":"sumanshu","comment_id":"397824","timestamp":"1625341440.0"},{"poster":"rgpalop","content":"I think B","upvote_count":"2","timestamp":"1601697240.0","comment_id":"191969"},{"timestamp":"1598042340.0","content":"B is correct","upvote_count":"2","comment_id":"163169","poster":"haroldbenites"},{"upvote_count":"9","timestamp":"1595761740.0","poster":"VishalB","content":"Correct Answer: B\n\nStreaming Data -- Pub/Sub\nANSI SQL access -- Big Query\nBatch historical exports -- Cloud Storage","comment_id":"144055"},{"comment_id":"112933","upvote_count":"4","timestamp":"1592457240.0","content":"\"Processing logs at scale using Cloud Dataflow\"\n\nAs far as this document is concerned.\n\n・Cloud Storage for storing exported logs in batch mode.\n・Pub/Sub for streaming exported logs in streaming mode.\n・Dataflow for processing log data.\n・BigQuery for storing processing output and supporting rich queries on that output.\n\nIt is stated that.\nIt says the data is collected from the market, and the problem is that the methods are defined as requirements.\nTherefore, I think a close answer is B.\n\nhttps://cloud.google.com/solutions/processing-logs-at-scale-using-dataflow?hl=ja\nHowever, it's Spanner that's good at finance.","poster":"kino2020"},{"comments":[{"content":"So, why do we need Cloud Storage in this pipeline?","comments":[{"comment_id":"119690","content":"because of the requirement: Batch historical exports","poster":"jalk","timestamp":"1593108420.0","upvote_count":"6"}],"comment_id":"101932","poster":"serg3d","upvote_count":"3","timestamp":"1591222380.0"}],"comment_id":"70304","content":"Answer B.\nStreaming Data - Pub/Sub.\nQuery Batch and Live Streaming Data - BigQuery.","poster":"Rajokkiyam","upvote_count":"8","timestamp":"1585790340.0"}],"question_text":"You are building an application to share financial market data with consumers, who will receive data feeds. Data is collected from the markets in real time.\nConsumers will receive the data in the following ways:\n✑ Real-time event stream\n✑ ANSI SQL access to real-time stream and historical data\n✑ Batch historical exports\nWhich solution should you use?","url":"https://www.examtopics.com/discussions/google/view/17233-exam-professional-data-engineer-topic-1-question-134/","answer_ET":"B","question_images":[],"answers_community":["B (92%)","4%"],"timestamp":"2020-03-22 10:43:00","isMC":true}],"exam":{"isImplemented":true,"lastUpdated":"15 Feb 2025","id":10,"numberOfQuestions":319,"provider":"Google","isMCOnly":true,"name":"Professional Data Engineer","isBeta":false},"currentPage":8},"__N_SSP":true}