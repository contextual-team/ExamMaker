{"pageProps":{"questions":[{"id":"SEvZEbZX46mJErFeOHam","answers_community":["A (100%)"],"answer_images":[],"discussion":[{"upvote_count":"26","timestamp":"1601301180.0","comment_id":"68900","content":"Answer: A\nDescription: Huge amount of data with log network bandwidth, Transfer applicate is best for moving data over 100TB","poster":"[Removed]"},{"content":"Correct - A","upvote_count":"10","timestamp":"1600756560.0","comment_id":"66874","poster":"[Removed]"},{"upvote_count":"1","poster":"patitonav","comment_id":"1109735","content":"Selected Answer: A\nA . Easy, just by the amount of data","timestamp":"1719745260.0"},{"content":"Selected Answer: A\nIn 6 months - only 0.0290304 Petabytes will be uploaded. Right - compression might help, but we do not have any info to support the ration. Lets go for A","comment_id":"1039296","timestamp":"1712737140.0","poster":"Nirca","upvote_count":"1"},{"poster":"barnac1es","upvote_count":"1","comment_id":"1015449","content":"Selected Answer: A\nPhysical Transfer: Transfer Appliance is a physical device provided by Google Cloud that you can use to physically transfer large volumes of data to the cloud. It allows you to avoid the limitations of network bandwidth and transfer data much faster.\n\nCapacity: Transfer Appliance can handle large volumes of data, including the 2 PB you need to migrate, without the constraints of slow network speeds.\n\nEfficiency: It is highly efficient for large-scale data transfers and is a practical choice for transferring multi-terabyte or petabyte-scale datasets.","timestamp":"1711256880.0"},{"upvote_count":"2","timestamp":"1708424280.0","content":"it would take 34 years.\nOption A no doubt.\n\nhttps://cloud.google.com/static/architecture/images/big-data-transfer-how-to-get-started-transfer-size-and-speed.png","poster":"arien_chen","comment_id":"985639"},{"upvote_count":"1","poster":"vaga1","timestamp":"1699548180.0","comment_id":"893185","content":"Selected Answer: A\n2,000,000,000,000,000 bytes = 2 Petabytes\n20,000,000 bytes = 20 Megabytes\n\nOnce we do the math: \n 2 Petabytes / 20 Megabytes = 100,000,000 seconds forecasted to migrate the data.\n\n100,000,000 seconds =\n1,666,666.7 minutes = \n27,777.8 hours = \n1,157.4 days \n\n6 months = 180 days \n\n1,157.4 days > 180 days\n\nStill, with such amount Transfer Appliance is recommended."},{"poster":"musumusu","comment_id":"812436","timestamp":"1692302400.0","content":"Transfer alliance is a physical device of size like cabin luggage or slightly larger. \nIt has Seagate/WD harddisk (these are name of companies) size varries from 100 to 480 TB. \nIn our case 2PB (2000 TB) accordingly. Google send you this and transfer data into it by wire connection and then upload data from this to GCS and empty the appliance.","upvote_count":"1"},{"comment_id":"761629","poster":"AzureDP900","upvote_count":"2","timestamp":"1688085480.0","content":"This is no brainer question, A is right"},{"upvote_count":"2","comment_id":"732630","poster":"zellck","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview\nTransfer Appliance is a high-capacity storage device that enables you to transfer and securely ship your data to a Google upload facility, where we upload your data to Cloud Storage.","timestamp":"1685617260.0"},{"comment_id":"705593","timestamp":"1682602140.0","upvote_count":"3","content":"Selected Answer: A\nProblem: Transferring 2 peta data to Cloud Storage\nConsiderations: Bad network speed\n\n\nBad network = cannot initiate from client’s end through network. So, B, C is out\nD will still be super slow. At this speed it will take 27,777 hours to transfer the data","poster":"jkhong"},{"poster":"sumanshu","timestamp":"1641307320.0","content":"Vote for A\n\nA - Correct , Transfer Appliance for moving offline data, large data sets, or data from a source with limited bandwidth\nhttps://cloud.google.com/storage-transfer/docs/overview\nB - Eliminated (Not recommended for large storage). recommended for < 1TB\nC - Its ONLINE, but we have bandwidth issue - So eliminated.\nD - Eliminated (Not recommended for large storage). recommended for < 1TB","upvote_count":"9","comment_id":"398320"},{"upvote_count":"3","timestamp":"1616590740.0","content":"Correct answer is A. with little calculation we know the kind of data will require approx 19 months to transfer on 20Mbps bandwidth. Also, google recommends Transfer appliance for petabytes of data.","poster":"SteelWarrior","comment_id":"186085"},{"content":"A is correct","comment_id":"163560","upvote_count":"3","poster":"haroldbenites","timestamp":"1614000660.0"},{"poster":"Rajuuu","upvote_count":"3","comment_id":"128203","timestamp":"1609969200.0","content":"Correct - A"},{"upvote_count":"3","poster":"Rajokkiyam","comment_id":"70330","timestamp":"1601608560.0","content":"Answer A"}],"answer":"A","timestamp":"2020-03-22 09:36:00","unix_timestamp":1584866160,"url":"https://www.examtopics.com/discussions/google/view/17228-exam-professional-data-engineer-topic-1-question-144/","question_text":"You need to move 2 PB of historical data from an on-premises storage appliance to Cloud Storage within six months, and your outbound network capacity is constrained to 20 Mb/sec. How should you migrate this data to Cloud Storage?","answer_ET":"A","question_images":[],"topic":"1","choices":{"C":"Create a private URL for the historical data, and then use Storage Transfer Service to copy the data to Cloud Storage","A":"Use Transfer Appliance to copy the data to Cloud Storage","D":"Use trickle or ionice along with gsutil cp to limit the amount of bandwidth gsutil utilizes to less than 20 Mb/sec so it does not interfere with the production traffic","B":"Use gsutil cp ג€\"J to compress the content being uploaded to Cloud Storage"},"exam_id":10,"isMC":true,"answer_description":"","question_id":51},{"id":"K2nObckKd6S8TatYNxdg","choices":{"B":"Load each month's CSV data into BigQuery, and write a SQL query to transform the data to a standard schema. Merge the transformed tables together with a SQL query","C":"Help the analysts write a Dataflow pipeline in Python to perform the transformation. The Python code should be stored in a revision control system and modified as the incoming data's schema changes","A":"Use Dataprep by Trifacta to build and maintain the transformation recipes, and execute them on a scheduled basis","D":"Use Apache Spark on Dataproc to infer the schema of the CSV file before creating a Dataframe. Then implement the transformations in Spark SQL before writing the data out to Cloud Storage and loading into BigQuery"},"url":"https://www.examtopics.com/discussions/google/view/16674-exam-professional-data-engineer-topic-1-question-145/","exam_id":10,"topic":"1","question_id":52,"answer_ET":"A","discussion":[{"comment_id":"64364","upvote_count":"35","content":"A should be the answer","timestamp":"1600178820.0","poster":"madhu1171"},{"comment_id":"68902","content":"Answer: A\nDescription: Dataprep is used by non developers","poster":"[Removed]","upvote_count":"18","timestamp":"1601301300.0"},{"content":"Selected Answer: A\nA. Use Dataprep by Trifacta to build and maintain the transformation recipes, and execute them on a scheduled basis\n\nAddresses Requirements:\nScheduled Execution: Dataprep supports running transformations on a schedule.\nAnalyst-Friendly: Dataprep's visual interface is designed for non-developer analysts to build and modify transformations easily.\nGraphical Tool: It provides a drag-and-drop environment for designing data transformations.\nSchema Flexibility: Dataprep can handle schema changes. Analysts can adapt recipes using the visual interface","comment_id":"1189562","timestamp":"1728081120.0","upvote_count":"2","poster":"CGS22"},{"upvote_count":"1","timestamp":"1711257060.0","poster":"barnac1es","comment_id":"1015453","content":"Selected Answer: A\nScheduled Transformations: Dataprep by Trifacta allows you to design and schedule transformation recipes to process data on a regular basis. You can automate the data cleansing process by scheduling it to run monthly.\n\nUser-Friendly Interface: Dataprep provides a user-friendly graphical interface that enables non-developer analysts to design, modify, and maintain transformation recipes without writing code. This empowers analysts to work with the data effectively.\n\nTransformation Flexibility: Dataprep supports flexible data transformations, making it suitable for scenarios where the schema of the incoming data changes. Analysts can adapt the transformations to new schemas using the visual tools provided by Dataprep."},{"comment_id":"893193","poster":"vaga1","upvote_count":"4","content":"Selected Answer: A\nProviding a graphical tool for designing transformations is enough for A","timestamp":"1699548600.0"},{"upvote_count":"1","comment_id":"822133","content":"Your company receives a lot of financial data in CSV files. The files need to be processed, cleaned and transformed before they are made available for analytics. The schema of the data also changes every third month. The Data analysts should be able to perform the tasks\n 1. No prior knowledge of any language with no coding\n 2. Provided a GUI tool to build and modify the schema\nWhat solution best fits the need?","timestamp":"1693027140.0","poster":"Dhruv28"},{"content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/dataprep\nDataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning. Because Dataprep is serverless and works at any scale, there is no infrastructure to deploy or manage. Your next ideal data transformation is suggested and predicted with each UI input, so you don’t have to write code.","poster":"zellck","upvote_count":"3","timestamp":"1685617140.0","comment_id":"732627"},{"timestamp":"1678105860.0","poster":"arpitagrawal","upvote_count":"2","comment_id":"661094","content":"Selected Answer: A\nnon-developer analysts"},{"timestamp":"1669788420.0","comment_id":"609536","content":"Selected Answer: A\nDataprep is for non developers","poster":"devdimidved","upvote_count":"1"},{"timestamp":"1668843300.0","comment_id":"603613","upvote_count":"1","content":"Selected Answer: A\nOption A -- Dataprep is the right answer","poster":"amitsingla012"},{"poster":"Prasanna_kumar","timestamp":"1661410320.0","content":"Answer is A","comment_id":"555856","upvote_count":"1"},{"comment_id":"520404","upvote_count":"2","timestamp":"1657384380.0","content":"Selected Answer: A\nA: https://cloud.google.com/dataprep/","poster":"MaxNRG"},{"timestamp":"1657281300.0","upvote_count":"1","content":"Selected Answer: A\nCloud Dataprep is a tool to do the job.","poster":"medeis_jar","comment_id":"519559"},{"comments":[{"poster":"duytran_d","timestamp":"1674865140.0","content":"this comment is being repeated and i really appreciate this feeling :D","comment_id":"638347","upvote_count":"1"}],"timestamp":"1653550920.0","upvote_count":"7","content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: A","comment_id":"487229","poster":"JG123"},{"poster":"sandipk91","comment_id":"422082","content":"vote for option A","timestamp":"1644413340.0","upvote_count":"4"},{"timestamp":"1641308280.0","comment_id":"398344","upvote_count":"5","poster":"sumanshu","content":"Vote for 'A', because of requirement - Enabling non-developer analysts to modify transformations"},{"upvote_count":"3","timestamp":"1614001080.0","comment_id":"163563","poster":"haroldbenites","content":"A is correct"},{"upvote_count":"1","content":"Answer should be D. Dataprep will detect schema automatically in the initial recipe.After 3 months if the schema is changed the scheduled dataprep job cannot handle. So D should be the option.","timestamp":"1610297880.0","comments":[{"upvote_count":"8","poster":"atnafu2020","content":"A \nyou can use dataprep for continuously changing target schema\nIn general, a target consists of the set of information required to define the expected data in a dataset. Often referred to as a \"schema,\" this target schema information can include:\n\nNames of columns\nOrder of columns\nColumn data types\nData type format\nExample rows of data\nA dataset associated with a target is expected to conform to the requirements of the schema. Where there are differences between target schema and dataset schema, a validation indicator (or schema tag) is displayed.\nhttps://cloud.google.com/dataprep/docs/html/Overview-of-RapidTarget_136155049","comment_id":"167885","timestamp":"1614473760.0"},{"content":"spark is not graphical tool.","comment_id":"148760","poster":"Archy","timestamp":"1612223700.0","upvote_count":"5"}],"comment_id":"131532","poster":"SSV"},{"upvote_count":"5","content":"Answer A","timestamp":"1601608620.0","comment_id":"70331","poster":"Rajokkiyam"},{"content":"Should be A.","timestamp":"1600756740.0","poster":"[Removed]","comment_id":"66876","upvote_count":"3"}],"answer":"A","question_images":[],"answer_description":"","answer_images":[],"question_text":"You receive data files in CSV format monthly from a third party. You need to cleanse this data, but every third month the schema of the files changes. Your requirements for implementing these transformations include:\n✑ Executing the transformations on a schedule\n✑ Enabling non-developer analysts to modify transformations\n✑ Providing a graphical tool for designing transformations\nWhat should you do?","answers_community":["A (100%)"],"isMC":true,"unix_timestamp":1584288420,"timestamp":"2020-03-15 17:07:00"},{"id":"76fLv5Lj1Esq7nQhI0aO","question_images":[],"url":"https://www.examtopics.com/discussions/google/view/17224-exam-professional-data-engineer-topic-1-question-146/","discussion":[{"poster":"Sid19","comment_id":"501308","timestamp":"1639480140.0","comments":[{"poster":"Deepakd","upvote_count":"2","content":"How can master node store data ? C is wrong.","comment_id":"536547","timestamp":"1643585940.0"},{"content":"must go to the master node first...","upvote_count":"1","timestamp":"1697679420.0","comment_id":"1047429","poster":"KLei"},{"upvote_count":"2","comment_id":"518042","poster":"[Removed]","content":"option D is mentioned in the new release 2019: https://cloud.google.com/blog/products/data-analytics/new-release-of-cloud-storage-connector-for-hadoop-improving-performance-throughput-and-more","timestamp":"1641455820.0"},{"timestamp":"1686164700.0","upvote_count":"3","poster":"WillemHendr","comment_id":"917539","content":"I feel indeed this question is testing if you understand, that gsutil cannot transfer to HDFS directly (eliminate A&B), and need a intermediate step, (making C doable, with a good result). D is found on official google docs. E doesn't have good end result."},{"timestamp":"1686574020.0","content":"You can copy to worker nodes directly too by specyfing the specific flag.\n hdfs://<master node> is the default filesystem. You can explicitly specify the scheme and NameNode if desired:\nhdfs dfs -cp gs://<bucket>/<object> hdfs://<master node>/<hdfs path>\n\nSo the correct answer is A & D","comment_id":"921458","poster":"rohan0411","upvote_count":"1"},{"upvote_count":"4","poster":"Oleksandr0501","content":"In Cloud Dataproc, the master node does not store data; it's responsible for coordinating the overall work of the cluster, scheduling jobs, and managing communication between the worker nodes.","comment_id":"889334","timestamp":"1683193440.0"},{"comment_id":"910065","content":"According to Chat GPT => D, E\n\nCloud Storage connector for Hadoop allows Hadoop tools, including Hive, to work with data stored in Google Cloud Storage (GCS). It enables you to use GCS as you would HDFS, so you can directly mount ORC files stored in GCS as Hive tables without moving the data.\n\nLoading the ORC files (which also contain schema information) into BigQuery and using the BigQuery connector for Hadoop is another option. The connector allows Hadoop and Spark applications to read/write data directly from/to BigQuery, providing another path for your Hive applications to access the data.\n\nThe other options are less efficient or incorrect. Copying the ORC files directly to the Hadoop Distributed File System (HDFS) on the Dataproc cluster with gsutil (options A, B, and C) would require unnecessary data transfer and storage, not leveraging the benefits of separation of storage and compute that GCS provides.","upvote_count":"1","timestamp":"1685435220.0","poster":"cetanx"}],"upvote_count":"26","content":"Answer is C and D 100%.\nI know it says to transfer all the files but with the options provided c is the best choice.\nExplaination\nA and B cannot be true as gsutil can copy data to master node and the to hdfs from master node.\nC -> works\nD->works Recommended by google\nE-> Will work but as the question says maximize performance this is not a case. As bigquery hadoop connecter stores all the BQ data to GCS as temp and then processes it to HDFS. As data is already in GCS we donot need to load it to bq and use a connector then unloads it back to GCS and then processes it."},{"poster":"[Removed]","upvote_count":"17","timestamp":"1584862260.0","content":"Should be B C","comment_id":"66848"},{"upvote_count":"1","content":"Selected Answer: AD\nhttps://stackoverflow.com/questions/54429642/how-to-copy-a-file-from-a-gcs-bucket-in-dataproc-to-hdfs-using-google-cloud\nBased on here, you can copy a single file from Google Cloud Storage (GCS) to HDFS using the HDFS copy command. There is no need to copy to the master node first.","poster":"shangning007","timestamp":"1734509040.0","comment_id":"1328336"},{"timestamp":"1729952520.0","comment_id":"1303301","content":"Selected Answer: CD\nActually I think A is correct as well.","poster":"SamuelTsch","upvote_count":"1"},{"content":"Selected Answer: DE\nI think D and E are the best and easy way to go. For sure D, but I think that E can work too, the data can be loaded in BQ as an external table, so at the end the data will be always on the GCS.","upvote_count":"2","poster":"patitonav","comment_id":"1109738","timestamp":"1703941620.0"},{"poster":"barnac1es","content":"Selected Answer: DE\nD. Cloud Storage Connector for Hadoop: You can use the Cloud Storage connector for Hadoop to mount the ORC files stored in Cloud Storage as external Hive tables. This allows you to query the data without copying it to HDFS. You can replicate these external Hive tables to native Hive tables in Cloud Dataproc if needed.\n\nE. Load ORC Files into BigQuery: Another approach is to load the ORC files into BigQuery, Google Cloud's data warehouse. Once the data is in BigQuery, you can use the BigQuery connector for Hadoop to mount the BigQuery tables as external Hive tables in Cloud Dataproc. This leverages the power of BigQuery for analytics and allows you to replicate external Hive tables to native ones in Cloud Dataproc.","timestamp":"1695526980.0","comment_id":"1015467","upvote_count":"1"},{"content":"Selected Answer: DE\nD. Cloud Storage Connector for Hadoop: You can use the Cloud Storage connector for Hadoop to mount the ORC files stored in Cloud Storage as external Hive tables. This allows you to query the data without copying it to HDFS. You can replicate these external Hive tables to native Hive tables in Cloud Dataproc if needed.\n\nE. Load ORC Files into BigQuery: Another approach is to load the ORC files into BigQuery, Google Cloud's data warehouse. Once the data is in BigQuery, you can use the BigQuery connector for Hadoop to mount the BigQuery tables as external Hive tables in Cloud Dataproc. This leverages the power of BigQuery for analytics and allows you to replicate external Hive tables to native ones in Cloud Dataproc.","poster":"barnac1es","upvote_count":"1","comment_id":"1015465","timestamp":"1695526740.0"},{"comment_id":"963292","content":"Selected Answer: AD\nA is the most straightforward way to start using Hive in Cloud Dataproc. You can use the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDFS. Then, you can mount the Hive tables locally.\n\nD is another option that you can use to start using Hive in Cloud Dataproc. You can leverage the Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables. Then, you can replicate the external Hive tables to the native ones.","timestamp":"1690340100.0","upvote_count":"2","poster":"vamgcp"},{"upvote_count":"3","timestamp":"1688976720.0","content":"Selected Answer: BC\nAnswers are;\nB. Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluster. Mount the Hive tables locally.\nC. Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluster. Then run the Hadoop utility to copy them do HDFS. Mount the Hive tables from HDFS.\n\nYou need to replicate some data to the cluster's local Hadoop Distributed File System (HDFS) to maximize performance. HDFS lies on datanode, data on masternode needs to be copied on datanode.\nB for managed hive table option, C for external hive table","poster":"Qix","comment_id":"947892"},{"comment_id":"892015","upvote_count":"1","timestamp":"1683541860.0","content":"Selected Answer: AD\nAD is correct","poster":"izekc"},{"timestamp":"1683107220.0","upvote_count":"1","comment_id":"888347","poster":"Oleksandr0501","comments":[{"poster":"Oleksandr0501","upvote_count":"1","timestamp":"1683107340.0","content":"gpt: Yes, that is correct. Option A is a valid way to transfer the ORC files to HDFS, and then mount the Hive tables locally. Option D is also valid, as it suggests using the Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables and then replicating those external Hive tables to native ones.\n\nChatgpt agreed, after inserting quesion and variants, and said that AD are correct answers. And it agreed. It adds some confidence that these are good, but gpt can make mistakes","comment_id":"888352"}],"content":"Selected Answer: AD\ni choose AD. \nSearched in other w/s, read discussions here, and guess better AD."},{"upvote_count":"1","content":"Selected Answer: AD\nA will copy to HDFS and so will D","comment_id":"867018","timestamp":"1681195560.0","poster":"streeeber"},{"upvote_count":"3","content":"Selected Answer: AD\nC: master node doesn't make sense","comment_id":"734225","poster":"hauhau","timestamp":"1670046000.0","comments":[{"poster":"hauhau","comment_id":"734226","upvote_count":"1","timestamp":"1670046060.0","content":"B: from the Cloud Storage bucket to any node of the Dataproc cluster\n-> still on cloud not maxize the speed"}]},{"upvote_count":"4","comment_id":"732620","content":"Selected Answer: CD\nCD is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage\nThe Cloud Storage connector is an open source Java library that lets you run Apache Hadoop or Apache Spark jobs directly on data in Cloud Storage, and offers a number of benefits over choosing the Hadoop Distributed File System (HDFS).\n\nConnector Support. The Cloud Storage connector is supported by Google Cloud for use with Google Cloud products and use cases, and when used with Dataproc is supported at the same level as Dataproc.","timestamp":"1669899120.0","poster":"zellck"},{"comment_id":"707935","poster":"tikki_boy","upvote_count":"2","content":"I'll go with DE","timestamp":"1667149920.0"},{"upvote_count":"2","timestamp":"1662180300.0","poster":"ducc","content":"Selected Answer: CD\nCD is correct","comment_id":"658074"},{"poster":"BigDataBB","timestamp":"1644312120.0","upvote_count":"1","comment_id":"542960","content":"Selected Answer: BC\nYou need to replicate some data to the cluster's local Hadoop Distributed File System (HDFS) to maximize performance. HDFS lies on datanode, data on masternode needs to be copied on datanode.\nB for managed hive table option, C for external hive table"},{"content":"Selected Answer: CD\nas explained by Sid19","upvote_count":"3","poster":"medeis_jar","timestamp":"1641650460.0","comment_id":"519562"},{"timestamp":"1639145220.0","comments":[{"timestamp":"1639145460.0","content":"Prefer A & D. typo, moderator can you fix it?","upvote_count":"3","comment_id":"498677","poster":"bkovari"}],"comment_id":"498672","content":"Prefer A & E. Since master node does not store data in HDFS (it does checkpointing, but the data is always present on the slave nodes) -> B is out. \nFurthermore, when you copy using gsutil, you are defining a HDFS path instead of a node name (it is abstracted away to which node it is going to be put and replicated finally) \nso B just makes no sense. Since performance is key, we need to have a native table eventually (when data is on HDFS it can be used as external table only, but we create the \nnative variant of it..) -> is fulfilled by option D. (E is out also, since nobody cares about BigQuery, the question is how to use Hive which is possible right after the data is present on HDFS.. )","upvote_count":"2","poster":"bkovari"},{"timestamp":"1638785700.0","comments":[],"content":"Guys Listen. Here's how you solve this problem. In Question \"You need to replicate some data to the cluster's local Hadoop Distributed File System\" So, Reading all options in process of elimination. A,B,C are out Since they transfer all ORC Files from GCS to DataProc. So D is best ans because its possible and data is in GCS. 2nd best is E. So, Ans is D and E. Upvote if you agree.","poster":"BigQuery","upvote_count":"6","comment_id":"495033"},{"timestamp":"1626501240.0","upvote_count":"6","comment_id":"408270","content":"I vote for A, D","poster":"LORETOGOMEZ"},{"content":"You need to replicate some data to the cluster's local Hadoop Distributed File System\n(HDFS) to maximize performance\nThis requirement looks like the key for this question. so, i think answer is B,C","comment_id":"309216","poster":"daghayeghi","timestamp":"1615586160.0","upvote_count":"3"},{"poster":"apnu","upvote_count":"6","timestamp":"1609474200.0","content":"D and E are externam table which will have performance issue and which wil not fullfill the reqt, so D and E are eliminated. in ABC , A is eliminated because we cannot copy data directly from GCS bucket to HDFS through utility. so A is also eliminated. so only B and C are correct answer.","comment_id":"256650"},{"comment_id":"251434","content":"DE seems to be correct, BC looks irrelevant","upvote_count":"3","poster":"ashuchip","timestamp":"1608782760.0"},{"timestamp":"1606388880.0","poster":"Nams_139","content":"D,E As we don't want to replicate all files to hdfs - to the point.","upvote_count":"4","comment_id":"228243"},{"upvote_count":"2","comment_id":"219350","content":"As per the condition given in question - \"data has to be moved to hdfs from GCS\" . Depending on that BC seems to be right choice. B for managed hive table option, C for external hive table .","poster":"IT3008","timestamp":"1605389460.0"},{"upvote_count":"1","content":"I think correct answer is DE..\nSourec:- https://docs.cloudera.com/HDPDocuments/DLM1/DLM-1.4.0/administration/content/dlm_replication_on_premise_to_gcs_hive.html","comment_id":"200407","poster":"arghya13","timestamp":"1602754800.0"},{"poster":"BhadriRaju","comment_id":"192489","timestamp":"1601749860.0","upvote_count":"2","content":"You need to replicate some data to the cluster's local Hadoop Distributed File System\n(HDFS) to maximize performance\nThis requirement looks like the key for this question. so, i think answer is B,C"},{"upvote_count":"2","content":"Should be C D","comment_id":"182620","poster":"Tanmoyk","timestamp":"1600570800.0"},{"poster":"VIncent9261111","timestamp":"1600372560.0","upvote_count":"3","comments":[{"poster":"KokkiKumar","content":"Then provide your answer","timestamp":"1635641040.0","comment_id":"470445","upvote_count":"1"}],"content":"Why B and C in the discussion? In DataProc, Node does not store data right?","comment_id":"181189"},{"upvote_count":"5","content":"C, D is correct","comment_id":"177777","poster":"sh2020","timestamp":"1599841860.0"},{"poster":"haroldbenites","upvote_count":"7","content":"D , E is correct","timestamp":"1598096460.0","comment_id":"163564"},{"timestamp":"1597370280.0","comment_id":"157735","poster":"FARR","content":"D is for sure one of the ways\nOther could be C","upvote_count":"3"},{"timestamp":"1597251240.0","comment_id":"156620","content":"Should be A D","poster":"bab2020","upvote_count":"3"},{"content":"ANswer is DE","poster":"Archy","timestamp":"1596319500.0","upvote_count":"4","comment_id":"148763"},{"content":"Answer: B, C\nDescription: HDFS lies on datanode, data on masternode needs to be copied on datanode","comment_id":"68905","poster":"[Removed]","upvote_count":"9","timestamp":"1585411200.0"}],"answer_ET":"CD","answer_images":[],"timestamp":"2020-03-22 08:31:00","answers_community":["CD (37%)","AD (33%)","DE (15%)","Other"],"unix_timestamp":1584862260,"exam_id":10,"question_text":"You want to migrate an on-premises Hadoop system to Cloud Dataproc. Hive is the primary tool in use, and the data format is Optimized Row Columnar (ORC).\nAll ORC files have been successfully copied to a Cloud Storage bucket. You need to replicate some data to the cluster's local Hadoop Distributed File System\n(HDFS) to maximize performance. What are two ways to start using Hive in Cloud Dataproc? (Choose two.)","question_id":53,"choices":{"E":"Load the ORC files into BigQuery. Leverage BigQuery connector for Hadoop to mount the BigQuery tables as external Hive tables. Replicate external Hive tables to the native ones.","A":"Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDFS. Mount the Hive tables locally.","C":"Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluster. Then run the Hadoop utility to copy them do HDFS. Mount the Hive tables from HDFS.","D":"Leverage Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables. Replicate external Hive tables to the native ones.","B":"Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluster. Mount the Hive tables locally."},"answer":"CD","isMC":true,"answer_description":"","topic":"1"},{"id":"3QuTobHHZsZ9hEqeiKlR","answer":"D","isMC":true,"timestamp":"2020-03-15 17:11:00","answer_images":[],"unix_timestamp":1584288660,"answer_description":"","discussion":[{"upvote_count":"42","poster":"mario_ordinola","comment_id":"317726","content":"if someone are not sure that D is the answer, I suggest to don't take the exam","timestamp":"1632362100.0"},{"comment_id":"64365","poster":"madhu1171","timestamp":"1600179060.0","content":"D should be the answer","upvote_count":"23"},{"poster":"patitonav","content":"Selected Answer: D\nNo duobt","timestamp":"1719745740.0","comment_id":"1109739","upvote_count":"2"},{"timestamp":"1711259040.0","poster":"barnac1es","content":"Selected Answer: D\nWorkflow Orchestration: Cloud Composer is a fully managed workflow orchestration service based on Apache Airflow. It allows you to define, schedule, and manage complex workflows with multiple steps, including shell scripts, Hadoop jobs, and BigQuery queries.\n\nDependency Management: You can define dependencies between different steps in your workflow to ensure they are executed in a specific order.\n\nRetry Mechanism: Cloud Composer provides built-in retry mechanisms, so if any step fails, it can be retried a fixed number of times according to your configuration.\n\nScheduled Execution: Cloud Composer allows you to schedule the execution of your workflows on a regular basis, meeting the requirement for executing the jobs on a schedule.","upvote_count":"1","comment_id":"1015468"},{"upvote_count":"3","timestamp":"1688085180.0","content":"D is right","comment_id":"761626","poster":"AzureDP900"},{"content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/composer/docs/concepts/overview\nCloud Composer is a fully managed workflow orchestration service, enabling you to create, schedule, monitor, and manage workflows that span across clouds and on-premises data centers.","poster":"zellck","upvote_count":"4","comment_id":"732610","timestamp":"1685616120.0"},{"comment_id":"634219","content":"Selected Answer: A\nCloud Composer for sure.","poster":"DataEngineer_WideOps","comments":[{"poster":"[Removed]","comment_id":"683360","upvote_count":"2","content":"Composer is D","timestamp":"1680163800.0"}],"timestamp":"1674249900.0","upvote_count":"1"},{"upvote_count":"1","content":"D. \nper document \"Scheduler\" is aimed to a single service and composer for an ETL , in addition it's not even specified all jobs are on cloud so only composer can handle it.","poster":"nadavw","comments":[{"poster":"nadavw","content":"https://cloud.google.com/blog/topics/developers-practitioners/choosing-right-orchestrator-google-cloud","upvote_count":"1","comment_id":"612148","timestamp":"1670305980.0"}],"comment_id":"612147","timestamp":"1670305980.0"},{"timestamp":"1657281840.0","upvote_count":"2","poster":"medeis_jar","content":"Selected Answer: D\nCloud Composer","comment_id":"519565"},{"content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: D","timestamp":"1653554400.0","comment_id":"487257","poster":"JG123","upvote_count":"2"},{"timestamp":"1629388260.0","comment_id":"294490","poster":"daghayeghi","content":"D:\nthe main point is that Cloud Composer should be used when there is inter-dependencies between the job, e.g. we need the output of a job to start another whenever the first finished, and use dependencies coming from first job.","upvote_count":"4"},{"poster":"ashuchip","content":"D seems to be quiet relevant , because using composure you can do all things which are being asked to perform, even retry property is there in composure.","comment_id":"251435","upvote_count":"3","timestamp":"1624500720.0"},{"upvote_count":"5","content":"The correct answer is Option A : Cloud Scheduler . \nAlthough at first instance, I thought it should be Cloud Composer but then looking at the question and reading it few times - it concluded me to go for Option A. \n\nCloud Scheduler has built in retry handling so you can set a fixed number of times and doesn't have time limits for requests. The functionality is much simpler than Cloud Composer. Cloud Composer is managed Apache Airflow that \"helps you create, schedule, monitor and manage workflows. For automate scheduled jobs - the most preferred method would be Scheduler, Composer would typically be used when we want to orchestrate many managed services and automate the work flow.","timestamp":"1620910920.0","poster":"Alasmindas","comments":[{"timestamp":"1655801280.0","upvote_count":"1","content":"You forgot the \"These jobs have many interdependent steps\" which can be handled only though Composer","comment_id":"506039","poster":"baubaumiaomiao"},{"comments":[{"timestamp":"1623568860.0","comment_id":"242416","upvote_count":"3","content":"I think D , how scheduler can handle this part \" The jobs are expected to run for many minutes up to several hours\"","poster":"mumukshu"}],"poster":"kavs","comment_id":"222375","timestamp":"1621380900.0","content":"A seems to be right","upvote_count":"1"}],"comment_id":"218560"},{"upvote_count":"1","content":"should be A","comment_id":"218464","poster":"Abby1356","timestamp":"1620902580.0"},{"timestamp":"1618478460.0","comment_id":"200400","poster":"arghya13","content":"Answer should be A..Cloud scheduler..cloud composer is an workflow manager. Can't run unix,bigquery jobs","upvote_count":"2"},{"poster":"Tanmoyk","upvote_count":"3","content":"D should be the best option","timestamp":"1616216520.0","comment_id":"182622"},{"timestamp":"1614001260.0","upvote_count":"3","content":"D is correct","poster":"haroldbenites","comment_id":"163565"},{"content":"D!\n\"Cloud Scheduler is a fully managed enterprise-grade cron job scheduler\"\nhttps://cloud.google.com/scheduler","comment_id":"132093","poster":"lgdantas","upvote_count":"6","timestamp":"1610377560.0"},{"comment_id":"70332","content":"Answer D","timestamp":"1601608860.0","upvote_count":"6","poster":"Rajokkiyam"},{"poster":"[Removed]","timestamp":"1601041140.0","upvote_count":"13","content":"Correct: D\n\nAll interdependent tasks need to be run through cloud composer whereas small/adhoc tasks need to be run via scheduler","comment_id":"68150"},{"timestamp":"1600752840.0","poster":"[Removed]","content":"Should be D","comment_id":"66849","upvote_count":"7"}],"question_text":"You are implementing several batch jobs that must be executed on a schedule. These jobs have many interdependent steps that must be executed in a specific order. Portions of the jobs involve executing shell scripts, running Hadoop jobs, and running queries in BigQuery. The jobs are expected to run for many minutes up to several hours. If the steps fail, they must be retried a fixed number of times. Which service should you use to manage the execution of these jobs?","topic":"1","exam_id":10,"url":"https://www.examtopics.com/discussions/google/view/16675-exam-professional-data-engineer-topic-1-question-147/","answers_community":["D (90%)","10%"],"choices":{"D":"Cloud Composer","A":"Cloud Scheduler","C":"Cloud Functions","B":"Cloud Dataflow"},"answer_ET":"D","question_id":54,"question_images":[]},{"id":"v3gQEUiLEhSOtUTWoxgw","question_images":[],"topic":"1","question_id":55,"isMC":true,"answer":"B","timestamp":"2020-03-15 17:16:00","question_text":"You work for a shipping company that has distribution centers where packages move on delivery lines to route them properly. The company wants to add cameras to the delivery lines to detect and track any visual damage to the packages in transit. You need to create a way to automate the detection of damaged packages and flag them for human review in real time while the packages are in transit. Which solution should you choose?","answers_community":["B (75%)","C (25%)"],"answer_ET":"B","answer_images":[],"unix_timestamp":1584288960,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/16677-exam-professional-data-engineer-topic-1-question-148/","choices":{"B":"Train an AutoML model on your corpus of images, and build an API around that model to integrate with the package tracking applications.","A":"Use BigQuery machine learning to be able to train the model at scale, so you can analyze the packages in batches.","D":"Use TensorFlow to create a model that is trained on your corpus of images. Create a Python notebook in Cloud Datalab that uses this model so you can analyze for damaged packages.","C":"Use the Cloud Vision API to detect for damage, and raise an alert through Cloud Functions. Integrate the package tracking applications with this function."},"discussion":[{"comment_id":"66851","content":"Should be B.","poster":"[Removed]","upvote_count":"34","timestamp":"1600752960.0"},{"timestamp":"1601042820.0","upvote_count":"23","content":"AutoML is used to train model and do damage detection\nAuto Vision is used is a pre trained model used to detect objects in images","comment_id":"68153","poster":"[Removed]","comments":[{"timestamp":"1713860400.0","poster":"ga8our","upvote_count":"3","comment_id":"1051588","content":"1. Who said we have a labelled corpus that can be fed to AutoML? \n2. Auto Vision, as you say, is used to detect objects, like box, ship, human, etc. Now it depends only on our definition (parameters) of a \"box\" what the model should accept as intact or damaged.\n3. ChatGCP also chooses C. Vision API. \n\nGiven that the question does not say we have labelled data, and that demage recognition is not qualitatively different from object recognition, I'd go for C - C. Vision API.","comments":[{"content":"Seriously, I feel the question was transcribed incorrectly but the answers were. It was probably meant to include a labeled corpus of images. With the details presented you'd have to hope that Vision API would be able to guess objects as damage.","upvote_count":"1","timestamp":"1715367900.0","poster":"brokeasspanda","comment_id":"1067527"},{"poster":"emmylou","timestamp":"1716228060.0","upvote_count":"1","comment_id":"1075803","content":"Add the sentence, \"This is a practice exam question. Please assume no changes to the architecture\" and it brings back B"}]},{"comment_id":"571666","content":"Damage is an object in the image . So Auto Vision API can be used.","poster":"Deepakd","upvote_count":"3","timestamp":"1663677540.0"},{"content":"Correct : B","comment_id":"68154","upvote_count":"12","poster":"[Removed]","timestamp":"1601042880.0"}]},{"comment_id":"1108770","poster":"enthGCP","upvote_count":"2","content":"as per chat gpt One of the features of Cloud Vision API is damage detection, which can be used to identify and classify various types of damage in images, such as cracks, dents, scratches, stains, etc","timestamp":"1719663720.0"},{"upvote_count":"1","comment_id":"1099858","content":"Selected Answer: B\nFor this scenario, where you need to automate the detection of damaged packages in real time while they are in transit, the most suitable solution among the provided options would be B.\n\nHere's why this option is the most appropriate:\n\nReal-Time Analysis: AutoML provides the capability to train a custom model specifically tailored to recognize patterns of damage in packages. This model can process images in real-time, which is essential in your scenario.\n\nIntegration with Existing Systems: By building an API around the AutoML model, you can seamlessly integrate this solution with your existing package tracking applications. This ensures that the system can flag damaged packages for human review efficiently.\n\nCustomization and Accuracy: Since the model is trained on your specific corpus of images, it can be more accurate in detecting damages relevant to your use case compared to pre-trained models.","poster":"MaxNRG","timestamp":"1718720760.0","comments":[{"poster":"MaxNRG","content":"Let's briefly consider why the other options are less suitable:\n\nA. Use BigQuery machine learning: BigQuery is great for handling large-scale data analytics but is not optimized for real-time image processing or complex image recognition tasks like damage detection on packages.\n\nC. Use the Cloud Vision API: While the Cloud Vision API is powerful for general image analysis, it might not be as effective for the specific task of detecting damage on packages, which requires a more customized approach.\n\nD. Use TensorFlow in Cloud Datalab: While this is a viable option for creating a custom model, it might be more complex and time-consuming compared to using AutoML. Additionally, setting up a real-time analysis system through a Python notebook might not be as straightforward as an API integration.","timestamp":"1718720820.0","comment_id":"1099859","upvote_count":"1"}]},{"content":"Selected Answer: B\nI was leaning towards C but tested out uploading some damaged boxes to Vision API. It seems to have a lot of trouble detecting damaged boxes. It mislabeled boxes as a tire or toy. Also, there is no part of the API that seems to be able to detect damage. So I'll have to go with B. You should train a model to accomplish this then integrate with your app.","comment_id":"1092267","upvote_count":"3","poster":"juliorevk","timestamp":"1717982160.0"},{"content":"Selected Answer: B\nAutoML for Custom Models: AutoML (Auto Machine Learning) is designed to simplify the process of training custom machine learning models, including image classification models. It allows you to leverage Google Cloud's pre-built AutoML Vision service to train a model specifically for detecting package damage based on your corpus of images. This ensures accurate and customized results.\nReal-time API Integration: After training the AutoML model, you can create an API endpoint that integrates seamlessly with your package tracking applications. This means that as packages move on the delivery lines, you can send images in real-time to the API for immediate analysis.\nScalability: AutoML Vision is built to scale, so it can handle the analysis of images in real-time, even as packages move continuously on the delivery lines.","upvote_count":"2","comment_id":"1015471","poster":"barnac1es","timestamp":"1711259280.0"},{"timestamp":"1708426200.0","comments":[{"timestamp":"1708426380.0","upvote_count":"3","poster":"arien_chen","comment_id":"985656","content":"typo: Option C is better than B."}],"poster":"arien_chen","comment_id":"985654","content":"Selected Answer: C\nKeywords: realtime, camera streaming\n\nhttps://cloud.google.com/vision#:~:text=where%20you%20are-,Vertex%20AI%20Vision,-Vertex%C2%A0AI%20Vision \n\nOption B AutoML would be too complex and not time efficient.\n\nUsing Vision AI(Vertex AI Vision) first + AutoML \nOption D is better than B (just AutoML).","upvote_count":"5"},{"content":"B\nhttps://www.cloudskillsboost.google/focuses/22020?parent=catalog","poster":"piyush7777","comment_id":"983588","upvote_count":"1","timestamp":"1708179060.0"},{"timestamp":"1706244360.0","comment_id":"963279","poster":"vamgcp","upvote_count":"1","content":"Selected Answer: B\nOption B - AutoML"},{"comment_id":"893747","content":"will stay with B. Might be more reliable, accurate.\nas many says in duscission, Vision Api does not say it has defect detection. \ni remember labs with Auto ML, where models were trained. Vertex AI labs.","poster":"Oleksandr0501","timestamp":"1699611660.0","upvote_count":"3"},{"poster":"AzureDP900","upvote_count":"2","content":"B is right","timestamp":"1688084460.0","comment_id":"761617"},{"content":"B\nC-is not answer \nVision API currently allows you to use the following features:\nhttps://cloud.google.com/vision/docs/features-list#:~:text=Vision%20API%20currently%20allows%20you%20to%20use%20the%20following%20features%3A","poster":"Atnafu","comment_id":"727272","upvote_count":"3","timestamp":"1685070840.0"},{"timestamp":"1683402780.0","content":"Selected Answer: B\nIt looks like B is the only valid option, with the assumption that you have a corpus of images (the question does not say that you do not). \nIt would not be Cloud Vision API because that does not do damage detection (https://cloud.google.com/vision/docs/features-list).","poster":"cloudmon","comment_id":"712668","upvote_count":"3"},{"content":"https://cloud.google.com/solutions/visual-inspection-ai#all-features","upvote_count":"2","poster":"John_Pongthorn","comments":[{"content":"This is how you would do it nowadays, but the question is not referring to this solution. It only refers to \"Cloud Vision API\" (not Visual Inspection API). Cloud Vision API does not do damage detection (https://cloud.google.com/vision/docs/features-list) so you would need to do AutoML. It looks like they assume that you have your own corpus of images.","upvote_count":"2","timestamp":"1683402600.0","poster":"cloudmon","comment_id":"712667"}],"timestamp":"1679593800.0","comment_id":"677284"},{"timestamp":"1660011420.0","content":"Here it is mentioned that the company is planning to implement a camera system. So it does not have the training data yet. Without having training data , the only option left is to use pre- trained models through cloud API . C Is the answer. B is wrong as you dont have data to train the model.","comment_id":"543522","upvote_count":"1","comments":[{"content":"I would correct myself and go for B. I did not find any mention of cloud vision api being used for object detection.","timestamp":"1664421300.0","comment_id":"577201","upvote_count":"3","poster":"Deepakd"}],"poster":"Deepakd"},{"upvote_count":"2","comment_id":"530078","timestamp":"1658517720.0","content":"Selected Answer: B\nAutoML is used to train model and do damage detection Auto Vision is used is a pre trained model used to detect objects in images","poster":"sraakesh95"},{"content":"Selected Answer: B\nCloud Vision API -> pre-trained models to detect labels, faces, words\nAutoML -> custom specific models trained for specific use case","poster":"medeis_jar","timestamp":"1657282080.0","upvote_count":"3","comment_id":"519567"},{"comment_id":"487259","poster":"JG123","timestamp":"1653554580.0","upvote_count":"2","content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: B"},{"comment_id":"193444","comments":[{"upvote_count":"3","content":"B there is similar usecase in video for turbine image based fault detection..catch is if it is a pretraine or custom label...as damages has to be trained by us it has to be custom labels..on the other hand product search like the component usecase images are going to be same Pretrained hence vision api was right for the other question on 750 component..any thoughts","timestamp":"1621382340.0","comment_id":"222388","poster":"kavs"}],"upvote_count":"4","timestamp":"1617612900.0","content":"AutoML is right from the actual blogs I use.\nThe answer is B.\nhttps://www.fourcast.io/blog/google-automl-how-i-built-my-own-image-classification-algorithm","poster":"kino2020"},{"upvote_count":"8","content":"Answer is B. \nCloud Vision API detects lot of things for not damages. The description of Damages can be different for each business . So we need to train the model with test and training data to give our definition of Damages, so we need ML capabilities so answer is B, AutoML.","timestamp":"1615642380.0","poster":"nehaxlpb","comment_id":"178685"},{"upvote_count":"2","poster":"atnafu2020","timestamp":"1614494640.0","comment_id":"168080","content":"C is the correct answer"},{"comment_id":"163568","poster":"haroldbenites","content":"B is correct.","timestamp":"1614001440.0","upvote_count":"6"},{"content":"C it is ! period :)","comment_id":"152766","upvote_count":"3","poster":"clouditis","timestamp":"1612746240.0"},{"timestamp":"1612330500.0","poster":"Prakzz","upvote_count":"5","content":"Should be B","comment_id":"149493"},{"content":"AutoML is the way to go 'B'","timestamp":"1612225080.0","poster":"Archy","comment_id":"148770","upvote_count":"6"},{"comment_id":"129309","timestamp":"1610067060.0","content":"C - cloud vision API","poster":"dg63","upvote_count":"1"},{"content":"answer should be C","poster":"dass","upvote_count":"1","comment_id":"121837","timestamp":"1609164600.0"},{"timestamp":"1608954720.0","content":"Option [B] - looks to be correct","upvote_count":"10","comment_id":"120080","poster":"dambilwa"},{"comment_id":"64367","comments":[{"content":"I think B. C can't be because vision api cannot detect damage.","comments":[{"comments":[{"timestamp":"1624989300.0","content":"the api is pretrained which returns predefined labels... not sure if we can supply our own images...","upvote_count":"4","comment_id":"255074","poster":"g2000"}],"content":"Vision API can detect images, it can be trained with Good and Damage images.","poster":"Rajokkiyam","upvote_count":"2","timestamp":"1601608980.0","comment_id":"70334"}],"upvote_count":"10","timestamp":"1600412220.0","comment_id":"65550","poster":"jvg637"}],"timestamp":"1600179360.0","poster":"madhu1171","content":"C should be the answer","upvote_count":"5"}],"exam_id":10}],"exam":{"numberOfQuestions":319,"id":10,"name":"Professional Data Engineer","provider":"Google","isMCOnly":true,"isBeta":false,"isImplemented":true,"lastUpdated":"15 Feb 2025"},"currentPage":11},"__N_SSP":true}