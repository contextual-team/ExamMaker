{"pageProps":{"questions":[{"id":"43lAk1yXbMjciz0cPpdG","question_text":"You want to analyze hundreds of thousands of social media posts daily at the lowest cost and with the fewest steps.\nYou have the following requirements:\n✑ You will batch-load the posts once per day and run them through the Cloud Natural Language API.\n✑ You will extract topics and sentiment from the posts.\n✑ You must store the raw posts for archiving and reprocessing.\n✑ You will create dashboards to be shared with people both inside and outside your organization.\nYou need to store both the data extracted from the API to perform analysis as well as the raw social media posts for historical archiving. What should you do?","answer_ET":"C","answers_community":["C (100%)"],"answer_images":[],"topic":"1","question_images":[],"question_id":316,"answer":"C","isMC":true,"discussion":[{"upvote_count":"46","comments":[{"poster":"Shawvin","content":"Yes, the raw data needs to be archived too","upvote_count":"1","timestamp":"1681860300.0","comment_id":"464335"},{"upvote_count":"4","content":"but the posts are fed into cloud natural language api. which means we have to consider the posts to be text only","comment_id":"127980","timestamp":"1641494040.0","poster":"Devx198912233"},{"content":"Also to run batch queries data needs to be in Cloud Storage, so why not just store it there?","poster":"asksathvik","timestamp":"1677492480.0","upvote_count":"1","comment_id":"432871"}],"comment_id":"68759","poster":"[Removed]","timestamp":"1632799020.0","content":"Answer: C\nDescription: Social media posts can images/videos which cannot be stored in bigquery"},{"upvote_count":"17","content":"Answer should be C, becose they ask you to save a copy of the raw posts for archival, which may not be possible if you directly feed the posts to the API.","poster":"psu","comment_id":"74202","timestamp":"1634150700.0"},{"upvote_count":"2","content":"can any one help me with the rest of question from 101 to 209 as i dont have a contributor access","poster":"itz_me_sudhir","timestamp":"1725165600.0","comment_id":"825575"},{"upvote_count":"2","content":"Selected Answer: C\nC is the answer.","timestamp":"1717667460.0","poster":"zellck","comment_id":"736765"},{"timestamp":"1711045320.0","comment_id":"675310","upvote_count":"5","content":"Selected Answer: C\nI got this question on sept 2022. Answer is C","poster":"sedado77"},{"poster":"Erso","upvote_count":"1","timestamp":"1709996940.0","comment_id":"664593","content":"Selected Answer: C\nC is the correct one"},{"content":"Selected Answer: C\nOnly C make sense.","upvote_count":"2","comment_id":"518491","poster":"medeis_jar","timestamp":"1688662920.0"},{"comment_id":"513459","content":"Selected Answer: C\nYou must store the raw posts for archiving and reprocessing, Store the raw social media posts in Cloud Storage.\nB is expensive\nD is not valid since you have to store the raw posts for archiving\nBetween A and C I’s say C, since we’re going to make dashboards and Data Studio will connect well with big query.\nand besides A would probably be more expensive.","timestamp":"1688129400.0","poster":"MaxNRG","upvote_count":"3"},{"poster":"BigQuery","timestamp":"1685891280.0","comment_id":"493831","content":"SAY MY NAME!","upvote_count":"4"},{"comment_id":"490524","content":"Selected Answer: C\nAnalysis BQ\nStorage GCS","poster":"StefanoG","upvote_count":"2","timestamp":"1685428320.0"},{"upvote_count":"1","poster":"fire558787","timestamp":"1676632260.0","content":"I believe the API accesses data only from GCS Buckets not BigQuery (but I'm not entirely sure)","comment_id":"426221"},{"comment_id":"396124","poster":"sumanshu","content":"Vote for C","upvote_count":"2","timestamp":"1672594440.0"},{"timestamp":"1658012460.0","content":"Answer should be C because we need to consider storage archival","poster":"DPonly","upvote_count":"2","comment_id":"269205"},{"upvote_count":"2","content":"I'll go with option C","poster":"arghya13","comment_id":"221719","timestamp":"1652859120.0"},{"timestamp":"1652086560.0","comment_id":"215873","upvote_count":"3","poster":"Alasmindas","content":"I will go with Option C, because of the following reasons:-\na) Social media posts are \"raw\" - which means - it can be of any format (blob/object storage) is preferred. \nb) The output from the application (assuming the application is Cloud NLP) is to be future stored for archival purpose - and hence again Google Cloud storage is the best option - so option C\nOption A &C - Incorrect, although Option D fulfils the requirement of \"fewest step\" but storing data in big query for archival purpose is not a google recommended approach \nOption B : Cloud SQL rules out as it does not solve either for archival storage or for analytics purpose."},{"upvote_count":"1","timestamp":"1648164300.0","content":"cost of long term storing is almost same in GCS and BQ, so answer D makes sense from that angle..","comment_id":"186543","poster":"singhkrishna"},{"poster":"Tanmoyk","content":"The job is supposed to run in batch process once in a day , so there is no requirement of stream data. The best economical and less complex steps is answer C","timestamp":"1646740200.0","upvote_count":"2","comment_id":"175782"},{"timestamp":"1645530840.0","poster":"Ravivarma4786","content":"Bigquery is suitable for social media post ans should be C","comment_id":"163509","upvote_count":"2"},{"content":"C.\nFirst store the data on GCS, then extract only the relavant info for analysis and load into BQ. This way, huge data ie., audio, videos can stay on GCS (not lost). BQ cannot store audio/video. And note that Cloud Natural Language API is used for analysis which uses Text as it's source","upvote_count":"3","comment_id":"134207","timestamp":"1642105620.0","poster":"tprashanth"},{"content":"Answer is C:-\nReason :- You must store the raw posts for archiving and reprocessing.","upvote_count":"3","timestamp":"1641995040.0","comment_id":"132895","poster":"Rajuuu"},{"comment_id":"128463","content":"my option is C","poster":"SSV","upvote_count":"3","timestamp":"1641522960.0"},{"upvote_count":"3","comment_id":"121129","timestamp":"1640606400.0","poster":"PRC","content":"C is correct...store the raw data in Cloud Storage"},{"content":"Option [C] is more appropriate than Option [A] because the question mentions archiving & reprocessing","timestamp":"1640406420.0","upvote_count":"3","poster":"dambilwa","comment_id":"119040"},{"timestamp":"1632316680.0","upvote_count":"4","comment_id":"66983","content":"Answer - C","poster":"[Removed]"},{"content":"Answer D","timestamp":"1632280980.0","comment_id":"66808","upvote_count":"1","comments":[{"timestamp":"1659051900.0","poster":"[Removed]","comment_id":"278933","upvote_count":"1","content":"Why D?"}],"poster":"Rajokkiyam"},{"timestamp":"1631861460.0","comment_id":"65101","comments":[{"upvote_count":"2","comment_id":"130261","content":"Raw Data might be unstructured so BigQuery cannot be a storage option for RAW data. It can only hold refined data. So Option C is correct.","poster":"Rajokkiyam","timestamp":"1641697860.0"}],"poster":"rickywck","content":"Not C?","upvote_count":"1"}],"choices":{"A":"Store the social media posts and the data extracted from the API in BigQuery.","B":"Store the social media posts and the data extracted from the API in Cloud SQL.","C":"Store the raw social media posts in Cloud Storage, and write the data extracted from the API into BigQuery.","D":"Feed to social media posts into the API directly from the source, and write the extracted data from the API into BigQuery."},"url":"https://www.examtopics.com/discussions/google/view/16843-exam-professional-data-engineer-topic-1-question-96/","timestamp":"2020-03-17 09:51:00","unix_timestamp":1584435060,"exam_id":10,"answer_description":""},{"id":"yVYYZ3WZ1ufp7SaekN8p","question_text":"You store historic data in Cloud Storage. You need to perform analytics on the historic data. You want to use a solution to detect invalid data entries and perform data transformations that will not require programming or knowledge of SQL.\nWhat should you do?","choices":{"C":"Use Cloud Dataproc with a Hadoop job to detect errors and perform transformations.","A":"Use Cloud Dataflow with Beam to detect errors and perform transformations.","B":"Use Cloud Dataprep with recipes to detect errors and perform transformations.","D":"Use federated tables in BigQuery with queries to detect errors and perform transformations."},"url":"https://www.examtopics.com/discussions/google/view/16109-exam-professional-data-engineer-topic-1-question-97/","question_images":[],"answer_description":"","answer":"B","isMC":true,"discussion":[{"comment_id":"61727","content":"Use Dataprep ! It's THE tool for this","upvote_count":"56","poster":"cleroy","timestamp":"1583849400.0"},{"poster":"rickywck","timestamp":"1584435180.0","comment_id":"65103","comments":[{"timestamp":"1611884820.0","upvote_count":"6","content":"True.. might be legal issue?!","poster":"[Removed]","comment_id":"278934"}],"content":"Yes B. \n\nHonest speaking, sometime I thought the answers being posted here were intentionally to mislead people whose do not have proper knowledge on the subject, but just memorizing answers to pass the exam.","upvote_count":"53"},{"content":"Selected Answer: A\nA is the right way to do it... dataprepo is clumsy","upvote_count":"1","comments":[{"upvote_count":"7","poster":"Wudihero2","comment_id":"1042130","content":"...Did you even read through the question? It says \"not require programming or knowledge of SQL\". YOU are the one who's clumsy, not dataprep.","timestamp":"1697153760.0"}],"timestamp":"1694049180.0","comment_id":"1001086","poster":"sergiomujica"},{"timestamp":"1691093160.0","content":"Selected Answer: B\nno programming -> B","comment_id":"971437","poster":"crazycosmos","upvote_count":"1"},{"poster":"FP77","comment_id":"967283","timestamp":"1690734360.0","content":"Selected Answer: B\nI honestly do not understand what is the deal with this website. The correct answer is obviously Dataprep. How can they say it's A?","upvote_count":"2"},{"timestamp":"1673688300.0","upvote_count":"2","content":"Selected Answer: B\nIt'B. DataPrep it's the right tool. \nhttps://cloud.google.com/dataprep","comment_id":"775217","poster":"Besss"},{"content":"Selected Answer: B\nB is the answer.\n\nhttps://cloud.google.com/dataprep\nDataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning.","comment_id":"736764","poster":"zellck","upvote_count":"2","timestamp":"1670327400.0"},{"upvote_count":"4","content":"I got this question on sept 2022.","poster":"sedado77","comment_id":"675314","timestamp":"1663777380.0"},{"content":"Selected Answer: B\nB. Actually there are two tools to fix this problem. \nDataprep rely on dataflw\nDatafusion rely on dataproc","comments":[{"content":"Datafusion is lo-code but not no-code. The only no-code system is dataprep.","poster":"baimus","upvote_count":"2","timestamp":"1727254800.0","comment_id":"1288940"}],"poster":"John_Pongthorn","timestamp":"1663219680.0","upvote_count":"2","comment_id":"669495"},{"content":"Selected Answer: A\nA is the best answer !","poster":"diagniste","upvote_count":"1","comments":[{"comment_id":"779387","timestamp":"1673997120.0","upvote_count":"1","content":"dataflow IS Apache Beam...","poster":"desertlotus1211"}],"comment_id":"598981","timestamp":"1652091480.0"},{"comment_id":"555403","content":"Selected Answer: B\nB Dataprep","upvote_count":"2","timestamp":"1645721700.0","poster":"Venkat007"},{"comment_id":"518492","timestamp":"1641495780.0","upvote_count":"2","poster":"medeis_jar","content":"Selected Answer: B\nhttps://cloud.google.com/dataprep/"},{"comment_id":"513448","poster":"MaxNRG","upvote_count":"6","timestamp":"1640875320.0","content":"Selected Answer: B\nB, “Cloud Dataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning”\nhttps://cloud.google.com/dataprep/","comments":[{"timestamp":"1658311560.0","upvote_count":"2","content":"max you rock man!","poster":"dattatray_shinde","comment_id":"633971"}]},{"timestamp":"1634942400.0","comment_id":"466354","content":"Answer is B. Data prep. The keyword here is no programming skills required.","upvote_count":"4","poster":"GirijaSrinivasan"},{"upvote_count":"2","poster":"nguyenmoon","comment_id":"446283","content":"B- Dataprep","timestamp":"1631848020.0"},{"content":"Use Dataprep ....is the answer","timestamp":"1628231820.0","poster":"pass_gcp","upvote_count":"2","comment_id":"420633"},{"comment_id":"396129","content":"Vote for B","poster":"sumanshu","timestamp":"1625153760.0","upvote_count":"2","comments":[{"comment_id":"396149","upvote_count":"1","content":"Cloud Dataprep - almost fully automated","poster":"sumanshu","timestamp":"1625155080.0"}]},{"poster":"AnilKr","content":"Most of the answers are wrong, this is simple even.","upvote_count":"1","comment_id":"265455","timestamp":"1610451180.0"},{"content":"Answer is B (Dataprep), for remaining option programming and/or SQL is involved.","comment_id":"246287","poster":"mbiy","timestamp":"1608192240.0","upvote_count":"2"},{"timestamp":"1605692100.0","poster":"arghya13","content":"Dataprep-B","upvote_count":"2","comment_id":"221722"},{"content":"B Dataprep","upvote_count":"2","poster":"abhi0706","timestamp":"1600965300.0","comment_id":"186330"},{"content":"Should be B","poster":"Tanmoyk","timestamp":"1599558660.0","upvote_count":"2","comment_id":"175783"},{"upvote_count":"2","timestamp":"1598090100.0","content":"Cloud DataPrep will be used Ans B","comment_id":"163512","poster":"Ravivarma4786"},{"timestamp":"1598004360.0","poster":"haroldbenites","content":"Correct B.\nNo programming or not knowledge SQL","upvote_count":"2","comment_id":"162852"},{"timestamp":"1596687660.0","comment_id":"151665","upvote_count":"3","content":"not sure who thought A could be even remotely correct! yes its B for sure","poster":"clouditis"},{"upvote_count":"8","poster":"[Removed]","timestamp":"1585372680.0","content":"Answer: B\nDescription: No programming knowledge required for Dataprep","comment_id":"68760"},{"comment_id":"66985","timestamp":"1584890400.0","content":"Should be B","upvote_count":"5","poster":"[Removed]"},{"poster":"Rajokkiyam","timestamp":"1584854700.0","upvote_count":"7","content":"Options B.(considering no SQL skill i needed)\nNext closest option is Federated source and transform the data to the permanent table","comment_id":"66809"}],"exam_id":10,"answers_community":["B (90%)","10%"],"topic":"1","answer_ET":"B","answer_images":[],"question_id":317,"unix_timestamp":1583849400,"timestamp":"2020-03-10 15:10:00"},{"id":"USbu1sS4gjTwyuS0mHiT","unix_timestamp":1584890580,"answer_description":"","question_id":318,"discussion":[{"comment_id":"66986","upvote_count":"24","poster":"[Removed]","content":"Should be A","timestamp":"1584890580.0"},{"comment_id":"76417","content":"should be A, dataflow is on cloud is external; \"don't allow access from external IPs to their on-premises resources\" so no dataflow.","upvote_count":"14","poster":"itche_scratche","timestamp":"1587306120.0"},{"timestamp":"1722009360.0","content":"The gcloud storage command is the standard tool for small- to medium-sized transfers over a typical enterprise-scale network, from a private data center or from another cloud provider to Google Cloud.","upvote_count":"2","comment_id":"1255778","poster":"shroffshivangi"},{"comment_id":"775219","upvote_count":"1","poster":"Besss","content":"Selected Answer: A\nA is correct","timestamp":"1673688540.0"},{"timestamp":"1670327160.0","poster":"zellck","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#gsutil_for_smaller_transfers_of_on-premises_data\nThe gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB) over a typical enterprise-scale network, from a private data center to Google Cloud. We recommend that you include gsutil in your default path when you use Cloud Shell. It's also available by default when you install the Google Cloud CLI. It's a reliable tool that provides all the basic features you need to manage your Cloud Storage instances, including copying your data to and from the local file system and Cloud Storage. It can also move and rename objects and perform real-time incremental syncs, like rsync, to a Cloud Storage bucket.","comment_id":"736761","upvote_count":"5"},{"timestamp":"1665566100.0","upvote_count":"2","comment_id":"692896","poster":"somnathmaddi","content":"Selected Answer: A\nShould be A"},{"poster":"medeis_jar","timestamp":"1641495900.0","upvote_count":"1","content":"Selected Answer: A\nWithout this \"The security rules don't allow access from external IPs to their on-premises resources\" B would be an answer.","comment_id":"518494"},{"upvote_count":"6","comment_id":"513453","poster":"MaxNRG","content":"Selected Answer: A\nA is the better and most simple IF there is no problem in having gsutil in our servers.\nB and C no way, the comms will go GCP–Home, which sais is not allowed.\nD is valid, we can send the files with http://ftp…BUT ftp is not secure, and we’ll need to move them to the cloud storage afterwards, which is not detailed in the answer.\nhttps://cloud.google.com/storage/docs/gsutil/commands/rsync","timestamp":"1640875440.0"},{"comment_id":"508539","upvote_count":"1","content":"I am confused . which one is correct A or B ???","poster":"am2005","timestamp":"1640356920.0"},{"comment_id":"504509","content":"Selected Answer: A\nA is correct.","timestamp":"1639869720.0","poster":"hendrixlives","upvote_count":"1"},{"content":"This is the link:https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#gsutil_for_smaller_transfers_of_on-premises_data","upvote_count":"2","poster":"Chelseajcole","timestamp":"1634404560.0","comment_id":"463204"},{"comment_id":"435577","timestamp":"1630333680.0","upvote_count":"3","content":"How rsynch will handle private network?\n \"..The security rules don't allow access from external IPs to their on-premises resources..\"","poster":"manocha_01887"},{"timestamp":"1625156220.0","comment_id":"396174","upvote_count":"2","poster":"sumanshu","content":"Vote for A"},{"poster":"daghayeghi","comment_id":"308407","upvote_count":"4","comments":[{"content":"How could gsutil connect to Cloud Storage, if there is not access from external IPs? Should I understand that there is not access from outside to inside, but it is possible to send from inside to outside?","poster":"maurodipa","upvote_count":"1","timestamp":"1638547140.0","comments":[{"timestamp":"1639822800.0","upvote_count":"3","poster":"szefco","content":"Yes. There is no access to on-prem from external IPs, but on prem can talk to external","comment_id":"504148"}],"comment_id":"493219"}],"timestamp":"1615501320.0","content":"A:\nhttps://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google"},{"content":"gsutil rsync will be used to transfer the files ANS A","poster":"Ravivarma4786","upvote_count":"4","comment_id":"163514","timestamp":"1598090220.0"},{"upvote_count":"3","poster":"haroldbenites","comment_id":"162854","timestamp":"1598004480.0","content":"A is correct"},{"timestamp":"1594119960.0","poster":"VishalB","content":"Ans : A\nThe gsutil rsync command makes the contents under dst_url the same as the contents under src_url, by copying any missing files/objects (or those whose data has changed), and (if the -d option is specified) deleting any extra files/objects. src_url must specify a directory, bucket, or bucket subdirectory","comment_id":"128845","upvote_count":"6"},{"upvote_count":"4","comment_id":"128817","poster":"Devx198912233","content":"option A\nhttps://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google","timestamp":"1594118040.0"}],"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/17255-exam-professional-data-engineer-topic-1-question-98/","answer_ET":"A","answer":"A","topic":"1","answers_community":["A (100%)"],"answer_images":[],"choices":{"B":"Use Dataflow and write the data to Cloud Storage.","C":"Write a job template in Dataproc to perform the data transfer.","D":"Install an FTP server on a Compute Engine VM to receive the files and move them to Cloud Storage.","A":"Execute gsutil rsync from the on-premises servers."},"timestamp":"2020-03-22 16:23:00","question_text":"Your company needs to upload their historic data to Cloud Storage. The security rules don't allow access from external IPs to their on-premises resources. After an initial upload, they will add new data from existing on-premises applications every day. What should they do?","exam_id":10,"isMC":true},{"id":"kPRwWlhpz1rm4Zd2ZRDl","choices":{"B":"Use the LIMIT keyword to reduce the number of rows returned.","D":"Use the bq query --maximum_bytes_billed flag to restrict the number of bytes billed.","C":"Recreate the table with a partitioning column and clustering column.","A":"Create a separate table for each ID."},"timestamp":"2020-03-17 10:05:00","isMC":true,"unix_timestamp":1584435900,"url":"https://www.examtopics.com/discussions/google/view/16845-exam-professional-data-engineer-topic-1-question-99/","answer_images":[],"answer_description":"","discussion":[{"timestamp":"1647507900.0","poster":"rickywck","comment_id":"65108","upvote_count":"43","content":"should be C:\n\nhttps://cloud.google.com/bigquery/docs/best-practices-costs"},{"poster":"[Removed]","timestamp":"1647962880.0","upvote_count":"17","content":"Correct - C","comment_id":"66987"},{"comment_id":"735938","poster":"zellck","timestamp":"1733402940.0","upvote_count":"4","content":"Selected Answer: C\nC is the answer.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables\nA partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.\n\nhttps://cloud.google.com/bigquery/docs/clustered-tables\nlustered tables in BigQuery are tables that have a user-defined column sort order using clustered columns. Clustered tables can improve query performance and reduce query costs."},{"poster":"Fezo","content":"Selected Answer: C\nC is the answer\nhttps://cloud.google.com/bigquery/docs/best-practices-costs","timestamp":"1720081380.0","comment_id":"626906","upvote_count":"3"},{"content":"Selected Answer: C\nC only make sense","comment_id":"518495","poster":"medeis_jar","upvote_count":"2","timestamp":"1704568020.0"},{"upvote_count":"5","poster":"MaxNRG","timestamp":"1703947500.0","comment_id":"513454","content":"Selected Answer: C\nhttps://cloud.google.com/bigquery/docs/best-practices-costs\nApplying a LIMIT clause to a SELECT * query does not affect the amount of data read. You are billed for reading all bytes in the entire table, and the query counts against your free tier quota.\nA and D doesnt make sense\nIts C, when you want to select by a partition you should write something like:\nCREATE TABLE `blablabla.partitioned`\nPARTITION BY\nDATE(timestamp)\nCLUSTER BY id\nAS\nSELECT * FROM `blablabla`"},{"content":"this is a trap to make people fail by giving wrong answer as B.","timestamp":"1702569780.0","comment_id":"501485","poster":"Anilgcp980","upvote_count":"3"},{"upvote_count":"1","poster":"snadaf","content":"It's D, here is the link\nhttps://cloud.google.com/bigquery/docs/best-practices-costs","comments":[{"content":"Well, you mean C, isn't it?","comment_id":"493223","timestamp":"1701619320.0","upvote_count":"1","poster":"maurodipa"}],"comment_id":"492377","timestamp":"1701514500.0"},{"comment_id":"475055","content":"Are they having a laugh at us by giving so many bad answers?","upvote_count":"5","poster":"Crudgey","timestamp":"1699561260.0"},{"content":"Answer: B\nNote: minimal change to sql","poster":"tsoetan001","comment_id":"465356","upvote_count":"1","comments":[{"comment_id":"472289","timestamp":"1699046220.0","poster":"szefco","upvote_count":"3","content":"Not B. LIMIT will not reduce amount of data scanned - only limit the final output, but you will still be billed for scanning whole table.\nC is correct. After applying partitioning ans clustering amount of bytes scanned will decrease"}],"timestamp":"1697832060.0"},{"comment_id":"453450","timestamp":"1695909300.0","comments":[{"comment_id":"472290","poster":"szefco","timestamp":"1699046340.0","upvote_count":"1","content":"I don't agree. Question says \"minimal changes to existing SQL queries\" - if you recreate table with partitioning and clustering you don't need to change SQLs that read from that table.\nC is correct answer here."},{"timestamp":"1697586780.0","content":"D would just block your query. The answer is C.","upvote_count":"1","comment_id":"463775","poster":"squishy_fishy"}],"poster":"Ysance_AGS","upvote_count":"2","content":"\"You want to reduce the amount of data scanned by BigQuery with minimal changes to existing SQL queries\" that doesn't mean that you can create or edit existing tables ! you only can edit the SQL query !!! so answer D is the correct one."},{"timestamp":"1694920920.0","upvote_count":"2","comment_id":"446289","poster":"nguyenmoon","content":"C - create partition table"},{"content":"Vote for C","upvote_count":"4","timestamp":"1688228340.0","comment_id":"396176","poster":"sumanshu"},{"poster":"felixwtf","upvote_count":"4","content":"LIMIT keyword is applied only at the end, i.e., only to limit the results already calculated. Therefore, a full table scan will have already happened. The where clause on the other hand would provide the desired filtering depending on the case. So, C is the correct answer.","timestamp":"1671128160.0","comment_id":"244830"},{"upvote_count":"2","poster":"learnazureportal","timestamp":"1669481280.0","content":"Not sure, why option C selected! The correct Answer is B. the question clearly says \"minimal changes to existing SQL queries\". who said that, recreate the table, with partitioning layout is minimal and is PART of SQL queries!","comment_id":"228473","comments":[{"content":"In addition to the previous reply, the LIMIT statement applies to the output (what you see in the UI), the full table scan will still happen. C is correct according to best practices.","comment_id":"415335","poster":"hdmi_switch","upvote_count":"1","timestamp":"1690451520.0"},{"content":"recreating table will not affect existing sql queries as they will still be selecting the same table name, but the scan will hugely decrease. so, option C is the correct answer.","timestamp":"1669821480.0","comment_id":"231157","poster":"ceak","upvote_count":"5"},{"timestamp":"1697586900.0","poster":"squishy_fishy","upvote_count":"1","comment_id":"463776","content":"Recreating table is recommended by Google."}]},{"comment_id":"221747","poster":"arghya13","upvote_count":"2","timestamp":"1668765960.0","content":"should be C:"},{"poster":"gyclop","comment_id":"180491","upvote_count":"3","timestamp":"1663349640.0","content":"Correct - C :\n\"Limit\" keyword restricts the final dataset to \"n\" rows, but is not able to restrict full table scan"},{"upvote_count":"2","poster":"Ravivarma4786","comment_id":"163516","timestamp":"1661162340.0","content":"Partitioning will help reduce the scanning time"},{"poster":"haroldbenites","content":"C is correct","timestamp":"1661076540.0","upvote_count":"2","comment_id":"162856"},{"upvote_count":"3","comment_id":"151672","timestamp":"1659759840.0","poster":"clouditis","content":"C it is! not sure who even thought B could be right here.."},{"comment_id":"121130","upvote_count":"4","content":"C is correct...Partitioning and Clustering are the best practices to limit the data scanning","timestamp":"1656324180.0","poster":"PRC"},{"upvote_count":"4","timestamp":"1656123600.0","comment_id":"119037","poster":"dambilwa","content":"Option [C] is correct"},{"upvote_count":"6","comment_id":"68761","poster":"[Removed]","timestamp":"1648437660.0","content":"Answer: C\nDescription: Partition helps in reducing the number of columns scanned"},{"timestamp":"1647927060.0","upvote_count":"3","comment_id":"66810","content":"Why not option D?\nhttps://cloud.google.com/bigquery/docs/best-practices-costs\nsection : Limit query costs by restricting the number of bytes billed","poster":"Rajokkiyam","comments":[{"timestamp":"1650755280.0","poster":"arnabbis4u","comment_id":"78860","upvote_count":"4","content":"Because if you limit the number of bytes, the query will fail if it reaches that limit. This does not guarantees a successful query execution."}]}],"answers_community":["C (100%)"],"question_images":[],"question_id":319,"topic":"1","answer_ET":"C","exam_id":10,"question_text":"You have a query that filters a BigQuery table using a WHERE clause on timestamp and ID columns. By using bq query `\"-dry_run you learn that the query triggers a full scan of the table, even though the filter on timestamp and ID select a tiny fraction of the overall data. You want to reduce the amount of data scanned by BigQuery with minimal changes to existing SQL queries. What should you do?","answer":"C"}],"exam":{"isMCOnly":true,"name":"Professional Data Engineer","id":10,"numberOfQuestions":319,"lastUpdated":"15 Feb 2025","isBeta":false,"provider":"Google","isImplemented":true},"currentPage":64},"__N_SSP":true}