{"pageProps":{"questions":[{"id":"9KlFe5AUehUDLnFxwjC0","timestamp":"2022-09-02 19:15:00","isMC":true,"question_text":"You have uploaded 5 years of log data to Cloud Storage. A user reported that some data points in the log data are outside of their expected ranges, which indicates errors. You need to address this issue and be able to run the process again in the future while keeping the original data for compliance reasons. What should you do?","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/79501-exam-professional-data-engineer-topic-1-question-176/","answer_ET":"C","question_id":86,"choices":{"B":"Create a Compute Engine instance and create a new copy of the data in Cloud Storage. Skip the rows with errors.","D":"Create a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to the same dataset in Cloud Storage.","C":"Create a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to a new dataset in Cloud Storage.","A":"Import the data from Cloud Storage into BigQuery. Create a new BigQuery table, and skip the rows with errors."},"question_images":[],"answer":"C","discussion":[{"content":"Selected Answer: C\nC. Create a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to a new dataset in Cloud Storage.\n\nYou can't filter out data using BQ load commands. You must imbed the logic to filter out data (i.e. time ranges) in another decoupled way (i.e. Dataflow, Cloud Functions, etc.). Therefore, A and B add additional complexity and deviates from the Data Lake design paradigm. D is wrong as the question strictly implies that the existing data set needs to be retained for compliance.","comment_id":"657711","timestamp":"1677788700.0","upvote_count":"9","poster":"AWSandeep"},{"timestamp":"1708121040.0","comment_id":"982970","upvote_count":"5","content":"Strange answers... Since when does cloud storage have datasets? Lol\nKeeping this in mind, the answer must be C, but none is really correcg","poster":"FP77"},{"content":"why not D if the versioning is activated while creating your bucket ?","poster":"ea2023","timestamp":"1723452780.0","comment_id":"1147996","upvote_count":"1"},{"poster":"MaxNRG","comment_id":"1101928","timestamp":"1718908980.0","content":"Selected Answer: C\nOption C is the best approach in this situation. Here is why:\n\nOption A would remove data which may be needed for compliance reasons. Keeping the original data is preferred.\nOption B makes a copy of the data but still removes potentially useful records. Additional storage costs would be incurred as well.\nOption C uses Dataflow to clean the data by setting out of range values while keeping the original data intact. The fixed records are written to a new location for further analysis. This meets the requirements.\nOption D writes the fixed data back to the original location, overwriting the original data. This would violate the compliance needs to keep the original data untouched.\nSo option C leverages Dataflow to properly clean the data while preserving the original data for compliance, at reasonable operational costs. This best achieves the stated requirements.","upvote_count":"3"},{"timestamp":"1688223840.0","upvote_count":"2","poster":"AzureDP900","content":"C. Create a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to a new dataset in Cloud Storage.","comment_id":"763270"},{"comment_id":"730481","upvote_count":"3","poster":"zellck","timestamp":"1685361900.0","content":"Selected Answer: C\nC is the answer."},{"upvote_count":"2","comment_id":"657641","content":"Selected Answer: C\nC is correct","poster":"PhuocT","timestamp":"1677784500.0"}],"topic":"1","answers_community":["C (100%)"],"exam_id":10,"unix_timestamp":1662138900,"answer_description":""},{"id":"oD4LlhitMytVDRgQB65z","exam_id":10,"isMC":true,"timestamp":"2022-09-02 20:30:00","url":"https://www.examtopics.com/discussions/google/view/79540-exam-professional-data-engineer-topic-1-question-177/","question_id":87,"question_images":[],"answer_ET":"C","question_text":"You want to rebuild your batch pipeline for structured data on Google Cloud. You are using PySpark to conduct data transformations at scale, but your pipelines are taking over twelve hours to run. To expedite development and pipeline run time, you want to use a serverless tool and SOL syntax. You have already moved your raw data into Cloud Storage. How should you build the pipeline on Google Cloud while meeting speed and processing requirements?","discussion":[{"content":"Selected Answer: C\nThe question is C but not because the SQL Syntax, as you can perfectly use SparkSQL on Dataproc reading files from GCS. It's because the \"serverless\" requirement.","comment_id":"686992","poster":"devaid","upvote_count":"14","timestamp":"1680707520.0"},{"content":"Selected Answer: A\nA) Looks more suitable , serverless approach for handling and performance.","timestamp":"1721138940.0","upvote_count":"2","poster":"GCP001","comment_id":"1124344"},{"content":"Selected Answer: C\nOption C is the best approach to meet the stated requirements. Here's why:\n\nBigQuery SQL provides a fast, scalable, and serverless method for transforming structured data, easier to develop than PySpark.\nDirectly ingesting the raw Cloud Storage data into BigQuery avoids needing an intermediate processing cluster like Dataproc.\nTransforming the data via BigQuery SQL queries will be faster than PySpark, especially since the data is already loaded into BigQuery.\nWriting the transformed results to a new BigQuery table keeps the original raw data intact and provides a clean output.\nSo migrating to BigQuery SQL for transformations provides a fully managed serverless architecture that can significantly expedite development and reduce pipeline runtime versus PySpark. The ability to avoid clusters and conduct transformations completely within BigQuery is the most efficient approach here.","upvote_count":"3","timestamp":"1718909340.0","comment_id":"1101930","poster":"MaxNRG"},{"poster":"MoeHaydar","upvote_count":"3","timestamp":"1704961500.0","comment_id":"948733","content":"Selected Answer: C\nNote: Dataproc by itself is not serverless\nhttps://cloud.google.com/dataproc-serverless/docs/overview"},{"poster":"Prudvi3266","comment_id":"876431","upvote_count":"3","timestamp":"1697886060.0","content":"Selected Answer: C\nbecause of serverless nature"},{"content":"Answer C: need to setup SQL based job means transformation in not very complex. And Biqquery sql are faster than spark sql context. (google claims)\nHowever, i will make a test by myself to check it.","upvote_count":"1","comment_id":"813462","poster":"musumusu","timestamp":"1692381720.0"},{"comment_id":"786579","comments":[{"timestamp":"1701502800.0","poster":"evanfebrianto","content":"Dataproc is not a serverless tool unless it mentions \"Dataproc Serverless\" explicitly.","comment_id":"912556","upvote_count":"2"}],"poster":"maci_f","timestamp":"1690199580.0","content":"Selected Answer: A\nIn the GCP Machine Learning Engineer practice question (Q4) there's the same question with similar answers and the correct answer is A since B \"is incorrect, here transformation is done on Cloud SQL, which wouldn’t scale the process\" and C \"is incorrect as this process wouldn’t scale the data transformation routine. And, it is always better to transform data during ingestion\": https://medium.com/@gcpguru/google-google-cloud-professional-machine-learning-engineer-practice-questions-part-1-3ee4a2b3f0a4","upvote_count":"2"},{"content":"C\nD-is incorrect because you are rebuild your batch pipeline for structured data on Google Cloud.","poster":"Atnafu","upvote_count":"1","comments":[{"upvote_count":"2","timestamp":"1684858440.0","comment_id":"725324","content":"A could be answer if it was Dataproc serverless and no conversion of code. Dp serverless support: scala,pyspark,sparksql and SparkR","poster":"Atnafu"}],"comment_id":"725322","timestamp":"1684858260.0"},{"poster":"TNT87","comment_id":"675940","content":"Selected Answer: C\nThis same question is there on Google's Professional Machine Learning Engineer, Question 4\nAnswer is C.","upvote_count":"4","timestamp":"1679484780.0"},{"comment_id":"667255","timestamp":"1678647240.0","poster":"Wasss123","content":"Selected Answer: C\nI choose C \nBigQuery SQL is more performant but more expensive. Here, it's a performance issue ( time reduction)\nSource : https://medium.com/paypal-tech/comparing-bigquery-processing-and-spark-dataproc-4c90c10e31ac","upvote_count":"2"},{"content":"C is the most likely , bigquery is severless and sql \nD is dataflow severless but it is wrong at using python sdk but using sql beam rthen it will be correct","timestamp":"1678449540.0","poster":"John_Pongthorn","upvote_count":"1","comment_id":"665291"},{"poster":"TNT87","timestamp":"1678181280.0","comment_id":"662165","content":"Answer C","upvote_count":"2"},{"comment_id":"657939","poster":"ducc","timestamp":"1677812220.0","comments":[{"comments":[{"content":"A or C. I go for C because they said they want to use SQL syntax...","upvote_count":"1","comment_id":"658100","poster":"ducc","timestamp":"1677828780.0"}],"poster":"ducc","content":"After thinking a while, I think the question is not clear enough. To be honest","timestamp":"1677828720.0","upvote_count":"1","comment_id":"658099"}],"content":"Selected Answer: A\nA\n- You have to maintain PySpark Code -> Proc","upvote_count":"1"},{"content":"Selected Answer: C\nC. Ingest your data into BigQuery from Cloud Storage, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table.\n\nKeys: \"Serverless\" and \"SQL\"","upvote_count":"3","poster":"AWSandeep","comment_id":"657717","timestamp":"1677789000.0","comments":[{"timestamp":"1677835800.0","content":"The question said \"use SQL syntax\" \nC might still correct","upvote_count":"1","comment_id":"658180","poster":"ducc"},{"content":"Changing answer to A as this is a new question referring to Dataproc Serverless. Dataproc Serverless for Spark batch workloads supports Spark SQL. Why modify ETL to ELT and convert PySpark to BigQuery SQL when it can be similar to a lift-and-shift?","comments":[{"poster":"Atnafu","upvote_count":"3","content":"Dataproc is diffrent than Dataproc Serveless. This question is talking about dataproc.\nBy the way dp serverless support both pyspark and sparkSql no need of conversion. \nC is best answer","timestamp":"1684858140.0","comment_id":"725321"}],"comment_id":"657753","upvote_count":"3","poster":"AWSandeep","timestamp":"1677791700.0"}]}],"answer_description":"","unix_timestamp":1662143400,"answers_community":["C (86%)","14%"],"topic":"1","choices":{"B":"Ingest your data into Cloud SQL, convert your PySpark commands into SparkSQL queries to transform the data, and then use federated quenes from BigQuery for machine learning.","A":"Convert your PySpark commands into SparkSQL queries to transform the data, and then run your pipeline on Dataproc to write the data into BigQuery.","D":"Use Apache Beam Python SDK to build the transformation pipelines, and write the data into BigQuery.","C":"Ingest your data into BigQuery from Cloud Storage, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table."},"answer_images":[],"answer":"C"},{"id":"Vn2D22OZXwrOelclw0L2","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/79547-exam-professional-data-engineer-topic-1-question-178/","discussion":[{"comment_id":"679459","upvote_count":"16","timestamp":"1679811000.0","poster":"John_Pongthorn","content":"Selected Answer: D\nD: it is most likely.\nThere are a lot of reference doc to tell about comparison between them\nhttps://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#choose_correctly_between_side_inputs_or_cogroupbykey_for_joins\n\nhttps://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-2\n\nhttps://stackoverflow.com/questions/58080383/sideinput-i-o-kills-performance"},{"upvote_count":"11","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#choose_correctly_between_side_inputs_or_cogroupbykey_for_joins\nThe CoGroupByKey transform is a core Beam transform that merges (flattens) multiple PCollection objects and groups elements that have a common key. Unlike a side input, which makes the entire side input data available to each worker, CoGroupByKey performs a shuffle (grouping) operation to distribute data across workers. CoGroupByKey is therefore ideal when the PCollection objects you want to join are very large and don't fit into worker memory.\n\nUse CoGroupByKey if you need to fetch a large proportion of a PCollection object that significantly exceeds worker memory.","timestamp":"1685361480.0","comment_id":"730476","poster":"zellck"},{"comments":[{"comment_id":"1101940","comments":[{"upvote_count":"1","poster":"MaxNRG","content":"The other options may not be as effective:\n\n• A (Switch to compressed Avro files): While Avro is a good format for certain types of data processing, simply changing the file format from gzip to Avro may not address the underlying issue causing the delay, especially if the problem is related to the way data is being joined or processed.\n\n• B (Reduce the batch size): Reducing the batch size could potentially increase overhead and might not significantly improve the processing time, especially if the bottleneck is due to the method of data joining.\n\n• C (Retry records that throw an error): Retrying errors could be useful in certain contexts, but it's unlikely to speed up the pipeline if the delay is due to inefficiencies in data processing methods like the use of SideInputs.","comment_id":"1101941","timestamp":"1718910180.0"}],"poster":"MaxNRG","upvote_count":"1","content":"Here's why this approach is beneficial:\n\n1. Efficiency in Handling Large Datasets: SideInputs are not optimal for large datasets because they require that the entire dataset be available to each worker. This can lead to performance bottlenecks, especially if the dataset is large. CoGroupByKey, on the other hand, is more efficient for joining large datasets because it groups elements by key and allows the pipeline to process each key-group separately.\n\n2. Scalability: CoGroupByKey is more scalable than SideInputs for large-scale data processing. It distributes the workload more evenly across the Dataflow workers, which can significantly improve the performance of your pipeline.\n\n3. Better Resource Utilization: By using CoGroupByKey, the Dataflow job can make better use of its resources, as it doesn't need to replicate the entire dataset to each worker. This results in faster processing times and better overall efficiency.","timestamp":"1718910120.0"}],"comment_id":"1101938","poster":"MaxNRG","timestamp":"1718910120.0","content":"Selected Answer: D\nTo expedite the Dataflow job that involves ingesting and transforming text files, especially if the pipeline is taking longer than expected, the most effective strategy would be:\n\nD. Use CoGroupByKey instead of the SideInput.","upvote_count":"1"},{"poster":"musumusu","content":"Answer: B, \nreducing the batch size improve the speed performance also improve cpu utilisation. \nDead letter queues are generated for messages that are errorrly acknowledged and its good to use sideinputs for that to check small amount of errors in memory. \nCogroupbykey is not necessary for error messages. \nI see only batch size that can be customized to improve the performance. \nIn practical use case:\nyou check these tools Stackdriver Monitoring and Logging, Cloud Trace, and Cloud Profiler. and try to find the cause, if its file type issue in compression, or batch size.","upvote_count":"1","timestamp":"1692382560.0","comment_id":"813470"},{"poster":"Atnafu","comment_id":"747564","content":"D\nFlatten will just merge all results into a single PCollection. To join them you can use CoGroupByKey","upvote_count":"1","timestamp":"1686939900.0"},{"upvote_count":"2","comments":[{"comment_id":"696716","poster":"devaid","content":"that is for Big Query isn't?","upvote_count":"1","comments":[{"poster":"TNT87","upvote_count":"1","timestamp":"1689072900.0","content":"datflow can use avro format sir. streaming or batching to bigquery in avro format it can","comment_id":"772507"}],"timestamp":"1681692840.0"}],"comment_id":"688345","content":"Selected Answer: A\nWhen optimizing for load speed, Avro file format is preferred. Avro is a binary row-based format which can be split and read in parallel by multiple slots including compressed files.","poster":"TNT87","timestamp":"1680846060.0"},{"upvote_count":"1","timestamp":"1680846000.0","poster":"TNT87","comment_id":"688344","content":"https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-data-ingestion"},{"comment_id":"686993","timestamp":"1680707700.0","upvote_count":"1","content":"Selected Answer: D\nD probably, side inputs have to fit in memory. If the p-collection in the side input doesn't fit well in memory it's better to use CoGroupByKey.","poster":"devaid"},{"timestamp":"1680066840.0","poster":"TNT87","content":"Selected Answer: A\nAnswer A \nthe same question is in number 70 you transform the files to Avro using Dataflow","upvote_count":"1","comment_id":"682352","comments":[{"content":"Avro requires the data to be at least semi-structured, because it wants a fixed schema. Text files are unstructured data, therefore it doesn't make sense to use Avro files for them","comment_id":"931198","poster":"KC_go_reply","timestamp":"1703311380.0","upvote_count":"3"}]},{"content":"A switching to avro. No serialisation","comment_id":"675285","poster":"csd1fggfhfgvh234","upvote_count":"1","timestamp":"1679421420.0"},{"upvote_count":"2","comment_id":"662172","content":"Switch to Avro format \nAnswer A","poster":"TNT87","timestamp":"1678181400.0","comments":[{"timestamp":"1679484900.0","comment_id":"675941","upvote_count":"1","poster":"TNT87","content":"https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-avro.html"}]},{"comment_id":"661056","upvote_count":"3","content":"Selected Answer: D\nD probably, side inputs have to fit in memory. If the p-collection in the side input doesn't fit well in memory it's better to use CoGroupByKey.","poster":"YorelNation","timestamp":"1678103580.0"},{"comment_id":"657748","upvote_count":"4","timestamp":"1677790920.0","content":"Selected Answer: B\nB. Reduce the batch size.","poster":"AWSandeep"}],"answers_community":["D (82%)","Other"],"question_id":88,"isMC":true,"answer_ET":"D","choices":{"C":"Retry records that throw an error.","B":"Reduce the batch size.","A":"Switch to compressed Avro files.","D":"Use CoGroupByKey instead of the SideInput."},"unix_timestamp":1662145320,"question_images":[],"topic":"1","timestamp":"2022-09-02 21:02:00","exam_id":10,"question_text":"You are testing a Dataflow pipeline to ingest and transform text files. The files are compressed gzip, errors are written to a dead-letter queue, and you are using\nSideInputs to join data. You noticed that the pipeline is taking longer to complete than expected; what should you do to expedite the Dataflow job?","answer":"D","answer_description":""},{"id":"mDVAKk9QjVMFFhS0ypeJ","question_images":[],"discussion":[{"content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/dlp/docs/pseudonymization#supported-methods\nFormat preserving encryption: An input value is replaced with a value that has been encrypted using the FPE-FFX encryption algorithm with a cryptographic key, and then prepended with a surrogate annotation, if specified. By design, both the character set and the length of the input value are preserved in the output value. Encrypted values can be re-identified using the original cryptographic key and the entire output value, including surrogate annotation.","upvote_count":"7","poster":"zellck","timestamp":"1669729980.0","comment_id":"730472"},{"poster":"Pime13","content":"Selected Answer: D\nD. Create a pseudonym by replacing PII data with a cryptographic format-preserving token.\n\nA: Storing non-tokenized data in a locked-down button is not a standard practice and might not provide the necessary security.\nB: Redacting all PII data and storing an unredacted version separately can lead to data management complexities and potential security risks.\nC: Scanning every table in BigQuery and masking data post-ingestion is less efficient and might not ensure real-time protection.","comment_id":"1337613","timestamp":"1736264700.0","upvote_count":"1"},{"upvote_count":"1","content":"Selected Answer: D\nWhy other options are not as suitable:\n\nA (Cryptogenic tokens and locked-down bucket): While this provides some protection, storing the non-tokenized data in a separate bucket adds complexity and risk.\nB (Redaction and locked-down bucket): Redaction removes sensitive data entirely, which might limit its usefulness for analysis and other purposes.\nC (Scanning and masking in BigQuery): This approach might be less efficient than masking the data during the streaming process before it reaches BigQuery.","poster":"ToiToi","timestamp":"1730467440.0","comment_id":"1305821"},{"poster":"SamuelTsch","upvote_count":"1","timestamp":"1730108640.0","content":"Selected Answer: D\nI would like to go with D. If the data could be deindentified later by the token, why should we store the data in a locked-down bucket?","comment_id":"1303903"},{"timestamp":"1705620180.0","content":"Selected Answer: D\nD> Looks more suitable as it will handle Referential integrity. https://cloud.google.com/dlp/docs/pseudonymization","poster":"GCP001","upvote_count":"1","comment_id":"1126279"},{"comment_id":"1077806","timestamp":"1700686560.0","poster":"pss111423","content":"answer A\nhttps://cloud.google.com/dlp/docs/transformations-reference Replaces an input value with a token, or surrogate value, of the same length using AES in Synthetic Initialization Vector mode (AES-SIV). This transformation method, unlike format-preserving tokenization, has no limitation on supported string character sets, generates identical tokens for each instance of an identical input value, and uses surrogates to enable re-identification given the original encryption key.","upvote_count":"2"},{"content":"Selected Answer: D\nD is correct.","timestamp":"1691923920.0","upvote_count":"1","comment_id":"979947","poster":"akg001"},{"content":"Selected Answer: B\nI've also asked to GPT but I had to remind the hard condition \"names and emails are often used as join keys\".\nIt changed the answer to \"B\" after 3rd iteration.\n\nmasking all PII data may not satisfy the requirement of using names and emails as join keys, as the data is obfuscated and cannot be used for accurate join operations.\n\nIn this approach, you would redact or remove the sensitive PII data, such as names and emails, from the dataset that will be used for real-time processing and analysis. The redacted data would be stored in the primary dataset to ensure that sensitive information is not accessible.\n\nAdditionally, you would create a copy of the original dataset with the PII data still intact, but this copy would be stored in a locked-down bucket with restricted access. This ensures that authorized individuals who need access to the unredacted data for specific purposes, such as join operations, can retrieve it from the secured location.","upvote_count":"2","poster":"cetanx","comment_id":"931694","comments":[{"timestamp":"1688564040.0","content":"made a typo up there, it has to be A","upvote_count":"2","poster":"cetanx","comment_id":"943786"}],"timestamp":"1687529880.0"},{"poster":"Oleksandr0501","comments":[{"comments":[{"upvote_count":"1","timestamp":"1683726240.0","comment_id":"894020","content":"that`s why i always mark \"gpt\", when copy from there... i know, thx\n\nalso, it might be A. Or D... Confusing question.","poster":"Oleksandr0501"}],"comment_id":"892906","content":"You shouldn't use ChatGPT as a source, the data used are not up to date and for such complex question a predicting text chatbot can help but, it's better to refer to the google documentation.","upvote_count":"5","timestamp":"1683618540.0","poster":"loicrichonnier"}],"upvote_count":"1","content":"gpt: \nThe recommended approach for using the Cloud Data Loss Prevention API (DLP API) to protect sensitive PII data while maintaining referential integrity is to create pseudonyms by replacing the PII data with cryptographic format-preserving tokens.\nThis approach ensures that sensitive data is not accessible by unauthorized individuals, while still preserving the format and length of the original data, which is essential for maintaining referential integrity.\n\nReplacing PII data with cryptogenic tokens, as mentioned in option A, is not recommended because cryptogenic tokens are not necessarily format-preserving, and this could affect the accuracy of data joins.\n\nTherefore, option D is the best approach for using the DLP API to ensure that PII data is not accessible by unauthorized individuals while still maintaining referential integrity.","timestamp":"1682860860.0","comment_id":"885249"},{"poster":"Prudvi3266","upvote_count":"3","content":"Selected Answer: D\nhere catch is \"cryptographic\" key","comment_id":"876522","timestamp":"1682080440.0"},{"upvote_count":"1","poster":"musumusu","comment_id":"813478","content":"Answer D, \nkey word - \"referential integrity\" use format preserve option, it keeps same length of the value and last four digits of your value in column","timestamp":"1676751780.0"},{"content":"Selected Answer: D\nThe answer is D","poster":"tunstila","comment_id":"779938","upvote_count":"1","timestamp":"1674041400.0"},{"comment_id":"759367","poster":"nkit","content":"Selected Answer: D\nI believe \"Format preserving token\" in option D makes it easier choice for me","timestamp":"1672203660.0","upvote_count":"1"},{"content":"Selected Answer: D\nD looks right","timestamp":"1672155300.0","poster":"PrashantGupta1616","upvote_count":"1","comment_id":"758718"},{"comment_id":"751033","poster":"jkhong","content":"Selected Answer: A\nQuestion is super tricky, B and C are not the answers since they do not maintain referential integrity.\n\nFor D, it does preserve the length of input. But since we are only concerned with referencing during joins, there is no point of maintaining the length anyway. Also, characters must be encoded as ASCII, this means that the name and email must be within the 256 character set. which is further limited to the alphabet characters, i.e. 94 characters. (https://cloud.google.com/dlp/docs/transformations-reference#crypto)\n\nNames nowadays do not just have ASCII characters but unicode as well, so D will not necessarily work all the time.","timestamp":"1671547620.0","upvote_count":"2"},{"comments":[{"timestamp":"1679499000.0","content":"\"button\" is just a typo for \"bucket\"","poster":"juliobs","upvote_count":"1","comment_id":"847205"}],"content":"D is the answer\nPseudonymization is a de-identification technique that replaces sensitive data values with cryptographically generated tokens.\n\nKeywords: You want to ensure that the sensitive data is masked but still maintains referential integrity\nPart1- data is masked-Create a pseudonym by replacing PII data with a cryptographic token\nPart 2 still maintains referential integrity- with a cryptographic format-preserving token\nA Not an answer because\nthe locked-down button does not seem to google cloud word","timestamp":"1671222120.0","poster":"Atnafu","comment_id":"747563","upvote_count":"4"},{"comment_id":"722701","timestamp":"1668954840.0","upvote_count":"1","poster":"dish11dish","content":"Selected Answer: D\nThough both option A nad D maintains referential integrity,question is why you wnat to keep untokenize data in GCS,best way is option D which even support Reversible feature which is not supported by option A refer chart in reference document.\n\nreference:-\nhttps://cloud.google.com/dlp/docs/pseudonymization"},{"comment_id":"712678","comments":[{"poster":"cloudmon","content":"And, answer A does not include format preservation, which would lose referential integrity.","upvote_count":"1","comment_id":"712679","timestamp":"1667774460.0","comments":[{"upvote_count":"1","poster":"NicolasN","content":"I think that this isn't true.\nLook at the table https://cloud.google.com/dlp/docs/transformations-reference#transformation_methods and notice the 6th line \"Pseudonymization by replacing input value with cryptographic hash\" (which refers to the case of answer [A]). Referential integrity is preserved.","comment_id":"712732","timestamp":"1667783520.0"}]}],"content":"Selected Answer: D\nIt's D.\n\"You want to ensure that the sensitive data is masked but still maintains referential integrity.\"\nThey don't ask you to also keep the original data (which answer A relates to). \nAlso, format-preservation is important in this case.","upvote_count":"3","timestamp":"1667774400.0","poster":"cloudmon"},{"content":"Selected Answer: A\n[B] and [C] aren't correct since they don't preserve referential integrity.\n[A] describes, in other words, Cryptographic hashing, where the sensitive data is replaced with a hashed value. The hashed value can't be reversed (https://cloud.google.com/dlp/docs/transformations-reference#crypto-hashing) so the phrase \"store the non-tokenized data in a locked-down button (bucket)\" ensures that data can be restored if needed.\n[D] seems to be a valid option too. However, in https://cloud.google.com/dlp/docs/pseudonymization#fpe-ffx, there is a warning: \n\"FPE provides fewer security guarantees compared to other deterministic encryption methods such as AES- SIV ... ... For these reasons, Google strongly recommends using deterministic encryption with AES-SIV instead of FPE for all security sensitive use cases\"\nSince there is no option to select Deterministic Encryption, and the question doesn't require to preserve the format of the data (keep the same length of data), I choose [A] as a more secure approach.","upvote_count":"3","comment_id":"711373","timestamp":"1667596560.0","comments":[{"content":"The table in https://cloud.google.com/dlp/docs/transformations-reference#transformation_methods shows which transformation preserves referential integrity","comment_id":"711376","upvote_count":"1","poster":"NicolasN","timestamp":"1667596680.0"},{"timestamp":"1691505540.0","content":"From here I see if A really meant Cryptographic hashing then it also satisfy referential integrity https://cloud.google.com/dlp/docs/pseudonymization#:~:text=following%20the%20table.-,Deterministic%20encryption%20using%20AES%2DSIV,-Format%20preserving%20encryption\nHowever, I cant see why A means Crpyptographic Hashing, no defition I can find online at all.","upvote_count":"1","poster":"wan2three","comment_id":"975768"}],"poster":"NicolasN"},{"poster":"PoCk3T","content":"All answers are obsolete since https://cloud.google.com/bigquery/docs/column-data-masking-intro","upvote_count":"2","comment_id":"701021","timestamp":"1666365060.0"},{"comment_id":"696723","upvote_count":"3","content":"Selected Answer: D\nAnswer D makes more sense to me. \nSource: https://cloud.google.com/dlp/docs/pseudonymization","poster":"devaid","timestamp":"1665968820.0"},{"poster":"Ray0506","timestamp":"1663259100.0","upvote_count":"1","content":"Selected Answer: D\nD is correct \n\nRefer: https://cloud.google.com/dlp/docs/transformations-reference#fpe","comment_id":"670108"},{"poster":"Thobm","timestamp":"1663004340.0","comment_id":"667289","content":"Selected Answer: D\nFormat-preserving token makes more sense, no bucket needed","upvote_count":"1","comments":[{"poster":"Thobm","comment_id":"667292","content":"Never mind, indeed A for the data that's still in GCS","comments":[{"poster":"learner2610","content":"Why isn't it D ? we have to maintain referential integrity so isnt it D ?","upvote_count":"1","timestamp":"1663068180.0","comment_id":"667932"}],"timestamp":"1663004400.0","upvote_count":"1"}]},{"content":"https://cloud.google.com/dlp/docs/pseudonymization#fpe-ffx","upvote_count":"1","timestamp":"1662183660.0","comment_id":"658105","poster":"ducc"},{"upvote_count":"3","comment_id":"657942","content":"Selected Answer: A\nAWSandeep is correct","poster":"ducc","timestamp":"1662167040.0","comments":[{"content":"can you specify why he is correct? why A and not D?","poster":"Ray0506","timestamp":"1663096380.0","upvote_count":"1","comment_id":"668350"}]},{"comment_id":"657755","upvote_count":"4","poster":"AWSandeep","comments":[{"comments":[{"poster":"Atnafu","content":"Changed my mind - This should be D","timestamp":"1671221760.0","upvote_count":"1","comment_id":"747559"}],"timestamp":"1669418640.0","comment_id":"727135","poster":"Atnafu","upvote_count":"1","content":"A\nreplacing with cryptographic token not with format preserve stuff\nhttps://cloud.google.com/dlp/docs/pseudonymization#:~:text=with%20its%20corresponding%20token"}],"timestamp":"1662146280.0","content":"Selected Answer: A\nA. Create a pseudonym by replacing the PII data with cryptogenic tokens, and store the non-tokenized data in a locked-down button."}],"timestamp":"2022-09-02 21:18:00","choices":{"D":"Create a pseudonym by replacing PII data with a cryptographic format-preserving token.","A":"Create a pseudonym by replacing the PII data with cryptogenic tokens, and store the non-tokenized data in a locked-down button.","C":"Scan every table in BigQuery, and mask the data it finds that has PII.","B":"Redact all PII data, and store a version of the unredacted data in a locked-down bucket."},"unix_timestamp":1662146280,"topic":"1","question_id":89,"answer_description":"","question_text":"You are building a real-time prediction engine that streams files, which may contain PII (personal identifiable information) data, into Cloud Storage and eventually into BigQuery. You want to ensure that the sensitive data is masked but still maintains referential integrity, because names and emails are often used as join keys.\nHow should you use the Cloud Data Loss Prevention API (DLP API) to ensure that the PII data is not accessible by unauthorized individuals?","answer_images":[],"answer":"D","answer_ET":"D","answers_community":["D (66%)","A (29%)","5%"],"isMC":true,"exam_id":10,"url":"https://www.examtopics.com/discussions/google/view/79550-exam-professional-data-engineer-topic-1-question-179/"},{"id":"7DYSkrHQrXsuLAXKaaJ4","url":"https://www.examtopics.com/discussions/google/view/16654-exam-professional-data-engineer-topic-1-question-18/","answer_description":"","unix_timestamp":1584271920,"question_id":90,"question_images":[],"topic":"1","exam_id":10,"answer_images":[],"question_text":"Business owners at your company have given you a database of bank transactions. Each row contains the user ID, transaction type, transaction location, and transaction amount. They ask you to investigate what type of machine learning can be applied to the data. Which three machine learning applications can you use? (Choose three.)","choices":{"D":"Supervised learning to predict the location of a transaction.","F":"Unsupervised learning to predict the location of a transaction.","B":"Unsupervised learning to determine which transactions are most likely to be fraudulent.","E":"Reinforcement learning to predict the location of a transaction.","C":"Clustering to divide the transactions into N categories based on feature similarity.","A":"Supervised learning to determine which transactions are most likely to be fraudulent."},"timestamp":"2020-03-15 12:32:00","isMC":true,"discussion":[{"comments":[{"timestamp":"1630334280.0","comments":[{"content":"yes. multiclass classification model should be applied","poster":"hellofrnds","comment_id":"459440","timestamp":"1633740780.0","upvote_count":"5"}],"poster":"sergio6","upvote_count":"4","comment_id":"435583","content":"D make sense, but i have a doubt: location is a discrete value (no regression), so a multiclass classification model should be applied ... to predict locations?"}],"comment_id":"64232","timestamp":"1584271920.0","content":"BCD makes more sense to me. Its for sure not unsupervised, since locations are in the data already. Reinforcement also doesn't fit, as there no AI and no interactions with data from the observer.","poster":"jvg637","upvote_count":"70"},{"poster":"[Removed]","upvote_count":"8","comment_id":"64714","content":"BCD looks more appropriate","timestamp":"1584363480.0"},{"poster":"Bulleen","timestamp":"1727155800.0","upvote_count":"7","content":"BCD makes sense, but I now agree that BCE is the correct answer. \nSay the model predict a location, guessing US or Sweden are both wrong when the answer is Canada. But US is closer, the distance from the correct location can be used to calculate a reward. Through reinforcement learning (E) the model could guess a location with better accuracy than supervised (D).","comment_id":"351647"},{"upvote_count":"2","poster":"anji007","timestamp":"1727155800.0","comment_id":"462668","content":"Ans: B, C and D\ni) Fraudulent transaction, is nothing but anomaly detection which falls under Unsupervised.\nii) All transactions can be categorized using type etc - clustering algorithm.\niii) Using location as a label, supervised classification can be developed to predict location."},{"timestamp":"1727155740.0","upvote_count":"7","comment_id":"487242","content":"Selected Answer: BCD\nAs wrote by RP123\nB - Not labelled as Fraud or not. So Unsupervised.\nC - Clustering can be done based on location, amount etc.\nD - Location is already given. So labelled. Hence supervised.","poster":"StefanoG"},{"poster":"ler_mp","timestamp":"1727155740.0","content":"Selected Answer: BCD\nBCD makes more sense. B, C should not be controversial. For D vs E, in this use case D fits better than usage of reinfocement learning","comment_id":"754051","upvote_count":"1"},{"upvote_count":"1","poster":"Kyr0","content":"Selected Answer: BCD\nBCD makes more sens to me","timestamp":"1727155740.0","comment_id":"757341"},{"timestamp":"1727155740.0","upvote_count":"5","comment_id":"819124","content":"Anwer: BCD\nThings to understand:\nSupervised learning will only predict the column that is labeled. In this case, there is not Fraud or not Fraud column inside which he will train on. So Option A, wrong. \noption D: Supervised learning for column (transaction location) is possible as column exist to train on. \nOption C: Custering N-type is possible and also an unsupervised learning to make cluster of similar pattern. \nOption B: Its a weaker point here, User should be able to know which clusters are fraud in history. As it doesn't give enough information about past analysis whether user knows potential frauds or not. Ignore this option, if question asked for 2 right options only.","poster":"musumusu"},{"timestamp":"1722362100.0","content":"Selected Answer: ABC\nWhy would you need to predict a location...","poster":"iooj","upvote_count":"2","comment_id":"1258331"},{"poster":"Roulle","timestamp":"1720520160.0","comment_id":"1244854","upvote_count":"1","content":"Selected Answer: ACD\nC and D are good for sureCliquez pour utiliser cette solution et E, F wrong for sure.\n \nThen, to choose between A and B. Both options indicate that we know which transactions are fraudulent and which are not. Indeed, in order to use unsupervised classification to determine the characteristics of fraudulent transactions, we must already know which ones are fraudulent, either because all transactions in the dataset are fraudulent, or because a variable allows us to identify them. If all transactions were fraudulent, this would probably have been specified in the statement. It is therefore more likely that the \"type of transaction\" variable can be used to distinguish fraudulent transactions from others.\n\nIn this case, we have a target variable to predict, enabling us to build interpretable supervised models to understand the typology of fraudulent transactions. I therefore opt for A, C and D"},{"comment_id":"1078284","content":"Options B, E, and F are not as suitable for the given scenario:\n\nB. Unsupervised learning to determine which transactions are most likely to be fraudulent.\n\nUnsupervised learning, while useful for anomaly detection, might not be as effective for fraud detection without labeled data indicating which transactions are fraudulent.\nE. Reinforcement learning to predict the location of a transaction.\n\nReinforcement learning is more suitable for scenarios where an agent learns to make decisions through trial and error, which doesn't seem to align with predicting transaction locations.\nF. Unsupervised learning to predict the location of a transaction.\n\nUnsupervised learning typically doesn't involve predicting specific values (like location) without labeled data for training.\nIn summary, A, C, and D are the most appropriate machine learning applications for investigating the provided bank transactions dataset.","upvote_count":"3","timestamp":"1700731140.0","poster":"TVH_Data_Engineer"},{"content":"Selected Answer: BCD\nAnswer: BCD","upvote_count":"1","timestamp":"1699216620.0","poster":"rocky48","comment_id":"1063261"},{"content":"Location is already given as attribite so what value is served with predicting location?","timestamp":"1694598060.0","poster":"Waqasghaloo","upvote_count":"2","comment_id":"1006426"},{"content":"A, B: Data features without the definition of fraudulent, so we can not obtain the answer even if using the unsupervise learning.\nC: Kmeans solve this.\nD: logistic regression. Just put the location into target.\nE: Give the positive reward when the model predicts correct location.\nF: Same as C. Use all features but locations, and use similarity to predict new data.","timestamp":"1691738400.0","poster":"youare87","comment_id":"978401","upvote_count":"1"},{"poster":"xiaofeng_0226","comment_id":"974452","timestamp":"1691390580.0","content":"Selected Answer: BCD\nAbsolutely","upvote_count":"1"},{"content":"Selected Answer: BCD\nmakes more sense","poster":"Dip1994","upvote_count":"1","timestamp":"1691126160.0","comment_id":"971699"},{"poster":"Mark_86","comment_id":"963488","timestamp":"1690355760.0","upvote_count":"1","content":"Selected Answer: BCD\nBCD make sense and does not require anything that is not given in the question data."},{"content":"should be BCD.\nE doesn't make sense because re-enforcement learning is used only when you want to reach a optimal solution to a problem. Like optimized solution for reaching point A to point B and etc. You don't need re-enforcement learning to predict a location.","poster":"hpvb","upvote_count":"1","comment_id":"932362","timestamp":"1687595760.0"},{"content":"Selected Answer: BCE\nI'd go for BCE instead of BCD, assuming that location is georgraphical location or the geographical location can be found from location using some side input. \nWith so limited features (there is no even transaction date/time given!) and so huge and variant label as location it is impossible to get any convergence in supervised learning(D).\nReinforced lerning(E) with reinforcment inversely proportional to the distance squared between predicted and the real location could get some reasonable results.","timestamp":"1683040560.0","comment_id":"887617","poster":"Jarek7","upvote_count":"2"},{"poster":"budgier","timestamp":"1683009600.0","comment_id":"887080","upvote_count":"1","content":"According to GPT A,B,C","comments":[{"comment_id":"990342","poster":"FP77","timestamp":"1692989700.0","upvote_count":"4","content":"Well, GPT is stupid then"}]},{"comment_id":"842179","poster":"juliobs","upvote_count":"1","content":"Selected Answer: BCD\nBCD. E does not make sense.","timestamp":"1679074980.0"},{"content":"Selected Answer: BCD\nBCD is correct","poster":"bha11111","comment_id":"835667","upvote_count":"1","timestamp":"1678510320.0"},{"timestamp":"1678267260.0","comment_id":"832756","poster":"betterForGo","upvote_count":"1","content":"I would like to choose ABC."},{"poster":"ThorstenStaerk","upvote_count":"4","comments":[{"content":"Exactly, I do not think A or B can be correct here","upvote_count":"2","poster":"chxnge","timestamp":"1675766460.0","comment_id":"800765"}],"content":"How is a machine supposed to know if a transaction is fraudulent if it only has ID, type, location and amount?","comment_id":"780918","timestamp":"1674118260.0"},{"timestamp":"1673070600.0","content":"Selected Answer: ABC\nABC make more sense to me; \nA --> we can use supervise learning to predict fraudulent\nB --> we can use anomaly detection to guess fraudulent\nC --> Clustering can also be used for customer segmentation\n\nWe have no need to predict location of the transation because it is already in the data.","poster":"korntewin","comment_id":"768264","upvote_count":"2"},{"upvote_count":"2","poster":"Chuckq","comment_id":"731389","content":"AB Discarded no data to do fraude detection. Al the others are possible to do, but predict location is clearly a stupid biased idea.\nProbably a bad or tricky designed question.","comments":[{"poster":"Catweazle1983","comment_id":"759775","timestamp":"1672228500.0","content":"I was thinking the same regarding A and B. But probably B is correct because you can do unsupervised anomaly detection. \nhttps://cloud.google.com/blog/products/data-analytics/bigquery-ml-unsupervised-anomaly-detection","upvote_count":"2"}],"timestamp":"1669807320.0"},{"upvote_count":"1","comment_id":"653168","content":"Selected Answer: BCD\nBCD is correct","timestamp":"1661722860.0","poster":"ducc"},{"comment_id":"618306","timestamp":"1655564760.0","upvote_count":"1","content":"Selected Answer: BCD\nBCD makes sense","poster":"SnowLearner"},{"comment_id":"588574","poster":"alecuba16","upvote_count":"2","content":"Selected Answer: BCD\nA, E ,F discarded, BCD are the possible cases","timestamp":"1650452760.0"},{"poster":"sraakesh95","timestamp":"1642101360.0","comment_id":"523028","upvote_count":"1","content":"Selected Answer: BCD\ncomment from @jvg637"},{"poster":"morpho4444","upvote_count":"2","comment_id":"489264","timestamp":"1638115440.0","content":"Selected Answer: BCD\nD are locations that we know, that makes it supervised."},{"content":"Selected Answer: BCD\nBCD makes more sense","timestamp":"1637237460.0","upvote_count":"1","comment_id":"480675","poster":"dsyouness"},{"poster":"ArturVemado","upvote_count":"2","comment_id":"480035","timestamp":"1637154840.0","content":"Selected Answer: BCD\nRL does not make any sense, since it is used for learning to take action according to rewards."},{"upvote_count":"1","content":"A is not an option, you do not have the labels to solve a supervised fraud detection problem. \nE is not an option too, RL use cases are totally different. \nD suites instead of E.","poster":"MaxNRG","comment_id":"475022","timestamp":"1636485240.0"},{"timestamp":"1634869020.0","upvote_count":"1","comment_id":"465950","content":"ACF are the right answers","poster":"Shawvin"},{"upvote_count":"1","comment_id":"426361","content":"BCE is only correct answer","timestamp":"1629207480.0","poster":"sandipk91"},{"poster":"sumanshu","upvote_count":"3","timestamp":"1624646400.0","content":"Vote for BCD","comment_id":"390712"},{"content":"BCD makes more sense","timestamp":"1620108660.0","comment_id":"349174","poster":"anudeepgupta42","upvote_count":"1"},{"content":"BCD is correct","timestamp":"1616560320.0","poster":"yoginisj","comment_id":"318765","upvote_count":"2"},{"poster":"naga","upvote_count":"2","timestamp":"1612630500.0","comment_id":"284974","content":"Correct BCE"},{"content":"Correct BCD : A cant be as fraud is not a label. Not E as Location is existing hence not reinforcemnt. Location is present hence cant be unsupervised","poster":"someshsehgal","timestamp":"1612277640.0","comment_id":"281984","upvote_count":"3"},{"content":"BCF makes more sense. This should be dealt with unsupervised learning.","comment_id":"261945","timestamp":"1610033940.0","upvote_count":"2","poster":"DeepakSai010101"},{"comments":[{"content":"Transaction location is already stored and available in the database. Why should we consider it as unsupervised here?","comment_id":"282574","timestamp":"1612341720.0","poster":"shilpa","upvote_count":"3"},{"content":"That's why he works at Microsoft :-P Unsupervised learning should be to discover things you don't know yet. Location is not one of them, as you have it in the database.","comment_id":"425846","poster":"fire558787","timestamp":"1629124740.0","upvote_count":"4"}],"content":"BCF , I discussed this question with one of the best data scientist working at microsoft, he agreed on BCF.","upvote_count":"2","comment_id":"255367","poster":"apnu","timestamp":"1609302960.0"},{"poster":"zh31427","timestamp":"1609022220.0","comment_id":"252947","content":"Surprisingly - similar question on LinuxAcademy suggests A & F as correct solutions:\n\nUnsupervised learning does not use labels but does look for patterns (or clustering) of data in order to make predictions based on the patterns it learns.\n\nSupervised learning takes labeled training data to predict future results on new test data. Classification models apply categorized variables (i.e. \"fraudulent\", \"not fraudulent\").","upvote_count":"2"},{"poster":"Madd","timestamp":"1608171180.0","content":"It's BCD as it is more appropriate","upvote_count":"2","comment_id":"246120"},{"content":"The correct answer should be - A,C, D, reasons as follows:-\nA is right because - Fraudulent or not Fraudulent is a \"Classification\" type of machine learning. And Classification is part of \"Supervised\" learning. Supervised learning can be of two type - regression or classification. \nC is right because - Clustering is a \"unsupervised learning in which we look for patterns. \nD is right because - predicting a location will be a \"Supervised regression\" type of machine learning.","poster":"Alasmindas","comment_id":"211241","comments":[{"content":"a fraud can be considered as an anomaly so we can use an anomaly detection which is unsupervised learning technique.","comment_id":"243593","timestamp":"1607951820.0","upvote_count":"3","poster":"celineZ"},{"poster":"Toto2020","upvote_count":"6","timestamp":"1608113100.0","comment_id":"245431","content":"A would be fine if we have a column saying if the transaction is fraudulent or not. Without a label column we cannot do Supervised learning, in this case. BCD I think."}],"upvote_count":"1","timestamp":"1604323680.0"},{"upvote_count":"3","comments":[{"poster":"Mushuskath","upvote_count":"3","content":"If there was a column with the information or whether the transaction is fraudulent or not, then what you are saying it would be true. However, We do not have any such column. Therefore, we cannot perform supervised learning for this task. And that’s why we can’t do A and we have to do B","timestamp":"1601962980.0","comment_id":"194013"}],"content":"Why not A? Because the bank has many transactions labeled as normal and fraudulent. I think A is correct.","comment_id":"173660","poster":"AaronLee","timestamp":"1599262260.0"},{"timestamp":"1598268780.0","upvote_count":"2","content":"Can't be E, I don't see how one can define a Reinforcement Learning problem: environment, agent, reward, etc.","poster":"Koja","comment_id":"165132"},{"content":"BCD:\nB, C - I'm working on exactly that project and we use either clustering or autoencoder on an unlabeled data to detect outliers - possible fraud transactions.\nC -","timestamp":"1596280680.0","comments":[{"timestamp":"1596280800.0","upvote_count":"3","poster":"MadHolm","comment_id":"148525","content":"C - we have location data labelled, so we can try predicting location based on other features. We can of course use unsupervised training to do that - but why would you? \nE - reinforced learning doesn't seem to be applicable to this data."}],"comment_id":"148524","upvote_count":"4","poster":"MadHolm"},{"content":"B cannot be correct. With unsupervised learning you can just cluster the data. There is no way you know if these clusters are fraud vs. not fraud not. With A there needs to be additional step of labeling the rows for fraud vs not fraud","timestamp":"1595853720.0","poster":"sh2020","comments":[{"poster":"zonc","upvote_count":"6","timestamp":"1597298820.0","comment_id":"157015","content":"But we may suppose that a fraud is an anomaly so we can use an anomaly detection which is unsupervised learning technique."}],"upvote_count":"3","comment_id":"144965"},{"comment_id":"143064","content":"BCD.\nB - Not labelled as Fraud or not. So Unsupervised.\nC - Clustering can be done based on location, amount etc.\nD - Location is already given. So labelled. Hence supervised.","timestamp":"1595641260.0","upvote_count":"6","poster":"RP123"},{"upvote_count":"2","poster":"Devx198912233","content":"BCF looks correct to me","timestamp":"1594389960.0","comment_id":"131500"},{"comment_id":"121622","content":"BCD is correct","upvote_count":"2","timestamp":"1593318840.0","poster":"PRC"},{"comment_id":"120757","content":"ACF are correct.","poster":"yxyj","upvote_count":"2","timestamp":"1593192720.0"},{"poster":"scn3","upvote_count":"6","timestamp":"1591282020.0","content":"BCD makes more sense to me, but I have several questions as I prepare for this exam. When going through these practice questions there seems to be some disparity between answers marked \"CORRECT\" and the logic/answers provided in the discussions about each question.\n\n1) Has Google verified these are the CORRECT answers or should I put more weight on the discussions?\n1a) Does someone go back through these questions and change them if there is a mistake?\n2) I've read different marks, but does anyone know if you need a 70% or 80% to pass?\n\nI apologize if this is the right place to put this, but couldn't find a general forum for GCP Data Engineering Exam. Thanks!","comment_id":"102447","comments":[{"content":"As per my take - these answers marked as 'correct' are not google verified and don't seem to be correct all the time--i would rather depend on the discussion and make most informed guess based on your own subject expertise.","comment_id":"117240","timestamp":"1592901840.0","upvote_count":"6","poster":"AJKumar"}]}],"answers_community":["BCD (78%)","13%","6%"],"answer":"BCD","answer_ET":"BCD"}],"exam":{"numberOfQuestions":319,"isMCOnly":true,"provider":"Google","lastUpdated":"15 Feb 2025","id":10,"isBeta":false,"isImplemented":true,"name":"Professional Data Engineer"},"currentPage":18},"__N_SSP":true}