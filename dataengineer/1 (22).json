{"pageProps":{"questions":[{"id":"rWc6mKWKowaq72jT767q","answer":"A","unix_timestamp":1662170520,"question_text":"You are using BigQuery and Data Studio to design a customer-facing dashboard that displays large quantities of aggregated data. You expect a high volume of concurrent users. You need to optimize the dashboard to provide quick visualizations with minimal latency. What should you do?","question_images":[],"timestamp":"2022-09-03 04:02:00","answer_ET":"A","answers_community":["A (97%)","3%"],"answer_images":[],"isMC":true,"answer_description":"","discussion":[{"content":"Selected Answer: A\nA. Use BigQuery BI Engine with materialized views.","timestamp":"1677824820.0","upvote_count":"10","comment_id":"658061","poster":"AWSandeep"},{"comment_id":"729315","upvote_count":"7","poster":"zellck","timestamp":"1685280240.0","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/bigquery/docs/materialized-views-intro\nIn BigQuery, materialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency. BigQuery leverages precomputed results from materialized views and whenever possible reads only delta changes from the base tables to compute up-to-date results. Materialized views can be queried directly or can be used by the BigQuery optimizer to process queries to the base tables.\n\nQueries that use materialized views are generally faster and consume fewer resources than queries that retrieve the same data only from the base tables. Materialized views can significantly improve the performance of workloads that have the characteristic of common and repeated queries."},{"content":"Option A is the better one.\n\nBut keep in mind for real life:\nhttps://cloud.google.com/bigquery/docs/bi-engine-preferred-tables\n\nLimitations\nBI Engine preferred tables have the following limitations:\n\nYou cannot add views into the preferred tables reservation list. BI Engine preferred tables only support tables.\nQueries to materialized views are only accelerated if both the materialized views and their base tables are in the preferred tables list.","comment_id":"1155114","upvote_count":"1","timestamp":"1724194620.0","poster":"maic01234"},{"timestamp":"1706049360.0","comments":[{"poster":"sporch08","comment_id":"992844","upvote_count":"1","timestamp":"1709196000.0","content":"If we take minimal latency into consideration, I am not sure a materialized view will be the right answer since the user gets data from the cache but is not up to date."}],"content":"Selected Answer: A\nMaterialized views are precomputed query results that are stored in memory, allowing for faster retrieval of aggregated data. When you create a materialized view in BigQuery, it stores the results of a query as a table, and subsequent queries that can leverage this materialized view can be significantly faster compared to computing them on the fly.","upvote_count":"2","comment_id":"960813","poster":"vamgcp"},{"timestamp":"1702331460.0","poster":"phidelics","upvote_count":"1","comment_id":"920947","content":"Selected Answer: A\nperiodically cache the results for perfomance"},{"upvote_count":"3","timestamp":"1682861100.0","content":"Selected Answer: A\nA.\nhttps://cloud.google.com/bigquery/docs/materialized-views-intro\nIn BigQuery, materialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency","comment_id":"708531","poster":"LPIT"},{"content":"Selected Answer: A\nI vote A\nhttps://cloud.google.com/bigquery/docs/bi-engine-intro#:~:text=Materialized%20views%20%2D%20Materialized%20views%20in%20BigQuery%20perform%20precomputation%2C%20thereby%20reducing%20query%20time.%20You%20should%20create%20materialized%20views%20to%20improve%20performance%20and%20to%20reduce%20processed%20data%20by%20using%20aggregations%2C%20filters%2C%20inner%20joins%2C%20and%20unnests.","poster":"Julionga","upvote_count":"2","comment_id":"681940","timestamp":"1680022740.0"},{"poster":"MounicaN","timestamp":"1678516980.0","comment_id":"665818","content":"Selected Answer: A\nuse materialized views is better option here","upvote_count":"3"},{"comments":[{"content":"Sorry, A is correct\nAs AWSandeep mention","comment_id":"658167","upvote_count":"2","timestamp":"1677835320.0","poster":"ducc"}],"poster":"ducc","content":"Selected Answer: C\nBy integrating BI Engine with BigQuery streaming, you can perform real-time data analysis over streaming data without sacrificing write speeds or data freshness.\n\nhttps://cloud.google.com/bigquery/docs/bi-engine-intro","comment_id":"657975","timestamp":"1677816120.0","upvote_count":"1"}],"url":"https://www.examtopics.com/discussions/google/view/79649-exam-professional-data-engineer-topic-1-question-199/","topic":"1","question_id":111,"choices":{"D":"Use BigQuery BI Engine with authorized views.","C":"Use BigQuery BI Engine with streaming data.","B":"Use BigQuery BI Engine with logical views.","A":"Use BigQuery BI Engine with materialized views."},"exam_id":10},{"id":"3cTXiOCatnOb52900h7v","answer_ET":"B","question_images":[],"answer_description":"","exam_id":10,"question_text":"You are building a model to make clothing recommendations. You know a user's fashion preference is likely to change over time, so you build a data pipeline to stream new data back to the model as it becomes available. How should you use this data to train the model?","url":"https://www.examtopics.com/discussions/google/view/15911-exam-professional-data-engineer-topic-1-question-2/","discussion":[{"comment_id":"98567","poster":"serg3d","upvote_count":"37","comments":[{"upvote_count":"4","timestamp":"1592789520.0","poster":"dambilwa","content":"Yes - The training set should be shuffled well to represent data across all scenarios","comment_id":"115950"}],"content":"I think it should be B because we have to use a combination of old and new test data as well as training data","timestamp":"1590798600.0"},{"poster":"jagadamba","comment_id":"121871","timestamp":"1593349140.0","upvote_count":"11","content":"B, as we need to train the data with new data, so that it will keep learning, and as well as used for test"},{"upvote_count":"1","poster":"cqrm3n","content":"Selected Answer: B\nWe should continuously retrain existing data with latest data to balance the need to adapt to new changes while not overriding historical knowledge. This is called Continuous Retraining where the model is periodically updated with latest data so that recommendations remain accurate over time.","comment_id":"1339948","timestamp":"1736781720.0"},{"upvote_count":"1","comment_id":"1330890","content":"Selected Answer: B\nContinuously retrain the model on a combination of existing data and the new data.","poster":"jaimecalderon","timestamp":"1734974640.0"},{"poster":"SamuelTsch","comment_id":"1300753","timestamp":"1729485300.0","content":"Selected Answer: B\nFrom my point of view, we should take both datasets.","upvote_count":"1"},{"timestamp":"1698956280.0","content":"Selected Answer: B\nOption A is not recommended because retraining the model on just new data will cause the model to lose the information it has learned from the historical data.\n\nOption C and D are not recommended because they are using the new data as test set and this approach will lead to a model that is overfitting and not generalize well to new users.\n\nSo answer is B","comment_id":"1060861","upvote_count":"2","poster":"rocky48"},{"poster":"rajkinz","upvote_count":"2","timestamp":"1698681000.0","comment_id":"1057993","content":"Answer is C. It is time sensitive data so latest data should be used for testing.\nReference: https://cloud.google.com/automl-tables/docs/prepare#ml-use"},{"poster":"rtcpost","upvote_count":"1","content":"Selected Answer: B\nThis approach allows the model to benefit from both the historical data (existing data) and the new data, ensuring that it adapts to changing preferences while retaining knowledge from the past. By combining both types of data, the model can learn to make recommendations that are up-to-date and relevant to users' evolving preferences.","comment_id":"1050460","timestamp":"1697971620.0"},{"content":"Selected Answer: B\ntrain on old and new data","timestamp":"1691491740.0","poster":"Websurfer","comment_id":"975503","upvote_count":"1"},{"poster":"AmmarFasih","upvote_count":"1","comment_id":"904813","timestamp":"1684837560.0","content":"Selected Answer: B\nOption B is the right answer. Since the questions states the models needs to be updated since the clothing preference changes. Hence we need the new data to be utilized for training/ updating model."},{"poster":"bha11111","comment_id":"835650","timestamp":"1678507980.0","upvote_count":"1","content":"Selected Answer: B\nHave verified this"},{"poster":"jin0","comment_id":"816823","content":"there are two point first when retraining second what data. I think retraining should be occur when the model could not predict well in this case there is monitoring metric should be needed first but no one said, second what data? in this case I think the answer is A. because when the model could not predict well it means the data variance and bias are changed so, it's no make sense what is combination new data with old data because the data being not be changed is not necessary anymore..","upvote_count":"1","comments":[{"content":"And the questions should explain in detail.. whether it's deep learning or tree based machine learning model.. and how large of new dataset is.. I think","comment_id":"816839","upvote_count":"1","poster":"jin0","timestamp":"1676996880.0"}],"timestamp":"1676996400.0"},{"content":"Selected Answer: C\nThe trend keep changing, so must mix new and old data...","timestamp":"1676510640.0","poster":"Morock","comment_id":"810159","upvote_count":"1"},{"comments":[{"comment_id":"1060859","poster":"rocky48","timestamp":"1698956160.0","content":"Nice explanation bro.","upvote_count":"1"}],"timestamp":"1674430680.0","poster":"samdhimal","comment_id":"784781","content":"Selected Answer: B\nB. Continuously retrain the model on a combination of existing data and the new data.\n\nThis approach will help to ensure that the model remains up-to-date with the latest fashion preferences of the users, while also leveraging the historical data to provide context and improve the accuracy of the recommendations. Retraining the model on a combination of existing and new data will help to prevent the model from being overly influenced by the new data and losing its ability to generalize to users with different preferences.\n\nOption A is not recommended because retraining the model on just new data will cause the model to lose the information it has learned from the historical data.\n\nOption C and D are not recommended because they are using the new data as test set and this approach will lead to a model that is overfitting and not generalize well to new users.","upvote_count":"5"},{"upvote_count":"1","comment_id":"768226","poster":"korntewin","content":"The answer can be A, if we implement online learning! But for regular model which can't implement online learning (everything with no gradient descent) the answer should be B.","timestamp":"1673064660.0"},{"content":"Correct answer is B","upvote_count":"1","comment_id":"767527","timestamp":"1673001720.0","poster":"testoneAZ"},{"upvote_count":"2","poster":"javibadillo","comment_id":"713642","timestamp":"1667900820.0","content":"Selected Answer: B\nB is the correct answer"},{"upvote_count":"1","content":"B is the correct answer. Model training using existing data + new data will allow model to learn new behaviour of users while retaining some of the older behaviour to make model.","poster":"Maeel92","comment_id":"698621","timestamp":"1666146180.0"},{"timestamp":"1654530720.0","poster":"Hikky_111","content":"B seems like the best option because we need to make the prediction based on the latest preferences by an individual also we need not forget how the user used to prefer before .It's highly unlikely that a person's fashion sense changes a lot.","comment_id":"612425","upvote_count":"1"},{"timestamp":"1654515720.0","comment_id":"612326","poster":"noob_master","content":"Selected Answer: B\n\"You know a user's fashion preference is likely to change over time\" - It is necessary to understand old data and new inputs for a good model - B is the best option","upvote_count":"1"},{"content":"B would be correct ans","comment_id":"610286","upvote_count":"1","poster":"Subhranil1010","timestamp":"1654105980.0"},{"comment_id":"538196","upvote_count":"1","timestamp":"1643748960.0","poster":"nana1995","content":"Selected Answer: B\nCorrect"},{"comment_id":"473986","upvote_count":"4","timestamp":"1636306440.0","comments":[{"timestamp":"1644744060.0","comment_id":"546352","poster":"ML_Novice","upvote_count":"1","content":"it seems like a fair answer, although its says , the user is LIKELY to change preference. Its likely to not a fact when you continously retrain the model on the new data you are assuming that he s completely going to change his clothe preference"},{"poster":"samdhimal","comments":[{"comment_id":"547754","upvote_count":"1","content":"Answer B is fair, because the user fashion preference is likely to change overtime and question says we need to build the data pipeline to stream new data back to the model when it is available. So retraining the model with new and existing data would be fair.","poster":"Palanee","timestamp":"1644929220.0"}],"timestamp":"1642791300.0","content":"Couldn't Agree More. The question seems little vast. It should've specified how a timeframe could play a role as well.","comment_id":"529311","upvote_count":"2"}],"content":"I would go with B (but A is also possible If the market is fast changing)\nhttps://datascience.stackexchange.com/questions/12761/should-a-model-be-re-trained-if-new-observations-are-available:\nSuppose that your model attempts to predict customers' behavior, e.g. how likely is a customer to purchase your product given an offer tailored for him. Clearly, the market changes over time, customers' preferences change, and your competitors adjust. You should adjust as well, so you need to retrain periodically. In such a case I would recommend to add new data, but also omit old data that is not relevant anymore. If the market is fast changing, you should even consider retraining periodically based on new data only.\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/retraining-models-on-new-data.html:\nIt is a good practice to continuously monitor the incoming data and retrain your model on newer data if you find that the data distribution has deviated significantly from the original training data distribution","poster":"MaxNRG"},{"content":"Ans: B","timestamp":"1634214360.0","comment_id":"462011","poster":"anji007","upvote_count":"2"},{"comments":[{"comment_id":"326977","timestamp":"1617394860.0","upvote_count":"3","content":"Amazon Recommend Method - \n\nhttps://docs.aws.amazon.com/machine-learning/latest/dg/retraining-models-on-new-data.html","poster":"sumanshu"}],"content":"Vote for B.\n\nIn Recommendation System - Matrix stored in database - which store the users rating and other details like how much time user spent on that page.We generally recommend on basis of matrix. And In production that matrix updated (or model get retrained once a day or once a week) - Because there could be new users rating. or we can say new data.\n\nSo model retrained fully i.e. on Existing Data + New Data => and generate a new matrix (user/item matrix)","comment_id":"326967","timestamp":"1617394140.0","poster":"sumanshu","upvote_count":"3"},{"upvote_count":"1","timestamp":"1615806300.0","content":"It's B.","comment_id":"311339","poster":"sumaiya96"},{"comment_id":"302834","upvote_count":"2","poster":"Arimaverick","timestamp":"1614800460.0","content":"B seems to be correct. As new data can be with new features. Hence the new data can be split to both training and test data to retrain as well as with existing data."},{"content":"Should be B","timestamp":"1614519960.0","comment_id":"300802","upvote_count":"1","poster":"sid091"},{"upvote_count":"1","content":"It should be D","timestamp":"1612625580.0","poster":"naga","comment_id":"284911"},{"content":"answer is D","timestamp":"1603610520.0","poster":"Athanasia","comment_id":"205490","upvote_count":"2"},{"upvote_count":"3","comment_id":"196619","timestamp":"1602234540.0","poster":"Jai_Deep_Learning","content":"The question says that you know the user preferences keeps changing so new data should also be used. Using existing data is also necessary to understand past behavior. So option B."},{"content":"Quoted from that article because it says to build the PiPeLine.\n\nProducing evaluation metric values using the trained model on a test dataset to assess the model's predictive quality.\nComparing the evaluation metric values produced by your newly trained model to the current model, for example, production model, baseline model, or other business-requirement models. You make sure that the new model produces better performance than the current model before promoting it to production.\n\nhttps://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning\n\nTherefore, B is the correct answer.","comments":[{"upvote_count":"2","poster":"jvg637","comment_id":"193711","timestamp":"1601921460.0","content":"It can be A: Frequently retrain your production models: To capture the evolving and emerging patterns, you need to retrain your model with the most recent data. For example, if your app recommends fashion products using ML, its recommendations should adapt to the latest trends and products."},{"comment_id":"189296","upvote_count":"6","timestamp":"1601328960.0","content":"This article seems to support answer B. https://medium.com/codait/keeping-your-machine-learning-models-up-to-date-f1ead546591b. Here's a quote from this article: \"If you see the accuracy of your model degrading over time, use the new data, or a combination of the new data and old training data to build and deploy a new model.\"","poster":"GregDT"}],"timestamp":"1601114100.0","comment_id":"187581","poster":"kino2020","upvote_count":"7"},{"poster":"Ravivarma4786","upvote_count":"1","comment_id":"160223","timestamp":"1597684740.0","content":"It should be D"},{"poster":"PRABHUKKARTHI","upvote_count":"1","comment_id":"159783","timestamp":"1597654380.0","content":"Option B: We cannot keep the old data for training, new data for testing & vice versa. Both the data required to train to generalize the model for both the set of data."},{"comment_id":"150636","timestamp":"1596563520.0","content":"B ,as until data is not too old, you need to incorporate along with new data.","poster":"Archy","upvote_count":"2"},{"comment_id":"149744","content":"Should be D","poster":"lokeee","timestamp":"1596456840.0","upvote_count":"1"},{"timestamp":"1596121260.0","poster":"Archy","upvote_count":"6","comment_id":"147489","content":"B, continuous training is needed to keep your model upto date"},{"content":"Should be C\nThe google document says for splitting time sensitive data the most recent data being used as the test set.\nhttps://cloud.google.com/automl-tables/docs/prepare","upvote_count":"1","comment_id":"127378","timestamp":"1594002840.0","poster":"Unmesh93","comments":[{"content":"I thought also C but that's not an answer b/c training and testing is not true with this statement \"You know a user's fashion preference is likely to change over time\". If it changes all tests in the trained data will fail. it could be B, b/c you cannot/can depend on either of training data only for testing","poster":"atnafu2020","upvote_count":"2","timestamp":"1597852860.0","comment_id":"161631"}]},{"poster":"jeckie2877","upvote_count":"2","timestamp":"1589307240.0","comment_id":"87880","content":"Could be A, but you don't guarantee that will be enough: in practice is B, since you use new and old data( up to some extent) to train your model. So it should be B"},{"poster":"anton_royce","content":"Answer D","upvote_count":"2","comment_id":"73193","timestamp":"1586589900.0"},{"timestamp":"1586410380.0","upvote_count":"2","content":"should be D","poster":"Nidie","comment_id":"72548"},{"content":"it shud be A","comment_id":"61002","upvote_count":"1","timestamp":"1583743200.0","poster":"RP123"}],"answer_images":[],"answer":"B","question_id":112,"answers_community":["B (95%)","5%"],"unix_timestamp":1583743200,"choices":{"D":"Train on the new data while using the existing data as your test set.","C":"Train on the existing data while using the new data as your test set.","A":"Continuously retrain the model on just the new data.","B":"Continuously retrain the model on a combination of existing data and the new data."},"timestamp":"2020-03-09 09:40:00","topic":"1","isMC":true},{"id":"8moWq3y62ahVhgwbW9CB","exam_id":10,"url":"https://www.examtopics.com/discussions/google/view/16282-exam-professional-data-engineer-topic-1-question-20/","question_id":113,"topic":"1","answer_images":[],"discussion":[{"poster":"jvg637","content":"The Answer should be D. The custom endpoint is not acknowledging the message, that is the reason for Pub/Sub to send the message again and again. Not B.","comment_id":"62584","upvote_count":"88","timestamp":"1583948220.0"},{"upvote_count":"10","comment_id":"71346","content":"D : Doubt should be only between B & D. But B is not possible because if SSL is expired then endpoint URL will not receive any messages forget about duplicates. So It should be D for Duplicates.","timestamp":"1586072640.0","poster":"MauryaSushil"},{"timestamp":"1727155920.0","poster":"atnafu2020","comment_id":"161802","upvote_count":"5","content":"D\nWhy are there too many duplicate messages?\n\nPub/Sub guarantees at-least-once message delivery, which means that occasional duplicates are to be expected. However, a high rate of duplicates may indicate that the client is not acknowledging messages within the configured ack_deadline_seconds, and Pub/Sub is retrying the message delivery. This can be observed in the monitoring metrics pubsub.googleapis.com/subscription/pull_ack_message_operation_count for pull subscriptions, and pubsub.googleapis.com/subscription/push_request_count for push subscriptions. Look for elevated expired or webhook_timeout values in the /response_code. This is particularly likely if there are many small messages, since Pub/Sub may batch messages internally and a partially acknowledged batch will be fully redelivered."},{"content":"D is correct\nAs per google docs- When you do not acknowledge a message before its acknowledgement deadline has expired, Pub/Sub resends the message. As a result, Pub/Sub can send duplicate messages. Use Google Cloud's operations suite to monitor acknowledge operations with the expired response code to detect this condition","timestamp":"1727155920.0","upvote_count":"3","comment_id":"166842","poster":"ganesh2121"},{"poster":"Radhika7983","timestamp":"1727155920.0","upvote_count":"6","comment_id":"218380","content":"The correct answer is D. Look for the link\nhttps://cloud.google.com/pubsub/docs/faq\n\nWhy are there too many duplicate messages?\nPub/Sub guarantees at-least-once message delivery, which means that occasional duplicates are to be expected. However, a high rate of duplicates may indicate that the client is not acknowledging messages within the configured ack_deadline_seconds, and Pub/Sub is retrying the message delivery. This can be observed in the monitoring metrics pubsub.googleapis.com/subscription/pull_ack_message_operation_count for pull subscriptions, and pubsub.googleapis.com/subscription/push_request_count for push subscriptions. Look for elevated expired or webhook_timeout values in the /response_code. This is particularly likely if there are many small messages, since Pub/Sub may batch messages internally and a partially acknowledged batch will be fully redelivered.\n\nAnother possibility is that the subscriber is not acknowledging some messages because the code path processing those specific messages fails, and the Acknowledge call is never made; or the push endpoint never responds or responds with an error."},{"timestamp":"1727155920.0","comment_id":"475028","content":"D as the Cloud Pub/Sub will deliver duplicate messages only if there has been no acknowledgement from the subscriber.\nRefer GCP documentation - Cloud Pub/Sub FAQs - Duplicates:\nhttps://cloud.google.com/pubsub/docs/faq#duplicates\nWhy are there too many duplicate messages?\nCloud Pub/Sub guarantees at-least-once message delivery, which means that occasional duplicates are to be expected. However, a high rate of duplicates may indicate that the client is not acknowledging messages within the configured ack_deadline_seconds, and Cloud Pub/Sub is retrying the message delivery.","poster":"MaxNRG","upvote_count":"2"},{"comment_id":"665256","poster":"crismo04","timestamp":"1727155860.0","upvote_count":"1","content":"Selected Answer: D\nIf the answer had been B the messages had not reached the HTTP page since the push subscription requires an SSL certificate (https://cloud.google.com/pubsub/docs/push#:~:text=Endpoint%20URL%20(required)), so the answer must be D.\n\nI think it is due to a bad response due to overcharge in the HTTPS page, which is sending a non correct status code (https://cloud.google.com/pubsub/docs/push#:~:text=VPC%20Service%20Controls.-,Receive%20messages,-When%20Pub/Sub)"},{"comment_id":"1050521","poster":"rtcpost","content":"Selected Answer: D\nIn Google Cloud Pub/Sub, when you use a push subscription, messages are delivered to the specified endpoint (in this case, your custom HTTPS endpoint). The acknowledgment deadline is the time given to the endpoint to acknowledge that it has received and processed the message. If the acknowledgment is not received within the deadline, Pub/Sub may consider the message as unacknowledged and may attempt redelivery, which can lead to duplicate messages.\n\nYou should ensure that your custom HTTPS endpoint acknowledges messages within the acknowledgment deadline to prevent duplicate messages from being sent. Additionally, it's essential to handle messages in an idempotent way, so even if duplicates do occur, the action taken by your endpoint doesn't have unintended consequences.","upvote_count":"2","timestamp":"1727155860.0"},{"poster":"petergjohnson","upvote_count":"1","timestamp":"1727155860.0","content":"Selected Answer: B\nAfter re-reading the question it seem to me that it is asking for a root cause. It is possible that the most common cause of this symptom is and expired certificate. Once expired duplicates would be received for every message.","comment_id":"1238391"},{"poster":"VictorBa","upvote_count":"1","comment_id":"1217189","content":"Selected Answer: D\nAgree with previous explanations regarding validity of D","timestamp":"1716518340.0"},{"poster":"searching4alicense","comment_id":"1163342","timestamp":"1709283900.0","content":"D - If a message has not been acknowledged within its acknowledgement deadline, Dataflow attempts to maintain the lease on the message by repeatedly extending the acknowledgement deadline to prevent redelivery from Pub/Sub. However this is best effort and there is a possibility that messages may be redelivered. This can be monitored using metrics listed here. https://cloud.google.com/blog/products/data-analytics/handling-duplicate-data-in-streaming-pipeline-using-pubsub-dataflow","upvote_count":"1"},{"poster":"philli1011","content":"D should be the answer. If acknowlegement is not received back to pub/sub , pub/sub may resend meassages.","comment_id":"1131703","timestamp":"1706189940.0","upvote_count":"1"},{"content":"The correct answer is:\nD. Your custom endpoint is not acknowledging messages within the acknowledgement deadline.","comment_id":"1027041","poster":"imran79","upvote_count":"2","timestamp":"1696648080.0"},{"timestamp":"1696351260.0","upvote_count":"1","comment_id":"1024080","content":"if there were an out of date certificate then nothing would get through. D","poster":"emmylou"},{"upvote_count":"3","timestamp":"1690532340.0","poster":"FP77","comment_id":"965380","content":"Selected Answer: D\nIt should be D\nhttps://cloud.google.com/pubsub/docs/troubleshooting#dupes"},{"upvote_count":"3","comment_id":"919240","poster":"dgteixeira","content":"Selected Answer: D\nThe correct answer is D, because it's how Pub/Sub works.\nDocumentation here: https://cloud.google.com/pubsub/docs/troubleshooting#dupes","timestamp":"1686310560.0"},{"comment_id":"886409","upvote_count":"2","poster":"boca_2022","content":"Selected Answer: D\nD for sure","timestamp":"1682953020.0"},{"upvote_count":"2","timestamp":"1679079240.0","comment_id":"842224","content":"Selected Answer: D\nD for sure","poster":"juliobs"},{"upvote_count":"1","poster":"bha11111","comment_id":"835669","content":"Selected Answer: D\nd is correct","timestamp":"1678510800.0"},{"timestamp":"1677133800.0","poster":"niketd","upvote_count":"2","content":"Selected Answer: D\nNo acknowledgment -> Answer B. Moderators please update your answer","comment_id":"818864"},{"comment_id":"771387","content":"D is the correct answer.","upvote_count":"2","poster":"GCPpro","timestamp":"1673353080.0"},{"timestamp":"1673107680.0","content":"D. Your custom endpoint is not acknowledging messages within the acknowledgement deadline.","poster":"AzureDP900","comment_id":"768710","upvote_count":"1"},{"timestamp":"1671587280.0","upvote_count":"1","comment_id":"751756","poster":"connorscion","content":"D, with a push delivery method if not acknowledge the sensor will keep sending the message."},{"poster":"DGames","upvote_count":"1","timestamp":"1670886060.0","content":"Selected Answer: D\nwithout ack message deliver multiple time.","comment_id":"743399"},{"timestamp":"1668519060.0","comment_id":"718749","upvote_count":"1","content":"Selected Answer: D\nWhen you do not acknowledge a message before its acknowledgement deadline has expired, Pub/Sub resends the message. As a result, Pub/Sub can send duplicate messages.","poster":"assU2"},{"content":"wy the B answer is marked as correct? it clear its some Ack problem the message goes back to the queu","upvote_count":"1","comment_id":"715430","poster":"Jasar","timestamp":"1668101280.0"},{"poster":"t11","content":"It has to be D. \n\nWith an out of date SSL cert, no message should be pushed to the HTTPS endpoint.","upvote_count":"1","timestamp":"1660791240.0","comment_id":"648259"},{"timestamp":"1656566700.0","content":"Selected Answer: D\nA message is resent if subscriber takes more than ackDeadline to respond.","upvote_count":"1","comment_id":"625037","poster":"KundanK973"},{"comment_id":"614570","comments":[{"comments":[{"timestamp":"1694873820.0","comment_id":"1009154","poster":"Vijayandra","content":"If* a negative","upvote_count":"1"}],"poster":"Vijayandra","content":"Push also need acknowledgment. Is a negative acknowledgement is received or the acknowledgment deadline expires, Pub/Sub resends the message. This can create duplication. https://cloud.google.com/pubsub/docs/push","upvote_count":"1","timestamp":"1694873760.0","comment_id":"1009153"}],"upvote_count":"5","content":"Selected Answer: B\nDefinitely not D\n\nThis is push, and acknowledgment is via HTTP response code, which is messed up by wrong certs\n\nNon http code ACK by client is only for pull, which would be B\n\nBut it is push","poster":"rr4444","timestamp":"1654868700.0"},{"timestamp":"1650452880.0","poster":"alecuba16","upvote_count":"2","content":"Selected Answer: D\nhttps://cloud.google.com/pubsub/docs/troubleshooting#dupes","comment_id":"588576"},{"timestamp":"1649190780.0","poster":"mallain","upvote_count":"1","comment_id":"581454","content":"Selected Answer: D\nhttps://cloud.google.com/pubsub/docs/troubleshooting#dupes"},{"content":"https://cloud.google.com/blog/products/data-analytics/handling-duplicate-data-in-streaming-pipeline-using-pubsub-dataflow\n\nPub/Sub guarantees at least once delivery for every subscription. This means that a message may be delivered more than once by the same subscription if Pub/Sub doesn‚Äôt receive acknowledgement within the acknowledgement deadline. The subscriber may acknowledge after the acknowledgement deadline or the acknowledgement may be lost due to transient network issues. In such scenarios the same message would be redelivered and subscribers may see duplicate data. It is the responsibility of the subscribing system (for example Dataflow) to detect such duplicates and handle accordingly.","timestamp":"1649151060.0","comment_id":"581143","poster":"jagan79","upvote_count":"2"},{"content":"Selected Answer: D\nThe Answer should be D. The custom endpoint is not acknowledging the message, that is the reason for Pub/Sub to send the message again and again. Not B.","timestamp":"1648233480.0","comment_id":"575221","poster":"910","upvote_count":"2"},{"poster":"samdhimal","comment_id":"530089","content":"correct answer -> Your custom endpoint is not acknowledging messages within the acknowledgement deadline.\n\nWhen you do not acknowledge a message before its acknowledgement deadline has expired, Pub/Sub resends the message. As a result, Pub/Sub can send duplicate messages.\n\nReference:\nhttps://cloud.google.com/pubsub/docs/troubleshooting#dupes","upvote_count":"2","timestamp":"1642888200.0"},{"upvote_count":"1","content":"Selected Answer: D\nAnswer is D","comment_id":"526075","poster":"Nico1310","timestamp":"1642450620.0"},{"content":"Selected Answer: D\n@jvg637","upvote_count":"1","poster":"sraakesh95","timestamp":"1642101840.0","comment_id":"523035"},{"poster":"mananksh","timestamp":"1641810840.0","content":"Selected Answer: D\nThe answer is D","comment_id":"520789","upvote_count":"1"},{"comments":[{"upvote_count":"1","timestamp":"1640598060.0","content":"Answer is D","comment_id":"510197","poster":"prasanna77"}],"poster":"prasanna77","content":"if SSL certificate faily,you can no longer connect to HTTPS (secure network)","comment_id":"510196","upvote_count":"3","timestamp":"1640598060.0"},{"content":"Selected Answer: D\nNot acknowledging a message makes Pub/Sub to think it has not been received, so it sends duplicate messages.","timestamp":"1639711980.0","poster":"hendrixlives","comment_id":"503332","upvote_count":"2"},{"content":"Selected Answer: D\nWhen you do not acknowledge a message before its acknowledgement deadline has expired, Pub/Sub resends the message. As a result, Pub/Sub can send duplicate messages.","timestamp":"1637921100.0","poster":"StefanoG","comment_id":"487243","upvote_count":"5"},{"timestamp":"1637563260.0","comment_id":"483911","poster":"bunty2021","upvote_count":"3","content":"Selected Answer: D\nIt should be D"},{"poster":"Track22","upvote_count":"4","comment_id":"469158","content":"Answer should be D , B is not correct becuase\n A managed certificate for HTTPS connections is automatically issued and renewed when you map a service to a custom domain.\nhttps://cloud.google.com/run/docs/mapping-custom-domains\nwe could not use own cetificat so we dont use any date","timestamp":"1635414120.0"},{"timestamp":"1634311260.0","content":"Ans D:\nIf a message is not acknowledged with in ackDeadline, then the message will be replayed hence causing duplicate messages.","comment_id":"462673","poster":"anji007","upvote_count":"3"},{"upvote_count":"1","timestamp":"1632977040.0","content":"B\nThe problem is inordinate amount of duplicate messages. \nIn the case of D, custom endpoint is not acknowledging messages within the acknowledgement deadline, so Pub/Sub can send duplicate messages, but, it may be solved by sending an ack again.\nHowever, in the case of B, the 417 HTTP STATUS Code will be sent because the ssl certificate has expired.\nIf you send a negative acknowledgement or the acknowledgement deadline expires, Pub/Sub resends the message.","poster":"iwanttogohome","comment_id":"454692"},{"comment_id":"447026","timestamp":"1631959620.0","upvote_count":"3","content":"Answer is D.","poster":"ramakin"},{"comment_id":"441125","content":"D. The custom endpoint is not Acknowledging .","timestamp":"1631052900.0","poster":"Nittin","upvote_count":"3"},{"upvote_count":"3","comment_id":"426362","poster":"sandipk91","timestamp":"1629207540.0","content":"D is the correct answer"},{"content":"D: \"When you do not acknowledge a message before its acknowledgement deadline has expired, Pub/Sub resends the message. As a result, Pub/Sub can send duplicate messages.\"","timestamp":"1628787900.0","comment_id":"423768","upvote_count":"5","poster":"CuchoLeo"},{"timestamp":"1624646880.0","upvote_count":"4","poster":"sumanshu","comment_id":"390720","content":"Vote for 'D\""},{"upvote_count":"3","comment_id":"319253","poster":"JustHuman","timestamp":"1616599200.0","content":"D for sure"},{"content":"D:\nif SSL certificate has expired, then endpoint will not receive the payload, there is no way of knowing that message was received earlier or not.\nhttps://cloud.google.com/pubsub/docs/faq#duplicates","upvote_count":"5","poster":"daghayeghi","comment_id":"305993","timestamp":"1615240200.0"},{"comment_id":"284986","content":"correct B","timestamp":"1612631160.0","poster":"naga","upvote_count":"2"},{"poster":"muraric","comment_id":"256512","content":"I will go for D.\nhttps://cloud.google.com/pubsub/docs/push\n\" If you send a negative acknowledgement or the acknowledgement deadline expires, Pub/Sub resends the message. \"","timestamp":"1609452780.0","upvote_count":"4"},{"content":"D - \nif SSL certificate has expired, then endpoint will not receive the payload, there is no way of knowing that message was received earlier or not.","upvote_count":"2","poster":"DeepakKhattar","comment_id":"213964","timestamp":"1604654220.0"},{"timestamp":"1603094460.0","poster":"KSSaurabh","upvote_count":"3","comment_id":"202437","content":"Pub/Sub guarantees at-least-once message delivery, which means that occasional duplicates are to be expected. However, a high rate of duplicates may indicate that the client is not acknowledging messages within the configured ack_deadline_seconds, and Pub/Sub is retrying the message delivery. This can be observed in the monitoring metrics pubsub.googleapis.com/subscription/pull_ack_message_operation_count for pull subscriptions, and pubsub.googleapis.com/subscription/push_request_count for push subscriptions. Look for elevated expired or webhook_timeout values in the /response_code. This is particularly likely if there are many small messages, since Pub/Sub may batch messages internally and a partially acknowledged batch will be fully redelivered.\n\nAnother possibility is that the subscriber is not acknowledging some messages because the code path processing those specific messages fails, and the Acknowledge call is never made; or the push endpoint never responds or responds with an error."},{"comment_id":"180795","poster":"SteelWarrior","content":"D. As pub/sub will resend the message in case not acknowledged by the client.","upvote_count":"2","timestamp":"1600333920.0"},{"content":"D \nLots duplicate messages may happen when:\nendpoint is not acknowledging messages\nwithin the acknowledgement deadline","comment_id":"180090","upvote_count":"2","poster":"VIncent9261111","timestamp":"1600217880.0"},{"timestamp":"1599471180.0","upvote_count":"1","content":"If a subscription uses push delivery, the Pub/Sub service delivers messages to a push endpoint. The push endpoint must be a publicly accessible HTTPS address. The server for the push endpoint must have a valid SSL certificate signed by a certificate authority.","comment_id":"175084","poster":"vickutk"},{"poster":"saurabh1805","upvote_count":"3","content":"D is correct answer here. Refer below link\n\nhttps://cloud.google.com/pubsub/docs/troubleshooting#dupes","comment_id":"160251","timestamp":"1597687200.0"},{"comment_id":"160018","timestamp":"1597668840.0","content":"Option D: Author can please correct this ?","upvote_count":"3","poster":"PRABHUKKARTHI"},{"content":"Answer: D","timestamp":"1595251500.0","poster":"enq","upvote_count":"1","comment_id":"139551"},{"content":"Correct Ans : D\nThis option is correct as the Cloud Pub/Sub will deliver\nduplicate messages only if there has been no acknowledgement from the\nsubscriber.","upvote_count":"2","poster":"VishalB","comment_id":"138655","timestamp":"1595160780.0"},{"upvote_count":"1","content":"B\nYour push endpoint needs to handle incoming messages and return an HTTP status code to indicate success or failure. A success response is equivalent to acknowledging messages. The status codes interpreted as message acknowledgements by the Cloud Pub/Sub system are 200, 201, 202, 204, or 102.\nIn pull sub When you do not acknowledge a message before its acknowledgement deadline has expired, Pub/Sub resends the message. As a result, Pub/Sub can send duplicate messages.","comment_id":"133038","comments":[{"comments":[{"comment_id":"173424","timestamp":"1599225120.0","poster":"vickutk","upvote_count":"1","content":"https://cloud.google.com/pubsub/docs/push"}],"upvote_count":"1","poster":"atnafu2020","comment_id":"161805","content":"Hi, where did you get this about http status code specifically with Cloud pub/sub? If you share ref material that would be appropriated","timestamp":"1597873080.0"}],"poster":"Vipinyadav03","timestamp":"1594569240.0"},{"poster":"Rajuuu","content":"SSL has nothing to do with duplicates.Correct answer is D.","upvote_count":"5","timestamp":"1593836760.0","comment_id":"125947"},{"comment_id":"73858","timestamp":"1586733540.0","poster":"arnabbis4u","upvote_count":"2","content":"It should be D"},{"timestamp":"1584428280.0","comment_id":"65056","poster":"Snobid","content":"Answer should be B\nDue to the question mentioning inordinate (unusually or disproportionately large; excessive) amount of duplicate messages, it is most likely due to a system fault. B is indicative of a system fault since a valid SSL certificate is required (see https://cloud.google.com/pubsub/docs/push).\nD is indicative of the application not responding within the required time allocated. Assuming that the application is working normally (since it is not mentioned otherwise in the question), creating an inordinate amount of is more unlikely to happen","upvote_count":"4"}],"choices":{"A":"The message body for the sensor event is too large.","B":"Your custom endpoint has an out-of-date SSL certificate.","D":"Your custom endpoint is not acknowledging messages within the acknowledgement deadline.","C":"The Cloud Pub/Sub topic has too many messages published to it."},"answer_ET":"D","timestamp":"2020-03-11 18:37:00","isMC":true,"answers_community":["D (86%)","14%"],"answer_description":"","answer":"D","question_images":[],"unix_timestamp":1583948220,"question_text":"You work for a car manufacturer and have set up a data pipeline using Google Cloud Pub/Sub to capture anomalous sensor events. You are using a push subscription in Cloud Pub/Sub that calls a custom HTTPS endpoint that you have created to take action of these anomalous events as they occur. Your custom\nHTTPS endpoint keeps getting an inordinate amount of duplicate messages. What is the most likely cause of these duplicate messages?"},{"id":"6FmdWDB3sXsloS5AOaqR","exam_id":10,"url":"https://www.examtopics.com/discussions/google/view/79650-exam-professional-data-engineer-topic-1-question-200/","unix_timestamp":1662170640,"topic":"1","answer":"D","question_id":114,"question_images":[],"answer_images":[],"answer_ET":"D","discussion":[{"upvote_count":"18","poster":"NicolasN","comments":[{"poster":"MaxNRG","timestamp":"1718993220.0","upvote_count":"3","content":"A single shared service account or granting every employee direct access violates security best practices, so not [A].","comment_id":"1102929"},{"comment_id":"931399","content":"Rejecting C + D solely based on Cloud Storage, which CAN be used in this scenario, is not sound reasoning.","upvote_count":"5","poster":"KC_go_reply","timestamp":"1703329800.0"}],"content":"Selected Answer: A\n‚úÖ[A] is the only acceptable answer.\n‚ùå[B] rejected (no need to elaborate)\n‚ùå[C] and [D] rejected. Why should we be obliged to use Cloud Storage? Other storage options in Google Cloud aren't compliant with \"major data protection standards\"?\n=============================================\n‚ùó[D] has another rejection reason, the following quotes:\nüî∏From <https://cloud.google.com/iam/docs/service-accounts>: \"You can add service accounts to a Google group, then grant roles to the group. However, adding service accounts to groups is not a best practice. Service accounts are used by applications, and each application is likely to have its own access requirements\"\nüî∏From <https://cloud.google.com/iam/docs/best-practices-service-accounts#groups>: \"Avoid using groups for granting service accounts access to resources\"","comment_id":"735753","timestamp":"1685945280.0"},{"timestamp":"1690110480.0","upvote_count":"14","comment_id":"785364","content":"Selected Answer: D\nfor A: please refer to this link below which suggests \"Sharing a single service account across multiple applications can complicate the management of the service account\" - meaning it's not a best practice.\nhttps://cloud.google.com/iam/docs/best-practices-service-accounts#single-purpose \nAlso, what if we have hundreds of users, does it really make sense to manage each user's IAM individually?\n\n\nfor D: it's indeed not one of the best practices but I believe it's much more managable and better than A","poster":"cetanx"},{"poster":"MaxNRG","content":"Selected Answer: D\nTo align with Google's recommended practices for managing access to personally identifiable information (PII) in compliance with banking industry regulations, let's analyze the options:\n\nA. Assign the required IAM roles to every employee, and create a single service account to access project resources: While assigning specific IAM roles to employees is a good practice for access control, using a single service account for all access to PII is not ideal. Service accounts should be used for applications and automated processes, not as a shared account for multiple users or employees.","upvote_count":"2","timestamp":"1718993340.0","comments":[{"timestamp":"1718993340.0","upvote_count":"1","content":"B. Use one service account to access a Cloud SQL database, and use separate service accounts for each human user: Again, service accounts are intended for automated tasks or applications, not for individual human users. Assigning separate service accounts to each human user is not a recommended practice and does not align with the principle of least privilege.","comment_id":"1102933","poster":"MaxNRG","comments":[{"timestamp":"1718993340.0","comment_id":"1102934","comments":[{"upvote_count":"1","content":"D. Use Cloud Storage to comply with major data protection standards. Use multiple service accounts attached to IAM groups to grant the appropriate access to each group: This approach is more aligned with best practices. Using Cloud Storage can ensure compliance with data protection standards. Creating multiple service accounts, each with specific access controls attached to different IAM groups, allows for more granular and controlled access to PII. This setup adheres to the principle of least privilege, ensuring that each service (or group of services) only has access to the resources necessary for its function.\n\nBased on these considerations, option D is the most appropriate choice. It ensures compliance with data protection standards, uses Cloud Storage for secure data management, and employs multiple service accounts tied to IAM groups for granular access control, aligning well with Google-recommended practices and regulatory requirements in the banking industry.","timestamp":"1718993340.0","poster":"MaxNRG","comment_id":"1102935"}],"upvote_count":"1","poster":"MaxNRG","content":"C. Use Cloud Storage to comply with major data protection standards. Use one service account shared by all users: Using Cloud Storage can indeed help comply with data protection standards, especially when configured correctly with encryption and access controls. However, sharing a single service account among all users is not a best practice. It goes against the principle of least privilege and does not provide adequate granularity for access control."}]}],"comment_id":"1102932"},{"comment_id":"976484","poster":"[Removed]","timestamp":"1707481500.0","upvote_count":"2","content":"Selected Answer: D\nD. Not the best, but seems most reasonable out of 4."},{"poster":"vamgcp","timestamp":"1706049060.0","upvote_count":"2","comment_id":"960809","content":"Selected Answer: D\nOption D - Using multiple service accounts attached to IAM groups helps enforce the principle of least privilege. Each group can be assigned only the necessary permissions, reducing the risk of unauthorized access to sensitive data."},{"timestamp":"1704872340.0","poster":"MoeHaydar","comment_id":"947777","upvote_count":"2","content":"Selected Answer: D\nGoogle Cloud Storage is designed to comply with major data protection standards. Creating multiple service accounts and attaching them to IAM groups provides granular control over who has access to the data. This approach is aligned with the principle of least privilege, a security best practice where a user is given the minimum levels of access necessary to complete their tasks."},{"timestamp":"1703329860.0","poster":"KC_go_reply","upvote_count":"2","content":"Selected Answer: D\nIt¬¥s not A because \n1. assigning IAM roles to single users instead of groups is not Google best practice, and\n2. the question explicitly states that we want to use multiple service accounts.","comment_id":"931400"},{"comments":[{"content":"‚ùå D. Use Cloud Storage to comply with major data protection standards. Use one service account shared by all users.\n\n- Sharing one service account among all users is not a secure practice. It goes against the principle of least privilege and does not allow for granular control over access permissions. If the shared service account were to be compromised, all resources accessible by the account would be at risk.","comment_id":"915571","timestamp":"1701800640.0","upvote_count":"1","poster":"Ender_H"}],"comment_id":"915570","content":"Selected Answer: D\nD. Use Cloud Storage to comply with major data protection standards. Use multiple service accounts attached to IAM groups to grant the appropriate access to each group.\n\n- Google Cloud Storage is built for secure and compliant data storage. It supports compliance with major data protection standards, which is essential in the banking industry where data protection regulations are stringent.\n- Service accounts in Google Cloud represent non-human users (applications or services) that need to authenticate and be authorized to access specific Google Cloud resources.\n- Creating multiple service accounts attached to IAM groups allows you to manage access control in a granular manner. This follows the principle of least privilege, providing each group with only the permissions they need to perform their tasks, which is a recommended practice for managing access to sensitive data like PII.","poster":"Ender_H","upvote_count":"2","timestamp":"1701800580.0"},{"upvote_count":"6","comments":[{"upvote_count":"1","comment_id":"1083593","content":"I would like to ask your question to those who decide the questions on the exams. I don't understand what they're trying to do, many of the questions cause divided responses, because they don't have a clear answer. The certification process is a waste of time.","timestamp":"1716987900.0","poster":"alfguemat"}],"poster":"juliobs","content":"Why are so many questions like this?\nNone of the answers is best practice.","comment_id":"847542","timestamp":"1695411540.0"},{"poster":"SuperVee","content":"Selected Answer: D\nI could be wrong but I think the wording in D caused this confusion, so it is an English problem. -- \"Use multiple service accounts attached to IAM groups to grant the appropriate access to each group\"\nI believe what D really means is that you can create a group for a bunch of people who only need access to resource A, so attach a Service account to the group and service account only have access to A.\nThen you create another group for another bunch of people who only need access to resource B, so attach a service account to this group. this service account can only access to B.\nSo each group/service account has a very specific access target, and purpose of the group is very narrowly defined which is allowed by best practice. However, wording in option D merged all these into one sentence causing confusions.\nOption A is an administrative nightmare to manage IAM for a larger user population which is actually also against GCP best practices.","comment_id":"838910","timestamp":"1694692680.0","upvote_count":"5"},{"comment_id":"808233","poster":"Aamir185","timestamp":"1691997000.0","content":"Selected Answer: D\nD it is","upvote_count":"2"},{"content":"D is right","comment_id":"763426","upvote_count":"1","timestamp":"1688252880.0","poster":"AzureDP900"},{"poster":"Amar2022","content":"Selected Answer: A\nA is the correct one","upvote_count":"1","timestamp":"1687606200.0","comment_id":"754895"},{"content":"Selected Answer: A\nAgree with NicolasN, D is bad practice. For D this may result in permission creep, where a group is granted access to an increasing number of resources. Only grant service accounts specific access to resources.","timestamp":"1687425180.0","comment_id":"753213","upvote_count":"1","poster":"jkhong"},{"timestamp":"1687365480.0","upvote_count":"1","content":"Selected Answer: A\nA is the answer, as NicalsN says.\nhttps://cloud.google.com/iam/docs/service-accounts#groups","comment_id":"752644","poster":"odacir"},{"timestamp":"1686907140.0","comment_id":"747096","poster":"Andrix2405","comments":[{"timestamp":"1686907200.0","content":"Sorry A","comment_id":"747097","poster":"Andrix2405","upvote_count":"1"}],"content":"Selected Answer: A\nAvoid using groups for granting service accounts access to resources -> D","upvote_count":"2"},{"timestamp":"1685279460.0","comment_id":"729255","content":"Selected Answer: D\nD is the answer.","upvote_count":"1","poster":"zellck"},{"timestamp":"1684706220.0","upvote_count":"1","content":"Why GCS?","poster":"hauhau","comment_id":"723993"},{"content":"Selected Answer: D\nI vote D","comment_id":"667495","timestamp":"1678667280.0","upvote_count":"4","poster":"Wasss123"},{"timestamp":"1677824940.0","upvote_count":"2","poster":"AWSandeep","content":"Selected Answer: D\nD. Use Cloud Storage to comply with major data protection standards. Use multiple service accounts attached to IAM groups to grant the appropriate access to each group.\nReveal Solution","comment_id":"658065"},{"comment_id":"657976","timestamp":"1677816240.0","upvote_count":"2","content":"Selected Answer: D\nD is the best for me","poster":"ducc"}],"question_text":"Government regulations in the banking industry mandate the protection of clients' personally identifiable information (PII). Your company requires PII to be access controlled, encrypted, and compliant with major data protection standards. In addition to using Cloud Data Loss Prevention (Cloud DLP), you want to follow\nGoogle-recommended practices and use service accounts to control access to PII. What should you do?","isMC":true,"answers_community":["D (65%)","A (35%)"],"answer_description":"","choices":{"C":"Use Cloud Storage to comply with major data protection standards. Use one service account shared by all users.","B":"Use one service account to access a Cloud SQL database, and use separate service accounts for each human user.","A":"Assign the required Identity and Access Management (IAM) roles to every employee, and create a single service account to access project resources.","D":"Use Cloud Storage to comply with major data protection standards. Use multiple service accounts attached to IAM groups to grant the appropriate access to each group."},"timestamp":"2022-09-03 04:04:00"},{"id":"7JrTB510DaoGJ82mOGrP","timestamp":"2022-09-03 04:12:00","unix_timestamp":1662171120,"choices":{"C":"Create a Dataflow job to read the Redis database from the on-premises data center and write the data to a Memorystore for Redis instance.","B":"Make a secondary instance of the Redis database on a Compute Engine instance and then perform a live cutover.","D":"Write a shell script to migrate the Redis data and create a new Memorystore for Redis instance.","A":"Make an RDB backup of the Redis database, use the gsutil utility to copy the RDB file into a Cloud Storage bucket, and then import the RDB file into the Memorystore for Redis instance."},"exam_id":10,"answer_ET":"A","topic":"1","question_id":115,"question_images":[],"question_text":"You need to migrate a Redis database from an on-premises data center to a Memorystore for Redis instance. You want to follow Google-recommended practices and perform the migration for minimal cost, time and effort. What should you do?","discussion":[{"timestamp":"1693715520.0","upvote_count":"13","poster":"AWSandeep","content":"Selected Answer: A\nA. Make an RDB backup of the Redis database, use the gsutil utility to copy the RDB file into a Cloud Storage bucket, and then import the RDB file into the Memorystore for Redis instance.\n\nThe import and export feature uses the native RDB snapshot feature of Redis to import data into or export data out of a Memorystore for Redis instance. The use of the native RDB format prevents lock-in and makes it very easy to move data within Google Cloud or outside of Google Cloud. Import and export uses Cloud Storage buckets to store RDB files.\n\nReference:\nhttps://cloud.google.com/memorystore/docs/redis/import-export-overview","comment_id":"658066"},{"upvote_count":"2","comment_id":"960808","timestamp":"1721766360.0","content":"Selected Answer: A\nOption A","poster":"vamgcp"},{"poster":"AzureDP900","upvote_count":"1","timestamp":"1704157740.0","content":"A. Make an RDB backup of the Redis database, use the gsutil utility to copy the RDB file into a Cloud Storage bucket, and then import the RDB file into the Memorystore for Redis instance.","comment_id":"763427"},{"timestamp":"1701184020.0","comment_id":"729253","upvote_count":"4","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/memorystore/docs/redis/about-importing-exporting\nThe import and export feature uses the native RDB snapshot feature of Redis to import data into or export data out of a Memorystore for Redis instance. The use of the native RDB format prevents lock-in and makes it very easy to move data within Google Cloud or outside of Google Cloud. Import and export uses Cloud Storage buckets to store RDB files.","poster":"zellck"},{"upvote_count":"1","content":"Selected Answer: A\nA\nhttps://cloud.google.com/memorystore/docs/redis/import-data","timestamp":"1700820240.0","poster":"gudiking","comment_id":"725723"},{"comment_id":"725634","content":"A\nImport and export uses Cloud Storage buckets to store RDB files.\nhttps://cloud.google.com/memorystore/docs/redis/about-importing-exporting#:~:text=Import%20and%20export%20uses%20Cloud%20Storage%20buckets%20to%20store%20RDB%20files.","timestamp":"1700811540.0","poster":"Atnafu","upvote_count":"2"},{"content":"Selected Answer: A\nA\nhttps://cloud.google.com/memorystore/docs/redis/general-best-practices","poster":"ducc","timestamp":"1693707120.0","comment_id":"657978","upvote_count":"2"}],"url":"https://www.examtopics.com/discussions/google/view/79651-exam-professional-data-engineer-topic-1-question-201/","answer":"A","answer_images":[],"answer_description":"","answers_community":["A (100%)"],"isMC":true}],"exam":{"isBeta":false,"lastUpdated":"15 Feb 2025","numberOfQuestions":319,"id":10,"isMCOnly":true,"isImplemented":true,"provider":"Google","name":"Professional Data Engineer"},"currentPage":23},"__N_SSP":true}