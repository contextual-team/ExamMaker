{"pageProps":{"questions":[{"id":"UNJeL6fYVlnloHvHKi0v","isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/130292-exam-professional-data-engineer-topic-1-question-289/","timestamp":"2024-01-04 11:12:00","question_id":211,"answer":"A","discussion":[{"comment_id":"1121903","upvote_count":"8","poster":"Matt_108","content":"Selected Answer: A\nDefinitely A, cloud data fusion and wrangler to setup the clean up pipeline with no coding required","timestamp":"1705166580.0"},{"content":"Selected Answer: D\nThe question say \"You want a quick solution that requires no coding.\". The data is in BQ, then is most easy normalize the data, and schedule recurring queries in BigQuery.","poster":"marlon.andrei","upvote_count":"2","timestamp":"1736196480.0","comment_id":"1337358"},{"content":"Selected Answer: A\nA. Use Cloud Data Fusion and Wrangler to normalize the data, and set up a recurring job.\n\nExplanation\nNo Coding Required: Cloud Data Fusion's Wrangler offers a no-code interface for data transformation tasks. You can visually design data normalization workflows without writing any code.\nRecurring Jobs: Cloud Data Fusion allows you to schedule these data normalization tasks to run on a recurring basis, meeting your need for automation.","timestamp":"1721581140.0","upvote_count":"2","poster":"987af6b","comment_id":"1252611"},{"comments":[{"poster":"carmltekai","upvote_count":"1","content":"Why other options aren't as suitable:\n\nA. Cloud Data Fusion and Wrangler: While powerful, these tools might be overkill for a simple normalization task and could involve a steeper learning curve.\nB. Dataflow SQL: Dataflow is primarily for stream processing and might not be the most efficient for batch transformations on data already in BigQuery.\nC. Dataproc Serverless: This involves using a Spark job, which requires coding and might be more complex than necessary for this task.","comment_id":"1249160","timestamp":"1721155320.0"}],"content":"Selected Answer: D\nThe best solution here is D. Use BigQuery and GoogleSQL to normalize the data, and schedule recurring queries in BigQuery.\n\nHere's why:\n\n* No-code solution: BigQuery's built-in capabilities and GoogleSQL offer a no-code way to transform and standardize data. You can leverage functions like REGEXP_REPLACE to normalize phone numbers and FORMAT to ensure consistent formatting across fields.\n* Recurring jobs: BigQuery allows you to schedule queries to run regularly, which is perfect for maintaining data consistency over time.\n* Quick and efficient: BigQuery is designed for large-scale data processing, making it fast and efficient for normalization tasks.","upvote_count":"2","comment_id":"1249158","timestamp":"1721155260.0","poster":"carmltekai"},{"timestamp":"1718620380.0","poster":"fitri001","content":"Selected Answer: A\nhttps://cloud.google.com/data-fusion/docs","upvote_count":"2","comment_id":"1231876"},{"comment_id":"1158648","comments":[{"timestamp":"1708859580.0","comments":[{"content":"Wouldn't writing the SQL transformation be considered coding? The question specifically states that a solution requiring no coding is needed.","timestamp":"1708943340.0","poster":"RenePetersen","comment_id":"1159597","comments":[{"comment_id":"1177023","upvote_count":"1","poster":"jreale64","timestamp":"1710828780.0","content":"While Cloud Data Fusion with Wrangler offers a visual interface for data wrangling, it requires setting up the environment and potentially writing code for ransformations. So it its not appropriate. I think D"}],"upvote_count":"6"}],"poster":"SohiniV","content":"Any views on this ?","upvote_count":"1","comment_id":"1158649"}],"timestamp":"1708859520.0","upvote_count":"1","content":"As per chatGPT, Option D allows you to utilize BigQuery's SQL capabilities to write queries that normalize the data according to company standards.\nYou can then schedule these queries to run on a recurring basis using BigQuery's scheduled queries feature. This feature allows you to specify a schedule (e.g., weekly) for executing SQL queries automatically.\nThis approach requires no additional setup or coding outside of BigQuery, making it a quick and straightforward solution to address the issue of data normalization.","poster":"SohiniV"},{"content":"Selected Answer: A\nOption A","timestamp":"1708533300.0","poster":"JyoGCP","comment_id":"1155704","upvote_count":"1"},{"content":"Selected Answer: A\nCloud Data Fusion and Wrangler","poster":"Sofiia98","timestamp":"1704877440.0","upvote_count":"2","comment_id":"1118387"},{"timestamp":"1704363120.0","poster":"scaenruy","comment_id":"1113523","content":"Selected Answer: A\nA. Use Cloud Data Fusion and Wrangler to normalize the data, and set up a recurring job.","upvote_count":"2"}],"unix_timestamp":1704363120,"choices":{"B":"Use Dataflow SQL to create a job that normalizes the data, and that after the first run of the job, schedule the pipeline to execute recurrently.","A":"Use Cloud Data Fusion and Wrangler to normalize the data, and set up a recurring job.","D":"Use BigQuery and GoogleSQL to normalize the data, and schedule recurring queries in BigQuery.","C":"Create a Spark job and submit it to Dataproc Serverless."},"answers_community":["A (81%)","D (19%)"],"question_text":"You have data located in BigQuery that is used to generate reports for your company. You have noticed some weekly executive report fields do not correspond to format according to company standards. For example, report errors include different telephone formats and different country code identifiers. This is a frequent issue, so you need to create a recurring job to normalize the data. You want a quick solution that requires no coding. What should you do?","topic":"1","answer_ET":"A","exam_id":10,"answer_images":[],"question_images":[]},{"id":"liRlxmvUURLAO6bwrcCn","url":"https://www.examtopics.com/discussions/google/view/17029-exam-professional-data-engineer-topic-1-question-29/","answer":"D","choices":{"D":"Use a row key of the form >#<sensorid>#<timestamp>.","A":"Use a row key of the form <timestamp>.","C":"Use a row key of the form <timestamp>#<sensorid>.","B":"Use a row key of the form <sensorid>."},"answer_images":[],"isMC":true,"answer_description":"","question_id":212,"question_text":"Your company is streaming real-time sensor data from their factory floor into Bigtable and they have noticed extremely poor performance. How should the row key be redesigned to improve Bigtable performance on queries that populate real-time dashboards?","topic":"1","unix_timestamp":1584688080,"exam_id":10,"question_images":[],"answer_ET":"D","answers_community":["D (100%)"],"timestamp":"2020-03-20 08:08:00","discussion":[{"timestamp":"1601193000.0","poster":"[Removed]","content":"Description: Best practices of bigtable states that rowkey should not be only timestamp or have timestamp at starting. It’s better to have sensorid and timestamp as rowkey","upvote_count":"33","comment_id":"68555"},{"upvote_count":"19","timestamp":"1600578480.0","poster":"[Removed]","comment_id":"66207","content":"Answer D"},{"comment_id":"1076375","comments":[{"poster":"mark1223jkh","timestamp":"1731827040.0","comment_id":"1212704","upvote_count":"1","content":"Thank you that is right."}],"content":"Selected Answer: D\nLooks like D is the best option\nReference: https://cloud.google.com/bigtable/docs/schema-design#time-based","timestamp":"1716294900.0","poster":"axantroff","upvote_count":"2"},{"content":"Selected Answer: D\nD. Use a row key of the form <sensorid>#<timestamp>.\n\nBy using the sensor ID as the prefix in the row key, you can achieve better distribution of data across Bigtable tablets. This can help balance the workload and prevent hotspots in the table. Additionally, placing the timestamp after the sensor ID allows you to perform range scans for a specific sensor and retrieve data efficiently within a time frame.\n\nOption C (using a row key of the form <timestamp>#<sensorid>) can work for some use cases but may not be as efficient for range scans when you want to retrieve data for a specific sensor within a time range.\n\nOption A (using a row key of the form <timestamp>) may lead to hotspots and inefficient range scans because it doesn't consider sensor IDs.\n\nOption B (using a row key of the form <sensorid>) is not optimal because it doesn't allow for efficient time-based filtering and could lead to uneven data distribution in Bigtable.","comment_id":"1050538","upvote_count":"2","poster":"rtcpost","timestamp":"1713788040.0"},{"content":"D is right\nBest practices of bigtable states that rowkey should not be only timestamp or have timestamp at starting. It’s better to have sensorid and timestamp as rowkey.\n\nReference:\nhttps://cloud.google.com/bigtable/docs/schema-design","upvote_count":"1","comment_id":"766085","poster":"AzureDP900","timestamp":"1688497200.0"},{"poster":"Nirca","timestamp":"1686653640.0","comment_id":"744018","content":"Selected Answer: D\n#<sensorid>#<timestamp> ------> low cardinality # high cardinality \nThis is current Bigtable Best Practice (to avoid Hotspots on the inserts)","upvote_count":"5"},{"comment_id":"689230","upvote_count":"1","poster":"maxdataengineer","content":"Selected Answer: D\nDiscard:\nA -> timestamp unique id could not be unique in the case that sensors transmit data at the same time.\nB -> sensorId repeated id for messages coming from the same sensor\nC -> a bad performance choice\n\nD -> BEST CHOICE. Each time BigTable looks for data in a table it does a scan and sort operations. By starting each unique id by sensorId it will make it easier to group and sort data since it has the lowest cardinality\nhttps://cloud.google.com/bigtable/docs/schema-design#general-concepts","timestamp":"1680952500.0"},{"poster":"John_Pongthorn","content":"as I look at https://cloud.google.com/bigtable/docs/schema-design#row-keys\n asia#india#bangalore\n asia#india#mumbai\nthey didn't have # ahead of this first value.\nasia#india#bangalore OR #asia#india#bangalore\nAre both valid?","upvote_count":"2","timestamp":"1678260540.0","comment_id":"663153"},{"content":"ANSWER: D","poster":"crisimenjivar","upvote_count":"1","timestamp":"1676913840.0","comment_id":"649479"},{"poster":"som_420","content":"Selected Answer: D\nAnswer is D","timestamp":"1671190560.0","upvote_count":"1","comment_id":"617190"},{"comment_id":"530873","timestamp":"1658609520.0","poster":"samdhimal","upvote_count":"9","content":"A. Use a row key of the form <timestamp>. \n---> Incorrect, because google says don't use a timestamp by itself or at the beginning of a row key.\nB. Use a row key of the form <sensorid>.\n--->Incorrect, because google says Include a timestamp as part of your row key.\nC. Use a row key of the form <timestamp>#<sensorid>.\n---> Incorrect, because google says don't use a timestamp by itself or at the beginning of a row key.\nD. Use a row key of the form >#<sensorid>#<timestamp>.\n---> Correct answer, because of option A,B,C reasons.\n - Timestamp isn't by itself, neither at the beginning.\n - Timestamp is included.\n\nReference: https://cloud.google.com/bigtable/docs/schema-design#row-keys"},{"comment_id":"461146","content":"Ans: D","poster":"anji007","timestamp":"1649779020.0","upvote_count":"2"},{"poster":"sumanshu","upvote_count":"9","comment_id":"391401","timestamp":"1640542980.0","content":"Vote for 'D' - Store multiple delimited values in each row key. (But avoid starting with Timestamp)\n\n\"Row keys to avoid\"\nhttps://cloud.google.com/bigtable/docs/schema-design","comments":[{"content":"A is not correct because this will cause most writes to be pushed to a single node (known as hotspotting)\nB is not correct because this will not allow for multiple readings from the same sensor as new readings will overwrite old ones.\nC is not correct because this will cause most writes to be pushed to a single node (known as hotspotting)\nD is correct because it will allow for retrieval of data based on both sensor id and timestamp but without causing hotspotting.","comment_id":"401851","poster":"sumanshu","timestamp":"1641648600.0","upvote_count":"7"}]},{"upvote_count":"2","comment_id":"285009","timestamp":"1628265900.0","content":"Correct D","poster":"naga"},{"comment_id":"243362","poster":"NamitSehgal","timestamp":"1623649020.0","upvote_count":"3","comments":[{"content":"reverse TS or hashing is not always first choice or better. never.","poster":"Tanzu","timestamp":"1658670660.0","comment_id":"531428","upvote_count":"1"}],"content":"Should be D\nReverse of timestamp even better but no options for that.\nAlso changing sensor ID if they are in sequential to hash or changing data to bits even better.\nIdea is not to use timestamp or sequential ID as first key."},{"upvote_count":"3","poster":"Radhika7983","comment_id":"220259","timestamp":"1621155000.0","content":"The correct answer is D. \nRefer to the link https://cloud.google.com/bigtable/docs/schema-design for Big table schema design.\n\nC is not the right answer becuase\nTimestamps\nIf you often need to retrieve data based on the time when it was recorded, it's a good idea to include a timestamp as part of your row key. Using the timestamp by itself as the row key is not recommended, as most writes would be pushed onto a single node. For the same reason, avoid placing a timestamp at the start of the row key.\n\nFor example, your application might need to record performance-related data, such as CPU and memory usage, once per second for a large number of machines. Your row key for this data could combine an identifier for the machine with a timestamp for the data (for example, machine_4223421#1425330757685)."},{"upvote_count":"2","timestamp":"1619876520.0","content":"answer would be D to avoid hotspoting..","poster":"arghya13","comment_id":"210571"},{"poster":"ch3n6","upvote_count":"4","timestamp":"1608450240.0","content":"correct: D\nwhy not C? Using the timestamp by itself as the row key is not recommended, as most writes would be pushed onto a single node. For the same reason, avoid placing a timestamp at the start of the row key. https://cloud.google.com/bigtable/docs/schema-design#row-keys","comment_id":"114456"}]},{"id":"O50gIBf9sx77MX9jrzaz","exam_id":10,"answer":"D","isMC":true,"choices":{"C":"Use exponential backoff as the subscription retry policy, and configure dead lettering to the same source topic with maximum delivery attempts set to 10.","A":"Increase the acknowledgement deadline to 10 minutes.","D":"Use exponential backoff as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10.","B":"Use immediate redelivery as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10."},"answer_images":[],"unix_timestamp":1704363300,"timestamp":"2024-01-04 11:15:00","url":"https://www.examtopics.com/discussions/google/view/130293-exam-professional-data-engineer-topic-1-question-290/","discussion":[{"timestamp":"1720563180.0","comment_id":"1117956","poster":"raaad","upvote_count":"15","content":"Selected Answer: D\n- Exponential Backoff: This retry policy gradually increases the delay between retries, which helps to avoid overloading the consumer app. \n- Dead Lettering to a Different Topic: Configuring dead lettering sends messages that couldn't be processed after the specified number of delivery attempts (10 in this case) to a separate topic. This allows for handling of failed messages without interrupting the regular flow of new messages.\n- Maximum Delivery Attempts Set to 10: This setting ensures that the system retries each message up to 10 times before considering it a failure and moving it to the dead letter topic."},{"comment_id":"1337150","timestamp":"1736168520.0","upvote_count":"1","content":"Selected Answer: D\nhttps://cloud.google.com/pubsub/docs/subscription-overview\n\nD. Use exponential backoff as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10.\nExponential Backoff: This retry policy helps to avoid overloading the consumer app by gradually increasing the time between retries, which is more efficient than immediate redelivery.\nDead Lettering: Configuring dead lettering to a different topic ensures that messages that cannot be processed after the maximum number of retries (10 in this case) are stored separately. This allows you to handle these messages later without losing any data.\nReliability: This configuration ensures that your messaging system is reliable and can handle temporary downtime of the consumer app while maintaining data integrity","poster":"Pime13"},{"comment_id":"1155709","timestamp":"1724251380.0","upvote_count":"1","content":"Selected Answer: D\nOption D","poster":"JyoGCP"},{"upvote_count":"1","comment_id":"1121905","poster":"Matt_108","timestamp":"1720884300.0","content":"Selected Answer: D\nOption D - agree with other comments explanation"},{"upvote_count":"3","timestamp":"1720360740.0","comment_id":"1115989","content":"D. Use exponential backoff as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10\n\nBest suitable options for graceful retry and storing failed messages","poster":"GCP001"},{"timestamp":"1720080900.0","comment_id":"1113524","poster":"scaenruy","upvote_count":"2","content":"Selected Answer: D\nD. Use exponential backoff as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10.","comments":[{"content":"Exponential backoff will help in managing the load on the consumer app by gradually increasing the delay between retries. Configuring dead lettering to a different topic after a maximum of 10 delivery attempts ensures that undeliverable messages are stored separately, preventing them from being retried endlessly and cluttering the main message flow.","upvote_count":"2","timestamp":"1720363920.0","poster":"Smakyel79","comment_id":"1116023"}]}],"answer_description":"","question_images":[],"topic":"1","answer_ET":"D","question_id":213,"answers_community":["D (100%)"],"question_text":"You are designing a messaging system by using Pub/Sub to process clickstream data with an event-driven consumer app that relies on a push subscription. You need to configure the messaging system that is reliable enough to handle temporary downtime of the consumer app. You also need the messaging system to store the input messages that cannot be consumed by the subscriber. The system needs to retry failed messages gradually, avoiding overloading the consumer app, and store the failed messages after a maximum of 10 retries in a topic. How should you configure the Pub/Sub subscription?"},{"id":"zuzcChet38LFgQWv1Emx","answers_community":["A (100%)"],"url":"https://www.examtopics.com/discussions/google/view/130296-exam-professional-data-engineer-topic-1-question-291/","discussion":[{"comments":[{"poster":"cloud_rider","comments":[{"timestamp":"1737647460.0","comment_id":"1345497","content":"I assume \"low-maintenance\" is the main problem on B","poster":"Ryannn23","upvote_count":"1"}],"content":"What is wrong with the option B?","upvote_count":"1","comment_id":"1319469","timestamp":"1732830720.0"}],"timestamp":"1704845880.0","poster":"raaad","comment_id":"1117960","upvote_count":"7","content":"Selected Answer: A\nAnalytics Hub offers a centralized platform for managing data sharing and access within the organization. This simplifies access control management."},{"content":"Selected Answer: A\nA. is the answer I select","timestamp":"1721581560.0","upvote_count":"1","comment_id":"1252614","poster":"987af6b"},{"timestamp":"1708533840.0","poster":"JyoGCP","upvote_count":"1","content":"Selected Answer: A\nAnalytics Hub","comment_id":"1155710"},{"poster":"Matt_108","timestamp":"1705166760.0","upvote_count":"1","content":"Selected Answer: A\nDefinitely A","comment_id":"1121906"},{"content":"Selected Answer: A\nA. Create an Analytics Hub private exchange, and publish the sales dataset.","timestamp":"1704363720.0","poster":"scaenruy","upvote_count":"1","comment_id":"1113535"}],"answer_ET":"A","question_id":214,"unix_timestamp":1704363720,"question_images":[],"timestamp":"2024-01-04 11:22:00","isMC":true,"topic":"1","answer_description":"","exam_id":10,"question_text":"You designed a data warehouse in BigQuery to analyze sales data. You want a self-serving, low-maintenance, and cost- effective solution to share the sales dataset to other business units in your organization. What should you do?","answer_images":[],"answer":"A","choices":{"D":"Use the BigQuery Data Transfer Service to create a schedule that copies the sales dataset to the other business units’ projects.","A":"Create an Analytics Hub private exchange, and publish the sales dataset.","C":"Create and share views with the users in the other business units.","B":"Enable the other business units’ projects to access the authorized views of the sales dataset."}},{"id":"Z0o4eBGSfIeBzMd2BK46","answers_community":["C (100%)"],"timestamp":"2024-01-04 11:33:00","answer_ET":"C","discussion":[{"comment_id":"1119679","upvote_count":"11","timestamp":"1704974520.0","content":"Selected Answer: C\n- Datastream: It's a fully managed, serverless service for real-time data replication. It allows to stream data from various sources, including Cloud SQL, into BigQuery.\n- Reduced Load on Cloud SQL: By replicating the required tables from both Cloud SQL databases into BigQuery, you minimize the load on the Cloud SQL instances. The marketing team's queries will be run against BigQuery, which is designed to handle high-volume analytics workloads.\n- Frequency of Queries: BigQuery can easily handle the high frequency of queries (100 times daily, up to 300 during sales events) due to its powerful data processing capabilities.\n- Combining Data Sources: Once the data is in BigQuery, you can efficiently combine it with the Google Analytics data for comprehensive analysis and campaign planning.","comments":[{"comment_id":"1178437","content":"Why not A ? Federrated queries will downgrade Cloud SQL perf?","timestamp":"1710948060.0","upvote_count":"1","poster":"SanjeevRoy91"}],"poster":"raaad"},{"poster":"987af6b","content":"Selected Answer: C\nInitially I said A, but this question was how I learned about Datastream, which I think would be the better solution in this scenario. So my answer is C","timestamp":"1721581860.0","upvote_count":"2","comment_id":"1252616"},{"poster":"AlizCert","timestamp":"1717597980.0","content":"Selected Answer: C\nC, noting that federated queries on read replicas would be the ideal solution","comment_id":"1224774","upvote_count":"1"},{"content":"Its option C.\n\n\"Performance. A federated query is likely to not be as fast as querying only BigQuery storage. BigQuery needs to wait for the source database to execute the external query and temporarily move data from the external data source to BigQuery. Also, the source database might not be optimized for complex analytical queries.\"\n\nSo, it will load the Cloud SQL external sources with the queries, impacting performance on those.\n\nLink: https://cloud.google.com/bigquery/docs/federated-queries-intro","timestamp":"1712818980.0","upvote_count":"1","comment_id":"1193510","poster":"joao_01"},{"comment_id":"1170867","upvote_count":"1","poster":"datasmg","content":"Selected Answer: C\nC is make sense","timestamp":"1710142680.0"},{"comment_id":"1155713","upvote_count":"1","timestamp":"1708534260.0","poster":"JyoGCP","content":"Selected Answer: C\nOption C"},{"poster":"scaenruy","comment_id":"1113544","content":"Selected Answer: C\nC. Create streams in Datastream to replicate the required tables from both Cloud SQL databases to BigQuery for these queries.","timestamp":"1704364380.0","comments":[{"poster":"Smakyel79","upvote_count":"3","comment_id":"1116026","timestamp":"1704646620.0","content":"Datastream is a serverless, easy-to-use change data capture (CDC) and replication service. By replicating the necessary tables from the Cloud SQL databases to BigQuery, you can offload the query load from the Cloud SQL databases. The marketing team can then run their queries directly on BigQuery, which is designed for large-scale data analytics. This approach seems to balance both efficiency and performance, minimizing load on the Cloud SQL instances."}],"upvote_count":"3"}],"question_id":215,"isMC":true,"answer_description":"","answer_images":[],"choices":{"C":"Create streams in Datastream to replicate the required tables from both Cloud SQL databases to BigQuery for these queries.","A":"Create BigQuery connections to both Cloud SQL databases. Use BigQuery federated queries on the two databases and the Google Analytics data on BigQuery to run these queries.","D":"Create a Dataproc cluster with Trino to establish connections to both Cloud SQL databases and BigQuery, to execute the queries.","B":"Create a job on Apache Spark with Dataproc Serverless to query both Cloud SQL databases and the Google Analytics data on BigQuery for these queries."},"url":"https://www.examtopics.com/discussions/google/view/130298-exam-professional-data-engineer-topic-1-question-292/","question_text":"You have terabytes of customer behavioral data streaming from Google Analytics into BigQuery daily. Your customers’ information, such as their preferences, is hosted on a Cloud SQL for MySQL database. Your CRM database is hosted on a Cloud SQL for PostgreSQL instance. The marketing team wants to use your customers’ information from the two databases and the customer behavioral data to create marketing campaigns for yearly active customers. You need to ensure that the marketing team can run the campaigns over 100 times a day on typical days and up to 300 during sales. At the same time, you want to keep the load on the Cloud SQL databases to a minimum. What should you do?","unix_timestamp":1704364380,"answer":"C","exam_id":10,"topic":"1","question_images":[]}],"exam":{"isMCOnly":true,"name":"Professional Data Engineer","numberOfQuestions":319,"isBeta":false,"isImplemented":true,"lastUpdated":"15 Feb 2025","id":10,"provider":"Google"},"currentPage":43},"__N_SSP":true}