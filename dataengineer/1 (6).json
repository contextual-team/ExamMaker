{"pageProps":{"questions":[{"id":"GZWIS51szOk6Nd99k37y","choices":{"D":"Train your own image recognition model leveraging transfer learning techniques.","C":"Use Cloud Vision API by providing custom labels as recognition hints.","B":"Use Cloud Vision AutoML, but reduce your dataset twice.","A":"Use Cloud Vision AutoML with the existing dataset."},"answer_description":"","unix_timestamp":1584870720,"topic":"1","discussion":[{"comment_id":"114675","poster":"Callumr","upvote_count":"54","timestamp":"1608465660.0","content":"B - You only need a PoC and it has be done quickly"},{"timestamp":"1600761120.0","comment_id":"66891","upvote_count":"20","poster":"[Removed]","content":"Correct - A"},{"content":"Selected Answer: A\nThe key difference between Google Cloud Vision AutoML and Cloud Vision API is that Cloud Vision API provides pre-trained models for basic image analysis tasks like object detection and labeling, while Cloud Vision AutoML allows you to train custom machine learning models to identify specific objects or concepts within images that are unique to your dataset, requiring you to provide labeled training data.","poster":"grshankar9","upvote_count":"1","comment_id":"1342801","timestamp":"1737244020.0"},{"upvote_count":"3","timestamp":"1732574580.0","comment_id":"1218563","content":"Selected Answer: A\nAutoML Vision is deprecated since march 31, 2024. The question will refer to Vertex AI AutoML. And as bet practice, the minimum dataset size for each label is 1000. So, with an updated question, the answer would be A.","poster":"josech"},{"timestamp":"1728076560.0","content":"Selected Answer: A\nA. Use Cloud Vision AutoML with the existing dataset.\n\nHere's why this is the most suitable option:\n\nSpeed and Ease: AutoML simplifies model building. You simply upload your labeled images, and AutoML takes care of model selection, training, and evaluation.\nExisting Dataset Sufficiency: Your dataset (750 components x 1000 images each) is a decent starting point for AutoML, allowing you to quickly test its effectiveness.\nMinimal Custom Development: AutoML's out-of-the-box deployment options let you integrate the model into your app without extensive coding.","poster":"CGS22","upvote_count":"1","comment_id":"1189548"},{"content":"Selected Answer: B\nOption B is the fastest way to train a model that can be used to recognize the 750 different components.","upvote_count":"1","poster":"saado9","timestamp":"1709986680.0","comment_id":"1003088"},{"timestamp":"1692269820.0","comments":[{"content":"Adding custom labels to Vision API is done by training an AutoML model! That's the formal recommendation. And you don't need a big dataset for AutoML as it uses transfer learning.","comment_id":"911775","timestamp":"1701417360.0","upvote_count":"4","poster":"forepick"},{"upvote_count":"1","comment_id":"963313","timestamp":"1706247120.0","content":"it is a labeled dataset and why do you need to label it once again? So no C","poster":"knith66"}],"content":"Whats wrong with C, its fast, cheap and add your 750 labels which is not big work. \nAutoML is good to train on big dataset and costly as compared to APIs","comment_id":"811877","poster":"musumusu","upvote_count":"2"},{"timestamp":"1691458740.0","content":"A - https://cloud.google.com/vertex-ai/docs/beginner/beginners-guide Target at least 1000 examples per target","poster":"techtitan","upvote_count":"8","comments":[{"content":"The quick POC part can be achieved by using Auto ML instead of creating and training your own model","poster":"techtitan","upvote_count":"1","timestamp":"1691458860.0","comment_id":"801629"}],"comment_id":"801628"},{"content":"Selected Answer: A\nFirst I think in Vision API, but that is a pre-trained AI, will not recognize my labels, so because you have 1000 samples per item, AUTO ML is perfect. B cannot be because have not sensed to reduce your dataset if you have the recommended number of info.\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label.","comment_id":"738826","poster":"odacir","upvote_count":"8","comments":[{"timestamp":"1688089740.0","upvote_count":"2","comment_id":"762463","poster":"AzureDP900","content":"A is correct"}],"timestamp":"1686209340.0"},{"content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label.","upvote_count":"4","poster":"zellck","comments":[{"timestamp":"1701289860.0","content":"So how are you going to test that the model was able to adequately learn from the sample? The point of splitting a dataset is to train the model on one part of the data (say 80%), and then test it on the other part (20%). If your model is able to predict the outcome of (most of) the sample points in your test dataset, you can be confident that it will work well on future data. Without a test data set, however, you have no such feedback. Therefore, the answer is B.","poster":"ga8our","upvote_count":"2","comments":[{"poster":"NewDE2023","upvote_count":"1","content":"I believe that the ideal would be to reduce the number of components for the POC and preserve the number of examples, so my answer is A.","timestamp":"1707061500.0","comment_id":"972212"}],"comment_id":"909633"}],"timestamp":"1685784780.0","comment_id":"734393"},{"timestamp":"1684418340.0","upvote_count":"1","poster":"gudiking","comment_id":"721388","content":"A - https://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category"},{"upvote_count":"5","timestamp":"1683978840.0","poster":"MarielaYBird","comment_id":"717339","content":"Selected Answer: B\nBased on this:\n\"As a rule of thumb, we recommend to have at least 100 training samples per class if you have distinctive and few classes, and more than 200 training samples if the classes are more nuanced and you have more than 50 different classes\"\n\n750 different components = more than 50 different classes. That means we need more than 200 training samples. If we used 250 training samples out of the 1000 samples and multiply it to 750 different classes we get a total of 187,500 which is the equivalent of reducing the dataset twice. \n\nhttps://cloud.google.com/vision/automl/object-detection/docs/prepare#how_big_does_the_dataset_need_to_be"},{"content":"Selected Answer: A\nI choose A because on the vertex AI documentation (https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data), on the best practices of preparing data for image recognition recommend this: We recommend about 1000 training images per label. The minimum per label is 10. In general, it takes more examples per label to train models with multiple labels per image, and resulting scores are harder to interpret.\n\nI know that is PoC, but if you do it without enough accuracy, you maybe discard the solution because it isn't fit for your requirements. So is better to do it with enough data to be sure that the model is or not accuracy enough with this data, because you maybe haven't enough accuracy and the problem is the quality of the data and not the amount of it.","timestamp":"1681737300.0","comment_id":"697420","upvote_count":"3","poster":"josrojgra"},{"comments":[{"timestamp":"1680313680.0","poster":"John_Pongthorn","content":"The more labels, the more accurate the result.","upvote_count":"1","comment_id":"683921"}],"timestamp":"1680313620.0","upvote_count":"3","poster":"John_Pongthorn","comment_id":"683920","content":"Selected Answer: A\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category\n\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label."},{"comment_id":"661901","content":"Selected Answer: B\n750*1000 are a lot.","upvote_count":"1","poster":"changsu","timestamp":"1678165620.0"},{"poster":"ducc","upvote_count":"1","timestamp":"1677553440.0","comment_id":"653721","content":"Selected Answer: A\nIt is labeled, so A is correct"},{"content":"It's A. \nhttps://cloud.google.com/vision/automl/docs/beginners-guide#data_preparation \n\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label.","comment_id":"649455","poster":"civilizador","comments":[{"timestamp":"1676908080.0","comment_id":"649456","poster":"civilizador","upvote_count":"1","content":"So even for POC better to use 1000 . There would be no significant time differences anyway between 500 and 1000"}],"timestamp":"1676908020.0","upvote_count":"5"},{"comment_id":"641304","upvote_count":"3","timestamp":"1675355880.0","content":"Option A & B are quite close. Refer: https://cloud.google.com/vision/automl/docs/beginners-guide#data_preparation – Says to target at least 1000 images per label for training.","poster":"TheRealBsh"},{"upvote_count":"1","poster":"czokwe","content":"Selected Answer: B\nB\ncant choose A because model needs to pass through the dataset several times for a proof of concept, existing data set samples might not be all seen in several working days causing over generalization","comment_id":"609481","timestamp":"1669771860.0"},{"poster":"Kriegs","comment_id":"607218","content":"I don't get it, why B rather than A? I know it's a proof of concept, but it's not like a bigger dataset is any kind of a dealbreaker in training a model of that size, and more data provides more accuracy to the model.\n\nI would choose A or C.","upvote_count":"2","timestamp":"1669386480.0"},{"comment_id":"606903","timestamp":"1669331400.0","poster":"[Removed]","upvote_count":"1","content":"Selected Answer: A\nAutoML work well with more than 100 training images/label but ideally with 1000 of each."},{"upvote_count":"2","comment_id":"602169","timestamp":"1668534240.0","poster":"michalsosn","content":"To keep the training time low, you should set the node hour budget to the desired number of hours, right? Not reduce the training set. Unless there's some limit in AutoML that makes 375k images acceptable and 750k not, I think I'd pick A rather than B."},{"content":"Selected Answer: A\nhttps://cloud.google.com/vision/automl/object-detection/docs/prepare\n\nAnnotation requirements \nFor each label you must have at least 10 images, each with at least one annotation (bounding box and the label).\n\nHowever, for model training purposes it's recommended you use about 1000 annotations per label. In general, the more images per label you have the better your model will perform.","upvote_count":"3","poster":"VGalan","timestamp":"1668417900.0","comments":[{"comment_id":"604112","timestamp":"1668890880.0","upvote_count":"1","poster":"homaj","content":"IT'S A POC"}],"comment_id":"601468"},{"poster":"sw52099","upvote_count":"1","content":"Selected Answer: C\nC.\nTraining a model only for POC is not cost-effective. Further, vision model is very hard to train, not mention to train a workable model only in few days.","timestamp":"1668261060.0","comment_id":"600598"},{"timestamp":"1664266800.0","content":"If you're gonna reduce the size, will you get better results? because, in the cloud.google.com/vision/automl/object-detection/docs/prepare it's clearly said \"As a rule of thumb, we recommend to have at least 100 training samples per class if you have distinctive and few classes, and more than 200 training samples if the classes are more nuanced and you have more than 50 different classes.\" hence, as we have 750 classes on average with 1000 images, if you reduce it twice? Yes, the training will become faster but with better accuracy? Anyone please clarify if its' A/B?","poster":"tavva_prudhvi","upvote_count":"2","comment_id":"576067"},{"comment_id":"559412","poster":"rbeeraka","upvote_count":"2","timestamp":"1662115200.0","content":"B is correct. C is wrong as Vision API's are pretrained models for generic use cases."},{"content":"My opinion is A. \n1000 labeled images for each item is the best fit with A. https://cloud.google.com/vision/automl/docs/prepare","comment_id":"558734","timestamp":"1662028860.0","poster":"Aslkdup","upvote_count":"3"},{"poster":"Prasanna_kumar","content":"Selected Answer: A\nAnswer : A","upvote_count":"1","timestamp":"1661094660.0","comment_id":"553039"},{"poster":"sagar_dis","upvote_count":"2","timestamp":"1658941140.0","content":"Selected Answer: C\nhttps://cloud.google.com/vision\nAutoML Vision enables you to train machine learning models to classify your images according to your own defined labels.\n\nVision API\nVision API offers powerful pre-trained machine learning models through REST and RPC APIs. Assign labels to images and quickly classify them into millions of predefined categories","comment_id":"534043"},{"content":"Selected Answer: B\nB is the answer","poster":"davidqianwen","timestamp":"1658661600.0","comment_id":"531319","upvote_count":"3"},{"upvote_count":"4","content":"Got this question in 17 Oct 2021 exam","poster":"GCP2022","comment_id":"531145","timestamp":"1658644260.0"},{"upvote_count":"2","comments":[{"timestamp":"1682255160.0","upvote_count":"1","poster":"muky31dec","content":"What is correct Ans? A or B","comment_id":"702188"}],"content":"Got this question in 17 Oct 2022 exam","comment_id":"531144","timestamp":"1658644200.0","poster":"GCP2022"},{"poster":"Bhawantha","content":"Selected Answer: B\nAns B\nVision API is giving just a rough idea about the image. Not exactly what we want. Because of that, we can exclude C. Since D is time-consuming D also can exclude.\n\nFight between A and B. For Auto, ML minimum is required 100 images per label. Even we get half or full the dataset is completely OK. But remember this is POC. In industry after POC most of the time we add new features or there are some changes. We just need to verify that by using this dataset and technologies we can implement. So no needs to waste your resources. \n\nAns B\nSee the small Comparisum.\nhttps://www.youtube.com/watch?v=GbLQE2C181U","comment_id":"525959","timestamp":"1658070360.0","upvote_count":"7"},{"timestamp":"1657371720.0","content":"Selected Answer: A\nA: https://cloud.google.com/automl/","upvote_count":"4","comment_id":"520268","poster":"MaxNRG"},{"content":"Selected Answer: C\nIn A and B with Cloud Vision AutoML you will make a model based on data and in question it clearly says build a PoC app ASAP, so Cloud Vision API (with prebuild image models ) + labeled data seems to be the fastest option.","poster":"medeis_jar","timestamp":"1657275480.0","comments":[{"content":"good point, C make sense","poster":"MaxNRG","upvote_count":"2","comments":[{"timestamp":"1658636580.0","poster":"MaxNRG","content":"https://cloud.google.com/vision/docs\nCloud Vision allows developers to easily integrate vision detection features within applications, including image labeling, face and landmark detection, optical character recognition (OCR), and tagging of explicit content.","comment_id":"531085","upvote_count":"1"}],"timestamp":"1658636460.0","comment_id":"531084"}],"upvote_count":"3","comment_id":"519502"},{"timestamp":"1657175340.0","content":"Answer is C","upvote_count":"1","comment_id":"518840","poster":"Kalyan1206"},{"content":"Selected Answer: C\nyou have a labeled the dataset. You are able to use the train the model.","poster":"Yonghai","upvote_count":"1","comment_id":"510027","timestamp":"1656298800.0"},{"timestamp":"1648438680.0","content":"Maximum data set size should be 150,000 only. So need to split data for one iteration of training --\"Images in each dataset : 150,000 maximum\"","upvote_count":"1","poster":"navemula","comment_id":"453004"},{"timestamp":"1644451920.0","poster":"Sushil_123","comment_id":"422333","upvote_count":"2","comments":[{"comments":[{"comment_id":"494108","comments":[{"comment_id":"518693","timestamp":"1657149660.0","content":"Have you taken exam? how did it go? what percentage of questions came from examtopics? with all the questions there is so many discussions happening, for the topics which are new to me, getting confused which option to choose, even if the same question comes in actual exam.","poster":"GCPLearning2021","upvote_count":"1"}],"timestamp":"1654402380.0","content":"LMAO. NO ONE ANSWERING. I AM ALSO HAVING EXAM IN A MONTH. I WILL UPDATE YOU GUYS. AFTER MY EXAM.","upvote_count":"1","poster":"BigQuery"}],"timestamp":"1645019100.0","content":"how was your exam ? did you find good reference from here ?","poster":"KalyanSinha","upvote_count":"3","comment_id":"425763"},{"upvote_count":"1","poster":"Ral17","content":"@Sushil_123 How was your exam? How many questions did you get from examtopics?","timestamp":"1646512380.0","comment_id":"439870"},{"comment_id":"442721","poster":"tainangao","upvote_count":"2","content":"I have the exam in a week or so. How was your exam? How helpful were examtopics questions?","timestamp":"1646959860.0"}],"content":"Hi All,\nI have exam in few days. How many questions we can expect from examtopics"},{"content":"Vote for B, POC is a small-scale experiments","comments":[{"upvote_count":"1","comment_id":"759491","poster":"Wonka87","content":"but it has to be working version so no need to reduce the training data","timestamp":"1687929720.0"}],"upvote_count":"3","timestamp":"1641216300.0","comment_id":"397527","poster":"sumanshu"},{"comment_id":"385965","upvote_count":"2","timestamp":"1639984800.0","poster":"vbondoo7","content":"Should be B - as its just a POC"},{"timestamp":"1639386720.0","content":"I think it's A as well. In general, don't reduce the dataset to speed up the PoC, it's a very bad habit. PoC should just direct you to AutoML rather than training own model. B could work but since you have 750 Classes, it is not recommended to reduce the size of the dataset. Rule of thumb is 100 images / class only if you have very few distinctive classes. here you have 750 classes and no idea if they are different. \" \nFor each label you must have at least 10 images, each with at least one annotation (bounding box and the label).\n\nHowever, for model training purposes it's recommended you use about 1000 annotations per label. In general, the more images per label you have the better your model will perform.\" https://cloud.google.com/vision/automl/object-detection/docs/prepare","poster":"mkport1","comment_id":"380920","upvote_count":"3"},{"comment_id":"238727","comments":[{"timestamp":"1656057660.0","content":"x1000 average examples (set of 750.000 images). So B) it's enougth for a PoC.","comment_id":"508419","upvote_count":"1","poster":"Jlozano"}],"timestamp":"1623184500.0","content":"its says 750 diffrent components...so A","poster":"beedle","upvote_count":"4"},{"upvote_count":"2","comments":[{"comment_id":"221893","content":"But vision api supports rpc and api while automl has ui interface so no much coding required...confused between a and c","timestamp":"1621336320.0","upvote_count":"1","poster":"kavs"}],"comment_id":"221880","timestamp":"1621335660.0","poster":"kavs","content":"C..similar vision api product search exists..pretrained labels are used while in automl we need to train so that skillset is required to customize..in vision api just in excel we can write the label as component and image n can use this to classify the new incoming images"},{"content":"Option A , reason:-\n- Training data should be as much as possible for better prediction.","poster":"Alasmindas","upvote_count":"4","comments":[{"comments":[],"content":"But, the problem is they didnt ask for accuracy! they only want to the POC to be done within few working days!","comment_id":"589858","timestamp":"1666429980.0","poster":"tavva_prudhvi","upvote_count":"1"}],"comment_id":"216724","timestamp":"1620654360.0"},{"comment_id":"216081","content":"A: \nhttps://cloud.google.com/vision/automl/docs/beginners-guide\n\n\"The bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label.\"","upvote_count":"8","comments":[{"comment_id":"243412","content":"Somewhat right, but the question is about POC which should not be as accurate as possible. Instead of 1000 per class, 500 per class is still much better than the bare minimum (100) but allows for faster training.","timestamp":"1623655500.0","upvote_count":"3","poster":"xrun"}],"poster":"Cloud_Enthusiast","timestamp":"1620571500.0"},{"upvote_count":"4","poster":"paragkhetam","content":"B as it is POC","comment_id":"196961","timestamp":"1617995820.0"},{"comment_id":"185175","upvote_count":"6","poster":"SteelWarrior","content":"Answer should be A: Refer the below documentation. Even with complete data set total record set would be 750K. For record set between 100K to 1000K the suggested time by Google is 1-6 Hrs.","timestamp":"1616501460.0","comments":[{"comments":[{"timestamp":"1616569920.0","poster":"Diqtator","upvote_count":"1","comment_id":"185882","content":"Does one picture compare to a single row though? A picture is more complicated than a normal table row"}],"poster":"SteelWarrior","timestamp":"1616501520.0","upvote_count":"3","comment_id":"185176","content":"https://cloud.google.com/automl-tables/docs/train"}]},{"upvote_count":"3","timestamp":"1616401800.0","poster":"Diqtator","content":"Hmm, to reduce the dataset like that we'd have to delete pictures for each product by the same amount. We can't just reduce the whole dataset since that would probably skew the amount of data for each product.\n\nThe easiest way would be A and just let it chew through all of the pictures. I am not sure about the time needed to do that though.","comment_id":"184236","comments":[{"content":"But for a PoC it might not matter, so B might be the best and fastest option anyway if we assume they reduce the dataset evenly over all products?","comment_id":"184237","upvote_count":"2","timestamp":"1616401980.0","poster":"Diqtator"}]},{"poster":"rajnishd","timestamp":"1614796560.0","content":"C is correct, Automl requires customized dataset","upvote_count":"1","comment_id":"172792"},{"comment_id":"163129","content":"C is correct. This scenario don't require some customize trining.\nAuto ML is used better when the recognition is customized and it requires some more specialized.","poster":"haroldbenites","upvote_count":"2","timestamp":"1613940180.0"},{"comment_id":"120098","upvote_count":"5","timestamp":"1608955860.0","comments":[{"poster":"dragon123","content":"The bare minimum required by AutoML Vision training is 100 image examples per category/label. So it should be B","timestamp":"1614184500.0","comment_id":"165283","upvote_count":"7"}],"content":"Option [A] & [B] are quite close. https://cloud.google.com/vision/automl/docs/beginners-guide#data_preparation - Says to target atleast 1000 images per label for training.. Hence, I'll got with Option [A]","poster":"dambilwa"},{"poster":"Rajokkiyam","timestamp":"1601599680.0","comment_id":"70295","upvote_count":"4","content":"Answer A"}],"question_id":31,"answer":"A","answers_community":["A (57%)","B (31%)","12%"],"isMC":true,"answer_ET":"A","url":"https://www.examtopics.com/discussions/google/view/17235-exam-professional-data-engineer-topic-1-question-126/","question_images":[],"timestamp":"2020-03-22 10:52:00","question_text":"You work for a manufacturing company that sources up to 750 different components, each from a different supplier. You've collected a labeled dataset that has on average 1000 examples for each unique component. Your team wants to implement an app to help warehouse workers recognize incoming components based on a photo of the component. You want to implement the first working version of this app (as Proof-Of-Concept) within a few working days. What should you do?","exam_id":10,"answer_images":[]},{"id":"BJQ1MIelYWCWCtPG4U9O","topic":"1","choices":{"C":"Use Cloud GPUs after implementing GPU kernel support for your customs ops.","B":"Use Cloud TPUs after implementing GPU kernel support for your customs ops.","D":"Stay on CPUs, and increase the size of the cluster you're training your model on.","A":"Use Cloud TPUs without any additional adjustment to your code."},"url":"https://www.examtopics.com/discussions/google/view/17236-exam-professional-data-engineer-topic-1-question-127/","question_id":32,"timestamp":"2020-03-22 11:01:00","exam_id":10,"question_text":"You are working on a niche product in the image recognition domain. Your team has developed a model that is dominated by custom C++ TensorFlow ops your team has implemented. These ops are used inside your main training loop and are performing bulky matrix multiplications. It currently takes up to several days to train a model. You want to decrease this time significantly and keep the cost low by using an accelerator on Google Cloud. What should you do?","discussion":[{"comments":[{"comments":[{"timestamp":"1703778780.0","content":"It does. TPU is good for \"Models with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\".","poster":"Helinia","upvote_count":"1","comment_id":"1107933"}],"upvote_count":"1","content":"the link doesn't say TPU does not support custom C++ tensorflow ops","timestamp":"1698181560.0","poster":"ffggrre","comment_id":"1053168"}],"content":"The correct answer is C\nTPU does not support custom C++ tensorflow ops\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus","upvote_count":"69","poster":"dhs227","timestamp":"1585764000.0","comment_id":"70205"},{"comment_id":"70171","timestamp":"1585757100.0","upvote_count":"44","content":"D:\nCloud TPUs are not suited to the following workloads: [...] Neural network workloads that contain custom TensorFlow operations written in C++. Specifically, custom operations in the body of the main training loop are not suitable for TPUs.","poster":"aiguy","comments":[{"upvote_count":"11","poster":"gopinath_k","comment_id":"309431","timestamp":"1615611300.0","content":"B:\n1. You need to provide support for the matrix multiplication - TPU\n2. You need to provide support for the Custom TF written in C++ - GPU"},{"comments":[{"upvote_count":"1","content":"Chat GPT says C\nOption D is not the most cost-effective or efficient solution. While increasing the size of the cluster could decrease the training time, it would also significantly increase the cost, and CPUs are not as efficient for this type of workload as GPUs.","poster":"cetanx","comments":[{"upvote_count":"3","comments":[{"upvote_count":"2","comment_id":"1056229","poster":"squishy_fishy","timestamp":"1698499560.0","content":"Totally agree. ChatGPT is garbage. It is still learning."}],"poster":"FP77","comment_id":"982643","timestamp":"1692193800.0","content":"chatgpt will give you different answers if you ask 10 times. The correct answer is B"}],"timestamp":"1685426580.0","comment_id":"909957"}],"content":"But, in the question it also says we have to decrease the time significantly?? If you gonna use the CPU, it will take more time to train, right?","upvote_count":"1","timestamp":"1648370640.0","comment_id":"576078","poster":"tavva_prudhvi"}]},{"poster":"SamuelTsch","timestamp":"1729789980.0","content":"Selected Answer: D\nAccording to the official documentation. Models that contain many custom TensorFlow operations written in C++ should keep using CPUs.","upvote_count":"2","comment_id":"1302549"},{"content":"I think this is D. I recently did the ML professional exam and they ask that there, and it's always \"c++ custom ops = CPU\", it's in fact the only scenario for non-small models on CPU. It's written in black and white here: https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus, check out the CPU/GPU/TPU \"when to use\" section.","timestamp":"1727351400.0","poster":"baimus","comment_id":"1289411","upvote_count":"3"},{"content":"Selected Answer: C\nWhy Not Other Options?\nA. Use Cloud TPUs without any additional adjustment to your code:\n\nTPUs are optimized for standard TensorFlow operations and require custom TensorFlow ops to be adapted to TPU-compatible kernels, which is not trivial.\nWithout modifications, your custom C++ ops will not run efficiently on TPUs.\nB. Use Cloud TPUs after implementing GPU kernel support for your customs ops:\n\nImplementing GPU kernel support alone is not sufficient for running on TPUs. TPUs require specific optimizations and adaptations beyond GPU kernels.\nD. Stay on CPUs, and increase the size of the cluster you're training your model on:\n\nWhile increasing the CPU cluster size might reduce training time, it is not as efficient or cost-effective as using GPUs, especially for matrix multiplication tasks.","upvote_count":"1","timestamp":"1717833660.0","comment_id":"1226620","poster":"Anudeep58"},{"comment_id":"1224523","upvote_count":"1","timestamp":"1717563960.0","poster":"AlizCert","content":"Selected Answer: C\nC: TPUs are out of the picture due to the custom ops, so the next best option for accelerating matrix operations is using GPU. Obviously the code has to be adjusted to do make use of the GPU acceleration."},{"comment_id":"1218377","poster":"GCP_data_engineer","upvote_count":"1","content":"CPU : Simple models \nGPU: Custom TensorFlow/PyTorch/JAX operations","timestamp":"1716647880.0"},{"upvote_count":"1","poster":"CGS22","content":"Selected Answer: C\nThe best choice here is C. Use Cloud GPUs after implementing GPU kernel support for your customs ops. Here's why:\n\nCustom Ops & GPUs: Since your model relies heavily on custom C++ TensorFlow ops focused on matrix multiplications, GPUs are the ideal accelerators for this workload. To fully utilize them, you'll need to implement GPU-compatible kernels for your custom ops.\nSpeed and Cost-Efficiency GPUs offer a significant speed improvement for matrix-intensive operations compared to CPUs. They provide a good balance of performance and cost for this scenario.\nTPUs: Limitations Although Cloud TPUs are powerful, they aren't designed for arbitrary custom ops. Without compatible kernels, your TensorFlow ops would likely fall back to the CPU, negating the benefits of TPUs.","comment_id":"1189553","timestamp":"1712265840.0"},{"comment_id":"1159014","upvote_count":"2","content":"Selected Answer: C\nTPU:\nModels with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\nLink: https://cloud.google.com/tpu/docs/intro-to-tpu#TPU\n\nSo, A&B eliminated\nCPU is very slow or built for simple operations. So C: GPU","timestamp":"1708885380.0","poster":"Preetmehta1234"},{"content":"Selected Answer: C\nto me, it's C","comment_id":"1122041","timestamp":"1705178640.0","upvote_count":"1","poster":"Matt_108"},{"timestamp":"1701492840.0","content":"Requirement 1: Significantly reduce the processing time while keeping costs low. \nRequirement 2: Bulky matrix multiplication takes up to several days.\n\nFirst, eliminate A & D:\nA: Cannot guarantee running on Cloud TPU without modifying the code.\nD: Cannot ensure performance improvement or cost reduction, and additionally, CPUs are not suitable for bulky matrix multiplication.\n\nIf it can be ensured that customization is easily deployable on both Cloud TPU and Cloud GPU,it seems more feasible to first try Cloud GPU.\n\nBecause:\nIt provides a better balance between performance and cost.\nModifying custom C++ on Cloud GPU should be easier than on Cloud TPU, which should also save on manpower costs.","comment_id":"1085704","poster":"Kimich","upvote_count":"3"},{"comment_id":"1075595","upvote_count":"1","poster":"emmylou","timestamp":"1700497680.0","content":"Answer D\nI did use Chat GPT and discovered that if you put at the beginning of the question -- \"Do not make assumption about changes to architecture. This is a practice exam question.\" All other answers require changes to the code and architecture."},{"upvote_count":"1","content":"Selected Answer: B\nI think it should use tensor flow processing unit along with GPU kernel support.","timestamp":"1700316660.0","poster":"DataFrame","comment_id":"1074047"},{"upvote_count":"1","timestamp":"1696591200.0","content":"Selected Answer: B\nTo use Cloud TPUs, you will need to:\n\nImplement GPU kernel support for your custom TensorFlow ops. This will allow your model to run on both Cloud TPUs and GPUs.","comment_id":"1026527","poster":"Nirca"},{"content":"Refer https://www.linkedin.com/pulse/cpu-vs-gpu-tpu-when-use-your-machine-learning-models-bhavesh-kapil","timestamp":"1696144620.0","upvote_count":"1","poster":"kumarts","comment_id":"1022071"},{"upvote_count":"1","poster":"IrisXia","comment_id":"980379","timestamp":"1691975460.0","content":"Answer C\nTPU not for custom C++ but GPU can"},{"poster":"KC_go_reply","comment_id":"956270","content":"Selected Answer: C\nA + B: TPU doesn't support custom TensorFlow ops\nThen it says 'decrease training time significantly' and literally 'use accelerator'. Therefore, use GPU -> C, *not* D!","timestamp":"1689745920.0","upvote_count":"3"},{"timestamp":"1689033900.0","upvote_count":"5","comment_id":"948478","poster":"ZZHZZH","content":"Selected Answer: C\nD shouldn't be the answer b/c the question statement clearly said you should use accelerators."},{"upvote_count":"4","timestamp":"1688975820.0","content":"Selected Answer: C\nAnswer is C\nUse Cloud GPUs after implementing GPU kernel support for your customs ops.\n\nTPU support Models with no custom TensorFlow operations inside the main training loop so Option-A and B are eliminated as question says that 'These ops are used inside your main training loop'\nNow choices remain 'C' & 'D'. CPU is for Simple models that do not take long to train. Since question says that currently its taking up to several days to train a model and hence existing infra may be CPU and taking so many days. GPUs are for \"Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\" as question says that model is dominated by TensorFlow ops leading to correct option as 'C'\n\nReference:\nhttps://cloud.google.com/tpu/docs/tpus\nhttps://www.tensorflow.org/guide/create_op#gpu_kernels","poster":"Qix","comment_id":"947881"},{"upvote_count":"5","content":"Selected Answer: C\nC. Use Cloud GPUs after implementing GPU kernel support for your customs ops.\n\nSince your model relies on custom C++ TensorFlow ops, using Cloud TPUs without any code adjustment (option A) would not be feasible, as TPUs might not support these custom operations. To significantly decrease training time while keeping costs low, you should use Cloud GPUs. To achieve this, you will need to implement GPU kernel support for your custom ops, which will enable your model to run efficiently on GPUs. Once the GPU kernel support is added, you can leverage the power of GPUs on Google Cloud to speed up","poster":"lucaluca1982","comment_id":"850821","timestamp":"1679818500.0"},{"comment_id":"850814","content":"Selected Answer: C\nC GPU support custom option","timestamp":"1679818020.0","upvote_count":"2","poster":"lucaluca1982"},{"poster":"midgoo","content":"Selected Answer: B\nThe correct answer is: B. Use Cloud TPUs after implementing GPU kernel support for your customs ops.\n\nThe model is dominated by custom C++ TensorFlow ops, so it will not run on Cloud TPUs or GPUs without modification. Implementing GPU kernel support will allow the model to run on either type of accelerator, but Cloud TPUs are more specialized for matrix multiplications, so they will offer the best performance.\n\nCloud TPUs are also more cost-effective than Cloud GPUs, so they are the best option for reducing the cost of training the model.\n\nStaying on CPUs and increasing the size of the cluster would be the most expensive option, and it would not offer the same performance benefits as using Cloud TPUs.","comment_id":"847973","upvote_count":"3","timestamp":"1679559960.0"},{"timestamp":"1679315940.0","upvote_count":"2","poster":"juliobs","content":"Selected Answer: C\nAnswer C: \"image recognition domain\", \"bulky matrix multiplications\", \"accelerator\", \"several days to train a model\". All screams for GPUs.","comment_id":"844867"},{"comment_id":"820850","poster":"musumusu","upvote_count":"1","timestamp":"1677265320.0","content":"Answer C:\nWhy not D: its already taking several days on CPUs, cmon, Time for parallel processing and this is GPUs concept. \nWhy not B: However its the fastest approach for tensorflow work specially, but we need to keep the cost lowest. \nOption C will reduce the time and overall cost."},{"upvote_count":"8","timestamp":"1670066940.0","comments":[{"upvote_count":"4","poster":"odacir","comment_id":"738835","content":"But GPU is not a step between CPU and TPU in cost and performance? Why not use GPU in this case?","timestamp":"1670491920.0"},{"poster":"AzureDP900","timestamp":"1672458720.0","content":"D is right","comment_id":"762464","upvote_count":"1"},{"content":"And how on earth is a 'CPU' an accelerator? Do you even know what that means?","poster":"KC_go_reply","upvote_count":"1","comment_id":"929441","timestamp":"1687348620.0"}],"comment_id":"734390","poster":"zellck","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nCloud TPUs are optimized for specific workloads. In some situations, you might want to use GPUs or CPUs on Compute Engine instances to run your machine learning workloads. In general, you can decide what hardware is best for your workload based on the following guidelines:\nCPUs\n- Models that are dominated by custom TensorFlow operations written in C++"},{"comment_id":"703861","upvote_count":"9","poster":"Azlijaffar","timestamp":"1666701780.0","content":"CPUs\n-Quick prototyping that requires maximum flexibility\n-Simple models that do not take long to train\n-Small models with small effective batch sizes\n-Models that are dominated by custom TensorFlow operations written in C++\n-Models that are limited by available I/O or the networking bandwidth of the host system\nGPUs\n-Models for which source does not exist or is too onerous to change\n-Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\n-Models with TensorFlow ops that are not available on Cloud TPU (see the list of available TensorFlow ops)\n-Medium-to-large models with larger effective batch sizes\nTPUs\n-Models dominated by matrix computations\n-Models with no custom TensorFlow operations inside the main training loop\n-Models that train for weeks or months\n-Larger and very large models with very large effective batch sizes","comments":[{"comments":[{"comment_id":"753820","poster":"Prakzz","upvote_count":"1","timestamp":"1671765120.0","content":"That's a different question in this series. This is a diffferent question/scenario"}],"timestamp":"1667142660.0","upvote_count":"3","poster":"Azlijaffar","comment_id":"707882","content":"Hi. I just did the exam. A similar question came up. It said that the model partially runs on custom TensorFlow ops. They need to speed up the time it takes for the model to complete its run. What type of accelerators to use etc. I chose GPUs as supported above. Hope this helps."},{"upvote_count":"2","comment_id":"703863","content":"-Use TPUs if model takes weeks or months. This model only takes days. Since model is DOMINATED by custom TF ops in C++, can stick with CPUs. Ans should be D.","poster":"Azlijaffar","timestamp":"1666701840.0"}]},{"upvote_count":"3","poster":"AmirNaik204","comment_id":"650592","timestamp":"1661231640.0","content":"Vote for C. GPU as the question says \"performing bulky matrix multiplications\" and \"low cost\""},{"upvote_count":"3","comment_id":"632917","timestamp":"1658130420.0","poster":"Pime13","content":"Selected Answer: D\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus"},{"content":"Selected Answer: D\nhttps://cloud.google.com/tpu/docs/tpus\n\nCPUs\n Models that are dominated by custom TensorFlow operations written in C++\nCloud TPUs are not suited to the following workloads:\n Neural network workloads that contain custom TensorFlow operations written in C++. \n Specifically, custom operations in the body of the main training loop are not suitable for \n TPUs.","poster":"VGalan","comment_id":"601477","timestamp":"1652514420.0","upvote_count":"2"},{"upvote_count":"1","content":"Selected Answer: D\nD according to documentation","poster":"wences","timestamp":"1648699200.0","comment_id":"578649","comments":[{"content":"Which documentation, please share the reference.","comment_id":"584310","timestamp":"1649694360.0","upvote_count":"2","poster":"tavva_prudhvi"}]},{"upvote_count":"1","timestamp":"1648214460.0","comment_id":"575038","content":"B: TPU: performing bulky matrix multiplications\nGPU: support CPP","poster":"PJG_worm"},{"timestamp":"1646139300.0","upvote_count":"1","comment_id":"558737","poster":"Aslkdup","content":"Answer is B.\nTPUs are optimized to perform fast, bulky matrix multiplication and developed model is using bulky matrix multiplication."},{"poster":"AM1O593","content":"Selected Answer: D\nD: CPU is recommended for models that use custom TensorFlow operations written in C++ [Dan Sullivan]","upvote_count":"5","comment_id":"550076","timestamp":"1645168140.0"},{"upvote_count":"3","timestamp":"1642441860.0","comment_id":"525978","poster":"Bhawantha","content":"D. \n\"Cloud TPUs are not suited to Neural network workloads that contain custom TensorFlow operations written in C++. Specifically, custom operations in the body of the main training loop are not suitable for TPUs.\"\nbecause TPUs are not suited for this application.\n\nThe battle between GPU vs CPU.\nSpecially mentioned that the C++ part in the description. Since we can go with CPU."},{"upvote_count":"1","poster":"eagle_fang","comment_id":"524411","content":"D \nhttps://cloud.google.com/tpu/docs/tpus","timestamp":"1642278000.0"},{"poster":"MaxNRG","timestamp":"1641740760.0","comment_id":"520269","upvote_count":"8","content":"Selected Answer: D\nD:\nhttps://cloud.google.com/tpu/docs/tpus\nCloud TPUs are optimized for specific workloads. In some situations, you might want to use GPUs or CPUs on Compute Engine instances to run your machine learning workloads. In general, you can decide what hardware is best for your workload based on the following guidelines:","comments":[{"poster":"MaxNRG","content":"CPUs:\n• Quick prototyping that requires maximum flexibility\n• Simple models that do not take long to train\n• Small models with small effective batch sizes\n• Models that are dominated by custom TensorFlow operations written in C++\n• Models that are limited by available I/O or the networking bandwidth of the host system","comment_id":"520270","upvote_count":"4","timestamp":"1641740760.0","comments":[{"upvote_count":"3","comment_id":"520271","timestamp":"1641740820.0","content":"GPUs:\n• Models that are not written in TensorFlow or cannot be written in TensorFlow\n• Models for which source does not exist or is too onerous to change\n• Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\n• Models with TensorFlow ops that are not available on Cloud TPU (see the list of available TensorFlow ops)\n• Medium-to-large models with larger effective batch sizes","comments":[{"comment_id":"520272","content":"TPUs:\n• Models dominated by matrix computations\n• Models with no custom TensorFlow operations inside the main training loop\n• Models that train for weeks or months\n• Larger and very large models with very large effective batch sizes\nTPUs are optimized to perform fast, bulky matrix multiplication.\nCloud TPUs are not suited to Neural network workloads that contain custom TensorFlow operations written in C++. Specifically, custom operations in the body of the main training loop are not suitable for TPUs.\nhttps://cloud.google.com/tpu/docs/tpus","timestamp":"1641740820.0","poster":"MaxNRG","upvote_count":"3"}],"poster":"MaxNRG"}]}]},{"poster":"medeis_jar","upvote_count":"2","comment_id":"519509","timestamp":"1641644640.0","content":"Selected Answer: D\nOnly D -> https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus"},{"poster":"kishanu","upvote_count":"3","comment_id":"506298","timestamp":"1640106780.0","content":"Selected Answer: B\nConfused between B and D. Refer to the below link\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nCustom Tensorflow operations - CPUs\nBulky Matrix calc - TPUs"},{"upvote_count":"4","comment_id":"491531","timestamp":"1638353820.0","content":"Selected Answer: C\nI vote C, to know why see: https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\n- CPUs\n -- Quick prototyping that requires maximum flexibility\n -- Simple models that do not take long to train\n\n- GPUs\n -- Models for which source does not exist or is too onerous to change\n -- Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\n\n- TPUs\n -- Models with no custom TensorFlow operations inside the main training loop","comments":[{"content":"Just copy pasting the options doesn't mean it's C, as in the same link there;'s also a statement saying about custom Tensorflow operations written in C++ in CPU section. Pleas explain about that!","comment_id":"576079","timestamp":"1648370820.0","poster":"tavva_prudhvi","upvote_count":"1"}],"poster":"StefanoG"},{"comment_id":"455060","content":"It could only be D. https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nModels that are dominated by custom TensorFlow operations written in C++","comments":[{"upvote_count":"1","poster":"squishy_fishy","timestamp":"1634481600.0","content":"The answer is C. \nhttps://www.tensorflow.org/guide/create_op","comment_id":"463573"}],"timestamp":"1633023720.0","upvote_count":"4","poster":"Chelseajcole"},{"upvote_count":"3","poster":"Ysance_AGS","timestamp":"1632900420.0","comment_id":"453912","content":"C seem to be correct => TPU doesn't work on custom Tensorflow C++ so Not A or B,\nD will take more time"},{"comment_id":"428860","timestamp":"1629567660.0","poster":"bruno1942","upvote_count":"5","content":"Cpu are recommended for the following: Models that heavivly use custom TensorFlow operation witten in c++. Page 233 Dan Sullivan."},{"upvote_count":"2","poster":"EricM22","content":"Should Be C:\nwe have a significant number of custom TensorFlow operations in main training loop","comment_id":"420389","timestamp":"1628180700.0"},{"poster":"sumanshu","comments":[{"content":"Could be 'D' as well because we are increasing cluster and CPU suggested for Models that are dominated by custom TensorFlow operations written in C++","comment_id":"397538","upvote_count":"2","timestamp":"1625312460.0","poster":"sumanshu","comments":[{"upvote_count":"4","poster":"sumanshu","comments":[{"timestamp":"1627927020.0","content":"Did you pass this exam?","comment_id":"418873","upvote_count":"3","poster":"GCP_Guru"}],"content":"Selected - D","comment_id":"397542","timestamp":"1625312520.0"}]}],"content":"vote for C\n\nA - wrong\nB - elimnated (cost will be high for TPU also TPU suggested for Models with no custom TensorFlow operations inside the main training loop), but here custome ops are inside main training loop.\nC - correct (time decreases on GPU and cost also not very high)\nD - eleminated ( it wil take more time for calculations and we need to decrease time)","timestamp":"1625312280.0","comment_id":"397534","upvote_count":"5"},{"upvote_count":"4","comment_id":"352993","timestamp":"1620560820.0","content":"C \nkeep the cost low by using an accelerator on Google Cloud\nTPU - Eliminated \nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nModels with a significant number of custom TensorFlow operations that must run at least partially on CPUs","poster":"userd83"},{"comment_id":"325066","poster":"Sumanth09","content":"Should be : A\n(From Dan Sullivan)\nGraphic processing units are accelerators that have multiple\narithmetic logic units (ALUs) that implement adders and multipliers. This architecture is\nwell suited to workloads that benefit from massive parallelization, such as training deep\nlearning models. GPUs and CPUs are both subject to the von Neumann bottleneck, which\nis the limited data rate between a processor and memory, and slow processing. TPUs are\nspecialized accelerators based on ASICs and created by Google to improve training of deep\nneural networks. These accelerators are designed for the TensorFlow framework. TPUs\nreduces the impact of the von Neumann bottleneck by implementing matrix multiplication\nin the processor.","timestamp":"1617193380.0","upvote_count":"2"},{"comment_id":"302748","upvote_count":"5","timestamp":"1614789420.0","content":"D\n\nCPUs: Models that are dominated by custom TensorFlow operations written in C++\n\nsource: https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus","poster":"gcper"},{"comment_id":"293748","content":"B:\nhttps://cloud.google.com/tpu/docs/tpus","timestamp":"1613684640.0","upvote_count":"1","poster":"daghayeghi"},{"comment_id":"287602","upvote_count":"3","poster":"bobby8521","timestamp":"1612968840.0","content":"I will got with C.\nTPUs- anyway its not supported with custom Tensorflow Operations\nGPUs- Can support\nCPU's - Will support\nConsidering it is taking serval days to run, best way is to increase to GPU \n\nhttps://cloud.google.com/tpu/docs/tpus\nhttps://www.tensorflow.org/guide/create_op#gpu_kernels"},{"content":"Correct-C\nTPU support Models with no custom TensorFlow operations inside the main training loop so Option-A and B are eliminated as question says that 'These ops are used inside your main training loop'\nNow choices remain 'C' & 'D'. CPU is for Simple models that do not take long to train. Since question says that currently its taking up to several days to train a model and hence existing infra may be CPU and taking so many days. GPUs are for \"Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\" as question says that model is dominated by TensorFlow ops leading to correct option as 'C'","poster":"AnilKr","timestamp":"1610944620.0","upvote_count":"3","comment_id":"270023"},{"timestamp":"1607522580.0","content":"https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nModels that are dominated by custom TensorFlow operations written in C++ -> CPU \nso answer is D","comments":[{"poster":"Jlozano","upvote_count":"1","comment_id":"508427","content":"Also: \"Simple models that do not take long to train\"","timestamp":"1640342100.0"}],"poster":"VM_GCP","comment_id":"239254","upvote_count":"1"},{"upvote_count":"1","comment_id":"238743","timestamp":"1607467500.0","content":"https://cloud.google.com/tpu/docs/tpus D it is","poster":"beedle"},{"timestamp":"1605874740.0","poster":"federicohi","comment_id":"223567","content":"C i think. D doesnt becuase its cheaper but its asking to reduce much time and CPU is only for simple models","upvote_count":"4"},{"timestamp":"1602659640.0","comment_id":"199600","poster":"aleedrew","content":"Should be C as there is no documentation on GPU kernels working with TPUs. I can see that B might work as you can set up a custom pip package for the C++ code, but this would be much easier if it was Cloud GPU with GPU kernel. Here is the doc on creating custom ops: https://www.tensorflow.org/guide/create_op#compiling_the_kernel_for_the_gpu_device\nThe question also adds bulky matrix computations which TPU is great for...so IDK. Tricky question.","upvote_count":"2"},{"poster":"Twinkletoes","timestamp":"1601470560.0","content":"It should be D...For operations written in C++ CPUs are recommended by Google in their Official Guide\n```\nCPUs are recommended for the following:\n■■ Prototyping\n■■ Simple models that train relatively quickly\n■■ Models that heavily use custom TensorFlow operations written in C++\n■■ Models that are limited by available I/O or network bandwidth of the host server\n```","upvote_count":"4","comment_id":"190350"},{"timestamp":"1600860180.0","poster":"SteelWarrior","comment_id":"185215","upvote_count":"1","content":"Should be C as TensorFlow operations with C++ custom code are not supported with TPUs."},{"comment_id":"181462","content":"Definitely C:\nKeep the cost Low and using an accelerator, considering these 2 main requirements","timestamp":"1600414020.0","poster":"shashankraj","upvote_count":"3"},{"content":"TPUs are optimized to perform fast, bulky matrix multiplication, Models that train for weeks or months , I will go for A","timestamp":"1599153120.0","poster":"rajnishd","upvote_count":"2","comment_id":"172818"},{"poster":"haroldbenites","upvote_count":"3","content":"I'm not sure , but I prefer the D.","comment_id":"163131","timestamp":"1598035680.0"},{"content":"C\nStaying on CPU may/may not reduce time\nGPU could be the answer as \"Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\"","comment_id":"157683","timestamp":"1597363740.0","comments":[{"comment_id":"157691","content":"also, you need hardware accelerator as per question. GPUs and TPUs (and not CPUs) are accelerators AFAIK","upvote_count":"2","timestamp":"1597364040.0","poster":"FARR"}],"poster":"FARR","upvote_count":"4"},{"comment_id":"148316","content":"answer is D, \"Model that heavily use custome Tensor flow operation written in C++\"","upvote_count":"1","poster":"Archy","timestamp":"1596248280.0"},{"content":"C\nhttps://stackoverflow.com/questions/50799510/how-to-run-custom-gpu-tensorflowop-from-c-code","timestamp":"1593354000.0","comment_id":"121914","poster":"norwayping","upvote_count":"3"},{"timestamp":"1592806860.0","upvote_count":"2","poster":"AJKumar","content":"C eliminated right away, A and B are both Cloud TPU's-which are not supported for C++. Answer D.","comment_id":"116076"},{"comment_id":"80086","poster":"arnabbis4u","content":"It should be D. Models dominated by Custom Tensorflow operations should be trained using CPUs.\nDescription : It cannot be B, because TPU does not support Custom Tensorflow operations.","upvote_count":"6","timestamp":"1587947520.0"},{"comment_id":"70297","timestamp":"1585789140.0","poster":"Rajokkiyam","comments":[{"poster":"droogie","comment_id":"126383","timestamp":"1593894480.0","upvote_count":"3","content":"D is NOT the correct answer \"You want to decrease this time significantly and keep the cost low by using an accelerator on Google Cloud\"","comments":[{"comments":[{"timestamp":"1605822780.0","upvote_count":"2","comment_id":"223106","content":"GPU kernel can be implemented as well: https://www.tensorflow.org/guide/create_op#gpu_kernels","poster":"snamburi3"}],"poster":"snamburi3","content":"I agree. also the emphasis is on accelerator, so maybe C","timestamp":"1605822540.0","comment_id":"223105","upvote_count":"2"}]}],"upvote_count":"4","content":"Answer D"},{"poster":"[Removed]","upvote_count":"3","comment_id":"66893","comments":[{"content":"https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus","timestamp":"1584871320.0","poster":"[Removed]","upvote_count":"2","comment_id":"66894"}],"content":"Should be B\nhttps://cloud.google.com/tpu/docs/tpus?hl=zh-tw#when_to_use_tpus","timestamp":"1584871260.0"}],"answer":"D","unix_timestamp":1584871260,"question_images":[],"isMC":true,"answer_images":[],"answer_description":"","answer_ET":"C","answers_community":["D (44%)","C (44%)","11%"]},{"id":"fRoubDLx2MxE5LdwDzuU","discussion":[{"upvote_count":"72","comments":[{"upvote_count":"4","timestamp":"1633149900.0","comment_id":"455853","comments":[{"poster":"hellofrnds","content":"So, answer should be C","upvote_count":"1","timestamp":"1633909920.0","comment_id":"460260","comments":[{"timestamp":"1649694240.0","upvote_count":"2","content":"If you training RMSE=0.2. and testing RMSE = 0.4, and we want the RMSE to be low as its the error, now is it overfitting or underfitting? think wisely!","comment_id":"584308","comments":[{"timestamp":"1660839480.0","poster":"alecuba16","comment_id":"648517","upvote_count":"1","content":"It's overfitting.\n\nOverfitting->low rmse in train / high accuracy-f1 score in train for classification.\n\nUnderfitting -> high rmse / low f1score or accuracy in train, you don't have to look into test set if there is an underfitting problem.","comments":[{"comment_id":"929114","upvote_count":"1","content":"But the question clearly states we have higher RMSE on the train than the test. So how would it be overfitting?","poster":"jfab","timestamp":"1687328640.0"}]},{"upvote_count":"2","comment_id":"929118","timestamp":"1687329300.0","poster":"jfab","content":"But in this scenario we'd have training RMSE = 0.4 & testing RMSE = 0.2 - you've not read the question properly"}],"poster":"tavva_prudhvi"}]},{"poster":"velliger","content":"High rmse: The model is underfitting the train data. To reduce overfitting, we increase the number of layers in the model or we change the type of layer.","upvote_count":"1","comments":[{"content":"*underfitting","poster":"velliger","upvote_count":"2","comment_id":"477452","timestamp":"1636808040.0"}],"timestamp":"1636807980.0","comment_id":"477451"},{"timestamp":"1670492820.0","content":"NO, its underfitting.","comment_id":"738853","upvote_count":"3","poster":"odacir"},{"poster":"jfab","comment_id":"929120","content":"\"Twice as high on the train\". So clearly means TRAINING error is twice as high vs testing. So underfitting","upvote_count":"3","timestamp":"1687329480.0"}],"poster":"hellofrnds","content":"@callumr , \"root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set.\" clearly means testing error is twice of training error. So, it is clearly overfitting. Isn't it?"},{"content":"Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data.","comments":[{"upvote_count":"2","comment_id":"1012032","content":"Wrong! This scenario indicates a case of underfitting. The RSME is twice as high on the training dataset compared to the test dataset, so the model is underfitting.","poster":"ckanaar","timestamp":"1695198120.0"}],"timestamp":"1691248260.0","upvote_count":"1","poster":"NeoNitin","comment_id":"973119"}],"timestamp":"1592655120.0","comment_id":"114761","poster":"Callumr","content":"This is a case of underfitting - not overfitting (for over fitting the model will have extremely low training error but a high testing error) - so we need to make the model more complex - answer is D"},{"comments":[],"comment_id":"66896","content":"should be D","poster":"[Removed]","timestamp":"1584871860.0","upvote_count":"20"},{"upvote_count":"1","comment_id":"1335644","poster":"samtestking","timestamp":"1735832760.0","content":"Selected Answer: B\nCould be B (data requirement for task is vague), but let's assume 100 million data points is enough and rule that out.\n\nIndication of overfitting is significantly better performance on training data compared to unseen data. Here we are told that the unseen data is performing significantly better which is the opposite of what we should see if it were overfitting. Rule out C.\n\nSymptoms of model underfitting is poor performance in BOTH training AND unseen data. While underfitting might be the issue, the more pressing concern is that the test set is clearly not representative of the overall data and could be skewed. This is further supported by the 90/10 split (academic/industry standard is 80/20 or 75/25 based on the Pareto principle: https://en.wikipedia.org/wiki/Pareto_principle). A 90/10 split would be useful if we were doing k-fold cross validation (https://machinelearningmastery.com/k-fold-cross-validation/), however there is no indication of such in the prompt.\n\nNote: The question does not explicitly say that the model is performing poorly/errors are significantly bad, just that the error is twice as high in the training set (they could both have low error values).\nSo whilst it could be a case of underfitting (D), the first step taken should be addressing the obviously problematic data representation by adjusting the train-test split (option A)."},{"upvote_count":"2","content":"Selected Answer: D\nIt is underfitting problem, which means that the used models is too easy.","comment_id":"1302551","timestamp":"1729790220.0","poster":"SamuelTsch"},{"comment_id":"1289423","timestamp":"1727352540.0","poster":"baimus","upvote_count":"1","content":"This is A. The key is that 90/10 is a weirdly small test set, that stood out to me straight away (I work professionally as a machine learning engineer and have the cert). Next tip, that everyone seems to be ignoring - this is not underfit OR overfit. The model outperforms on the TEST set, this is not a miswording. Test scores higher than train. The time you might expect to see this is if your test set is too small to be a representative sample, leading to unrepresentative results. Seeing as the question already set up this conclusion with the 90/10 thing, it's definitely A. None of the others (or indeed anything else) can address Test outperforming Train, and the conclusion of others below that this is due to a poorly worded question is a bizarre conclusion."},{"poster":"cuadradobertolinisebastiancami","comment_id":"1151063","timestamp":"1708008540.0","content":"Selected Answer: D\nUnderfitting scenario","upvote_count":"2"},{"timestamp":"1705070940.0","content":"Selected Answer: D\nIt is an underfitting situation - D","upvote_count":"2","comment_id":"1120858","poster":"Sofiia98"},{"comments":[{"comment_id":"1085691","upvote_count":"1","poster":"Kimich","content":"https://dooinnkim.medium.com/what-are-overfitting-and-underfitting-855d5952c0b6","timestamp":"1701490200.0"}],"poster":"Kimich","content":"Selected Answer: C\nShould be C\nC. Try out regularization techniques (e.g., dropout or batch normalization) to avoid overfitting:\n\nThis is a reasonable approach. Regularization techniques can help prevent overfitting, especially when the model shows a significantly higher error on the training set compared to the test set.\nD. Increase the complexity of your model (e.g., introducing an additional layer or increasing the size of vocabularies or n-grams):\n\nThis could potentially exacerbate the overfitting issue. Increasing model complexity without addressing overfitting concerns may lead to poor generalization on new data.","upvote_count":"2","comment_id":"1083273","timestamp":"1701246600.0"},{"upvote_count":"3","poster":"hallo","content":"Are the questions in this relevant for the new exam or are these all now outdated?","comment_id":"1076541","timestamp":"1700591220.0"},{"timestamp":"1700515980.0","content":"https://stats.stackexchange.com/questions/497050/how-big-a-difference-for-test-train-rmse-is-considered-as-overfit#:~:text=RMSE%20of%20test%20%3C%20RMSE%20of,is%20always%20overfit%20or%20underfit.\nRMSE of test > RMSE of train => OVER FITTING of the data.\nRMSE of test < RMSE of train => UNDER FITTING of the data.\nso for answer is D","comment_id":"1075856","poster":"pss111423","upvote_count":"1"},{"timestamp":"1699533900.0","poster":"steghe","content":"Underfitting models: In general High Train RMSE, High Test RMSE.\nOverfitting models: In general Low Train RMSE, High Test RMSE.\n\nhttps://daviddalpiaz.github.io/r4sl/regression-for-statistical-learning.html","upvote_count":"1","comment_id":"1066414"},{"comment_id":"1020904","upvote_count":"2","timestamp":"1696001580.0","poster":"ha1p","content":"I passed the exam today. I am pretty sure it is overfitting. Answer must be c"},{"upvote_count":"3","poster":"MULTITASKER","comment_id":"1014429","content":"Selected Answer: D\nRMSE is more on training. That means, model is not performing well on training dataset but performing well on testing dataset. This happens in the case of underfitting. So D.","timestamp":"1695410340.0"},{"content":"Selected Answer: D\nRMSE training = 2 x testing\nWhen training > testing, it is a case of underfitting\nHence D","upvote_count":"2","timestamp":"1694451360.0","poster":"[Removed]","comment_id":"1005007"},{"content":"chatGPT says option C","upvote_count":"1","comment_id":"1000173","poster":"pulse008","timestamp":"1693975380.0"},{"upvote_count":"2","poster":"stonefl","content":"Selected Answer: D\n\"root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set.\" means the RMSE of training set is two time of RMSE of test set, which indicates the training is not as good as test, then underfiting, so D.","comment_id":"995170","timestamp":"1693490520.0"},{"comment_id":"973117","content":"Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data.\n\nSo with dropout method we can overcome the overfitting so C is correct","poster":"NeoNitin","upvote_count":"1","timestamp":"1691248200.0"},{"timestamp":"1688824320.0","poster":"MoeHaydar","upvote_count":"2","content":"Selected Answer: D\nunderfitting","comment_id":"946529","comments":[]},{"poster":"neerajRathi","comment_id":"944469","timestamp":"1688633400.0","upvote_count":"1","content":"Selected Answer: C\nC sounds like a valid answer."},{"content":"Selected Answer: C\nIf the root-mean-squared error (RMSE) of a model is twice as high on the training set compared to the test set, it suggests that the model is overfitting.\n\nOverfitting occurs when a machine learning model becomes too specialized to the training data and fails to generalize well to new, unseen data. In this case, it means that the model is performing better on the test set, which represents new data, compared to the training set it was initially trained on.\n\nThe fact that the RMSE is higher on the training set implies that the model is struggling to accurately predict the training data points. This could be due to the model learning intricate details and noise from the training data, to the extent that it cannot generalize well to new examples.\n\nTo address this issue, you may consider employing regularization techniques or adjusting hyperparameters to reduce overfitting.","upvote_count":"1","comment_id":"929855","poster":"blathul","timestamp":"1687376040.0"},{"comment_id":"929446","comments":[],"poster":"KC_go_reply","content":"Selected Answer: D\nOverfitting = high performance on train split, low performance on test split\nIn this case, we have the opposite of that. Therefore, the model is actually underfit, and the performance on the test split is probably just coincidence.","upvote_count":"1","timestamp":"1687348860.0"},{"content":"Selected Answer: D\nUnderfitting","comment_id":"917312","upvote_count":"1","timestamp":"1686149280.0","comments":[{"upvote_count":"1","content":"Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data.","timestamp":"1691248320.0","poster":"NeoNitin","comment_id":"973123"}],"poster":"vaga1"},{"poster":"dataengineeruser34","content":"Selected Answer: C\nAnswer C","upvote_count":"1","comment_id":"908706","timestamp":"1685286420.0"},{"content":"Selected Answer: C\nTraining error is small and test error is big\" is an indication of overfitting.","poster":"shabfat","comment_id":"906531","upvote_count":"2","timestamp":"1685004600.0"},{"comment_id":"888226","upvote_count":"1","content":"it is D\nTraining dataset has many errors (RMSE twice as high). Test set - RMSE twice low (more correct). It tells about underfitting.\nSo, we need D. Increase the complexity of your model by, e.g., introducing an additional layer or increase sizing the size of vocabularies or n-grams used.\n\nOthers, who chose D, described it well. Read them also, and checked in internet how to differ overfitting and underf.","poster":"Oleksandr0501","comments":[],"timestamp":"1683095580.0"},{"upvote_count":"2","content":"The fact that the RMSE of the model is twice as high on the train set as on the test set suggests that the model is overfitting on the training data. This means that the model is too complex and is fitting the noise in the training data, rather than the underlying patterns in the data. To improve the performance of the model, we need to reduce overfitting.\n\nTherefore, the correct option is C","timestamp":"1681208940.0","poster":"muhusman","comment_id":"867189"},{"timestamp":"1679561700.0","upvote_count":"6","poster":"midgoo","content":"Selected Answer: C\nI have checked and verified that the answer is C. There is a trick here about overfit and underfit. Depends on how high the RMSE of training set comparing to test set, we may have different meaning.\n\nIf the RMSE is higher on the training set than on the test set, but only by a small amount, this usually means that the model is UNDERFITTING the data. This means that the model is not complex enough to learn the underlying patterns in the data. To improve the performance of the model, you can try to increase the complexity of the model, such as by adding more layers or neurons.\n\nWhen the RMSE is much higher on the training set than on the test set, it suggests that the model has OVERFITTED the data. An overfitted model has learned the noise from the training data, and it will not perform well on new data. To improve the performance of the model, you can try to decrease the complexity of the model, such as by removing some of the layers or neurons.","comments":[{"timestamp":"1682338980.0","content":"No. When the RMSE is much higher on the training set, it suggests exactly the opposite of what you said, since it means that the model is not capable of fully understand the training data. You have overfit when the RMSE is significantly lower on the training data than the test data, which is not our case. We can therefore boost model performance by making it more complex - answer D has this purpose.","comment_id":"879313","upvote_count":"1","poster":"tronunator"}],"comment_id":"847990"},{"content":"Answer D\nRMSE shows the variation from actual value and predictive value. If it is higher on RMSE, then you should definitely reject the model. In this case it is saying, RMSE is higher in training and lower in test. Which means model could be better trained and the guy was sleeping doing this work. \nSo, it means, it released the underfit model or poor model in simple term. So what you should do, to reduce RMSE, you increase complexity of model by adding more Neurons and Hidden layers. \n\nIf it was overfit, means very high RMSE in test while showing almost 0 during training, it means it was overfit. \nSo how you fix overfit, Feature Engineering, Regularisation, Hypertunning, dropout (reduce feature) .. bla bla bla","comment_id":"811895","upvote_count":"3","comments":[],"timestamp":"1676639760.0","poster":"musumusu"},{"poster":"jkh_goh","comment_id":"786312","timestamp":"1674548460.0","content":"Obviously the question meant to say \"error twice as high on the test set as on the train set\", thus overfitting.","upvote_count":"1"},{"comment_id":"786020","poster":"desertlotus1211","timestamp":"1674522120.0","content":"This will help in understanding overfitting vs. underfitting\nhttps://daviddalpiaz.github.io/r4sl/regression-for-statistical-learning.html","upvote_count":"1"},{"upvote_count":"1","poster":"Astrophile","comment_id":"763129","content":"Selected Answer: C\nThere are 100M labeled examples in the dataset and the train and test ratio is 90:10, hence the model is capturing all the noises with a pattern in the data. It is sufficient enough to indicate the overfitting the model as \"Training error is small and test error is big\" .","timestamp":"1672565940.0"},{"comment_id":"703881","poster":"Azlijaffar","timestamp":"1666702740.0","comments":[{"poster":"Azlijaffar","timestamp":"1666702800.0","upvote_count":"1","comment_id":"703883","content":"Answer should be D."},{"comment_id":"973128","timestamp":"1691248380.0","poster":"NeoNitin","upvote_count":"1","content":"Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data."}],"content":"The wording is tricky. \"RMSE twice as high on train set as on test set\" means Train RMSE is higher than Test RMSE. Means train data has more errors than test data. More errors = underfitting. \nFYI: i have twice as many apples as you means i have more apples than you. hahaha.","upvote_count":"3"},{"poster":"YorelNation","comment_id":"659988","timestamp":"1662369780.0","content":"I don't get how in an underfit context you can have twice the RMSE on the train set (meaning twice less on the test set ==> You have doubled performance on your testing set ?) The question seems very odd to me.","comments":[{"timestamp":"1666011180.0","upvote_count":"1","poster":"jkhong","content":"Many cases, it can just be a coincidence that your test data yields a better performance than your train, where your underfitted training data is just the right fit for this specific test data","comment_id":"697410"}],"upvote_count":"2"},{"timestamp":"1652518200.0","comment_id":"601499","poster":"VGalan","content":"Selected Answer: D\nThe lower the RMSE, the better a given model is able to “fit” a dataset \n\nOverfitting: Model perform well on train set and poorly on test set. In this case, model performs better in test set. Cant not be Overfitting.\nThe options B and C are for overfitting cases. \n\nWe need to improve the train perform so in my opinion, D is the correct answer","upvote_count":"2"},{"comment_id":"520273","content":"Selected Answer: D\nD:\nA is incorrect since test sample is large enough.\nB is incorrect since dataset is pretty large already, and having more data typically helps with overfitting and not with underfitting.\nC is incorrect since regularization helps to avoid overfitting and we have a clear underfitting case.\nD is correct since increasing model complexity generally helps when you have an underfitting problem.","comments":[{"timestamp":"1641740940.0","upvote_count":"1","comment_id":"520274","content":"https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting\nhttps://towardsdatascience.com/deep-learning-3-more-on-cnns-handling-overfitting-2bd5d99abe5d\nhttps://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization\nhttps://developers.google.com/machine-learning/crash-course/training-neural-networks/best-practices","poster":"MaxNRG"}],"upvote_count":"6","poster":"MaxNRG","timestamp":"1641740940.0"},{"content":"Can you add the link to say is underfit or overfit. I understand that is overfit because in the training set there are not significant error but there are in the testing set. That means that there are noise in the training data set.","upvote_count":"1","comment_id":"491752","poster":"Pupina","timestamp":"1638370920.0"},{"content":"\"Training error is small and test error is big\" is an indication of overfitting. Answer is C:\n\nDont add more complexity.","comment_id":"484738","timestamp":"1637640960.0","upvote_count":"3","poster":"lifebegins"},{"comment_id":"484736","timestamp":"1637640540.0","content":"RMSE of test > RMSE of train => OVER FITTING of the data.\nRMSE of test < RMSE of train => UNDER FITTING of the data.","upvote_count":"6","poster":"lifebegins"},{"poster":"sumanshu","content":"Vote for D","comment_id":"397557","timestamp":"1625314260.0","upvote_count":"6"},{"content":"question says 100M labels and saying high error on testing data - if it's Underfitting model, model might get high error on train data.\n1) Testing data might not representing the training data. if it is the case, need to add more data \n2) Error might be due to more labels. if it is the case, need to apply regularization\nAnswer might be B or C","comment_id":"325077","poster":"Sumanth09","timestamp":"1617194220.0","upvote_count":"3"},{"comment_id":"224763","timestamp":"1606023240.0","poster":"arghya13","content":"Problem of underfitting..D is the correct answer","upvote_count":"2"},{"content":"D. this question is in the sample exam and the correct answer shows as D.","timestamp":"1605823380.0","poster":"snamburi3","upvote_count":"8","comment_id":"223111"},{"timestamp":"1604760300.0","comments":[{"timestamp":"1672461900.0","poster":"AzureDP900","content":"Agreed","upvote_count":"1","comment_id":"762465"}],"upvote_count":"4","content":"Underfitting does not explain the higher test score. If the model is not complex enough it will also perform worse on the test data. A seems the only correct answer explaining the error differences between the sets.","poster":"JulesT","comment_id":"214682"},{"upvote_count":"2","timestamp":"1603678680.0","content":"should be D","comment_id":"205966","poster":"Athanasia"},{"content":"D is correct, as this is the case of underfitting.","timestamp":"1600924080.0","comment_id":"185881","upvote_count":"2","poster":"SteelWarrior"},{"upvote_count":"3","content":"C \nIf the RMSE for the test set is much higher than that of the training set, it is likely that you've badly over fit the data","timestamp":"1600314600.0","poster":"VIncent9261111","comment_id":"180663"},{"content":"This is a question from Google practice exam and according to Google it's D","comment_id":"180662","upvote_count":"5","timestamp":"1600313880.0","poster":"Tanmoyk"},{"poster":"Anonymous999","upvote_count":"3","timestamp":"1599775980.0","comment_id":"177335","content":"D is the right answer"},{"content":"C is correct.\nIt's overfiting.","comment_id":"163139","poster":"haroldbenites","upvote_count":"3","timestamp":"1598036940.0"},{"timestamp":"1594142820.0","upvote_count":"4","comment_id":"129160","poster":"dg63","content":"D - high RMSE indicates underfitting"},{"comment_id":"116075","poster":"AJKumar","upvote_count":"11","content":"small RMS Error means--overfitting--fits well--so make it complex by dropping features.\nbig RMS Error means--underfitting--not good fit--so increase complexity by adding layers/features. Answer D.","timestamp":"1592806680.0"},{"timestamp":"1588026900.0","poster":"arnabbis4u","comment_id":"80511","content":"Correct answer should be C. Dropout regularization method helps in overfitting problems. \nhttps://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e\nD is not correct because, to reduce overfitting you should try to make your models less complex. Adding complexity will not help.","upvote_count":"4"}],"answers_community":["D (63%)","C (35%)","3%"],"answer":"D","question_images":[],"answer_images":[],"unix_timestamp":1584871860,"exam_id":10,"question_text":"You work on a regression problem in a natural language processing domain, and you have 100M labeled examples in your dataset. You have randomly shuffled your data and split your dataset into train and test samples (in a 90/10 ratio). After you trained the neural network and evaluated your model on a test set, you discover that the root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set. How should you improve the performance of your model?","timestamp":"2020-03-22 11:11:00","isMC":true,"answer_ET":"D","topic":"1","question_id":33,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/17238-exam-professional-data-engineer-topic-1-question-128/","choices":{"B":"Try to collect more data and increase the size of your dataset.","A":"Increase the share of the test sample in the train-test split.","D":"Increase the complexity of your model by, e.g., introducing an additional layer or increase sizing the size of vocabularies or n-grams used.","C":"Try out regularization techniques (e.g., dropout of batch normalization) to avoid overfitting."}},{"id":"IkAhZXqpPwcQCHsM8IU8","answer_description":"","timestamp":"2020-03-22 11:14:00","question_images":[],"answer":"D","choices":{"D":"Organize your data in separate tables for each month, and use snapshot decorators to restore the table to a time prior to the corruption.","C":"Organize your data in separate tables for each month, and duplicate your data on a separate dataset in BigQuery.","A":"Organize your data in a single table, export, and compress and store the BigQuery data in Cloud Storage.","B":"Organize your data in separate tables for each month, and export, compress, and store the data in Cloud Storage."},"question_text":"You use BigQuery as your centralized analytics platform. New data is loaded every day, and an ETL pipeline modifies the original data and prepares it for the final users. This ETL pipeline is regularly modified and can generate errors, but sometimes the errors are detected only after 2 weeks. You need to provide a method to recover from these errors, and your backups should be optimized for storage costs. How should you organize your data in BigQuery and store your backups?","question_id":34,"exam_id":10,"url":"https://www.examtopics.com/discussions/google/view/17239-exam-professional-data-engineer-topic-1-question-129/","isMC":true,"unix_timestamp":1584872040,"answer_ET":"D","topic":"1","discussion":[{"poster":"[Removed]","comment_id":"66898","timestamp":"1584872040.0","upvote_count":"22","content":"Should be B"},{"comment_id":"74274","poster":"Ganshank","content":"B\nThe questions is specifically about organizing the data in BigQuery and storing backups.","upvote_count":"12","timestamp":"1586818440.0"},{"content":"Selected Answer: D\nWith Storage Decorators, BigQuery only stores the differences between a snapshot and its base table, minimizing storage costs.","comment_id":"1342831","upvote_count":"1","poster":"grshankar9","timestamp":"1737252240.0"},{"upvote_count":"2","timestamp":"1733028360.0","poster":"cloud_rider","content":"Selected Answer: D\nD is the most cost optimized solution to keep the backup. please read the link - https://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots","comment_id":"1320453"},{"poster":"SamuelTsch","timestamp":"1729790640.0","content":"Selected Answer: D\nI think D is better.","upvote_count":"2","comment_id":"1302560"},{"comment_id":"1241268","timestamp":"1719994200.0","upvote_count":"2","poster":"Lenifia","content":"Selected Answer: D\nThe best option is D. Organize your data in separate tables for each month, and use snapshot decorators to restore the table to a time prior to the corruption."},{"poster":"zevexWM","comment_id":"1201256","content":"Selected Answer: D\nAnswer is D: \nSnapshots are different from time travel. They can hold data as long as we want.\nFurthermore \"BigQuery only stores bytes that are different between a snapshot and its base table\" so pretty cost effective as well.\n\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots","upvote_count":"2","timestamp":"1713954120.0"},{"poster":"Farah_007","comment_id":"1193160","content":"Selected Answer: B\nFrom : https://cloud.google.com/architecture/dr-scenarios-for-data#BigQuery\nIt can't be D\nIf the corruption is caught within 7 days, query the table to a point in time in the past to recover the table prior to the corruption using snapshot decorators.\nStore the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table. => D","upvote_count":"1","timestamp":"1712766540.0"},{"timestamp":"1697963460.0","poster":"Nirca","content":"Selected Answer: D\nD - this solution in integrated. No core is needed","comment_id":"1050364","upvote_count":"5"},{"timestamp":"1696230840.0","content":"90% of questions are having multiple answers and its very hard to get into every discussion where the conclusion is not there","poster":"Bahubali1988","comment_id":"1022902","upvote_count":"7"},{"comment_id":"1013074","upvote_count":"6","content":"Selected Answer: B\nThe answer is B: \n\nWhy not D? Because snapshot costs can become high if a lot of small changes are made to the base table: https://cloud.google.com/bigquery/docs/table-snapshots-intro#:~:text=Because%20BigQuery%20storage%20is%20column%2Dbased%2C%20small%20changes%20to%20the%20data%20in%20a%20base%20table%20can%20result%20in%20large%20increases%20in%20storage%20cost%20for%20its%20table%20snapshot.\n\nSince the question specifically states that the ETL pipeline is regularly modified, this means that lots of small changes are present. In combination with the requirement to optimize for storage costs, this means that option B is the way to go.","timestamp":"1695300000.0","poster":"ckanaar"},{"content":"Selected Answer: D\nkeyword: detected after 2 weeks.\nonly snapshot could resolve the problem.","poster":"arien_chen","timestamp":"1692508980.0","comment_id":"985562","upvote_count":"2"},{"upvote_count":"8","comment_id":"967780","poster":"Lanro","timestamp":"1690780860.0","content":"Selected Answer: D\nFrom BigQuery documentation - Benefits of using table snapshots include the following:\n\n- Keep a record for longer than seven days. With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\n- Minimize storage cost. BigQuery only stores bytes that are different between a snapshot and its base table, so a table snapshot typically uses less storage than a full copy of the table.\n\nSo storing data in GCS will make copies of data for each table. Table snapshots are more optimal in this scenario."},{"content":"Selected Answer: B\nOrganizing your data in separate tables for each month will make it easier to identify the affected data and restore it.\nExporting and compressing the data will reduce storage costs, as you will only need to store the compressed data in Cloud Storage.\nStoring your backups in Cloud Storage will make it easier to restore the data, as you can restore the data from Cloud Storage directly","upvote_count":"1","timestamp":"1690431660.0","comment_id":"964323","poster":"vamgcp"},{"comment_id":"920522","upvote_count":"2","timestamp":"1686473460.0","poster":"phidelics","comments":[{"upvote_count":"1","content":"Just an additional info!\nHere is an example for an export job;\n\n$ bq extract --destination_format CSV --compression GZIP 'your_project:your_dataset.your_new_table' 'gs://your_bucket/your_object.csv.gz'","timestamp":"1686557460.0","poster":"cetanx","comments":[{"poster":"cetanx","comment_id":"943585","content":"I will update my answer to D.\nThink of a scenario that you are in the last week of June and an error occurred 3 weeks ago (so still in June) however you do not have an export of the June table yet therefore you cannot recover the data simply because you don't have an export just yet.\n\nSo snapshots are way to go!","timestamp":"1688553660.0","upvote_count":"3"}],"comment_id":"921237"}],"content":"Selected Answer: B\nOrganize in separate tables and store in GCS"},{"timestamp":"1686318480.0","poster":"sdi_studiers","upvote_count":"3","content":"Selected Answer: D\nD\n\"With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\" [source: https://cloud.google.com/bigquery/docs/table-snapshots-intro]","comment_id":"919374"},{"content":"\"Store your data in different tables for specific time periods. This method ensures that you need to restore only a subset of data to a new table, rather than a whole dataset.\"\n\n\"Store the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table.\"\n\nB","comment_id":"916987","timestamp":"1686124020.0","upvote_count":"2","poster":"WillemHendr"},{"comment_id":"841595","upvote_count":"3","poster":"lucaluca1982","timestamp":"1679029320.0","content":"Why not D?"},{"comment_id":"734381","upvote_count":"1","content":"Selected Answer: B\nB is the answer.","timestamp":"1670066400.0","poster":"zellck"},{"comment_id":"676871","timestamp":"1663917480.0","upvote_count":"2","content":"Selected Answer: B\nB\nhttps://cloud.google.com/architecture/dr-scenarios-for-data#BigQuery","poster":"John_Pongthorn"},{"upvote_count":"8","content":"Selected Answer: B\nB seems the best solution (but C is also good candidate)\nD is incorrect - table decorators allow time travel back only up to 7 days (see https://cloud.google.com/bigquery/table-decorators) - if you want to keep older snapshots, you would have to save them into separate table yourself (and pay for storage).","timestamp":"1641741360.0","poster":"MaxNRG","comment_id":"520280","comments":[{"poster":"MaxNRG","upvote_count":"3","comments":[{"comment_id":"520284","poster":"MaxNRG","upvote_count":"3","content":"BigQuery is replicated, but this won't help with corruption in your tables. Therefore, you need to have a plan to be able to recover from that scenario. For example, you can do the following:\n• If the corruption is caught within 7 days, query the table to a point in time in the past to recover the table prior to the corruption using snapshot decorators.\n• Export the data from BigQuery, and create a new table that contains the exported data but excludes the corrupted data.\n• Store your data in different tables for specific time periods. This method ensures that you will need to restore only a subset of data to a new table, rather than a whole dataset.\n• Store the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table.\nhttps://cloud.google.com/solutions/dr-scenarios-for-data#BigQuery","timestamp":"1641741480.0"}],"timestamp":"1641741480.0","content":"BigQuery. If you want to archive data, you can take advantage of BigQuery's long term storage. If a table is not edited for 90 consecutive days, the price of storage for that table automatically drops by 50 percent. There is no degradation of performance, durability, availability, or any other functionality when a table is considered long term storage. If the table is edited, though, it reverts back to the regular storage pricing and the 90 day countdown starts again.","comment_id":"520283"}]},{"content":"Selected Answer: B\n\"You need to provide a method to recover from these errors, and your backups should be optimized for storage costs\"\nCost -> GCS\nBackups -> Separate Tables + GCS","upvote_count":"4","comment_id":"519512","comments":[{"upvote_count":"1","comment_id":"762466","poster":"AzureDP900","content":"Agreed","timestamp":"1672459080.0"}],"poster":"medeis_jar","timestamp":"1641645180.0"},{"timestamp":"1637642220.0","poster":"lifebegins","content":"https://cloud.google.com/bigquery/docs/time-travel\n\nAnswer should be B only, because snapshot decorator u can time travel only for 7 days","comment_id":"484743","upvote_count":"3"},{"poster":"vintop95","comment_id":"463486","upvote_count":"4","content":"B:\nStore your data in different tables for specific time periods. This method ensures that you will need to restore only a subset of data to a new table, rather than a whole dataset.\nhttps://cloud.google.com/architecture/dr-scenarios-for-data#managed-database-services-on-gcp","comments":[{"poster":"Mcloudgirl","timestamp":"1668103920.0","upvote_count":"2","content":"Thanks for explaining this - I previously was unsure why separate tables was necessary.","comment_id":"715446"}],"timestamp":"1634461080.0"},{"upvote_count":"3","comment_id":"453922","timestamp":"1632901560.0","poster":"Ysance_AGS","content":"B since the question is about saving costs when storing backups : use Cloud Storage."},{"comment_id":"413507","poster":"raf2121","upvote_count":"1","content":"Point for discussion \n\nIs A an Option ? One of the information is \"BQ is Centralized analytics Platform\" - Will not be good to store the data in single table rather separate table for each month (for example, if One needs to analyze a trend for last two years, will not be complicated to have 24 tables in the query)","timestamp":"1627168980.0"},{"upvote_count":"3","content":"why not A","poster":"p111111111111","timestamp":"1625564220.0","comment_id":"399817"},{"upvote_count":"1","comment_id":"397566","content":"A & B - Not sure (what to choose)\nC - elimnated (because of cost)\nD - eliminated (because of error detected in 2 weeks)","poster":"sumanshu","timestamp":"1625314920.0"},{"content":"How about C?\nhttps://medium.com/google-cloud/how-to-backup-google-big-query-5f078138cedc\nSounds like alternative 2 in above article","timestamp":"1621912680.0","comments":[{"content":"Because \"your backups should be optimized for storage costs\"","timestamp":"1622819520.0","comment_id":"374445","upvote_count":"2","poster":"z8zhong"}],"poster":"shanjin14","upvote_count":"1","comment_id":"366066"},{"upvote_count":"4","comment_id":"285960","poster":"someshsehgal","content":"Why nobody supporting C. Data management will be easy with data reside only within BQ. Also long term storage cost in BQ is almost similar like GCS","timestamp":"1612758840.0"},{"content":"Should be B\nUsing snapshot decorators , recovery is valid only for a period of 7 days. Here it says 2 weeks so \"D\" is ruled out.\n\nYou can undelete a table within seven days of deletion, including explicit deletions and implicit deletions due to table expiration. After seven days, it is not possible to undelete a table using any method, including opening a support ticket. \nhttps://cloud.google.com/bigquery/docs/managing-tables","comment_id":"245667","timestamp":"1608130440.0","upvote_count":"8","poster":"ABM9"},{"comment_id":"199603","timestamp":"1602660120.0","poster":"aleedrew","upvote_count":"2","content":"It says sometimes it is discovered in two weeks, not all of it is discovered in two weeks. I am inclined to choose D."},{"timestamp":"1601388720.0","upvote_count":"7","comment_id":"189719","comments":[{"content":"The link you provided supports B!","upvote_count":"2","poster":"patitonav","comment_id":"399985","timestamp":"1625575740.0"}],"poster":"kino2020","content":"B\nBigQuery. If you want to archive data, you can take advantage of BigQuery's long term storage. If a table is not edited for 90 consecutive days, the price of storage for that table automatically drops by 50 percent. There is no degradation of performance, durability, availability, or any other functionality when a table is considered long term storage. If the table is edited, though, it reverts back to the regular storage pricing and the 90 day countdown starts again.\nhttps://cloud.google.com/solutions/dr-scenarios-for-data#managed-database-services-on-gcp"},{"content":"D\nYou can easily revert changes without having to request a recovery from backups If not deleted. (When a table is explicitly deleted, its history is flushed after 7 days.)","timestamp":"1598499420.0","comment_id":"167259","poster":"atnafu2020","upvote_count":"2"},{"content":"B is correct.","upvote_count":"5","poster":"haroldbenites","comment_id":"163141","timestamp":"1598037120.0"},{"comment_id":"129636","content":"Answer is A: single table is enough. Why should we go for monthly tables.","timestamp":"1594203540.0","poster":"SSV","comments":[{"comments":[{"content":"For a table that has Change data capture, is it even possible if data is maintained in separate monthly tables? I am having this question since question has this 'ETL pipeline modifies the original data and prepares it for the final users' this could refer to type 2 table operation. I am leaning towards A just because of this.","timestamp":"1641523920.0","comment_id":"518725","poster":"GCPLearning2021","upvote_count":"1"}],"comment_id":"183204","upvote_count":"3","timestamp":"1600624380.0","poster":"vakati","content":"Answer - B\n[A] would have been a good choice if the table was partitioned on timestamp. Since it's not explicitly mentioned, [B] is the correct option"},{"upvote_count":"7","content":"Monthly table is a good choice if you find the error two weeks later and need to restore the table, you don't want to restore the whole history data","poster":"dragon123","comments":[{"content":"A. you can restore only the desired ranges of data By partitions","upvote_count":"1","timestamp":"1633879920.0","comment_id":"460130","poster":"sergio6"}],"comment_id":"165317","timestamp":"1598282580.0"}],"upvote_count":"6"},{"poster":"dambilwa","comment_id":"119008","timestamp":"1593047820.0","upvote_count":"5","content":"Though Option [B] & [D] are closest, I think none of them are correct. Point-inTime recovery is valid only for a period of 7 days. For option [B] to be correct, new tables should be created & backed up on a daily basis as data is ingested / modified daily"},{"comment_id":"88559","content":"Correct B","poster":"arnabbis4u","upvote_count":"5","timestamp":"1589413860.0"},{"comment_id":"70298","comments":[{"content":"Decorators can only correct the data unto 7 days. Hence Option B","upvote_count":"10","comment_id":"103623","timestamp":"1591426860.0","poster":"Poojaji"}],"content":"Answer D. Point in Time decorators needed to correct the data that are older than 2 weeks.","upvote_count":"2","poster":"Rajokkiyam","timestamp":"1585789680.0"}],"answers_community":["D (52%)","B (48%)"],"answer_images":[]},{"id":"SLUrs6hYfVzidtoNmFwF","timestamp":"2020-03-11 18:22:00","answer_images":[],"answer":"D","question_text":"You want to process payment transactions in a point-of-sale application that will run on Google Cloud Platform. Your user base could grow exponentially, but you do not want to manage infrastructure scaling.\nWhich Google database service should you use?","choices":{"A":"Cloud SQL","D":"Cloud Datastore","C":"Cloud Bigtable","B":"BigQuery"},"exam_id":10,"answers_community":["D (57%)","A (35%)","8%"],"answer_ET":"D","unix_timestamp":1583947320,"question_id":35,"answer_description":"","discussion":[{"comment_id":"101332","content":"Initially, thinking D is the best answer but when question is re-re-read, A seems to be correct answer for following reasons\n1. Is payment TRANSACTION -- DB should able to perform full blown transaction (updating inventory, sales info etc, though not specified) , not just ATOMIC which DataStore provides\n2. Its point-of-sale application, not ONLINE STORE where HIGH number of concurrent users ordering stuff. \n3. User Base could grow exponentially - again more users does mot mean concurrent users and more processing power. Its only about storage.\n4. Do not want to Manage infrastructure scaling. - Cloud SQL can scale in terms of storage.\n5. CloudStore is poor selection for OLTP application \n - Each property is index - so higher latency\n \nNot sure, during exam 2 min is enough to think on various point.. \nI may be wrong or wrong path ... lets brainstrom..","poster":"DeepakKhattar","upvote_count":"77","comments":[{"poster":"canon123","timestamp":"1636211700.0","upvote_count":"10","comment_id":"473508","content":"CloudSql does not auto scale.","comments":[{"comments":[{"comments":[{"timestamp":"1666974060.0","content":"yes. and that is ok since this is a point of sale. an exponential increase in number of clients still means reduced parallel processing (how many customers can buy in the very same time) so an increase in memory and CPU is very unlikely to be necessary. yes, an exponential increase in the number of customers means more memory, and more storage, which in cloud SQL increases automatically.","comment_id":"706617","upvote_count":"3","poster":"MisuLava"}],"content":"That link explains how to set MySQL autoscaling with Google Compute Engine instances (you install and manage MySQL on the VM). This can not be applied to Cloud SQL (managed service). In Cloud SQL, only the storage can be automatically increased, and changing the Cloud SQL instance size requires a manual edit of the instance type.","timestamp":"1639708140.0","poster":"hendrixlives","comment_id":"503303","upvote_count":"4"},{"timestamp":"1664336580.0","comment_id":"681374","upvote_count":"2","content":"C SQL doesn't AUTO SCALE, you need to manually edit , Please show where does it says AUTO SCALING","poster":"nkunwar"}],"timestamp":"1638766260.0","upvote_count":"3","poster":"BigQuery","comment_id":"494899","content":"https://cloud.google.com/architecture/elastically-scaling-your-mysql-environment#objectives\n\nPlease read. It can be configured for autoscaling."}]},{"timestamp":"1629871260.0","upvote_count":"3","poster":"Blobby","content":"Can't online be considered PoS? CloudSQL does have constraints for scaling and Google seem to specifically be selling Datastore for transactional use cases so going with D: \nhttps://cloud.google.com/datastore/docs/concepts/transactions","comments":[{"poster":"Blobby","content":"Based on a re-read of the above comments and other later questions agree with A. \npls ignore my first answer.","upvote_count":"5","timestamp":"1631682540.0","comment_id":"445000"}],"comment_id":"431181"}],"timestamp":"1591145820.0"},{"comment_id":"62577","comments":[{"poster":"BigQuery","upvote_count":"2","content":"Cloud SQL does scale automatically. THERE IS A SETTING WHERE YOU DEFINE INCREASE MEMORY SPACE WHEN IT REACHED 70%. \n\nhttps://cloud.google.com/sql/docs/features#features_3\n\nHere it say's \n-> Fully managed SQL Server databases in the cloud.\n-> Custom machine types with up to 624 GB of RAM and 96 CPUs.\n-> Up to 64 TB of storage available, with the ability to automatically increase storage size as needed.","timestamp":"1638766020.0","comment_id":"494895","comments":[{"comment_id":"503306","poster":"hendrixlives","upvote_count":"5","timestamp":"1639708380.0","comments":[{"upvote_count":"2","comment_id":"739564","content":"I believe the key point is it's a POS, not an e-commerce. Keeping that in mind, exponential user increase in POS might not mean concurrent user increase, which could be a huge consideration in case of it is being e-commerce.\n\nI would rather go with 'Cloud SQL' as the best answer.","timestamp":"1670540220.0","poster":"imsaikat50"}],"content":"Storage scale is automatic (e.g. you begin with a 50GB disk and it grows automatically as needed), but the instance size (CPU/memory) will be the same. The questions states that the user base may increase exponentially. Even if you have enough disk space to store all your user data, the increase in users will cause problems if your instance (CPU/memory) is too small, since the instance will not be able to process all the queries at the required speed."}]}],"timestamp":"1583947320.0","poster":"jvg637","content":"D seems to be the right one. Cloud SQL doesn't automatically scale","upvote_count":"38"},{"content":"Selected Answer: D\nThe answer is D, Cloud Datastore (now Firestore in Datastore mode), because it supports auto scaling and low latency reads and writes. Cloud SQL is not the correct answer because it requires more active management for scaling.","poster":"cqrm3n","upvote_count":"1","timestamp":"1737179640.0","comment_id":"1342430"},{"poster":"GHill1982","content":"Selected Answer: C\nFor handling payment transactions in a point-of-sale application with potential exponential growth and without the need to manage infrastructure scaling, Cloud Bigtable would be the best choice.","upvote_count":"1","comment_id":"1301210","timestamp":"1729533780.0"},{"comment_id":"214051","upvote_count":"13","content":"D seems to be the answer. This is what I think based on my analysis below.\nPOS is OLTP system but now a days NOSQL with ACID properties also are used for OLTP,\nCloud sql is good for relational database and it would have been an option here but it clearly says that \"you do not want to manage infrastructure scaling\". In cloud SQL, which is managed service and not server less, you need to manually do vertical scaling(scale up and scale down). \nHence I believe CLOUD SQL is not the option here. \nI also tried creating a datastore using google cloud console and it gives 2 options now that is cloud firestore in native mode and cloud firestore in data store mode. automatic scaling is available in both where there is no manual scaling up or down is required. Also, both firestore in native and datastore provides ACID properties. Also, firestore is now optimized for OLTP. Please see below\nhttps://cloud.google.com/solutions/building-scalable-apps-with-cloud-firestore\nThough the question only talks about datastore, I am just providing additional information. \nConsidering all what I read through, D is the answer.","poster":"Radhika7983","comments":[{"upvote_count":"1","timestamp":"1625248320.0","poster":"awssp12345","comment_id":"397093","content":"I agree. I think people are missing the part of the question that mentions they don't want to maintain the DB."},{"comment_id":"397097","poster":"awssp12345","upvote_count":"1","timestamp":"1625248380.0","content":"This should be accepted and the highest voted answer."}],"timestamp":"1727155200.0"},{"content":"Selected Answer: D\nD is the hero here.\nThough Cloud SQL has an upper hand when it comes to transactions(OLTP), it does not autoscale its computing capabilities as compared to datastore.\nDo visit: https://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for","comment_id":"506855","upvote_count":"2","poster":"kishanu","timestamp":"1727155140.0"},{"comments":[{"timestamp":"1640648940.0","content":"May be some history can help to decide which is best answer.\nDatastore built by Google uses BigTable as it's storage, while the company who built FireStore uses Cloud Spanner as it's storage. Google decided that they like the FireStore technology and acquired it.\nIf Cloud Spanner is an option I would choose it. So, D for me, although it's json storage format, but the Cloud Spanner it uses as storage fits all the requirements.","comment_id":"510662","upvote_count":"2","poster":"kuik"}],"timestamp":"1727155140.0","poster":"hendrixlives","comment_id":"503320","content":"Selected Answer: D\nD is correct: Datastore (currently Firestore in native or datastore mode). It is a fully managed and serverless solution that allows for transactions and will autoscale (storage and compute) without the need to manage any infrastructure. \nA is wrong: Cloud SQL is fully a managed transactional DB, but only the storage grows automatically. As your user base increases, you will need to increase the CPU/memory of the instance, and to do that you must edit the instance manually (and the questions specifically says \"you do not want to manage infrastructure scaling\")\nB is wrong: Bigquery is OLAP (for analytics). NoOps, fully managed, autoscales and allows transactions, but it is not designed for this use case.\nC is wrong: Bigtable is a NoSQL database for massive writes, and to scale (storage and CPU) you must add nodes, so it is completely out of this use case.","upvote_count":"4"},{"upvote_count":"1","content":"Selected Answer: D\nB - not an option\nC - lack of ACID transactions\nA - lack of resource automatic scalability \nD - (correct, IMHO) support ACID, suitable for OLPT and scalable enough","timestamp":"1727154960.0","poster":"axantroff","comment_id":"1056987"},{"poster":"rocky48","comment_id":"1062181","timestamp":"1727154960.0","upvote_count":"3","content":"Selected Answer: D\nB - not an option\nC - lack of ACID transactions\nA - lack of resource automatic scalability\nD - (correct, IMHO) support ACID, suitable for OLPT and scalable enough"},{"upvote_count":"1","poster":"TVH_Data_Engineer","timestamp":"1727154960.0","comment_id":"1096236","content":"Selected Answer: D\nCloud Datastore (now part of Google Cloud Firestore in Datastore mode) is designed for high scalability and ease of management for applications. It is a NoSQL document database built for automatic scaling, high performance, and ease of application development. It's serverless, meaning it handles the scaling, performance, and management automatically, fitting your requirement of not wanting to manage infrastructure scaling.\n\nCloud SQL, while a fully-managed relational database service that makes it easy to set up, manage, and administer your SQL databases, is not as automatically scalable as Datastore. It's better suited for applications that require a traditional relational database."},{"comment_id":"1286842","upvote_count":"2","poster":"baimus","content":"Selected Answer: A\nThis actually is A. I was initially in the D camp, and have spent considerable time reading about it now (circa 1 hour). D is explicitly not suitable for payment transactions, as Datastore supports ACID transactions, but only within entity groups, which are small, localized sets of data. This restriction means that transactions are not suitable for scenarios requiring multi-entity consistency across the entire database.\nThe only two products recommended for payments in the Google ecosystem are Cloud Spanner and Cloud SQL. Is Cloud SQL managed? I'd say it wasn't really, due to the need to configure instances, but that is trumped by the fact it is the only choice suitable for a payment transaction system.","timestamp":"1726838940.0"},{"poster":"SatyamKishore","timestamp":"1723369140.0","upvote_count":"1","content":"Cloud Datastore is a fully managed, NoSQL document database that is highly scalable and designed to automatically handle large increases in traffic without requiring manual intervention. It's well-suited for applications with a rapidly growing user base.","comment_id":"1263997"},{"upvote_count":"1","poster":"iooj","content":"Selected Answer: D\nFirestore extension of Datastore can handle acid transactions and allows autoscaling","timestamp":"1722360060.0","comment_id":"1258318"},{"upvote_count":"1","timestamp":"1719494040.0","comment_id":"1238168","content":"Selected Answer: A\nA. Requires Transactions.","poster":"jamalkhan"},{"poster":"8ad5266","comment_id":"1222152","timestamp":"1717159080.0","upvote_count":"1","content":"Selected Answer: A\nTransactional = SQL. Anything with payment processors should NOT be NoSQL"},{"content":"Selected Answer: A\nTransactional DB","timestamp":"1716857820.0","poster":"brokeasspanda","comment_id":"1219920","upvote_count":"1"},{"timestamp":"1716079800.0","upvote_count":"2","content":"The best choice for this scenario is D. Cloud Datastore. Here's why:\n\nScalability: Cloud Datastore is a NoSQL database designed for automatic scaling. It can handle exponential growth without requiring manual intervention for infrastructure management.\nNoSQL: This type of database is well-suited for transactional data like payment processing, where the schema can be flexible.\nFully Managed: Cloud Datastore is a fully managed service, meaning Google takes care of the underlying infrastructure, maintenance, and updates.\nHere's why the other options aren't the best fit:\n\nCloud SQL: While suitable for relational data, it might not be the most scalable or cost-effective option for handling massive growth in a point-of-sale application.\nBigQuery: Primarily designed for data warehousing and analytics, not transactional processing.\nCloud Bigtable: A high-performance NoSQL database, but it's more suited for large-scale, low-latency applications and might be overkill for a point-of-sale system.","comment_id":"1213541","poster":"39405bb"},{"timestamp":"1714321740.0","upvote_count":"1","poster":"ilberg","comment_id":"1203655","content":"auto-scale is the key word to unfold the correct answer. D is the the correct answer"},{"timestamp":"1699383300.0","comment_id":"1065090","poster":"RT_G","upvote_count":"1","content":"Selected Answer: D\nD - Because DataStore supports massive scaling and ACID transactions which are two primary considerations in this scenario."},{"poster":"imran79","timestamp":"1696646580.0","content":"For a point-of-sale application where you anticipate exponential growth and want to ensure seamless scalability without managing the infrastructure scaling, the best choice among the provided options is:\n\nD. Cloud Datastore\n\nCloud Datastore is a NoSQL database service that's built for web, mobile, and IoT applications. It provides automatic scaling, high performance, and ease of application development, making it a suitable choice for applications where the user base could grow exponentially and where you don't want to manually handle infrastructure scaling.","upvote_count":"2","comment_id":"1027025"},{"poster":"gudguy1a","upvote_count":"1","content":"Selected Answer: C\nBigtable is the better answer.\nNeither Cloud SQL & Datastore (Firestore) can scale to exponential level....","timestamp":"1693919940.0","comment_id":"999540"},{"content":"A seems to be correct.\nUsecase here is \"process payment transactions in a point-of-sale application\". So this is OLTP. Datastore is not a relational database, and it is not an effective solution for analytic data. If you need a relational database with full SQL support for an online transaction processing (OLTP) system, consider Cloud SQL.\nhttps://cloud.google.com/datastore/docs/concepts/overview","upvote_count":"4","comment_id":"975085","poster":"GCP_PDE_AG","timestamp":"1691455320.0"},{"upvote_count":"2","timestamp":"1691388120.0","comment_id":"974433","content":"Selected Answer: D\nD is correct because you do not want to manage scaling in this case. Cloud SQL can also scale but it needs involvement unlike Datastore which is completely automatic.","poster":"pulse008","comments":[{"timestamp":"1691434080.0","poster":"bhavaneesh","comment_id":"974911","upvote_count":"1","content":"Do you by any chance have the contributor access - I need questions 205-209 :( Please"}]},{"poster":"alihabib","comment_id":"973132","timestamp":"1691248620.0","content":"Its D, though Datastore is NoSQL, but it supports ACID transaction concepts.","upvote_count":"1"},{"upvote_count":"2","timestamp":"1690687020.0","comment_id":"966808","poster":"yash12","content":"Correct Answer is D, Cloud Datastore is used for PoS\nhttps://cloud.google.com/blog/topics/developers-practitioners/your-google-cloud-database-options-explained"},{"poster":"Vipul1600","comment_id":"963655","content":"Datastore automatically scales and is highly opted for ACID transactions.","timestamp":"1690370700.0","upvote_count":"1"},{"poster":"hpvb","content":"cloud sql --> incorrect\nhttps://cloud.google.com/sql#section-8 --> you will need to manually do scalability\nScalability --> Easily scale up as your data grows—add processor cores, RAM and storage, and scale out by adding read replicas to handle increasing read traffic. Read replicas support high availability, can have their own read replicas, and can be located across regions and platforms.\n\nbig query , big table --> doesn't support ACID \n\nCloud DataStore --> CORRECT ( https://cloud.google.com/datastore/) check features section\n\nACID transactions\nEnsure the integrity of your data by executing multiple datastore operations in a single transaction with ACID characteristics, so all the grouped operations succeed or all fail.\n\nFully managed\nDatastore is fully managed, which means Google automatically handles sharding and replication in order to provide you with a highly available and consistent database.","timestamp":"1687547580.0","upvote_count":"3","comment_id":"931894"},{"content":"BigQuery --> OLAP , DataWareHouse \nCloud BigTable --> NOSQL, Doesn't support ACID.\nCloud DataStore --> NO SQL","timestamp":"1687546320.0","comment_id":"931876","upvote_count":"2","poster":"hpvb"},{"timestamp":"1686554940.0","comment_id":"921208","content":"Bard said it's cloud sql:\n\"If Cloud Spanner is not an option, Cloud SQL is a good choice for your point-of-sale application. It offers a managed service, flexible pricing, security, and global availability.\"","poster":"tal_","upvote_count":"2"},{"content":"Selected Answer: A\nIt should be cloud SQL","comment_id":"908985","upvote_count":"4","timestamp":"1685327400.0","poster":"biswa_b"},{"upvote_count":"2","comment_id":"902700","content":"Selected Answer: A\ntransaction data, OLTP, ACID plus Cloudsql can scale up","poster":"baiy","timestamp":"1684604100.0"},{"content":"Selected Answer: A\nThe description of the solution to me looks like an OLTP app, I would go with spanner but since it’s not an option I would go with cloud sql","upvote_count":"2","poster":"Kiroo","comment_id":"893370","timestamp":"1683658320.0"},{"comment_id":"890934","content":"Selected Answer: A\nCloud SQL is a fully-managed relational database service that is compatible with MySQL, PostgreSQL, and SQL Server, and provides high availability, scalability, and security. It can handle transaction processing with strong consistency, and supports SQL, which is a widely used database language that many developers are already familiar with.\n\nCloud Datastore, on the other hand, is a NoSQL document database that is designed for web and mobile applications with simpler data models. While Cloud Datastore can handle high read-intensive workloads, it may not be the best option for transaction processing that requires strong consistency and ACID compliance.\n\nIn summary, if you require a relational database for your payment processing application, Cloud SQL would be a better choice than Cloud Datastore.","poster":"plotry","timestamp":"1683397080.0","upvote_count":"2"},{"content":"This is a payment transaction application so it must support ACID backend and DataStore does support this feature along with Great scalability and high performance so I will go with Datastore.","poster":"pankajk0811","timestamp":"1681317300.0","upvote_count":"4","comment_id":"868559"},{"timestamp":"1680972540.0","comment_id":"864859","upvote_count":"1","poster":"RiteshSingh825","content":"Selected Answer: C\nGoing with answer C as 1)Auto-Scale property so no Cloud SQL 2) Real Time updates which DataStore cannot provide 3) OLTP transaction that means no Big Query"},{"comment_id":"845798","timestamp":"1679394900.0","content":"Selected Answer: C\nA - not auto scale\nB - not meant for payment transactions\nC - best choice here as it could support exponentially growing user base\nD - Support ACID but not in a large scale","upvote_count":"2","poster":"midgoo"},{"content":"Selected Answer: D\nD - I believe the use case is suitable for Datastore.\nhttps://cloud.google.com/datastore/docs/concepts/overview?hl=pt-br","comment_id":"839238","timestamp":"1678825200.0","upvote_count":"2","poster":"Chesternut999"},{"poster":"betterForGo","upvote_count":"1","timestamp":"1678225320.0","content":"with chatGPT : \nFor a point-of-sale application that may have an exponentially growing user base, but requires minimal infrastructure management, the recommended database service on Google Cloud Platform would be Cloud Spanner, as it provides high availability, horizontal scalability, and consistency across regions. It is designed to support high-volume, globally distributed transactions with low latency, and provides automatic scaling and replication of data across multiple regions.\n\nHowever, if Cloud Spanner is not an option for any reason, then the best database service to use would be Cloud SQL, which is a managed SQL database service that is compatible with most SQL client applications. It provides automated backups, replication, and scaling of the database instances with minimal effort.","comments":[{"comment_id":"836723","upvote_count":"1","content":"Question has not constrains on sql data.\nhttps://cloud.google.com/datastore/docs/concepts/overview#:~:text=Datastore%20is%20ideal%20for%20applications,product%20details%20for%20a%20retailer.\nDatastore is ideal for applications that rely on highly available structured data at scale. You can use Datastore to store and query all of the following types of data:\n\nProduct catalogs that provide real-time inventory and product details for a retailer.\nUser profiles that deliver a customized experience based on the user’s past activities and preferences.\nTransactions based on ACID properties, for example, transferring funds from one bank account to another.","poster":"unnamed12355","timestamp":"1678596720.0"}],"comment_id":"832314"},{"timestamp":"1677225540.0","poster":"Zosby","upvote_count":"1","content":"\" transactions\" # B and D are Wrong","comment_id":"820234"},{"poster":"musumusu","timestamp":"1677148320.0","content":"Answer: D, \nKey words: Point of sale (pos) you can compare it with fast payment methods ex, barcode scan of product or tickets and credit card scans. These transactions doesn't need SQL database as it gets information from the card/barcode itself. \nRight answer is Firestore as based on number of clients but alternative of Firestore is Datastore (old version of Firestore) you can say. \nIt has no limits of POS or Synchornization processing such as sensor. \nIf there a Question for fastest sync database, you can choose Firestore/datastore.","comment_id":"819064","upvote_count":"2"},{"timestamp":"1676817780.0","poster":"techtitan","comment_id":"814180","upvote_count":"4","content":"Selected Answer: D\nDatastore is a highly scalable NoSQL database for your applications. Datastore automatically handles sharding and replication, providing you with a highly available and durable database that scales automatically to handle your applications' load. Datastore provides a myriad of capabilities such as ACID transactions, SQL-like queries, indexes, and much more."},{"content":"Selected Answer: C\nWhizlabs says C","timestamp":"1675764000.0","upvote_count":"1","poster":"chxnge","comment_id":"800738"},{"comment_id":"799595","upvote_count":"2","poster":"cucinareblog","timestamp":"1675678080.0","content":"Selected Answer: A\nTransactional data i better to go into relational DB like CloudSql"},{"content":"A. Cloud SQL\n\nCloud SQL is a fully managed relational database service that is well suited for applications that require a structured and scalable database. It can handle high levels of concurrent connections and is designed to be highly available, so you do not need to manage infrastructure scaling. Since your payment transactions will likely require a structured database with tables and relationships, Cloud SQL is the best choice for this use case.","poster":"samdhimal","comments":[{"timestamp":"1675629060.0","upvote_count":"2","comment_id":"799153","content":"B. BigQuery is a data warehousing and analytics service designed for large-scale data processing and analysis. While it could be used to store payment transactions, it is not optimized for handling high levels of concurrent connections, making it less suitable for a point-of-sale application.\n\nC. Cloud Bigtable is a NoSQL database service designed for handling massive amounts of semi-structured and unstructured data. It is not well suited for a point-of-sale application that requires a structured database with tables and relationships, making it a less ideal choice.\n\nD. Cloud Datastore is a NoSQL document-oriented database service that is well suited for applications that require a flexible, scalable, and highly available data store. While it could be used to store payment transactions, it is not optimized for handling high levels of concurrent connections, making it less suitable for a point-of-sale application.","poster":"samdhimal"}],"upvote_count":"3","comment_id":"799151","timestamp":"1675629060.0"},{"upvote_count":"1","poster":"KGCP25","comment_id":"771410","content":"Referring to https://cloud.google.com/datastore/docs/concepts/overview#other_storage_and_database_options --- If you need a relational database with full SQL support for an online transaction processing (OLTP) system, consider Cloud SQL.","timestamp":"1673354400.0"},{"poster":"Nirca","upvote_count":"1","comment_id":"771367","timestamp":"1673352000.0","content":"Selected Answer: C\nBigTable is: \"A fully managed, scalable NoSQL database service for large analytical and operational workloads with up to 99.999% availability\"\n\n* Cloud SQL is NOT auto-scale for Writes (inserts). only for reads (select) \n* Datastore is not a stand-alone system. it is part on the Firestore/Data system. So I'm not \nexpecting GCP to suggest this option any more (starting from 2022) \n* BigQuery is only for analytics"},{"content":"The answer should be C, as BigTable is the most scalable solution with transaction capability. \n\nD is no sql DB with no strict schema which is not appropriate for fixed schema transaction workload.\n\nIMO, A is not scalable enough!","timestamp":"1673067360.0","upvote_count":"1","comment_id":"768242","poster":"korntewin"},{"content":"Selected Answer: C\nhigh load cloud application -> bigtable is recommended.\nA - cloudSQL will work but not scale as fast nor provide fast response time on high load \nD - datastore is designed for simple use cases","timestamp":"1672239720.0","upvote_count":"1","poster":"Jackalski","comment_id":"759979"},{"upvote_count":"3","poster":"Brillianttyagi","timestamp":"1671907680.0","comment_id":"755088","content":"Selected Answer: D\nD- Cloud datastore is the right answer as it autoscale automatically but only storage scale automatically in clout sql not cpu, as load increase you also have to increase load.\n\nYou can find the answer in this guide.\nhttps://towardsdatascience.com/how-i-passed-google-professional-data-engineer-exam-in-2020-2830e10658b6"},{"timestamp":"1670883300.0","poster":"DGames","comment_id":"743375","upvote_count":"1","content":"Answer - A This question related to RDMS (Point-of sales) database and in GCP - cloud SQL is option. also need to maintain ACID property."},{"poster":"permoon","comment_id":"738636","comments":[{"comment_id":"747882","upvote_count":"2","timestamp":"1671265860.0","poster":"jkhong","content":"I think u need to revisit how chatgpt gives answers lol, I've asked it and it gave me Datastore as an answer\n\nThe correct answer is D. Cloud Datastore\n\nCloud Datastore is a fully managed, NoSQL document database service that provides automatic scalability, high performance, and durability for your applications. It is designed to handle large volumes of structured and unstructured data, and can support rapid growth without requiring you to manage infrastructure scaling. Cloud Datastore is a good choice for storing and retrieving large amounts of data quickly and efficiently, making it a suitable option for a point-of-sale application that may experience rapid growth in user base."}],"content":"Selected Answer: A\nI' asked ChatGPT about this question, and it told me the correct answer is A.\n\nIf you want to process payment transactions in a point-of-sale application that will run on Google Cloud Platform and you don't want to worry about managing infrastructure scaling, you should use Google Cloud SQL. Cloud SQL is a fully-managed database service that makes it easy to set up, maintain, manage, and administer your relational databases on Google Cloud Platform.","upvote_count":"5","timestamp":"1670476200.0"},{"upvote_count":"4","poster":"Asheesh1909","content":"Selected Answer: D\nDatastore can autoscale and supports transactions, in case of exponential increase in sales too.","comment_id":"730656","timestamp":"1669741920.0"},{"poster":"Fabius","timestamp":"1668953100.0","upvote_count":"2","content":"Cloud Datastore is a document DB.\nCloud SQL is a correct answer","comment_id":"722672"},{"content":"Selected Answer: A\nOut of the two options being discussed (A or D) I strongly think it is A. Many of them who voted for A has already mentioned it.. but two strong points - Cloud SQL (A) is a relational DB which is much needed in this scenario (payment transaction) and it can grow when right auto growth settings be set.\nTake note Auto Growth settings is set for allowing the DB to grow to a certain defined limit and is a common feature across most prominent relational DBs and Cloud SQL is just like MySQL or SQL Server.","comment_id":"722157","upvote_count":"1","timestamp":"1668880860.0","poster":"Jay_Krish"},{"poster":"GCPSharon","comment_id":"704315","content":"Selected Answer: D\n\"infrastructure scaling\"! Cloud SQL only can provide auto storage sacle.","timestamp":"1666748760.0","upvote_count":"4"},{"content":"D: \nDatastore is a highly scalable NoSQL database for your applications. Datastore automatically handles sharding and replication, providing you with a highly available and durable database that scales automatically to handle your applications' load. Datastore provides a myriad of capabilities such as ACID transactions, SQL-like queries, indexes, and much more.","timestamp":"1666646940.0","comment_id":"703376","upvote_count":"2","poster":"Atnafu"},{"upvote_count":"4","poster":"nkunwar","content":"Selected Answer: D\nAs user base grows, write transaction grows since we are dealing with POS (that not the place for reading but writing). In order to accommodate more writes in transactional flavor which can be horizontally scalable DATASTORE should be preferred.","timestamp":"1664336340.0","comment_id":"681372"},{"content":"A\nDatastore is not ideal for every use case. For example, Datastore is not a relational database, and it is not an effective solution for analytic data.\nHere are some common scenarios where you should probably consider an alternative to Datastore:\nIf you need a relational database with full SQL support for an online transaction processing (OLTP) system, consider \nCloud SQL\nIf you don’t require support for ACID transactions or if your data is not highly structured, consider \nCloud Bigtable\nIf you need interactive querying in an online analytical processing (OLAP) system, consider \nBigQuery\nIf you need to store large immutable blobs, such as large images or movies, consider \nCloud Storage\nFor more information about other database options, see the overview of database services.https://cloud.google.com/products/databases","poster":"louise22","comment_id":"680524","comments":[{"poster":"Adi_tth","upvote_count":"1","timestamp":"1670236080.0","comment_id":"735848","content":"Cloud bigtable supports ACID."}],"timestamp":"1664267580.0","upvote_count":"2"},{"upvote_count":"1","comment_id":"670015","timestamp":"1663252440.0","content":"its b as it should e transactional..datastore holds semistructured data","poster":"1997deb"},{"content":"Selected Answer: A\nCloud SQl due to acid","timestamp":"1662969300.0","poster":"badrisrinivas9","upvote_count":"2","comment_id":"666718"},{"upvote_count":"2","timestamp":"1657800240.0","content":"Selected Answer: D\nA or D, but to store transitions, you have to write. With Cloud SQL, you can scale it out only for reading, not for writing, by adding more replications. Therefore D: datastore would be the best solution.","poster":"changsu","comment_id":"631343"},{"upvote_count":"1","comment_id":"630138","poster":"marsilveira","timestamp":"1657559460.0","content":"The answer here is Cloud SQL because it deals with data transactions. The most answer voted here refers to Cloud Datastore, which doesn't suit because there is a fixed schema and doesn't deal with document and unfixed schema."},{"timestamp":"1656649620.0","poster":"Puneet2022","comment_id":"625570","upvote_count":"3","content":"Selected Answer: D\nhttps://cloud.google.com/datastore/docs/concepts/transactions"},{"poster":"paauul","upvote_count":"4","comment_id":"620459","content":"This question is from over 2y ago, today it'd probably ask about Spanner. I highly doubt that you'd see this question in the exam","timestamp":"1655907300.0"},{"timestamp":"1654965900.0","content":"Selected Answer: A\nCorrect A","comment_id":"615022","poster":"alexsrc7","upvote_count":"3"},{"upvote_count":"2","content":"Selected Answer: A\nCloud SQL is transactional, SQL, storage autoscaling DB","poster":"FrankT2L","timestamp":"1654846980.0","comment_id":"614385"},{"poster":"Omi_04","content":"Selected Answer: D\nAuto-scalable and ACID compliant, Cloud SQL is not auto-scalable","upvote_count":"2","comment_id":"609808","timestamp":"1654013700.0"},{"timestamp":"1653601140.0","comment_id":"607804","poster":"NickNtaken","content":"Selected Answer: D\nI think D is correct, as it said the user base will grow EXPOTENTIALLY, cloud sql can only scale vertically, while datastore is distributed, thus able to scale horizontally to meet exponentially growth. Datastore also support transaction.","upvote_count":"3"},{"timestamp":"1652854860.0","upvote_count":"1","content":"You are doing payment transactions, therefore you must use a transactional database, therefore from that perspective it can only be A) Cloud SQL","poster":"JimJam1","comment_id":"603176"},{"upvote_count":"1","comment_id":"592056","poster":"Vip777","content":"Hi Moderator, please delete my comment, this comment and one with just one word.... relation.... i was searching in discussion and didn't notice i click submit button.","timestamp":"1650942060.0"},{"poster":"Vip777","comment_id":"592055","timestamp":"1650941940.0","content":"relation","upvote_count":"1"},{"poster":"richardchan66","content":"Answer D:\nhttps://cloud.google.com/datastore/docs/concepts/overview\nTransactional, available, scalable","upvote_count":"1","timestamp":"1649646720.0","comment_id":"583997"},{"comments":[{"poster":"tavva_prudhvi","content":"But, thats kinda extra setup, right but Data store does that instantly, why not D?","timestamp":"1649167560.0","comment_id":"581300","upvote_count":"1"}],"comment_id":"580906","upvote_count":"1","timestamp":"1649103420.0","poster":"devric","content":"CloudSQL can autoscale with CloudSQL proxy. I'm going with A"},{"comment_id":"576298","timestamp":"1648396080.0","upvote_count":"2","poster":"MangeshGCP","content":"Selected Answer: A\nIts a case of Transactional DB . Exponential growth is not properly specified"},{"timestamp":"1647400080.0","comments":[{"upvote_count":"2","timestamp":"1649167500.0","poster":"tavva_prudhvi","comment_id":"581299","content":"Datastore is good for ACID Transactions e.g., transferring funds between accounts, etc with Strong Consistency and also when it comes to scaling Datastore does that."}],"content":"Cloud Datastore is a NoSQL database which is not good for Transactions , so ill go with A","upvote_count":"1","poster":"Kamlakar","comment_id":"568744"},{"poster":"Arkon88","comment_id":"559394","upvote_count":"4","content":"Selected Answer: D\n1) Scaling and transactions are in Datastore \n2) here question tells about application so additional point for Datastore\n3) answer A looks very good but there scaling is limited and could spanner is not here \n\nso D, after reading all info from commets \n\nplease reffer https://cloud.google.com/datastore/docs/concepts/overview\n\n1)Transactions based on ACID properties, for example, transferring funds from one bank account to another.\n\n2)Massive scalability with high performance","timestamp":"1646223600.0"},{"comment_id":"559376","upvote_count":"3","poster":"Arkon88","content":"Selected Answer: A\nonly cloud sql is transactional","timestamp":"1646222640.0","comments":[{"content":"changed my mind after going through the comments, D is right one","comment_id":"559402","timestamp":"1646223960.0","poster":"Arkon88","upvote_count":"2"}]},{"poster":"YashwantKumar1402","timestamp":"1642613400.0","content":"Data store is not meant for transactional data. Cloud spanner would have been a good option but in this case cloud sql seems right.","upvote_count":"5","comment_id":"527774"},{"comment_id":"508188","poster":"prasanna77","upvote_count":"2","content":"its choice between datastore and CloudSQl.If scalable then Datastore","timestamp":"1640301540.0"},{"comment_id":"507560","timestamp":"1640229000.0","poster":"karlos_bhai","upvote_count":"1","content":"Selected Answer: D\nD seems more appropriate"},{"timestamp":"1640224080.0","content":"Bigtable","poster":"Yonghai","upvote_count":"5","comment_id":"507483"},{"poster":"etdevops","upvote_count":"2","timestamp":"1639900260.0","content":"Selected Answer: D\nAnswer: D, Datastore\nhttps://cloud.google.com/datastore/docs/concepts/overview","comment_id":"504703"},{"content":"Selected Answer: D\nCloudSql does not auto scale","timestamp":"1637918100.0","comment_id":"487208","upvote_count":"3","poster":"StefanoG"},{"content":"D as the payment transactions would need a transactional data service Datastore can support the same. Also, it is fully managed with NoOps required.\nA is wrong as Cloud SQL would need infrastructure scaling. Although storage can be automatically scaled (up to a limit), instance type needs to be changed as per the load manually.\nB is wrong as BigQuery is a data warehousing option.\nC is wrong as Bigtable is not a relational database but a NoSQL option.","comment_id":"474420","upvote_count":"5","timestamp":"1636394100.0","poster":"MaxNRG"},{"upvote_count":"4","comment_id":"469452","timestamp":"1635449640.0","content":"D: Datastore does autoscaling whereas Cloud SQL does not scale automatically \nhttps://cloud.google.com/datastore/docs/concepts/overview","poster":"SonuKhan1"},{"timestamp":"1634309460.0","content":"Ans: A","poster":"anji007","upvote_count":"1","comment_id":"462662"},{"comments":[{"content":"Hi @iwanttogohome where had you read that CloudSQL autoscale?","upvote_count":"1","timestamp":"1637918160.0","poster":"StefanoG","comment_id":"487209"}],"timestamp":"1632969000.0","content":"A seems to be the right one. Cloud SQL supports automatic scale up.","comment_id":"454648","poster":"iwanttogohome","upvote_count":"3"},{"comment_id":"453536","content":"D. Looking at doc from Google:\n\nWhat it's good for\nDatastore is ideal for applications that rely on highly available structured data at scale. You can use Datastore to store and query all of the following types of data:\n\nProduct catalogs that provide real-time inventory and product details for a retailer.\nUser profiles that deliver a customized experience based on the user’s past activities and preferences.\nTransactions based on ACID properties, for example, transferring funds from one bank account to another.\n\nhttps://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for\n\nThis payment and can be similar to a banking transactions...so D seems good for this use case","upvote_count":"8","poster":"Chelseajcole","timestamp":"1632848580.0"},{"comment_id":"452676","timestamp":"1632768120.0","upvote_count":"7","content":"D - the question is even example use-case mentioned here https://cloud.google.com/datastore/docs/concepts/transactions#uses_for_transactions","poster":"retep007"},{"poster":"Doubty92","comments":[{"comment_id":"469449","poster":"SonuKhan1","content":"Big Table is more of Analytical DB rather than for OLTP","upvote_count":"3","timestamp":"1635448980.0"}],"content":"why is it not Big Table? can anyone explain?","upvote_count":"4","comment_id":"427021","timestamp":"1629313140.0"},{"content":"D is the correct answer","comment_id":"426353","timestamp":"1629207120.0","upvote_count":"2","poster":"sandipk91"},{"timestamp":"1629122040.0","upvote_count":"7","poster":"fire558787","comment_id":"425830","content":"Very much Disagree with those saying A is right. D is right: Datastore / Firestore are for transactional workloads and can autoscale. A is wrong as Cloud SQL doesn't scale (\"grow exponentially\" is the hint). BigQuery can autoscale but is not a transactional DB. BigTable can scale only manually and is not a transactional database (but an analytical database https://cloud.google.com/architecture/data-lifecycle-cloud-platform#managed_wide-column_nosql)"},{"comment_id":"419526","content":"I would go for D there:\nhttps://cloud.google.com/datastore/docs/concepts/cloud-datastore-transactions","upvote_count":"2","timestamp":"1628055540.0","poster":"matix781"},{"upvote_count":"4","poster":"sumanshu","comment_id":"330764","content":"Vote for 'D'\nhttps://cloud.google.com/datastore/docs/concepts/overview\n\ncheck the 'What it's good for' section","timestamp":"1617837660.0"},{"comments":[{"content":"https://cloud.google.com/datastore/docs/concepts/overview\n\nCheck the 'What it's good for' section on above link","timestamp":"1617837600.0","poster":"sumanshu","upvote_count":"1","comment_id":"330763"}],"upvote_count":"1","poster":"daghayeghi","content":"answer A:\nCloud Datastore and Firestore are managed document databases that support flexible schemas and transactions. in our use case we don't have any clue of flexibility in database design. then we will go for Cloud SQL.","comment_id":"305738","timestamp":"1615210080.0"},{"timestamp":"1614520980.0","comment_id":"300823","poster":"sid091","content":"D is correct . Though datastore is a NOSQL Database but NOSQL with ACID properties can be used here as the data can be grown exponentially","upvote_count":"1"},{"upvote_count":"2","comment_id":"284951","content":"Correct A","poster":"naga","timestamp":"1612628640.0"},{"content":"I think you want to have a relational DB, ACID is not enough (https://www.ribice.ba/relational-datastore/). I would choose CLOUD SQL.","upvote_count":"2","poster":"rosetta","timestamp":"1610883720.0","comment_id":"269476"},{"poster":"apnu","timestamp":"1609997580.0","comment_id":"261557","upvote_count":"2","content":"D. As Datastore support auto scaling and can manage ACID transaction along with high load."},{"timestamp":"1609021320.0","content":"D - same question in Linux Academy","comment_id":"252941","poster":"zh31427","upvote_count":"3"},{"upvote_count":"3","timestamp":"1608735000.0","poster":"sweety40","comment_id":"251017","content":"D : Cloud Datastore is true no-ops, a serverless database that is ideal for non-relational, point-of-sale transactional data. It can grow exponentially without having to manage infra."},{"poster":"rajaCent","comment_id":"247331","timestamp":"1608296400.0","upvote_count":"2","content":"D is more apt as i feel datastore is Highly scalable NoSQL database for storing, syncing and querying of data on a global scale as it is Severless with ACID Suport https://cloud.google.com/datastore/docs/concepts/overview.\nCloud is scales vertically not horizontal"},{"poster":"beedle","comment_id":"238412","comments":[{"comment_id":"241555","poster":"kavs","comments":[{"timestamp":"1630674300.0","content":"I think as @kavs, as reported in GCP documentation https://cloud.google.com/datastore/docs/concepts/overview, this is one of case where Firestore (new Datastore) is the right solution.\nSo for me the right answer is D","upvote_count":"1","comment_id":"438528","poster":"StefanoG"}],"upvote_count":"1","content":"What it's good for\nDatastore is ideal for applications that rely on highly available structured data at scale. You can use Datastore to store and query all of the following types of data:\n\nProduct catalogs that provide real-time inventory and product details for a retailer.\nUser profiles that deliver a customized experience based on the user’s past activities and preferences.\nTransactions based on ACID properties, for example, transferring funds from one bank account to another.\n\nSo D is correct - it is a payment transaction","timestamp":"1607773860.0"}],"content":"I do think its A because its OLTP and cloud SQL can scale up automatically not scale out.","upvote_count":"2","timestamp":"1607440260.0"},{"upvote_count":"2","timestamp":"1605373560.0","comment_id":"219230","poster":"aviassu","content":"This is a straight forward A Cloud SQL for transactional data, and D is incorrect due to No-SQL database and others analytical and wide-column databases not used for transactional purposes. CloudSQL can scale well too."},{"timestamp":"1603102800.0","content":"Hey guys, D is not possible...the service is not available anymore. Firestore is the next generation of Datastore and you can use it in \"Datasotre mode\", probably the correcxt answer is A.","poster":"Erso","upvote_count":"2","comment_id":"202487"},{"upvote_count":"1","comment_id":"194837","content":"D is the most appropriate one, due to the following reasons \n1. Fully managed \n2. Support Exponential growth\n3. Support Transaction based ACID properties \n4. Globally available","poster":"Tanmoyk","timestamp":"1602044820.0"},{"comment_id":"184378","content":"Option A specific to this scenario as per Google Doc -\nCloud SQL offers high performance, scalability, and convenience. Hosted on Google Cloud Platform, Cloud SQL provides a database infrastructure for applications running anywhere.","poster":"PratikG","timestamp":"1600775700.0","upvote_count":"2"},{"timestamp":"1600464420.0","poster":"yurstev","comment_id":"181979","content":"A. keyword is transactions","upvote_count":"2"},{"comment_id":"176369","content":"A is correct as requirement is OLTP. It is automatic scale\nhttps://cloud.google.com/sql/docs/features","comments":[{"content":"I could not find anything related to 'automatic scaling' mentioned in provided link","poster":"sumanshu","upvote_count":"1","comment_id":"389998","timestamp":"1624577760.0"}],"poster":"ganesh2121","timestamp":"1599639960.0","upvote_count":"2"},{"content":"A Fits here for OLTP\nCloud SQL automatically ensures your databases are reliable, secure, and scalable so that your business continues to run without disruption. Cloud SQL automates all your backups, replication, encryption patches, and capacity increases—while ensuring greater than 99.95% availability, anywhere in the world.","poster":"oku","comment_id":"166997","upvote_count":"2","timestamp":"1598467860.0"},{"upvote_count":"3","timestamp":"1597660380.0","content":"Option A: Cloud SQL is fully managed and auto scalable. It is best for transactional databases whereas Cloud Datastore is for No-SQL Document Database.","comment_id":"159882","poster":"PRABHUKKARTHI"},{"timestamp":"1595354700.0","poster":"mikey007","upvote_count":"4","content":"D is correct ans..\n\nBelow link is from gcp docs where google itself unchecked for Scale insurance for Cloud SQL.","comment_id":"140492"},{"timestamp":"1593835680.0","upvote_count":"4","poster":"Rajuuu","content":"A is correct.","comment_id":"125933"},{"comment_id":"120517","upvote_count":"2","poster":"dambilwa","timestamp":"1593174060.0","content":"It is a tough choice between Datastore & Cloud SQL"},{"poster":"GauravGCP","upvote_count":"4","content":"A seems to be more appropriate \nhttps://cloud.google.com/products/databases","comment_id":"105129","timestamp":"1591607460.0"},{"comment_id":"73854","content":"D is the right answer","poster":"arnabbis4u","upvote_count":"3","timestamp":"1586730900.0"},{"comment_id":"73261","content":"I think is is A as this is payment transaction \nhttps://cloud.google.com/datastore/docs/concepts/overview\nDatastore is not ideal for every use case. For example, Datastore is not a relational database, and it is not an effective solution for analytic data.\n\nHere are some common scenarios where you should probably consider an alternative to Datastore:\n\nIf you need a relational database with full SQL support for an online transaction processing (OLTP) system, consider Cloud SQL.","poster":"anton_royce","timestamp":"1586599920.0","comments":[{"timestamp":"1597866840.0","upvote_count":"5","poster":"atnafu2020","comment_id":"161771","content":"Transactions based on ACID properties, for example, transferring funds from one bank account to another. For me, D is the correct answer. How about scalability? if the user growth exponentially scalable, that will force to use spanner :) i think"}],"upvote_count":"5"},{"upvote_count":"5","comment_id":"72560","poster":"Nidie","content":"D.https://cloud.google.com/datastore/docs/concepts/overview","timestamp":"1586413260.0"},{"content":"D is apt","timestamp":"1584810600.0","comment_id":"66604","poster":"Rajokkiyam","upvote_count":"4"}],"question_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/16279-exam-professional-data-engineer-topic-1-question-13/","topic":"1"}],"exam":{"isImplemented":true,"isMCOnly":true,"id":10,"provider":"Google","lastUpdated":"15 Feb 2025","name":"Professional Data Engineer","numberOfQuestions":319,"isBeta":false},"currentPage":7},"__N_SSP":true}