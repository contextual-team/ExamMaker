{"pageProps":{"questions":[{"id":"F16zrVz6qW7vooCasUUl","discussion":[{"upvote_count":"11","content":"Selected Answer: A\nto detect average noise levels from sensors, the best approach is to use session windows with a 15-minute gap duration (Option A). Session windows are ideal for cases like this where the events (sensor data) are sporadic. They group events that occur within a certain time interval (15 minutes in your case) and a new window is started if no data is received for the duration of the gap. This matches your requirement to end the window when no data is received for 15 minutes, ensuring that the average noise level is calculated over periods of continuous data","poster":"datapassionate","timestamp":"1705672020.0","comment_id":"1126707","comments":[{"comment_id":"1152223","upvote_count":"2","content":"But you are not fulfilling this requirement \"You need to detect the average noise level from a sensor when data is received for a duration of more than 30 minutes\". I would say C","timestamp":"1708117020.0","poster":"ashdam"}]},{"timestamp":"1707405780.0","upvote_count":"10","comment_id":"1144622","poster":"saschak94","content":"Selected Answer: A\nYou need a window that start when data for a sensor arrives and end when there's a gap in data. That would rule out hopping and tumbling windows. \n\n- > Windows need to stay open as long as there's data arriving - 30+ mins\n-> Window Should close when no data has been received for 15 mins -> Gap 15 mins"},{"timestamp":"1736417340.0","comment_id":"1338284","poster":"Pime13","content":"Selected Answer: D\nD for me, clearly stated on example: https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#hopping-windows","upvote_count":"1"},{"comment_id":"1271741","content":"Selected Answer: A\nThe problem requires detecting average noise levels when data is received for more than 30 minutes, but the window should end when no data has been received for 15 minutes.\nSession windows are ideal for this scenario because:\nThey are designed to capture bursts of activity followed by periods of inactivity.\nThey dynamically size based on the data received, which fits well with the variable duration of noise events.\nThe gap duration can be set to define when a session ends.\nThe 15-minute gap duration aligns perfectly with the requirement to end the window when no data has been received for 15 minutes.\nSession windows will naturally extend beyond 30 minutes if data keeps coming in, satisfying the requirement to detect levels for durations of more than 30 minutes.","poster":"shanks_t","timestamp":"1724516820.0","upvote_count":"3"},{"content":"Selected Answer: D\nD. Using a 15-minute window with a 15-minute tumbling window withAllowedLateness is the most suitable option for the following reasons:\n\nFlexibility: By allowing a 15-minute delay, it can accommodate various situations such as network latency or sensor failures.\nProcessing efficiency: Using a fixed window improves processing efficiency.\nCompliance with conditions: The window ends if no data is received for 15 minutes, meeting the specified condition.\nImplementation points:\n\n.withAllowedLateness: This operator allows delayed events to be included in the current window.\nTrigger: When 30 minutes of data is collected, a trigger event is generated, and the average value is calculated based on this event.\nWatermark: By setting a watermark, processing of old data can be terminated.","poster":"viciousjpjp","timestamp":"1723848660.0","upvote_count":"1","comment_id":"1267360"},{"upvote_count":"3","poster":"JamesKarianis","timestamp":"1722879480.0","content":"Selected Answer: A\nWithout a doubt A: https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#session-windows","comment_id":"1261139"},{"upvote_count":"1","content":"Selected Answer: A\nThe requirements are\n- recieve data for a duration of MORE than 30 minutes\n- end the window based on inactivity","comment_id":"1260013","timestamp":"1722628560.0","poster":"iooj"},{"poster":"josech","comment_id":"1213921","upvote_count":"3","content":"Correct answer: A.\nUse a session Window to cature data and create an aggregation when the Session is larger than 30 minutes. \nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#session-windows\nhttps://beam.apache.org/releases/javadoc/2.6.0/org/apache/beam/sdk/transforms/windowing/Sessions.html","timestamp":"1716142800.0"},{"content":"Selected Answer: C\nC- Running average: https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#hopping-windows","poster":"f74ca0c","upvote_count":"1","timestamp":"1716095100.0","comment_id":"1213609"},{"upvote_count":"2","timestamp":"1712476260.0","poster":"joao_01","comments":[{"comment_id":"1198392","timestamp":"1713510600.0","poster":"joao_01","content":"Actually I think with B, the window will never be closed because the probability of having a period of inactivity of 30 minutes is very low. In that case I think the option A is the more correct one.","upvote_count":"1"}],"comment_id":"1190843","content":"Guys, I think its B. I was considering C, but C is calculating every 30 min, the 15min window gap data.Thats not what the questions wants. The questions wants a solution to get the average data of a 30 min window. So Its B.\n\nLook at this relating to C:\n\"Use hopping windows with a 15-minute window, and a thirty-minute period\" --> Wrong\n(IS DIFFERENT THEN)\n\"Use hopping windows with a 30-minute window, and a 15-minute period\" --> Right.\n\nThats why I think the B is the right answer."},{"timestamp":"1709787840.0","upvote_count":"1","poster":"342f1c6","content":"Selected Answer: C\nTo take running averages of data, use hopping windows. You can use one-minute hopping windows with a thirty-second period to compute a one-minute running average every thirty seconds.","comment_id":"1167664"},{"timestamp":"1708864200.0","content":"Selected Answer: A\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#session-windows\nA session window contains elements within a gap duration of another element. The gap duration is an interval between new data in a data stream. If data arrives after the gap duration, the data is assigned to a new window.\nSo, we need","upvote_count":"3","comment_id":"1158699","poster":"kck6ra4214wm"},{"content":"Selected Answer: C\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#hopping-windows To take running averages of data, use hopping windows. You can use one-minute hopping windows with a thirty-second period to compute a one-minute running average every thirty seconds.","timestamp":"1706199060.0","comment_id":"1131837","upvote_count":"1","poster":"imiu"},{"poster":"Matt_108","comments":[{"comment_id":"1130667","timestamp":"1706104440.0","content":"Agree D.\nData comes -> 30 mts duration.\nData didn't come in 15 mts -> 15 mts duration","poster":"AllenChen123","upvote_count":"1"}],"upvote_count":"2","content":"Selected Answer: D\nOption D to me, It aligns with the specified criteria for detecting the average noise level within a 30-minute duration and handling the end of the window when no data is received for 15 minutes.","timestamp":"1705158120.0","comment_id":"1121771"},{"content":"Selected Answer: D\nOPTION D is correct for the specific scenario where we want to detect the average noise level for a duration of more than 30 minutes but end the window when no data has been received for 15 minutes.\n\nExplanation:\n\n- Tumbling windows are non-overlapping windows, and in this case, you want to capture data continuously for 30-minute intervals.\n\n- Using a tumbling window with a 15-minute window size aligns with your requirement to detect the average noise level for a duration of more than 30 minutes.\n\n- Adding a .withAllowedLateness operator with a duration of fifteen minutes ensures that the window will still consider late-arriving data within that time frame. After fifteen minutes of no data, the window will be closed, and any late-arriving data will not be considered.\n\nOption A and B invalid as they capture fixed logic with 15 or 30 mins. Option C captures only 15 min average with 30 min trigger hence not suitable.","poster":"BIGQUERY_ALT_ALT","comment_id":"1119356","timestamp":"1704947880.0","upvote_count":"2"},{"comment_id":"1117492","timestamp":"1704805320.0","upvote_count":"2","content":"Selected Answer: C\nHopping windows (called sliding windows in Apache Beam).\nTo take running averages of data, use hopping windows.","poster":"Sofiia98"},{"timestamp":"1704302580.0","content":"Selected Answer: C\nC. Use hopping windows with a 15-minute window, and a thirty-minute period.","upvote_count":"3","comment_id":"1112964","poster":"scaenruy"}],"question_text":"You are building a streaming Dataflow pipeline that ingests noise level data from hundreds of sensors placed near construction sites across a city. The sensors measure noise level every ten seconds, and send that data to the pipeline when levels reach above 70 dBA. You need to detect the average noise level from a sensor when data is received for a duration of more than 30 minutes, but the window ends when no data has been received for 15 minutes. What should you do?","url":"https://www.examtopics.com/discussions/google/view/130217-exam-professional-data-engineer-topic-1-question-266/","question_images":[],"isMC":true,"answer_ET":"A","exam_id":10,"question_id":186,"answer_description":"","topic":"1","choices":{"C":"Use hopping windows with a 15-minute window, and a thirty-minute period.","A":"Use session windows with a 15-minute gap duration.","B":"Use session windows with a 30-minute gap duration.","D":"Use tumbling windows with a 15-minute window and a fifteen-minute .withAllowedLateness operator."},"answer":"A","timestamp":"2024-01-03 18:23:00","answer_images":[],"unix_timestamp":1704302580,"answers_community":["A (69%)","C (18%)","13%"]},{"id":"7hnQxSkRh6wHF4p2UwXz","answer_images":[],"answer_description":"","answer_ET":"A","unix_timestamp":1704302820,"isMC":true,"question_images":[],"topic":"1","question_id":187,"url":"https://www.examtopics.com/discussions/google/view/130218-exam-professional-data-engineer-topic-1-question-267/","exam_id":10,"question_text":"You are creating a data model in BigQuery that will hold retail transaction data. Your two largest tables, sales_transaction_header and sales_transaction_line, have a tightly coupled immutable relationship. These tables are rarely modified after load and are frequently joined when queried. You need to model the sales_transaction_header and sales_transaction_line tables to improve the performance of data analytics queries. What should you do?","discussion":[{"comment_id":"1114654","poster":"raaad","content":"Selected Answer: A\n- In BigQuery, nested and repeated fields can significantly improve performance for certain types of queries, especially joins, because the data is co-located and can be read efficiently. - - This approach is often used in data warehousing scenarios where query performance is a priority, and the data relationships are immutable and rarely modified.","timestamp":"1720190160.0","upvote_count":"5"},{"upvote_count":"2","poster":"josech","content":"Selected Answer: A\nOption A https://cloud.google.com/bigquery/docs/best-practices-performance-nested","comment_id":"1213928","timestamp":"1732048080.0"},{"content":"Selected Answer: A\noptional A","comment_id":"1175605","upvote_count":"1","poster":"hanoverquay","timestamp":"1726544460.0"},{"upvote_count":"1","comment_id":"1155160","timestamp":"1724199300.0","poster":"JyoGCP","content":"Selected Answer: A\nOption A"},{"content":"Selected Answer: A\nOption A","upvote_count":"1","timestamp":"1720876020.0","comment_id":"1121776","poster":"Matt_108"},{"poster":"scaenruy","content":"Selected Answer: A\nA. Create a sales_transaction table that holds the sales_transaction_header information as rows and the sales_transaction_line rows as nested and repeated fields.","comment_id":"1112968","upvote_count":"1","timestamp":"1720020420.0"}],"timestamp":"2024-01-03 18:27:00","answer":"A","choices":{"A":"Create a sales_transaction table that holds the sales_transaction_header information as rows and the sales_transaction_line rows as nested and repeated fields.","C":"Create a sales_transaction table that stores the sales_transaction_header and sales_transaction_line data as a JSON data type.","D":"Create separate sales_transaction_header and sales_transaction_line tables and, when querying, specify the sales_transaction_line first in the WHERE clause.","B":"Create a sales_transaction table that holds the sales_transaction_header and sales_transaction_line information as rows, duplicating the sales_transaction_header data for each line."},"answers_community":["A (100%)"]},{"id":"n7qcvG4HebmHesvEofPM","answer":"C","question_images":[],"exam_id":10,"discussion":[{"content":"Selected Answer: C\n- Graceful Data Transition: Draining the old pipeline ensures it processes all existing data in its buffers and watermarks before shutting down, preventing data loss or inconsistencies.\n- Minimal Latency Increase: The latency increase will be limited to the amount of time it takes to drain the old pipeline, typically within the acceptable 10-minute threshold.","poster":"raaad","comment_id":"1114680","upvote_count":"8","timestamp":"1704474240.0"},{"poster":"AlizCert","timestamp":"1707679920.0","upvote_count":"6","comments":[{"upvote_count":"1","poster":"SamuelTsch","comment_id":"1305178","timestamp":"1730322120.0","content":"we don't restart the drained pipeline."},{"comment_id":"1181875","timestamp":"1711299960.0","content":"So why not B it is the better choice to save intermediate state and easy to use","poster":"d11379b","upvote_count":"2"}],"comment_id":"1147610","content":"I don't think C is correct, as it will immediately fire the window:\n\"Draining can result in partially filled windows. In that case, if you restart the drained pipeline, the same window might fire a second time, which can cause issues with your data. \"\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#effects\n\nMaybe \"A\" means launching a replacement job?\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Launching"},{"timestamp":"1732396140.0","comment_id":"1316803","content":"Why not B?\nhttps://cloud.google.com/dataflow/docs/guides/upgrade-guide#stop-and-replace","poster":"petulda","upvote_count":"1"},{"timestamp":"1722952680.0","content":"Selected Answer: C\nThere is requirement to avoid data loss.\n\nhttps://cloud.google.com/dataflow/docs/guides/upgrade-guide#stop-and-replace\n\"To avoid data loss, in most cases, draining is the preferred action.\"","upvote_count":"1","comment_id":"1261692","poster":"STEVE_PEGLEG"},{"timestamp":"1718182680.0","poster":"Ouss_123","comment_id":"1228910","content":"Selected Answer: C\n- Draining the old pipeline ensures that it finishes processing all in-flight data before stopping, which prevents data loss and inconsistencies.\n- After draining, you can start the new pipeline, which will begin processing new data from where the old pipeline left off.\n- This approach maintains a smooth transition between the old and new versions, minimizing latency increases and avoiding data gaps or overlaps.\n\n==> Other options, such as updating, snapshotting, or canceling, might not provide the same level of consistency and could lead to data loss or increased latency beyond the acceptable 10-minute window. Draining is the safest method to ensure a seamless transition.","upvote_count":"2"},{"timestamp":"1711300140.0","content":"Selected Answer: B\nI would choose B as mentioned by Alizcert, a simple drain may cause problem \nDataflow snapshots save the state of a streaming pipeline, which lets you start a new version of your Dataflow job without losing state. Snapshots are useful for backup and recovery, testing and rolling back updates to streaming pipelines, and other similar scenarios.","poster":"d11379b","comment_id":"1181876","upvote_count":"2"},{"timestamp":"1710570600.0","poster":"hanoverquay","comment_id":"1174782","upvote_count":"1","content":"Selected Answer: C\nC option"},{"content":"Selected Answer: C\nOption C, draining the old pipeline solves all requests","timestamp":"1705158540.0","poster":"Matt_108","comment_id":"1121779","upvote_count":"1"},{"comment_id":"1112973","timestamp":"1704303060.0","content":"Selected Answer: C\nC. Drain the old pipeline, then start the new pipeline.","poster":"scaenruy","upvote_count":"2"}],"unix_timestamp":1704303060,"question_id":188,"answer_description":"","topic":"1","question_text":"You created a new version of a Dataflow streaming data ingestion pipeline that reads from Pub/Sub and writes to BigQuery. The previous version of the pipeline that runs in production uses a 5-minute window for processing. You need to deploy the new version of the pipeline without losing any data, creating inconsistencies, or increasing the processing latency by more than 10 minutes. What should you do?","url":"https://www.examtopics.com/discussions/google/view/130219-exam-professional-data-engineer-topic-1-question-268/","isMC":true,"answer_images":[],"answers_community":["C (88%)","12%"],"timestamp":"2024-01-03 18:31:00","choices":{"D":"Cancel the old pipeline, then start the new pipeline.","C":"Drain the old pipeline, then start the new pipeline.","A":"Update the old pipeline with the new pipeline code.","B":"Snapshot the old pipeline, stop the old pipeline, and then start the new pipeline from the snapshot."},"answer_ET":"C"},{"id":"2A80gbGhKt8X8VenDULw","discussion":[{"upvote_count":"13","comment_id":"1114681","timestamp":"1704474480.0","content":"Selected Answer: B\n- It utilizes Data Catalog's native support for both BigQuery datasets and Pub/Sub topics. \n- For PostgreSQL tables running on a Compute Engine instance, you'd use Data Catalog APIs to create custom entries, as Data Catalog does not automatically discover external databases like PostgreSQL.","comments":[{"content":"Agree. https://cloud.google.com/data-catalog/docs/concepts/overview#catalog-non-google-cloud-assets","upvote_count":"4","poster":"AllenChen123","timestamp":"1705814160.0","comment_id":"1127636"}],"poster":"raaad"},{"timestamp":"1706516640.0","upvote_count":"12","content":"Selected Answer: C\nData Catalog is the best choice. But for catalogging PostgreSQL it is better to use a connector when available, instead of using API. \nhttps://cloud.google.com/data-catalog/docs/integrate-data-sources#integrate_unsupported_data_sources","comment_id":"1134752","poster":"datapassionate","comments":[{"content":"I agree. On the linked page:\nIf you can't find a connector for your data source, you can still manually integrate it by creating entry groups and custom entries.\nAs we can find a connector there, it should be used.","poster":"7787de3","comment_id":"1283155","upvote_count":"1","timestamp":"1726230180.0"},{"content":"Agree. If it doesn't have a connector, it must be manually built on the Data Catalog API.\nAs PostgreSQL already has a connector it's the best option is C","upvote_count":"4","comment_id":"1137469","timestamp":"1706780880.0","poster":"tibuenoc"}]},{"timestamp":"1736173800.0","comment_id":"1337181","content":"Selected Answer: A\nhttps://cloud.google.com/data-catalog/docs/concepts/overview#automatic_cataloging_of_assets\nhttps://cloud.google.com/data-catalog/docs/concepts/overview#catalog-non-google-cloud-assets","poster":"Pime13","upvote_count":"1"},{"timestamp":"1734911760.0","poster":"AWSandeep","comment_id":"1330621","content":"Selected Answer: B\nThis section explains it clearly: https://cloud.google.com/data-catalog/docs/integrate-data-sources#integrate_unsupported_data_sources.","upvote_count":"1"},{"upvote_count":"3","content":"Selected Answer: C\nThis is C. To clarify some issues below with B, the links provided by supporters of B actually do say that it's preferable to use a community connector where available, and to only use the API when the case is genuinely not supported by community connectors. \nIn this case it's Postgresql, so it's supported, see here for full list: https://cloud.google.com/data-catalog/docs/integrate-data-sources#integrate_on-premises_data_sources\n\nSo this would be B if it was something like Q+ or some genuinely unsupported database, but postgres is supported for community connector.","comment_id":"1294639","timestamp":"1728375660.0","poster":"baimus"},{"timestamp":"1724517300.0","comment_id":"1271742","comments":[{"timestamp":"1724517420.0","upvote_count":"1","comment_id":"1271744","content":"C. While similar to B, using custom connectors for PostgreSQL might involve more development effort than using the Data Catalog APIs directly.","poster":"shanks_t"}],"content":"Selected Answer: B\nData Catalog automatically catalogs metadata from Google Cloud sources such as BigQuery, Vertex AI, Pub/Sub, Spanner, Bigtable, and more.\n\nTo catalog metadata from non-Google Cloud systems in your organization, you can use the following:\n\nCommunity-contributed connectors to multiple popular on-premises data sources\nManually build on the Data Catalog APIs for custom entries","poster":"shanks_t","upvote_count":"2"},{"timestamp":"1723281840.0","content":"raaad mostly correct and we can check his description supporting his answer so we can go with it .Cheers mate","upvote_count":"1","comment_id":"1263405","poster":"meh_33"},{"poster":"987af6b","timestamp":"1722105420.0","comment_id":"1256420","upvote_count":"2","comments":[{"content":"Changed my mind. B.\n-This is not on premise, so the custom connector should not be applicable\n-Question says keep manual dev and config to a minimum","poster":"987af6b","comment_id":"1256788","timestamp":"1722170280.0","upvote_count":"1"}],"content":"Selected Answer: C\nI’m voting for C because the documentation states that Postgres is a custom connector developed by the community."},{"timestamp":"1718595420.0","upvote_count":"3","poster":"fitri001","comment_id":"1231691","content":"Selected Answer: B\nBigQuery Datasets and Pub/Sub Topics: Google Data Catalog can automatically catalog metadata from BigQuery and Pub/Sub, making it easy to discover and manage these data assets without additional development effort.\n\nPostgreSQL Tables: While Data Catalog does not have built-in connectors for PostgreSQL, you can use the Data Catalog APIs to manually catalog the PostgreSQL tables. This requires some custom development but is manageable compared to creating custom connectors for everything."},{"timestamp":"1716375900.0","poster":"virat_kohli","content":"Selected Answer: B\nB. Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables.","comment_id":"1215683","upvote_count":"1"},{"content":"Selected Answer: B\nOption B leverages Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics, which streamlines the process and reduces manual effort. Using Data Catalog APIs to manually catalog PostgreSQL tables ensures consistency across all data assets while minimizing development and configuration efforts.","comment_id":"1211340","timestamp":"1715683860.0","poster":"Cassim","upvote_count":"1"},{"timestamp":"1714022400.0","comments":[{"poster":"LaxmanTiwari","content":"data catalog api will come into effect if custom connectors are not available via community repos.","upvote_count":"1","timestamp":"1714022520.0","comment_id":"1201759"}],"content":"Selected Answer: C\nI vote for c as per Integrate on-premises data sources\nTo integrate on-premises data sources, you can use the corresponding Python connectors contributed by the community:\n\nunder the link\n\nhttps://cloud.google.com/data-catalog/docs/integrate-data-sources","upvote_count":"3","poster":"LaxmanTiwari","comment_id":"1201758"},{"timestamp":"1712478240.0","comment_id":"1190850","comments":[{"upvote_count":"1","poster":"joao_01","comment_id":"1190851","content":"Link: https://cloud.google.com/data-catalog/docs/concepts/overview#catalog-non-google-cloud-assets","timestamp":"1712478240.0"}],"poster":"joao_01","content":"In the opction C, the expression \"Use custom connectors to manually catalog PostgreSQL tables.\" is refering to the use case of Google when you want to use \"Community-contributed connectors to multiple popular on-premises data sources\". As you can see, this connectors are for ON-PREMISSES data sources ONLY. In this case the Postgres is in a VM in the cloud. Thus, the option correct is B.","upvote_count":"3"},{"comments":[{"content":"I think “custom connector” here may just infer that this is not official tools? as the doc mentioned “ connectors contributed by the community” \nAnd should not be B as “manually catalog by API “ this is a way even more basic than using connector","timestamp":"1711301280.0","comment_id":"1181886","poster":"d11379b","upvote_count":"1"}],"timestamp":"1710570420.0","comment_id":"1174778","content":"Selected Answer: B\noption B, there's no need to build a custom connector now, postgreSQL is now supported\nhttps://github.com/GoogleCloudPlatform/datacatalog-connectors-rdbms/tree/master/google-datacatalog-postgresql-connector","poster":"hanoverquay","upvote_count":"1"},{"content":"Selected Answer: B\nUse Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables.","poster":"Y___ash","upvote_count":"1","comment_id":"1172481","timestamp":"1710328800.0"},{"timestamp":"1708962600.0","content":"Selected Answer: B\nDatacatalog API contain the connector for postgresql with using it developer don't have to create the custom connectors","poster":"Harshzh12","comment_id":"1159898","upvote_count":"1"},{"upvote_count":"5","content":"Selected Answer: C\nGoogle Recommendation: If you can't find a connector for your data source, you can still manually integrate it by creating entry groups and custom entries. To do that, you can:\n- Use one of the Data Catalog Client Libraries in one of the following languages: C#, Go, Java, Node.js, PHP, Python, or Ruby.\n- Or manually build on the Data Catalog API.\n\nHowever, there is a connector for PostgreSQL, so option C.","comment_id":"1152871","poster":"ML6","timestamp":"1708207980.0"},{"upvote_count":"3","comment_id":"1145329","content":"Selected Answer: C\nIf you can't find a connector for your data source, you can still manually integrate it by creating entry groups and custom entries. To do that, you can:\n\n- Manually build on the Data Catalog API.","poster":"saschak94","timestamp":"1707467160.0"},{"poster":"Matt_108","timestamp":"1705158780.0","comment_id":"1121783","upvote_count":"3","content":"Selected Answer: B\nOption B - Data Catalog automatically maps out GCP resources and dev efforts are minimized by leveraging the data catalog API to do the same for postgresql db"},{"upvote_count":"2","comment_id":"1117038","poster":"GCP001","timestamp":"1704750480.0","content":"B.\n-- Looks much better option as needed low development efforts. \n-- C not looking right as it will need lot of dev efforts for custom connectors."}],"isMC":true,"answers_community":["C (50%)","B (48%)","2%"],"question_images":[],"exam_id":10,"unix_timestamp":1704474480,"timestamp":"2024-01-05 18:08:00","answer":"C","question_id":189,"answer_ET":"C","question_text":"Your organization's data assets are stored in BigQuery, Pub/Sub, and a PostgreSQL instance running on Compute Engine. Because there are multiple domains and diverse teams using the data, teams in your organization are unable to discover existing data assets. You need to design a solution to improve data discoverability while keeping development and configuration efforts to a minimum. What should you do?","answer_description":"","topic":"1","url":"https://www.examtopics.com/discussions/google/view/130424-exam-professional-data-engineer-topic-1-question-269/","answer_images":[],"choices":{"A":"Use Data Catalog to automatically catalog BigQuery datasets. Use Data Catalog APIs to manually catalog Pub/Sub topics and PostgreSQL tables.","C":"Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use custom connectors to manually catalog PostgreSQL tables.","B":"Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables.","D":"Use customer connectors to manually catalog BigQuery datasets, Pub/Sub topics, and PostgreSQL tables."}},{"id":"03OQANLAT9nAws69vMTl","isMC":true,"question_id":190,"question_images":[],"answer_description":"","timestamp":"2020-03-19 10:54:00","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/16969-exam-professional-data-engineer-topic-1-question-27/","topic":"1","discussion":[{"content":"Answer: B\nDescription: Best Choice out of given options.","poster":"[Removed]","upvote_count":"34","timestamp":"1601192460.0","comment_id":"68552"},{"upvote_count":"17","timestamp":"1600502040.0","content":"Should be B","comment_id":"65944","poster":"[Removed]"},{"comment_id":"1340408","timestamp":"1736868420.0","upvote_count":"1","content":"Selected Answer: B\nI think the best option is B.\nD could be an option but what if the feature is very correlated to the result ?","poster":"sofiane_kihal"},{"content":"B: combine the dependent features. It is more like PCA (principal component analysis). \nD: could be the answer, but what if that feature is very important, or how often do you get a feature with more than 50% NULL values?","poster":"mark1223jkh","comment_id":"1212699","timestamp":"1731825780.0","upvote_count":"1"},{"content":"Selected Answer: B\nI am not into ML, to be honest, so I will rely on community opinion and choose B","timestamp":"1716294300.0","poster":"axantroff","comment_id":"1076366","upvote_count":"2"},{"comment_id":"1050535","content":"Selected Answer: B\nB. Combine highly co-dependent features into one representative feature.\n\nCombining highly correlated features into a single representative feature can reduce the dimensionality of your dataset, making the training process faster while preserving relevant information. This approach often helps eliminate redundancy in the input data.\n\nOption A (eliminating features that are highly correlated to the output labels) can be counterproductive, as you want to maintain features that are informative for your prediction task. Removing features that are correlated with the output may reduce model accuracy.\n\nOption C (averaging feature values in batches of 3) is not a common technique for reducing dimensionality, and it could lead to loss of important information.\n\nOption D (removing features with null values for more than 50% of training records) can help reduce the dimensionality and may be useful if you have a large number of features with missing data, but it may not necessarily address co-dependency among features.","timestamp":"1713787740.0","poster":"rtcpost","upvote_count":"4"},{"timestamp":"1710555060.0","poster":"suku2","upvote_count":"1","content":"Selected Answer: B\nB. Combine highly co-dependent features into one representative feature.\nThis is the best choice.","comment_id":"1008767"},{"content":"\"D\" is wrong, and very dangerous. For instance, it might represent modern measurements only installed in <50% of weather stations, but very very precise and valuable. \n\nNulls are not a problem for models, out-of-the-box or with transformations models can handle nulls just fine.","timestamp":"1699517100.0","upvote_count":"1","comment_id":"892818","poster":"WillemHendr"},{"upvote_count":"1","content":"Selected Answer: D\nwrong question. there are two answers B, D","timestamp":"1692588420.0","comment_id":"816228","comments":[{"upvote_count":"2","timestamp":"1692589080.0","content":"B. Combine highly co-dependent features into one representative feature.\n-> Explainable feature should be dependent from each other feature. expecially not deep leanring. so, in this case normally eliminated or combined\n\nD. Remove the features that have null values for more than 50% of the training records.\n-> it's too large null data in the feature. normally the feature should be removed because it's too hard to fill up replacing data","poster":"jin0","comment_id":"816236"}],"poster":"jin0"},{"content":"Answer is Combine highly co-dependent features into one representative feature.\n\nA: correlated to output means that feature can contribute a lot to the model. so not a good idea.\nC: you need to run with almost same number, but you will iterate twice, once for averaging and second time to feed the averaged value.\nD: removing features even if it 50% nulls is not good idea, unless you prove that it is not at all correlated to output. But this is nowhere so can remove.","comments":[{"content":"But, if there are null datas more than 50% then, it should be eliminated because there are two ways to train the model. first, remove records containing having null but in this case there are too many records should be removed and second, replace null to other data but in this case cause of it's too large data having null then It's literally hard to replace. so normally the feature having too many null data should be removed. So, there are two answer in this question B, D I think","timestamp":"1692587820.0","comment_id":"816220","upvote_count":"2","poster":"jin0"}],"timestamp":"1688497020.0","poster":"AzureDP900","comment_id":"766082","upvote_count":"1"},{"comment_id":"723259","upvote_count":"2","timestamp":"1684648800.0","content":"I have a doubt, instead of combining highly corelated features why cant we remove corelated features which may give much more simplified dataset?","poster":"Thasni"},{"comment_id":"616947","content":"Selected Answer: B\nAnswer: B\n\nData that is co-dependent is high corelated is some kind of reduldant information in some cases. If the features x1, x2 and x3 are x2 = x1 + 1 and x3 = 2*x1, for example, x2 and x3 are reduldant because can be explained with x1 feature, so can be excluded of the the model. Other option is to group this features. There is a lot of ways to resolve, but the main ideia is to use data engineer in co-depedent features to reduce the number of features in the model","poster":"noob_master","timestamp":"1671142860.0","upvote_count":"2"},{"upvote_count":"1","timestamp":"1670423820.0","comment_id":"612754","content":"Selected Answer: B\nThis method is called Data Engineering, that you combine two or more values to get a custom info, this will avoid that the model read an extra column on the training and probably increase it's accuracy.","poster":"Ishiske"},{"timestamp":"1668349380.0","upvote_count":"1","comment_id":"601150","poster":"Yad_datatonic","content":"Answer: B"},{"upvote_count":"2","content":"Selected Answer: B\nCo-dependent -> correlated -> correlated info = already present info in other variable.","timestamp":"1666265460.0","poster":"alecuba16","comment_id":"588599"},{"comments":[{"comment_id":"606417","content":"Yes. But nearly 50% of the non-null data still seems to be a lot to ignore.","timestamp":"1669263120.0","poster":"Dayashankar_H_A","upvote_count":"2"}],"content":"Trying to find a reason why it is B and not D, found this and it seems the answer is D.\nhttps://cloud.google.com/architecture/data-preprocessing-for-ml-with-tf-transform-pt1\nFeature selection. Selecting a subset of the input features for training the model, and ignoring the irrelevant or redundant ones, using filter or wrapper methods. This can also involve simply dropping features if the features are missing a large number of values.","upvote_count":"7","comment_id":"544536","poster":"pamepadero","timestamp":"1660128780.0"},{"timestamp":"1658480160.0","content":"Selected Answer: B\nB\nnull values can have many meanings and need different approach to handle, otherwise it causes inaccurate model, so not D","upvote_count":"4","poster":"exnaniantwort","comment_id":"529764"},{"upvote_count":"1","poster":"ZIMARAKI","content":"Selected Answer: B\nFor me the best option is B","timestamp":"1657992240.0","comment_id":"525204"},{"content":"Selected Answer: B\nB is the correct option","poster":"aet2123","timestamp":"1655595960.0","comment_id":"504572","upvote_count":"1"},{"timestamp":"1654327260.0","poster":"BigQuery","comment_id":"493613","comments":[{"timestamp":"1654327320.0","comment_id":"493614","content":"Sorry B is correct. ignore my previous comment","upvote_count":"3","poster":"BigQuery"}],"content":"Selected Answer: D\nD is correct","upvote_count":"1"},{"content":"Ans: B\nA: correlated to output means that feature can contribute a lot to the model. so not a good idea.\nC: you need to run with almost same number, but you will iterate twice, once for averaging and second time to feed the averaged value.\nD: removing features even if it 50% nulls is not good idea, unless you prove that it is not at all correlated to output. But this is nowhere so can remove.","upvote_count":"8","timestamp":"1649779320.0","poster":"anji007","comment_id":"461151"},{"upvote_count":"6","timestamp":"1640540700.0","comment_id":"391374","content":"Vote for 'B'\n\ncombining features to createte a new feature is a step of \"Feature construction\"\nor\ndecomposing or splitting features to create new features.\n\nIdeally, PCA should be apply if we want to reduce the dimension.\n\nRemoving those columns / features - where Data is miss > 50% (may improve the speed) - but will decrease the accuracy as well. So instead of dropping features where we have missing data, we need to impute something","comments":[{"comment_id":"397195","upvote_count":"1","comments":[{"poster":"sumanshu","upvote_count":"2","comment_id":"401841","content":"PCA (principal component analysis) - a decomosition tehcnique apply on numerical data....this algo automatically check which features (columns) are more related with output and which are less related...and it keep only related one....(i.e. reduce dimension) or number of features...generally applied...when we have thouusands of columns (features)","timestamp":"1641647760.0","comments":[{"comment_id":"401842","timestamp":"1641647820.0","upvote_count":"2","poster":"sumanshu","content":"Otheriwse...we need to manually check all columns (using plotting the graphs) or using mathematical formula.."}]}],"content":"What is PCA?","poster":"awssp12345","timestamp":"1641173460.0"},{"content":"'D\" could not be the answer - If There is NO missing value OR if all features have missing values > 50% does it means we drop everything - NO , right ?.\n\nSo, B is more appropriate","comment_id":"391376","timestamp":"1640540820.0","upvote_count":"2","poster":"sumanshu"}],"poster":"sumanshu"},{"poster":"moonlightbeamer","content":"Answer: A\n\ndirectly eliminate redundant feature","timestamp":"1640051040.0","upvote_count":"1","comment_id":"386628","comments":[{"timestamp":"1640051220.0","poster":"moonlightbeamer","upvote_count":"2","content":"sorry, change mind, B. if corelated to other feature, can remove, but A is corelate to label...","comment_id":"386630"}]},{"poster":"naga","comment_id":"285005","timestamp":"1628265360.0","upvote_count":"3","content":"Correct B"},{"poster":"[Removed]","timestamp":"1627032720.0","comment_id":"274531","content":"Feature selection. Selecting a subset of the input features for training the model, and ignoring the irrelevant or redundant ones, using filter or wrapper methods. This can also involve simply dropping features if the features are missing a large number of values","upvote_count":"2"},{"comment_id":"243358","poster":"NamitSehgal","timestamp":"1623648780.0","upvote_count":"5","content":"Should be B"},{"upvote_count":"1","timestamp":"1621493820.0","poster":"Erso","comment_id":"223436","content":"The answer is D, read this article https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e"},{"comment_id":"194028","upvote_count":"4","timestamp":"1617689460.0","poster":"Mushuskath","content":"Should be B. Remember that combining three features doesn’t necessarily mean taking an average. While option number D is quite tempting, we must remember that null values do not mean missing values."},{"upvote_count":"2","timestamp":"1615980960.0","content":"Answer should be B. While option D seems to be logical but we can not decide on dropping a column unless we know exactly what information that column holds.","comment_id":"180804","poster":"SteelWarrior"},{"comment_id":"159733","timestamp":"1613554860.0","poster":"bkcs","upvote_count":"3","content":"Answer D:\nRefer Feature Selection point here : https://cloud.google.com/solutions/machine-learning/data-preprocessing-for-ml-with-tf-transform-pt1#preprocessing_operations"},{"content":"If three features are combined to one feature, you will still need all 3 features to transform in your scoring datasets to predict i.e. you have not removed them just transformed","poster":"Ankit267","comments":[{"timestamp":"1617689280.0","content":"Goal is to speed up training.","upvote_count":"3","comment_id":"194024","poster":"Mushuskath"}],"comment_id":"130292","upvote_count":"1","timestamp":"1610168700.0"},{"poster":"kaush","upvote_count":"2","comments":[{"comment_id":"194025","poster":"Mushuskath","timestamp":"1617689340.0","upvote_count":"3","content":"Null does not mean missing"},{"comment_id":"149516","poster":"YuriP","timestamp":"1612334760.0","content":"Null may be how the i.e. weather station reports mm of rainfall. It's unknown, so you can't drop it.","upvote_count":"2"}],"timestamp":"1609935360.0","comment_id":"127661","content":"This is a Question with very clear answer which is D, The first thing to do to reduce dimentionality is to remove features with more than 50% missing values , If we have two co-dependent variables we drop one ,"},{"poster":"kaush","timestamp":"1609935060.0","content":"Should be D , its is correct to eliminate co-dependent features but its incorrect to transform to single feature","upvote_count":"2","comments":[{"poster":"sumanshu","content":"Why it's incorrect? we did both things, i.e. convert single feature to multiple and vice-versa as well","upvote_count":"1","timestamp":"1640540940.0","comment_id":"391380"}],"comment_id":"127652"}],"answers_community":["B (90%)","10%"],"choices":{"D":"Remove the features that have null values for more than 50% of the training records.","C":"Instead of feeding in each feature individually, average their values in batches of 3.","B":"Combine highly co-dependent features into one representative feature.","A":"Eliminate features that are highly correlated to the output labels."},"exam_id":10,"answer":"B","unix_timestamp":1584611640,"question_text":"You are building a model to predict whether or not it will rain on a given day. You have thousands of input features and want to see if you can improve training speed by removing some features while having a minimum effect on model accuracy. What can you do?","answer_ET":"B"}],"exam":{"numberOfQuestions":319,"id":10,"isMCOnly":true,"isImplemented":true,"lastUpdated":"15 Feb 2025","provider":"Google","isBeta":false,"name":"Professional Data Engineer"},"currentPage":38},"__N_SSP":true}