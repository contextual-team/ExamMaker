{"pageProps":{"questions":[{"id":"SFcV3PgCLUwAkpDcmJUX","topic":"1","question_images":[],"question_text":"You've migrated a Hadoop job from an on-prem cluster to dataproc and GCS. Your Spark job is a complicated analytical workload that consists of many shuffling operations and initial data are parquet files (on average 200-400 MB size each). You see some degradation in performance after the migration to Dataproc, so you'd like to optimize for it. You need to keep in mind that your organization is very cost-sensitive, so you'd like to continue using Dataproc on preemptibles (with 2 non-preemptible workers only) for this workload.\nWhat should you do?","exam_id":10,"choices":{"C":"Switch from HDDs to SSDs, copy initial data from GCS to HDFS, run the Spark job and copy results back to GCS.","A":"Increase the size of your parquet files to ensure them to be 1 GB minimum.","B":"Switch to TFRecords formats (appr. 200MB per file) instead of parquet files.","D":"Switch from HDDs to SSDs, override the preemptible VMs configuration to increase the boot disk size."},"answers_community":["D (56%)","A (37%)","7%"],"answer":"D","answer_images":[],"isMC":true,"answer_ET":"D","timestamp":"2020-03-14 14:44:00","unix_timestamp":1584193440,"discussion":[{"comment_id":"65087","content":"Should be A:\n\nhttps://stackoverflow.com/questions/42918663/is-it-better-to-have-one-large-parquet-file-or-lots-of-smaller-parquet-files\nhttps://www.dremio.com/tuning-parquet/\n\nC & D will improve performance but need to pay more $$","comments":[{"comment_id":"1342556","poster":"grshankar9","content":"Switching to SSDs definitely increases the cost, eliminating C & D.","timestamp":"1737210660.0","upvote_count":"1"},{"comment_id":"458353","poster":"diluvio","content":"It is A . please read the links above","upvote_count":"5","timestamp":"1633536960.0"},{"content":"https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance","poster":"odacir","comment_id":"737845","timestamp":"1670415960.0","upvote_count":"1"},{"comments":[{"comment_id":"624757","timestamp":"1656517560.0","upvote_count":"1","poster":"rr4444","content":"You can have local SSDs for the dataproc normal or preemptible VMs https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-pd-ssd"},{"content":"Also for Shuffling Operations, one need to override the preemptible VMs configuration to increase boot disk size.\n(Second half of answer D is correct but first half is wrong)","poster":"raf2121","timestamp":"1627477260.0","upvote_count":"1","comment_id":"416203"}],"poster":"raf2121","content":"Point for discussion - Another reason why it can't be C or D.\nSSD's are not available on pre-emptible Worker nodes (answers didn't say whether they wanted to switch from HDD to SDD for Master nodes)\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs","comment_id":"416201","upvote_count":"8","timestamp":"1627476900.0"},{"timestamp":"1670456760.0","poster":"zellck","upvote_count":"3","content":"https://cloud.google.com/dataproc/docs/support/spark-job-tuning#limit_the_number_of_files\n\nStore data in larger file sizes, for example, file sizes in the 256MBâ€“512MB range.","comment_id":"738470"},{"upvote_count":"3","content":"you are right C&D will pay more $. the reason of this questions is shuffling I think. and to reduce shuffling between jobs then make file size larger","comment_id":"825629","timestamp":"1677657900.0","poster":"jin0"}],"poster":"rickywck","timestamp":"1584432840.0","upvote_count":"70"},{"poster":"madhu1171","timestamp":"1584193440.0","comment_id":"63867","comments":[{"comments":[{"poster":"ch3n6","timestamp":"1592734380.0","comments":[{"poster":"VishalB","comments":[{"poster":"FARR","upvote_count":"3","timestamp":"1597567980.0","content":"File sizes are already within the expected range for GCS (128MB-1GB) so not C.\nD seems most feasible","comment_id":"159104"}],"content":"C is recommended only -\nIf you have many small files, consider copying files for processing to the local HDFS and then copying the results back","timestamp":"1595256720.0","upvote_count":"1","comment_id":"139639"}],"comment_id":"115455","content":"C is correct. D is wrong. they are using 'dataproc and GCS', not related to boot disk at all .","upvote_count":"2"}],"timestamp":"1584703080.0","poster":"jvg637","comment_id":"66249","upvote_count":"15","content":"D: # By default, preemptible node disk sizes are limited to 100GB or the size of the non-preemptible node disk sizes, whichever is smaller. However you can override the default preemptible disk size to any requested size. Since the majority of our cluster is using preemptible nodes, the size of the disk used for caching operations will see a noticeable performance improvement using a larger disk. Also, SSD's will perform better than HDD. This will increase costs slightly, but is the best option available while maintaining costs."}],"content":"Answer should be D","upvote_count":"12"},{"timestamp":"1736236980.0","content":"Selected Answer: A\nA. \nNot D because it doesn't make sens to move to SSD when cost-senstive","upvote_count":"1","comment_id":"1337485","poster":"f74ca0c"},{"timestamp":"1730613600.0","comment_id":"1306439","upvote_count":"1","poster":"Javakidson","content":"A is the answer"},{"poster":"SamuelTsch","comment_id":"1302126","content":"Selected Answer: A\nI think either A or C. The problem is occured by I/O performance. Option A is feasible, which reduces the number of files leading better parallel processing. Option C tries to handle I/O performance issue. \nTaking other factors like budget and no mention of HDD/SSD, option A is possible the correct answer.","timestamp":"1729701540.0","upvote_count":"1"},{"poster":"baimus","comment_id":"1288158","content":"Selected Answer: A\nThere's no mention of a drive type used, only GCS. That means A is the only sensible option.","timestamp":"1727101380.0","upvote_count":"1"},{"timestamp":"1721926140.0","content":"Selected Answer: A\nQuestion doesn't actually say they are using HDD in the scenario, for that reason I choose A","poster":"987af6b","upvote_count":"2","comment_id":"1255092"},{"content":"A\nWe don't know if HDD was used, so we can know what to do about that, but we know that the parquet files are small and much, and we can act on that by increasing the sizes to have lesser number of it.","upvote_count":"2","comment_id":"1145420","timestamp":"1707479340.0","poster":"philli1011"},{"upvote_count":"1","timestamp":"1701513780.0","poster":"rocky48","comments":[{"content":"Given the scenario and the cost-sensitive nature of your organization, the best option would be:\n\nC. Switch from HDDs to SSDs, copy initial data from GCS to HDFS, run the Spark job, and copy results back to GCS.\n\nOption C allows you to leverage the benefits of SSDs and HDFS while minimizing costs by continuing to use Dataproc on preemptible VMs. This approach optimizes both performance and cost-effectiveness for your analytical workload on Google Cloud.","upvote_count":"1","timestamp":"1701693120.0","comment_id":"1087630","poster":"rocky48"}],"content":"Selected Answer: A\nShould be A:\nhttps://stackoverflow.com/questions/42918663/is-it-better-to-have-one-large-parquet-file-or-lots-of-smaller-parquet-files","comment_id":"1086069"},{"content":"Selected Answer: A\nhttps://stackoverflow.com/questions/42918663/is-it-better-to-have-one-large-parquet-file-or-lots-of-smaller-parquet-files\n\nCost effective is the key in the question.","poster":"Mathew106","timestamp":"1690105860.0","upvote_count":"1","comment_id":"960306"},{"content":"Selected Answer: D\nPreemptible VMs can't be used for HDFS storage.\nAs a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads.","timestamp":"1679488500.0","poster":"Nandhu95","upvote_count":"1","comment_id":"847057"},{"upvote_count":"1","timestamp":"1677741300.0","comment_id":"826584","poster":"midgoo","comments":[{"timestamp":"1690216980.0","comment_id":"961827","poster":"Mathew106","content":"Optimal size is 1GB","upvote_count":"1"}],"content":"Selected Answer: D\nShould NOT be A as:\n1. The file size is already at the optimal size\n2. If the current file size works well in the current Hadoop, it is expected to have similar performance in Dataproc\n\nThe only difference between the current and Dataproc is that Dataproc is using preemptible nodes. So yes, it may incur a bit more cost by using SSD but assuming using the preemptible already save most of it, so we want to save less to improve the performance"},{"poster":"[Removed]","upvote_count":"1","comment_id":"819977","timestamp":"1677201720.0","content":"Selected Answer: A\nCost sensitive is the keyword."},{"upvote_count":"2","content":"this question asked by Google, So option C is not correct otherwise, good approach to use initial data in hdfs and swtich from HDD to SDDs for 2 non-preemptible node. \nOption D is right but they are not mentioning that they will stop using 2 non-preemptible node. but i assume it :P","comment_id":"809426","timestamp":"1676460720.0","poster":"musumusu"},{"comment_id":"789527","comments":[{"comment_id":"791431","timestamp":"1674972060.0","content":"agreed C over D as\nswitching from HDDs to SSDs and overriding the preemptible VMs configuration to increase the boot disk size, may not be the best solution for improving performance in this scenario because it doesn't address the main issue which is the large number of shuffling operations that are causing performance degradation. While SSDs may have faster read and write speeds than HDDs, they may not provide significant performance improvements for a workload that is primarily CPU-bound and heavily reliant on shuffling operations. Additionally, increasing the boot disk size of the preemptible VMs may not be necessary or cost-effective for this particular workload.","poster":"ayush_1995","upvote_count":"1"}],"poster":"PolyMoe","content":"Selected Answer: C\nC.\nref : https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\n- size recommended is 128MB-1GB ==> so it is not size issue ==> not A\n- there is no issue mentioned with file format ==> not B\n- D. could be a good solution, but requires overriding preemptible VMs. however, the questions asks to continue using preemtibles ==> not D\n- C. is a good solution.","timestamp":"1674817680.0","upvote_count":"3"},{"timestamp":"1671542100.0","content":"Selected Answer: D\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\n\nManage Cloud Storage file sizes\nTo get optimal performance, split your data in Cloud Storage into files with sizes from 128 MB to 1 GB. Using lots of small files can create a bottleneck. If you have many small files, consider copying files for processing to the local HDFS and then copying the results back.\n\nSwitch to SSD disks\nIf you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.","comment_id":"750917","poster":"slade_wilson","upvote_count":"2"},{"poster":"zellck","comment_id":"736803","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#switch_to_ssd_disks\nIf you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#use_preemptible_vms\nAs a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads. For details, see the page on preemptible VMs in the Dataproc documentation.","timestamp":"1670329920.0","upvote_count":"1"},{"upvote_count":"2","content":"Answer id D\nnot C because cannot use HDFS with preemptible VMs\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#use_preemptible_vms","comment_id":"725885","poster":"sfsdeniso","timestamp":"1669297980.0"},{"comment_id":"724887","upvote_count":"4","timestamp":"1669178700.0","poster":"dish11dish","content":"Selected Answer: D\nOption D is correct\n\nElimination Strategy:-\nA. Increase the size of your parquet files to ensure them to be 1 GB minimum (doesnâ€™t make sense as the file size are fit for migration to proceed with given scenario, recommended size is between 128 MB to 1 GB.)\nB. Switch to TFRecords formats (appr. 200MB per file) instead of parquet files(doesnâ€™t make sense to make changes to file format )\nC. Switch from HDDs to SSDs, copy initial data from GCS to HDFS, run the Spark job and copy results back to GCS(doesnâ€™t make sense to copy the file from GCS to HDFS as the workload that consists of many shuffling operations)\nD. Switch from HDDs to SSDs, override the preemptible VMs configuration to increase the boot disk size(perfect fit as the workload that consists of many shuffling operations which requires attention to increase the performance reference doc:- https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance )"},{"content":"Selected Answer: A\nit's A.\nLarger parquet files will be more efficient and it's a non-cost solution to implement in contrary to the SSD drives.","comment_id":"723541","comments":[{"timestamp":"1670456820.0","upvote_count":"1","comment_id":"738471","content":"Recommended file size is not 1GB.\n\nhttps://cloud.google.com/dataproc/docs/support/spark-job-tuning#limit_the_number_of_files\nStore data in larger file sizes, for example, file sizes in the 256MBâ€“512MB range.","poster":"zellck"}],"upvote_count":"1","timestamp":"1669037280.0","poster":"piotrpiskorski"},{"poster":"gudiking","comment_id":"720519","content":"Selected Answer: A\nhttps://www.dremio.com/blog/tuning-parquet/","upvote_count":"1","timestamp":"1668694860.0"},{"comments":[{"poster":"NicolasN","upvote_count":"3","comment_id":"715353","content":"Not True\n\n\"As a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads.\"\nðŸ”— https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#use_preemptible_vms\n\nMoreover:\n\"If you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.\"\nðŸ”— https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#switch_to_ssd_disks","timestamp":"1668094380.0"}],"content":"Selected Answer: A\nIt's A.\nD doesn't make sense because Spark does shuffling in memory, and in any case, it has nothing to do with the BOOT disk size.","timestamp":"1667761080.0","comment_id":"712585","poster":"cloudmon","upvote_count":"1"},{"poster":"josrojgra","content":"Selected Answer: D\nI choose D because the files already fit at the recommended size (https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#manage_cloud_storage_file_sizes).\n\nAnd to improve the performance with a shuffle-heavy workload is recommended to increase the boot disk of the preemptible VMs (https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#use_preemptible_vms)","upvote_count":"2","timestamp":"1666085820.0","comment_id":"698085"},{"upvote_count":"1","content":"A it is, SSD makes it expensive & defeats the question's purpose of no increased cost requirement","comment_id":"676608","poster":"clouditis","timestamp":"1663888620.0"},{"comment_id":"673004","upvote_count":"1","content":"Selected Answer: D\nD. Switch from HDDs to SSDs, override the preemptible VMs configuration to increase the boot disk size\n\nRead the section, it is pretty straightforward.\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance","timestamp":"1663570320.0","poster":"John_Pongthorn"},{"poster":"ducc","content":"Selected Answer: D\nagree with gcd56","upvote_count":"1","timestamp":"1661257260.0","comment_id":"650805"},{"comment_id":"648334","timestamp":"1660806300.0","content":"Selected Answer: A\nVote for A","upvote_count":"1","poster":"rickantonais"},{"timestamp":"1659630120.0","content":"Selected Answer: A\nSpark based on ram and GCS as alternative for hdfs storage,so the options left are increasing the parquet file size","poster":"francisjoseph","comment_id":"642547","upvote_count":"1"},{"upvote_count":"1","comment_id":"611657","timestamp":"1654396260.0","poster":"Devx198912233","content":"Selected Answer: A\nHas to be A. D will increase the cost and unnecessary as well. optimal size of parquet file is 1GB\nhttps://stackoverflow.com/questions/42918663/is-it-better-to-have-one-large-parquet-file-or-lots-of-smaller-parquet-files"},{"poster":"devric","timestamp":"1649190840.0","content":"Selected Answer: A\nShould be A. There is a Spark job who runs over data on RAM who needs to be readed just one time.","upvote_count":"1","comment_id":"581455"},{"comment_id":"542625","upvote_count":"3","poster":"Deepakd","comments":[{"timestamp":"1649315580.0","comment_id":"582263","content":"As you already said, its cost-sensitive! How can you still choose C/D?","poster":"tavva_prudhvi","upvote_count":"1"},{"comment_id":"581457","upvote_count":"1","content":"What it's the relation between the boot speed and the spark processing speed?","poster":"devric","timestamp":"1649191020.0"}],"timestamp":"1644261300.0","content":"First please note that the file sizes are between 200-400 MB each. Googleâ€™s recommended best practice for file size is between 128 MB to 1GB. \nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\nSo going by Googleâ€™s recommended best practices , these file sizes are proper. \nThis makes A incorrect. The files are within the recommended range. Further A says the file size needs to be 1 GB minimum. The word minimum implies the file sizes can go bigger than 1 GB. This is not Googleâ€™s recommended best practice. \nC is incorrect as well. C is recommended only when there are too many small file sizes. The files here are within the recommended range. \nB is not relevant. \nD is correct . But it would increase the cost. This is difficult to accept as the organisation is very cost sensitive. But this is the only option that would increase the performance."},{"timestamp":"1643485200.0","comment_id":"535685","poster":"kped21","upvote_count":"1","content":"C - If you have many small files, consider copying files for processing to the local HDFS and then copying the results back."},{"content":"Selected Answer: D\nIt is definitely D\n\na) \"To get optimal performance, split your data in Cloud Storage into files with sizes from 128 MB to 1 GB.\"\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\n\nFiles are already within this size range, so increasing to 1GB or over would worsen performance.\n\nb) Same as a) - changing file type would not affect performance.\n\nc) \"Your Dataproc cluster needs non-HDFS local disk space for shuffling.\"\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#adjust_storage_size\n\nHDFS is not used for shuffling workloads, so copying data to it wouldn't do much.\n\nd) \"If you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.\"\n\n\"As a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads.\"\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance","poster":"gdc56","timestamp":"1643201040.0","upvote_count":"6","comment_id":"532884"},{"poster":"medeis_jar","upvote_count":"1","timestamp":"1641494460.0","comment_id":"518472","content":"Selected Answer: A\nhttps://stackoverflow.com/questions/42918663/is-it-better-to-have-one-large-parquet-file-or-lots-of-smaller-parquet-files"},{"timestamp":"1640281260.0","upvote_count":"2","comment_id":"508069","comments":[{"poster":"tavva_prudhvi","timestamp":"1649315760.0","comment_id":"582267","content":"In the question, there's a word \"very-cost sensitive\"... Did you see the phrase? Company is very cost sensitive.. then how would you convert hdd to ssd?","upvote_count":"2"}],"poster":"MaxNRG","content":"Selected Answer: D\nD:\n1)Switch to SSD disks\nIf you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.\n\n2) Use preemptible VMs\nAs a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance"},{"content":"Preemptible VMs can't be used for HDFS storage -- SO NOT OPTION C\nOPTION D INCREASES COST -- SO IT A NO.\nI DON'T KNOW ABOUT B,\nSO THE ANSWER IS OPTION A, AND ITS ALO MENTIONED IN BELOW LINK\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance","poster":"Abhi16820","comment_id":"477498","timestamp":"1636812780.0","upvote_count":"1"},{"poster":"gcp_k","comment_id":"466275","content":"From GCP Documentation -> https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\n\nMany options were provided. Since the question says, the company is cost sensitive, I would go with option #1 listed in the GCP documentation which is increase the size to 1 GB as my first trial.\n\nI am choosing A","upvote_count":"2","timestamp":"1634926680.0"},{"content":"Company is very cost sensitive.. then how would you convert hdd to ssd?","upvote_count":"3","comment_id":"462683","poster":"[Removed]","timestamp":"1634312820.0"},{"content":"C seem to be correct : Spark jobs use RDD ( stored in memory) so to make jobs faster we should copy the source files to Local Workers HDFS and switch HDD to SSD.\nSources: https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#manage_cloud_storage_file_sizes","poster":"Ysance_AGS","timestamp":"1632833760.0","comment_id":"453407","upvote_count":"1"},{"upvote_count":"6","poster":"fire558787","timestamp":"1629186720.0","content":"ATTENTION: It's D, not A! From GCP Documentation: \n\n1) \"As a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads\" \n2) If you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.\n\nReference: https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance","comments":[{"timestamp":"1629368700.0","upvote_count":"5","content":"Are first 140Q enough from this website for certification exam?","poster":"RajatVyas","comments":[{"upvote_count":"1","poster":"Ral17","comment_id":"440467","content":"Why do you think so? Did you appear for the exam already?","timestamp":"1630945620.0"}],"comment_id":"427394"}],"comment_id":"426180"},{"content":"s per GCPâ€™s optimization recommendations, A cannot be answer since it says split your data in Cloud Storage into files with sizes from 128 MB to 1 GB and the data is already within given limits. Hence the answer is D - Switch to SSD disks.","timestamp":"1625505900.0","poster":"awssp12345","upvote_count":"2","comment_id":"399332"},{"poster":"sumanshu","content":"Option A and D , both looks okay\n\nBut vote for 'D' - because in question its mention - many shuffling operations (so SSD and override this configuration are recommended)","comments":[{"upvote_count":"1","comment_id":"399324","content":"Makes sense.","poster":"awssp12345","timestamp":"1625505540.0"}],"timestamp":"1625095680.0","comment_id":"395296","upvote_count":"5"},{"timestamp":"1618140300.0","content":"Correct C:\n\nhttps://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#comparing_cloud_storage_and_hdfs","upvote_count":"1","poster":"timolo","comment_id":"333242","comments":[{"content":"Preemptible VMs can't be used for HDFS storage.\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance","poster":"sumanshu","comment_id":"395291","timestamp":"1625095320.0","upvote_count":"1"}]},{"poster":"daghayeghi","content":"D:\nhttps://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms","comment_id":"308316","timestamp":"1615494540.0","upvote_count":"2"},{"timestamp":"1612578480.0","comment_id":"284502","poster":"someshsehgal","comments":[{"poster":"sumanshu","comment_id":"395292","upvote_count":"1","timestamp":"1625095380.0","content":"Preemptible VMs can't be used for HDFS storage."}],"content":"Correct C:\nTo get optimal performance, split your data in Cloud Storage into files with sizes from 128 MB to 1 GB. Using lots of small files can create a bottleneck. If you have many small files, consider copying files for processing to the local HDFS and then copying the results back.\nSwitch to SSD disks\nIf you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.","upvote_count":"2"},{"upvote_count":"1","comment_id":"278682","poster":"[Removed]","timestamp":"1611850500.0","content":"People who say c and d, note that point given as 'very cost sensitive'"},{"upvote_count":"3","timestamp":"1610306700.0","poster":"tikna","content":"A is correct answer. C and D are wrong because SSD is costly. They talk about shuffling if you increase size of file there will be less shuffling. hence A","comment_id":"264225"},{"timestamp":"1600965060.0","comment_id":"186327","poster":"zxing233","content":"https://cloud.google.com/bigtable/docs/choosing-ssd-hdd in BT doc says SSD is cost effective. so even it's a bit expensive but it reduce a lot the computing time which is money also","upvote_count":"2"},{"comment_id":"180167","comments":[],"content":"C is correct\nPer GCP Documentation, local HDFS Storage is a good option if:\neach file size is relatively small and heave I/O work load","poster":"VIncent9261111","upvote_count":"2","timestamp":"1600229760.0"},{"comment_id":"176891","timestamp":"1599712920.0","poster":"kino2020","content":"https://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\n------------------------------------------------------------------------------------------\nOptimize performance\nThis section discusses ways to get better performance and reduce cost while running Spark jobs.\nãƒ»Manage Cloud Storage file sizes\nãƒ»Switch to SSD disks\nãƒ»Place VMs in the same zone\nãƒ»Use preemptible VMs\n------------------------------------------------------------------------------------------\nThere are four main types of performance optimization.\nAmong them, the cost reference is Use preemptible VMs So, the likelihood is that D is high.\n------------------------------------------------------------------------------------------\nDataproc cluster can use preemptible VM instances as workers. This results in lower per-hour compute costs for your non-critical workloads than by using normal instances. However, there are some factors to consider when you use preemptible VMs:\n------------------------------------------------------------------------------------------","upvote_count":"1","comments":[{"timestamp":"1605726360.0","poster":"snamburi3","comment_id":"222192","upvote_count":"2","content":"D. \nPreemptible disk size- \" This disk space is used for local caching of data and is not available through HDFS.\"\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms#using_preemptibles_in_a_cluster"}]},{"poster":"Tanmoyk","content":"Why not A ? To get optimal performance, split your data in Cloud Storage into files with sizes from 128 MB to 1 GB. Using lots of small files can create a bottleneck. So if the bottleneck can be avoided using a larger file size , this can solve the issue without increasing the cost .","upvote_count":"2","timestamp":"1599545940.0","comment_id":"175674"},{"upvote_count":"2","content":"Itâ€™s C, Data shuffling taking lots of times if you donâ€™t copy files from GCS to HDFS.","timestamp":"1598231280.0","comment_id":"164787","poster":"dragon123"},{"poster":"saurabh1805","comment_id":"160800","upvote_count":"6","content":"D is correct answer, Refer below link\n\nhttps://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#use_preemptible_vms","timestamp":"1597746480.0"},{"timestamp":"1596679920.0","comment_id":"151616","content":"its A guys, C & D are out as SSD is more expensive!","upvote_count":"1","poster":"clouditis"},{"content":"Option C is correct:\nRequirement clearly says dataproc & GCS. One of the google recommended solution for Shuffle Intensive jobs is to make use of GCS only as Source and Target and to use HDFS for all intermediate shuffling and other operations. Since Shuffling will use the HDFS for storing the data and Pre-emptible VM's doesn't have HDFS storage allocated there is no point in increasing the boot disk of preemptible VM's.Pre-emptible vm is completely out of scope.\n\nFrom Google:-\nPreemptible Worker Ratio:Since preemptible workers write their shuffle data to HDFS, it is important to make sure that your cluster contains enough primary workers to accommodate your job's shuffle data. \n\nPre-emptible VM's Persistent disk sizeâ€”As a default, secondary workers are created with the smaller of 100GB or the primary worker boot disk size. This disk space is used for local caching of data and is not available through HDFS. You can override the default disk size with the gcloud dataproc clusters create --secondary-worker-boot-disk-size command at cluster creation. You can specify this flag even if the cluster does will not have secondary workers when it is created","comment_id":"130235","poster":"Rajokkiyam","comments":[{"content":"How 'C' , if Preemptible VMs can't be used for HDFS storage.","timestamp":"1625095500.0","upvote_count":"1","poster":"sumanshu","comment_id":"395295"}],"upvote_count":"3","timestamp":"1594253700.0"},{"upvote_count":"2","content":"what about option B? using tensorflow.\nhttps://engineering.linkedin.com/blog/2020/spark-tfrecord#:~:text=The%20goal%20of%20Spark%2DTFRecord,%2C%20JSON%2C%20Parquet%2C%20etc.","comment_id":"127888","timestamp":"1594045800.0","poster":"Devx198912233"},{"poster":"Rajuuu","comment_id":"126885","timestamp":"1593956160.0","upvote_count":"5","content":"Organisation is cost sensitive ..Hence SSD will be more expensive.\nWill go for A."},{"upvote_count":"4","comment_id":"121741","poster":"norwayping","timestamp":"1593333240.0","content":"D\nhttps://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\nSwitch to SSD disks\nIf you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.\nAs a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads. For details, see the page on preemptible VMs in the Dataproc documentation."},{"upvote_count":"2","timestamp":"1593271500.0","poster":"PRC","comment_id":"121319","content":"Sorry D.."},{"poster":"PRC","timestamp":"1593271440.0","comment_id":"121317","content":"C&D are correct","upvote_count":"1"},{"timestamp":"1585369200.0","content":"Answer: D\nDescription: Google says that inorder to increase performance switch to SSD which will be costly, so to tackle this increase the boot disk size, bootsize is worker node cache size 100 Gb.","upvote_count":"5","comment_id":"68747","poster":"[Removed]"},{"comment_id":"67020","upvote_count":"4","comments":[{"poster":"[Removed]","content":"https://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance","timestamp":"1584919140.0","comment_id":"67125","upvote_count":"2"},{"timestamp":"1584943140.0","upvote_count":"7","comments":[{"timestamp":"1593150060.0","content":"Option [D] - Looks most appropriate - https://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance","poster":"dambilwa","upvote_count":"4","comment_id":"120210"}],"content":"Selected D","comment_id":"67184","poster":"[Removed]"}],"timestamp":"1584895620.0","poster":"[Removed]","content":"Should be C."}],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/16572-exam-professional-data-engineer-topic-1-question-87/","question_id":306},{"id":"DcqtlpAZR1pqAdlaG9dR","url":"https://www.examtopics.com/discussions/google/view/79771-exam-professional-data-engineer-topic-1-question-88/","topic":"1","discussion":[{"timestamp":"1693632480.0","comment_id":"826605","poster":"midgoo","content":"Selected Answer: D\nC is a big NO. Writing to PubSub in DoFn will cause bottleneck in the pipeline. For IO, we should always use those IO lib (e.g PubsubIO)\nUsing sideOutput is the correct answer here. There is a Qwiklab about this. It is recommended to do that lab to understand more.","upvote_count":"13"},{"upvote_count":"8","timestamp":"1692121200.0","poster":"jonathanthezombieboy","comment_id":"809892","content":"Selected Answer: D\nBased on the given scenario, option D would be the best approach to improve the reliability of the pipeline.\n\nAdding a try-catch block to the DoFn that transforms the data would allow you to catch and handle errors within the pipeline. However, storing erroneous rows in Pub/Sub directly from the DoFn (Option C) could potentially create a bottleneck in the pipeline, as it adds additional I/O operations to the data processing.\n\nOption A of filtering the erroneous data would not allow the pipeline to reprocess the failing data, which could result in data loss.\n\nOption D of using a sideOutput to create a PCollection of erroneous data would allow for reprocessing of the failed data and would not create a bottleneck in the pipeline. Storing the erroneous data in a separate PCollection would also make it easier to debug and analyze the failed data.\n\nTherefore, adding a try-catch block to the DoFn that transforms the data and using a sideOutput to create a PCollection of erroneous data that can be stored to Pub/Sub later would be the best approach to improve the reliability of the pipeline."},{"comment_id":"1191496","upvote_count":"2","poster":"Farah_007","timestamp":"1728381900.0","content":"Selected Answer: D\nI think it's D because here you can write data from Dataflow PCollection to pub/sub. https://cloud.google.com/dataflow/docs/guides/write-to-pubsub"},{"content":"Selected Answer: C\nAnswer is C. Here is the github repo and an example from the Qwiklab where they tag the output as 'parsed_rows' and 'unparsed_rows' before they send the data to GCS. I don't see how GCS or PubSub would make a difference at this point. It seems like a more maintanable solution to just parse the data in the DoFn.\n\n1) If the function does more than that then it serves multiple purposes and it's not good software engineering. Unless there is a good reason, writing to PubSub should be separated from the DoFn.\n\nii) It's faster to write in mini-batches or one batch than stream the errors. What's the need for streaming out errors 1 by 1? Literally no real advantage.\n\nhttps://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/7_Advanced_Streaming_Analytics/solution/streaming_minute_traffic_pipeline.py","upvote_count":"1","comment_id":"961839","poster":"Mathew106","timestamp":"1706122560.0"},{"poster":"tibuenoc","content":"Selected Answer: D\nOutput errors to new PCollection â€“ Send to collector for later analysis (Pub/Sub is a good target)","comment_id":"847959","upvote_count":"2","timestamp":"1695449460.0"},{"poster":"musumusu","timestamp":"1692092640.0","upvote_count":"3","content":"Option D is right approach to use to get errors as sideOutput. Apache beam has a special scripting docs not dynamic as python itself. So lets follow standard sideOutput(withoutputs in the code)\nsyntax be like in pipeline:\n'ProcessData' >> beam.ParDo(DoFn).with_outputs","comments":[{"timestamp":"1692877620.0","upvote_count":"1","comment_id":"820520","poster":"musumusu","content":"After using you try: Catch: you can also send the erroneous records to dead letter sink into BQ\n``` outputTuple.get(deadLetterTag).apply(BigQuery.write(...)) ```"}],"comment_id":"809431"},{"poster":"abwey","upvote_count":"3","comment_id":"801803","content":"Selected Answer: D\nblahblahblahblahblahblahblahblah","timestamp":"1691477220.0"},{"timestamp":"1690623480.0","content":"Selected Answer: D\nIt`s D.\nUse a try catch block to direct erroneous rows into a side output. The PCollection of the side output can be sent efficiently to the PubSub topic via Apache Beam PubSubIO.\n\nIt's not C because C means to sent every single invalid row in a separate request to PubSub which is very inefficient when working with Dataflow as now batching is involved.","upvote_count":"2","poster":"waiebdi","comment_id":"791590"},{"timestamp":"1686047100.0","comment_id":"736793","content":"Selected Answer: C\nC is the answer.","poster":"zellck","upvote_count":"1"},{"poster":"hauhau","upvote_count":"1","content":"C\nD: dataflow to pub/sub is weird","comment_id":"732669","timestamp":"1685620200.0"},{"timestamp":"1685002500.0","comment_id":"726639","upvote_count":"3","poster":"Atnafu","content":"D\nSide output is a great manner to branch the processing. Let's take the example of an input data source that contains both valid and invalid values. Valid values must be written in place #1 and the invalid ones in place#2. A naive solution suggests to use a filter and write 2 distinct processing pipelines. However this approach has one main drawback - the input dataset is read twice. If for the mentioned problem we use side outputs, we can still have 1 ParDo transform that internally dispatches valid and invalid values to appropriate places (#1 or #2, depending on value's validity).\n\n\nhttps://www.waitingforcode.com/apache-beam/side-output-apache-beam/read#:~:text=simple%20test%20cases.-,Side%20output%20defined,-%C2%B6"},{"content":"Answer is D","upvote_count":"1","timestamp":"1684929300.0","comment_id":"725886","poster":"sfsdeniso"},{"poster":"cloudmon","comment_id":"712588","timestamp":"1683392460.0","upvote_count":"2","content":"Selected Answer: C\nIt's C.\nIn D, \"storing to PubSub later\" doesn't really make sense."},{"upvote_count":"2","content":"Selected Answer: C\nAnswer is C. You need to reprocess all the failling data, and yes, you can use PubSub as a sink, according to the documentation: https://beam.apache.org/documentation/io/connectors/","timestamp":"1681518180.0","poster":"devaid","comment_id":"695068"},{"upvote_count":"4","timestamp":"1680002940.0","comment_id":"681639","poster":"nickyshil","content":"Answer C"},{"poster":"nickyshil","timestamp":"1680002820.0","comment_id":"681637","content":"The error records are directly written to PubSub from the DoFn (itâ€™s equivalent in python).\nYou cannot directly write a PCollection to PubSub. You have to extract each record and write one at a time. Why do the additional work and why not write it using PubSubIO in the DoFn itself?\nYou can write the whole PCollection to Bigquery though, as explained in\n\nReference:\nhttps://medium.com/google-cloud/dead-letter-queues-simple-implementation-strategy-for-cloud-pub-sub-80adf4a4a800","upvote_count":"6"},{"comment_id":"658404","poster":"AWSandeep","content":"Selected Answer: D\nD. Add a try-catch block to your DoFn that transforms the data, use a sideOutput to create a PCollection that can be stored to Pub/Sub later.","upvote_count":"3","timestamp":"1677851880.0"}],"answer_images":[],"question_images":[],"question_id":307,"exam_id":10,"unix_timestamp":1662206280,"answers_community":["D (85%)","C (15%)"],"timestamp":"2022-09-03 13:58:00","choices":{"D":"Add a try×’â‚¬Â¦ catch block to your DoFn that transforms the data, use a sideOutput to create a PCollection that can be stored to Pub/Sub later.","B":"Add a try×’â‚¬Â¦ catch block to your DoFn that transforms the data, extract erroneous rows from logs.","A":"Add a filtering step to skip these types of errors in the future, extract erroneous rows from logs.","C":"Add a try×’â‚¬Â¦ catch block to your DoFn that transforms the data, write erroneous rows to Pub/Sub PubSub directly from the DoFn."},"answer":"D","question_text":"Your team is responsible for developing and maintaining ETLs in your company. One of your Dataflow jobs is failing because of some errors in the input data, and you need to improve reliability of the pipeline (incl. being able to reprocess all failing data).\nWhat should you do?","answer_description":"","isMC":true,"answer_ET":"D"},{"id":"6Zv8lvpbED6aVde40Mdz","question_id":308,"isMC":true,"answer":"C","timestamp":"2022-09-02 10:26:00","answer_ET":"C","answer_images":[],"topic":"1","question_text":"You're training a model to predict housing prices based on an available dataset with real estate properties. Your plan is to train a fully connected neural net, and you've discovered that the dataset contains latitude and longitude of the property. Real estate professionals have told you that the location of the property is highly influential on price, so you'd like to engineer a feature that incorporates this physical dependency.\nWhat should you do?","url":"https://www.examtopics.com/discussions/google/view/79337-exam-professional-data-engineer-topic-1-question-89/","unix_timestamp":1662107160,"answer_description":"","answers_community":["C (68%)","B (16%)","Other"],"choices":{"A":"Provide latitude and longitude as input vectors to your neural net.","D":"Create a feature cross of latitude and longitude, bucketize it at the minute level and use L2 regularization during optimization.","B":"Create a numeric column from a feature cross of latitude and longitude.","C":"Create a feature cross of latitude and longitude, bucketize it at the minute level and use L1 regularization during optimization."},"exam_id":10,"question_images":[],"discussion":[{"timestamp":"1664222760.0","content":"Ans C, use L1 regularization becuase we know the feature is a strong feature. L2 will evenly distribute weights","comment_id":"680119","upvote_count":"9","poster":"AHUI"},{"content":"Selected Answer: C\nOption C is correct\n\nUse L1 regularization when you need to assign greater importance to more influential features. It\nshrinks less important feature to 0.\nL2 regularization performs better when all input features influence the output & all with the\nweights are of equal size.","poster":"dish11dish","upvote_count":"8","comment_id":"724240","timestamp":"1669112280.0"},{"upvote_count":"2","comment_id":"1302127","timestamp":"1729702440.0","poster":"SamuelTsch","content":"Selected Answer: D\nI would like choose D. L1 will ignore the irrelevant features. However, we know that lat and long are cruial for this model. We can't take away their influences. L2 helps in preventing overfitting."},{"upvote_count":"1","timestamp":"1728935220.0","comment_id":"1297803","poster":"MohaSa1","content":"Selected Answer: A\nThis does not seems to be useful, minute level bucketizing will create 3,600 possible buckets per degree squared, not logical, and sparse feature space, Option A seems to be a better choice."},{"content":"Selected Answer: B\nBucketing into minutes is inaccurate, up to 1.8 km are grouped. Way too much for real estste.\nTherefore B","comment_id":"1251957","timestamp":"1721503020.0","poster":"Snnnnneee","upvote_count":"1"},{"comment_id":"1015518","upvote_count":"2","timestamp":"1695535740.0","poster":"uday_examtopic","content":"Create a feature cross of latitude and longitude, bucketize it at the minute level and use L2 regularization during optimization.\n\n Like option C, we bucketize at the minute level, but this time we apply L2 regularization. L2 regularization, or Ridge Regression, discourages large values of weights in the model without forcing them to become sparse. It can help prevent overfitting, especially when we have a large number of features (as a result of bucketizing and crossing).\n\nGiven the options, D. Create a feature cross of latitude and longitude, bucketize it at the minute level and use L2 regularization during optimization seems to be the most appropriate. Bucketizing at the minute level captures localized patterns, and L2 regularization can help control the complexity of the model without enforcing sparsity."},{"content":"What does bucketizing at the minute level mean in the context of this question?","comment_id":"1013010","comments":[{"comment_id":"1065719","timestamp":"1699457220.0","content":"Coordinates are written with Degrees, minutes and seconds (one minute being equal to about 1.8 km). So you group your coordinates in buckets with a miute precision","upvote_count":"4","poster":"Surely1987"}],"timestamp":"1695294540.0","upvote_count":"3","poster":"ckanaar"},{"comment_id":"991358","upvote_count":"2","content":"Selected Answer: B\nI strongly believe it's B.","poster":"FP77","timestamp":"1693129380.0"},{"content":"Selected Answer: B\nThe right answer is B. What the hell does bucketize the feature cross of latitude and longtitude even mean? They are not a time feature. C and D don't even make sense. The L1 regularization is something that doesn't answer anything in the question. The only valid feature engineered here is option B. A is not an engineered feature.\n\n Create a feature cross of latitude and longitude, bucketize it at the minute level and use L1 regularization during optimization.","comment_id":"960310","comments":[{"poster":"baimus","comment_id":"1288163","upvote_count":"1","timestamp":"1727102160.0","content":"Bucketising means that we're saying \"anyone in this square 1.8km (minute) region is considered a single area\" - it's actually recommended as a default way to deal with lat/lon, as they don't really work as seperate columns (or at least we'd be hoping the FCNN buckets them intelligently itself, which it won't mostly)"}],"upvote_count":"1","poster":"Mathew106","timestamp":"1690106160.0"},{"upvote_count":"1","comment_id":"949122","content":"D\n\nYou have to use L2, since you have create a new variable with two already existing the risk of multicollinearity is high, L1 is good for selecting feature to avoid curse of dimensionality not for multicollinearity","timestamp":"1689091260.0","poster":"Jojo9400"},{"comment_id":"886321","poster":"ga8our","timestamp":"1682948640.0","comments":[{"upvote_count":"2","content":"L1 regression forces unimportant coefficients to zero. Since the location is extremely important, L1 will force less important coefficients to zero, thereby further increasing the importance of the location coefficient.","comment_id":"1013008","timestamp":"1695294480.0","poster":"ckanaar"}],"content":"Why not L2? L2 (Ridge) uses a squared value coefficient as a penalty term to the loss function, while L1 (Lasso) uses an absolute value coefficient. Isn't a squared penalty stronger than an absolute one? \nhttps://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c","upvote_count":"1"},{"comment_id":"880267","timestamp":"1682421120.0","poster":"Oleksandr0501","content":"gpt: Option C and D suggest bucketizing the feature cross of latitude and longitude at the minute level and using L1 or L2 regularization during optimization. While regularization can help prevent overfitting, bucketizing at such a granular level may not be necessary and could lead to overfitting. It's also not clear how bucketizing at the minute level would capture the spatial relationship between the latitude and longitude features.","upvote_count":"2"},{"content":"Selected Answer: D\nD. Create a feature cross of latitude and longitude, bucketize it at the minute level and use L2 regularization during optimization. This will create a new feature that captures the physical dependency of the location of the property on the price, and bucketing it at the minute level will reduce the number of unique values and prevent overfitting. L2 regularization will also help to prevent overfitting by penalizing large weights in the model.","comments":[{"upvote_count":"1","content":"chat-gpt also says D\nexplanation: \nThis approach effectively creates a grid of the geographical area in your data, allowing the model to learn weights for each grid cell (bucket). This helps capture the spatial relationship between latitude and longitude, which can be crucial for real estate prices. Additionally, using L2 regularization helps prevent overfitting by discouraging complex models, which can be particularly important when working with high-dimensional crossed features.","comment_id":"906509","timestamp":"1685002260.0","poster":"cetanx"}],"poster":"PolyMoe","upvote_count":"1","comment_id":"789538","timestamp":"1674819540.0"},{"poster":"zellck","content":"Selected Answer: C\nC is the answer.\n\nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture\nA feature cross is a synthetic feature formed by multiplying (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually.\n\nhttps://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization","upvote_count":"3","comment_id":"736792","timestamp":"1670329440.0"},{"poster":"crismo04","content":"https://medium.com/riga-data-science-club/geographic-coordinate-encoding-with-tensorflow-feature-columns-e750ae338b7c#:~:text=to%20the%20rescue!-,Feature%20Crosses,-Combining%20features%20into","timestamp":"1662907740.0","upvote_count":"2","comments":[{"upvote_count":"1","content":"Feature cross seems to be the right feature option","timestamp":"1662907800.0","comments":[{"timestamp":"1662907860.0","upvote_count":"4","content":"So it's B option","comment_id":"666217","poster":"crismo04"}],"poster":"crismo04","comment_id":"666214"}],"comment_id":"666212"},{"poster":"[Removed]","content":"Selected Answer: C\nRegularization + location into one","upvote_count":"1","timestamp":"1662428640.0","comment_id":"660685"},{"timestamp":"1662176580.0","comment_id":"658035","upvote_count":"5","content":"Selected Answer: C\nC. Create a feature cross of latitude and longitude, bucketize it at the minute level and use L1 regularization during optimization.","poster":"AWSandeep"},{"upvote_count":"1","timestamp":"1662107160.0","content":"C or D?\nhttps://medium.com/riga-data-science-club/geographic-coordinate-encoding-with-tensorflow-feature-columns-e750ae338b7c","poster":"nwk","comment_id":"657177"}]},{"id":"P3Mrd0XS82s0tDqlmfgz","question_text":"Your company is using WILDCARD tables to query data across multiple tables with similar names. The SQL statement is currently failing with the following error:\n//IMG//\n\nWhich table name will make the SQL statement work correctly?","discussion":[{"content":"None, the actual `bigquery-public-data.noaa_gsod.gsod*`\nwith back ticks at the beginning and at the end.","upvote_count":"34","comments":[{"poster":"Davijde13","upvote_count":"11","comment_id":"776766","timestamp":"1673798280.0","content":"I suspect there has been some typo with copy-paste of the option D"},{"comment_id":"1106649","poster":"jitvimol","timestamp":"1703663760.0","content":"yes, I see from another source that actually ans D has to be backtick. Probably a problem when this web do data ingestion.","upvote_count":"4"}],"timestamp":"1663943280.0","comment_id":"677244","poster":"Ender_H"},{"comment_id":"1330489","timestamp":"1734887880.0","upvote_count":"1","poster":"Mariaantonirajc","content":"Selected Answer: B\nB This is the correct syntax. The wildcard * is outside any quotes or string delimiters. This tells BigQuery to query all tables that match the pattern gsod* within the noaa_gsod dataset."},{"timestamp":"1734683700.0","content":"Selected Answer: B\nBig Query does not use quotations while fetching data from the table :\nexample: select * from project-id.dataset_name.table_name; is the syntax","upvote_count":"1","poster":"sravi1200","comment_id":"1329360"},{"comments":[{"poster":"vaga1","comment_id":"902000","timestamp":"1684505520.0","content":"Who used BQ knows that the backquote is necessary only for the project name, while it can be used for the whole string, and necessary only when the project name contains special (special in this specific context) characters.\n\n- is a special character. so\n`bigquery-public-data`.noaa_gsod.gsod1940 \nwould have worked too.\n\nThe question now turns out to be\n`bigquery-public-data`.noaa_gsod.gsod* \nstill works or due to the * presence we need to write \n`bigquery-public-data.noaa_gsod.gsod*`\n?\n\nI personally do not remember, and I do not have a BQ at my disposal at the moment. \nBut I know for sure that \n`bigquery-public-data.noaa_gsod.gsod*`\nworks while \n`bigquery-public-data`.noaa_gsod.gsod*\nis not in the options.","upvote_count":"2"}],"comment_id":"901998","poster":"vaga1","timestamp":"1727154660.0","content":"Selected Answer: D\nlet's forget the fact that in BQ is used ` instead than ' which retrieves an error in any case. ` is called backquote, backtick, or left quote while ' is simply an apostrophe. Let's consider ' to be ` in every answer, since moderators could have not been aware of such when they had received the question.","upvote_count":"1"},{"comment_id":"1050476","upvote_count":"2","poster":"rtcpost","timestamp":"1727154660.0","content":"Selected Answer: D\nOption D (assuming to have backticks)\n\nRefer: https://cloud.google.com/bigquery/docs/querying-wildcard-tables\nThe following query is NOT valid because it isn't properly quoted with backticks:\n```\n#standardSQL\n/* Syntax error: Expected end of statement but got \"-\" at [4:11] */\nSELECT\n max\nFROM\n # missing backticks\n bigquery-public-data.noaa_gsod.gsod*\nWHERE\n max != 9999.9 # code for missing data\n AND _TABLE_SUFFIX = '1929'\nORDER BY\n max DESC\n```"},{"upvote_count":"1","timestamp":"1727154660.0","poster":"RT_G","content":"Selected Answer: D\nReference: https://cloud.google.com/bigquery/docs/querying-wildcard-tables\nThe wildcard table name contains the special character (*), which means that you must enclose the wildcard table name in backtick (`) characters. For example, the following query is valid because it uses backticks:\n\n\n#standardSQL\n/* Valid SQL query */\nSELECT\n max\nFROM\n `bigquery-public-data.noaa_gsod.gsod*`\nWHERE\n max != 9999.9 # code for missing data\n AND _TABLE_SUFFIX = '1929'\nORDER BY\n max DESC","comment_id":"1065075"},{"timestamp":"1727154660.0","upvote_count":"4","comment_id":"1207163","poster":"ABKR1300","content":"Few might go with the Option B which will be a blunder because of the below reason.\n\nWhile querying the tables or views with the name, it is optional to surround with the backticks. But while querying the list of tables with Wild card character, it is must to surround with the backticks. \n\nWe can get the Syntax error: Expected end of input but got \"*\" with the below query \n\nSELECT * FROM bigquery-public-data.noaa_gsod.gsod*\nWHERE _TABLE_SUFFIX = \"2024\"\n\nSo, option D might be the correct one, provided if there is a typo."},{"timestamp":"1719332580.0","poster":"Chintu_573","comment_id":"1236984","content":"Selected Answer: B\nIN option D, there is differert ' ` on first and last. That's why right option is second.","upvote_count":"1"},{"timestamp":"1716291180.0","poster":"dsyouness","comment_id":"1214894","content":"Selected Answer: B\nbigquery-public-data.noaa_gsod.gsod* also works","upvote_count":"3"},{"poster":"RT_G","content":"Selected Answer: D\nAgree with others - Option D","timestamp":"1699381320.0","comment_id":"1065072","upvote_count":"1"},{"poster":"axantroff","content":"Selected Answer: D\nD. 'bigquery-public-data.noaa_gsod.gsod*` is the right answer with 1 typo","comment_id":"1056902","upvote_count":"1","timestamp":"1698592080.0"},{"content":"Answer is 'D'\nReference : https://cloud.google.com/bigquery/docs/wildcard-table-reference\n\nEnclose table names with wildcards in backticks\nThe wildcard table name contains the special character (*), which means that you must enclose the wildcard table name in backtick (`) characters.","timestamp":"1683895380.0","poster":"Pavaan","comment_id":"895917","upvote_count":"3"},{"poster":"Melampos","timestamp":"1682539440.0","comment_id":"881991","upvote_count":"2","content":"Selected Answer: B\nbigquery-public-data.noaa_gsod.gsod* works"},{"comment_id":"844617","poster":"hkhnhan","upvote_count":"1","content":"Selected Answer: B\nshould be B, the backtick at D answer is wrong ' instead of `","timestamp":"1679297700.0"},{"timestamp":"1679296560.0","poster":"hkhnhan","content":"should be B, the backtick at D answer is wrong ' instead of `","upvote_count":"1","comment_id":"844608"},{"timestamp":"1677163740.0","upvote_count":"1","comment_id":"819312","content":"D is correct","poster":"Zosby"},{"poster":"priluft","timestamp":"1662493500.0","content":"Selected Answer: D\nD. 'bigquery-public-data.noaa_gsod.gsod*`","upvote_count":"2","comment_id":"661617"},{"content":"Selected Answer: D\nD. 'bigquery-public-data.noaa_gsod.gsod*`","poster":"AWSandeep","upvote_count":"2","timestamp":"1662180480.0","comment_id":"658075"}],"isMC":true,"topic":"1","timestamp":"2022-09-03 06:48:00","unix_timestamp":1662180480,"exam_id":10,"answer_description":"","question_id":309,"question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0000600001.png"],"choices":{"D":"'bigquery-public-data.noaa_gsod.gsod*`","C":"'bigquery-public-data.noaa_gsod.gsod'*","A":"'bigquery-public-data.noaa_gsod.gsod'","B":"bigquery-public-data.noaa_gsod.gsod*"},"url":"https://www.examtopics.com/discussions/google/view/79679-exam-professional-data-engineer-topic-1-question-9/","answer_images":[],"answers_community":["D (53%)","B (47%)"],"answer_ET":"D","answer":"D"},{"id":"ksThm9ywgfNoejmy6qrH","isMC":true,"choices":{"D":"Install the StackDriver Agent and configure the MySQL plugin.","C":"Install the StackDriver Logging Agent and configure fluentd in_tail plugin to read MariaDB logs.","B":"Place the MariaDB instances in an Instance Group with a Health Check.","A":"Install the OpenCensus Agent and create a custom metric collection application with a StackDriver exporter."},"answer_ET":"D","answer":"D","answers_community":["D (87%)","13%"],"question_id":310,"topic":"1","url":"https://www.examtopics.com/discussions/google/view/17260-exam-professional-data-engineer-topic-1-question-90/","question_images":[],"discussion":[{"upvote_count":"26","comments":[{"upvote_count":"13","comment_id":"444675","poster":"fire558787","timestamp":"1663170540.0","content":"It is definitely A.\nB: can't be because Health Checks just checks that machine is online\nC: StackDriver Logging is for Logging. Here we talk of Monitoring / Alerting\nD: StackDriver Agent monitors default metrics of VMs and some Database stuff with the MySQL Plugin. Here you want to monitor some more custom stuff like Replication of MariaDB (I didn't find anything of this sort in the plugin page), and you may want to use Custom Metrics rather than default metrics. \"Cloud Monitoring automatically collects more than 1,500 built-in metrics from more than 100 monitored resources. But those metrics cannot capture application-specific data or client-side system data. Those metrics can give you information on backend latency or disk usage, but they can't tell you how many background routines your application spawned.\" https://cloud.google.com/monitoring/custom-metrics/open-census#monitoring_opencensus_metrics_quickstart-python"}],"content":"Answer : A\nMariaDB needs costume metrics , and stackdriver built-in monitoring tools will not provide these metrics . Opencensus Agent will do this for you \nFor more info , refer to :\nhttps://cloud.google.com/monitoring/custom-metrics/open-census","comment_id":"76116","poster":"Barniyah","timestamp":"1618761480.0"},{"comments":[{"timestamp":"1661005980.0","upvote_count":"2","comment_id":"428197","content":"I think its D, because its Selfmanaged DB and for this we use Stackdriver Agents. and in this question its asking about metrics not logs.","poster":"Atulthakur"}],"comment_id":"68751","upvote_count":"13","content":"Answer: C\nDescription: The GitHub repository named google-fluentd-catch-all-config which includes the configuration files for the Logging agent for ingesting the logs from various third-party software packages.","timestamp":"1616899440.0","poster":"[Removed]"},{"upvote_count":"2","content":"Selected Answer: D\nHere's the rationale:\nStackDriver Agent: The StackDriver Agent is designed to collect system and application metrics from virtual machine instances and send them to StackDriver Monitoring. It simplifies the process of collecting and forwarding metrics.\nMySQL Plugin: The StackDriver Agent has a MySQL plugin that allows you to collect MySQL-specific metrics without the need for additional custom development. This includes metrics related to network connections, disk IO, and replication status â€“ which are the specific metrics you mentioned.\n\nOption D is the most straightforward and least development-intensive approach to achieve the monitoring and alerting requirements for MariaDB on GCE VM Instances using StackDriver.","poster":"rocky48","comment_id":"1087643","timestamp":"1733316720.0"},{"timestamp":"1731101580.0","comment_id":"1065975","poster":"BlehMaks","content":"Selected Answer: A\nreplication status seems to be not included in sql agent metrics. but I do not like A in terms of efforts","upvote_count":"1"},{"upvote_count":"1","comment_id":"831614","poster":"ninjatech","timestamp":"1709794260.0","content":"it can't be A as it saying minimal development but for opencensus the development is needed."},{"comment_id":"750942","content":"Selected Answer: A\nTo use metrics collected by OpenCensus in your Google Cloud project, you must make the OpenCensus metrics libraries and the Stackdriver exporter available to your application. The Stackdriver exporter exports the metrics that OpenCensus collects to your Google Cloud project. You can then use Cloud Monitoring to chart or monitor those metrics.","poster":"slade_wilson","upvote_count":"1","timestamp":"1703079660.0"},{"content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/third-party/mariadb","timestamp":"1701865140.0","comment_id":"736790","comments":[{"timestamp":"1703514300.0","content":"For supplement, â€˜Stackdriver agent' now called as Ops agent, 'Operations Suite'","poster":"wan2three","upvote_count":"4","comment_id":"755709"}],"upvote_count":"9","poster":"zellck"},{"comment_id":"724225","content":"Selected Answer: D\nOption D is Correct\nMariaDB is a community-developed, commercially supported fork of the MySQL relational database management system (RDBMS). To collect logs and metrics for MariaDB, use the mysql receivers.\n\nThe mysql receiver connects by default to a local MariaDB server using a Unix socket and Unix authentication as the root user.\n\nreference:-https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/third-party/mariadb","poster":"dish11dish","upvote_count":"5","timestamp":"1700646300.0"},{"timestamp":"1698589320.0","poster":"girgu","upvote_count":"3","comment_id":"707186","content":"Selected Answer: D\nhttps://cloud.google.com/monitoring/agent/ops-agent/third-party/mariadb"},{"comment_id":"676605","poster":"clouditis","content":"C is the answer, fluentd plug in is needed as the DB is on GCE","timestamp":"1695424200.0","upvote_count":"2"},{"poster":"ducc","timestamp":"1693348080.0","upvote_count":"2","content":"Selected Answer: D\ngo for D","comment_id":"653670"},{"timestamp":"1692614040.0","poster":"eRaymox","upvote_count":"2","content":"A\nStackDriver Agent monitors default metrics of VMs and some Database stuff with the MySQL Plugin. Here you want to monitor some more custom stuff like Replication of MariaDB (I didnâ€™t find anything of this sort in the plugin page), and you may want to use Custom Metrics rather than default metrics. â€œCloud Monitoring automatically collects more than 1,500 built-in metrics from more than 100 monitored resources. But those metrics cannot capture application-specific data or client-side system data. Those metrics can give you information on backend latency or disk usage, but they canâ€™t tell you how many background routines your application spawned.â€ https://cloud.google.com/monitoring/custom-metrics/open-census#monitoring_opencensus_metrics_quickstart-python","comment_id":"649712"},{"upvote_count":"2","poster":"Kriegs","timestamp":"1687452720.0","comment_id":"620517","content":"I'm not 100% sure as I have no experience with that issue, but I would say it's D - both A and D should work, but the keyword is \"with minimal development effort\" (and using pre-built plugin > creating custom metric in terms of simplicity, that's obvious) and all of the relevant data (as per question) should be there: https://cloud.google.com/monitoring/api/metrics_agent#agent-mysql\n\nI'm not sure if C would work, but it also seems more advanced in implementation than D. B is 100% wrong and insufficient for that use case.\n\nFeel free to prove me wrong :)"},{"comment_id":"590529","content":"A and D both seem like viable options here, unsure which is Google's preferred method as that would be deemed the correct answer in the exam. Any opinions?","poster":"NR22","upvote_count":"1","timestamp":"1682244240.0"},{"content":"Selected Answer: D\nD\nmariaDB is an extension of mysql and mysql plugin must work fine to extract the metrics of mariaDB.","comments":[{"timestamp":"1684997700.0","poster":"ST42","upvote_count":"2","comment_id":"607091","content":"\"MariaDB is a community-developed, commercially supported fork of the MySQL relational database management system (RDBMS). To collect logs and metrics for MariaDB, use the mysql receivers.\"\n\nhttps://cloud.google.com/monitoring/agent/ops-agent/third-party/mariadb"}],"comment_id":"586364","timestamp":"1681565220.0","poster":"Didine_22","upvote_count":"5"},{"comment_id":"530814","poster":"rbeeraka","upvote_count":"2","timestamp":"1674507540.0","content":"Selected Answer: A\nOpencensus Agent is right one"},{"timestamp":"1663295040.0","upvote_count":"4","poster":"nguyenmoon","comment_id":"445579","comments":[{"comment_id":"521425","poster":"[Removed]","content":"agree; https://cloud.google.com/monitoring/agent/ops-agent/third-party/mariadb#configure-instance","upvote_count":"2","timestamp":"1673429700.0"}],"content":"D is correct. Answer : D\nmariaDB is an extension of mysql and mysql plugin must work fine to extract the metrics of mariaDB."},{"upvote_count":"6","content":"Answer : D\nmariaDB is an extension of mysql and mysql plugin must work fine to extract the metrics of mariaDB.\nhttps://cloud.google.com/monitoring/agent/plugins/mysql","poster":"safiyu","timestamp":"1660161840.0","comment_id":"422975"},{"timestamp":"1656633960.0","content":"Vote for C","poster":"sumanshu","upvote_count":"2","comment_id":"395320"},{"upvote_count":"6","content":"should be D \nhttps://cloud.google.com/monitoring/api/metrics_agent#agent-mysql - This plugins collects the replication status and other metrics can be collected from Stackdriver agent. And also MariaDB was not listed in that as this can be used as MySQL plugin.","timestamp":"1644522300.0","comment_id":"287821","poster":"bobby8521"},{"timestamp":"1632326820.0","upvote_count":"8","comments":[{"timestamp":"1637254980.0","content":"yes, I think so. Ref: https://cloud.google.com/monitoring/agent/plugins/mysql#configuring","comment_id":"222069","upvote_count":"4","poster":"snamburi3"}],"content":"Should be D. MySQL plugin should be compatible for MariaDB.","comment_id":"184565","poster":"SteelWarrior"},{"content":"I believe it should be D as MariaDb is just an extension of MySQL and the mysql plugin should work with sytacjdriver monitoring","upvote_count":"7","poster":"bnikunj","comment_id":"182259","timestamp":"1632052920.0"},{"poster":"shashankraj","timestamp":"1631950620.0","comment_id":"181468","upvote_count":"4","content":"A:\nFluentd doesn't having Plugin for MariaDB \nhttps://www.fluentd.org/plugins/all"},{"comment_id":"175685","content":"A should be the answer use OpenCensus to capture application related custom information and integrate with stackdriver","timestamp":"1631083440.0","upvote_count":"1","poster":"Tanmoyk"},{"poster":"haroldbenites","content":"C is correct","comment_id":"162534","upvote_count":"1","timestamp":"1629499920.0"},{"poster":"hemanth7","upvote_count":"6","content":"should be D as question talks more about monitoring and alerting than logging.","comment_id":"152816","timestamp":"1628390100.0"},{"comment_id":"120487","poster":"dambilwa","timestamp":"1624708320.0","upvote_count":"3","content":"I wonder why Option [D] is wrong - We can Install the MySQL Plugin & start monitoring & alerts. Isn't it?"},{"poster":"[Removed]","timestamp":"1616432340.0","comments":[{"content":"You can install Stackdriver Logging agent on compute engine and AWS EC2 instances to stream logs from third party applications into Stackdriver Logging.\nThe logging agent is an application based on Fluentd. When you write your logs to existing log files such as sys log on your VM instance, the login agent sends the logs to Stackdriver.","poster":"[Removed]","comment_id":"70012","upvote_count":"2","timestamp":"1617254340.0"}],"content":"Should be C","upvote_count":"1","comment_id":"67023"}],"question_text":"You are deploying MariaDB SQL databases on GCE VM Instances and need to configure monitoring and alerting. You want to collect metrics including network connections, disk IO and replication status from MariaDB with minimal development effort and use StackDriver for dashboards and alerts.\nWhat should you do?","answer_description":"","unix_timestamp":1584896340,"timestamp":"2020-03-22 17:59:00","exam_id":10,"answer_images":[]}],"exam":{"isMCOnly":true,"lastUpdated":"15 Feb 2025","id":10,"isBeta":false,"name":"Professional Data Engineer","numberOfQuestions":319,"isImplemented":true,"provider":"Google"},"currentPage":62},"__N_SSP":true}