{"pageProps":{"questions":[{"id":"dR8TTGjhDYU4wYBM8Ana","answer_images":[],"isMC":true,"answer":"B","question_id":261,"question_text":"You work for an economic consulting firm that helps companies identify economic trends as they happen. As part of your analysis, you use Google BigQuery to correlate customer data with the average prices of the 100 most common goods sold, including bread, gasoline, milk, and others. The average prices of these goods are updated every 30 minutes. You want to make sure this data stays up to date so you can combine it with other data in BigQuery as cheaply as possible.\nWhat should you do?","choices":{"A":"Load the data every 30 minutes into a new partitioned table in BigQuery.","C":"Store the data in Google Cloud Datastore. Use Google Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Cloud Datastore","D":"Store the data in a file in a regional Google Cloud Storage bucket. Use Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Google Cloud Storage.","B":"Store and update the data in a regional Google Cloud Storage bucket and create a federated data source in BigQuery"},"topic":"1","question_images":[],"unix_timestamp":1583933460,"answers_community":["B (72%)","10%","Other"],"answer_ET":"B","timestamp":"2020-03-11 14:31:00","exam_id":10,"discussion":[{"content":"this is one of the sample exam questions that google has on their website. The correct answer is B","timestamp":"1583933460.0","comments":[{"poster":"nadavw","timestamp":"1723449480.0","upvote_count":"2","content":"B - since it seems that not all data is in BigQuery but the analysis is done using BigQuery so federated query is the optimal approach","comment_id":"1264539"}],"comment_id":"62439","poster":"mmarulli","upvote_count":"43"},{"poster":"[Removed]","comment_id":"68616","upvote_count":"13","comments":[{"poster":"funtoosh","upvote_count":"9","content":"it's not only cheaper but the requirement is that the data keep updating every 30 min and you need to combine the data in bigquery, use external tables to do that is the recommended practice","comment_id":"294498","timestamp":"1613757840.0"}],"timestamp":"1585319220.0","content":"Answer: B\nDescription: B is correct because regional storage is cheaper than BigQuery storage."},{"content":"Selected Answer: A\nBigQuery is a powerful data warehouse designed for analyzing large datasets efficiently. Partitioning tables allows you to manage large datasets by splitting them into segments based on a key, such as time.\nBy creating a partitioned table and updating it every 30 minutes, you can load the new price data directly into the correct partitions. BigQueryâ€™s partitioned tables optimize both the storage and querying cost because BigQuery only scans the relevant partitions when querying, minimizing the amount of data read and hence reducing costs.\nPartitioning by time (e.g., timestamp or date columns) is particularly effective for datasets with periodic updates (like price data) since each batch of data will be loaded into the corresponding partition.","poster":"jatinbhatia2055","upvote_count":"1","timestamp":"1733908140.0","comment_id":"1324948"},{"poster":"SamuelTsch","comment_id":"1301608","timestamp":"1729605540.0","content":"Selected Answer: B\nActually, in this question, I think B is the most suitable. C, D are somehow overkill. A due to the minimum partition granularity. However, with B, the data could not be previewd also it is not possible to estimate the cost.","upvote_count":"2"},{"poster":"Pennepal","content":"D. Store the data in a file in a regional Google Cloud Storage bucket. Use Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Google Cloud Storage.\n\nHere's why this approach is ideal:\n\nCost-Effective Storage: Cloud Storage offers regional storage classes that are cost-effective for frequently accessed data. Storing the price data in a regional Cloud Storage bucket keeps it readily available.\n\nCloud Dataflow for Updates: Cloud Dataflow is a managed service for building data pipelines. You can create a Dataflow job that runs every 30 minutes to:\n\nDownload the latest economic data file from Cloud Storage.\nProcess and potentially transform the data as needed.\nLoad the updated data into BigQuery.\nBigQuery Integration: BigQuery seamlessly integrates with Cloud Dataflow. The Dataflow job can directly load the processed data into a BigQuery table for further analysis with your customer data.","upvote_count":"2","timestamp":"1713747300.0","comment_id":"1199894"},{"timestamp":"1702557000.0","comment_id":"1096457","upvote_count":"2","poster":"TVH_Data_Engineer","content":"Selected Answer: A\nBigQuery supports partitioned tables, which allow for efficient querying and management of large datasets that are updated frequently. By loading the updated data into a new partition every 30 minutes, you can ensure that only relevant partitions are queried, reducing the amount of data processed and thereby minimizing costs.\nWhat's wrong with B ? While creating a federated data source in BigQuery pointing to a Google Cloud Storage bucket is feasible, it might not be the most efficient for data that is updated every 30 minutes. Querying federated data sources can sometimes be more expensive and less performant than querying data stored directly in BigQuery."},{"content":"Selected Answer: D\nFederated queries let you send a query statement to Cloud Spanner or Cloud SQL databases not to cloud storage","poster":"Melampos","comment_id":"882068","comments":[{"poster":"sid_is_dis","upvote_count":"3","content":"Is you are right about \"federated queries\", but the option B says about \"federated data source\". These are different concepts","comment_id":"927990","timestamp":"1687213560.0"}],"timestamp":"1682549520.0","upvote_count":"1"},{"comment_id":"874589","poster":"Abhilash_pendyala","timestamp":"1681905540.0","content":"ChatGPT says partitioned tables is the best approach, The answers here are quite contrasting with that answer, Even i thought it has to be option A, I am so confused now? Any proper straight forward answer ?","upvote_count":"1"},{"content":"Answer B:\nUploading data into staging tables/ external tables or federated source in BQ is the best approach. \nOption A is also good approach, anyone can explain about his part what is wrong about this?","poster":"musumusu","upvote_count":"1","timestamp":"1677185340.0","comment_id":"819736","comments":[{"content":"we can't implement A, it's because biquery partition table can only be done minimun in range 1 hour, the requirement said it must be update every 30 minutes, so A is imposible option as the minimum partition is in hour level","upvote_count":"7","comment_id":"829796","poster":"yoga9993","timestamp":"1678008540.0"}]},{"upvote_count":"1","timestamp":"1672837800.0","comment_id":"765661","content":"B is right","poster":"AzureDP900"},{"poster":"Krish6488","timestamp":"1671492540.0","content":"Selected Answer: B\nDiscounting A due to limitations on partitions\nDiscounting C because datastore does not fit into the nature of data we are talking about and federation between BQ and datastore it an overkill\nBetween B and D, updating the price file on GCS and joining BQ tables and external tables sourcing data from GCS is most cost optimal way for this use case","comments":[{"content":"D is also overkill for this use case, so I'd pick B","comment_id":"773977","poster":"ler_mp","upvote_count":"1","timestamp":"1673572920.0"}],"comment_id":"750324","upvote_count":"2"},{"poster":"jkhong","content":"Selected Answer: B\nConsideration: As cheaply as possible. Make sure data stays up to date.\n\nInitially chose A. But in actuality there is no need to maintain or store past data so storage of past data and partitioning doesn't seem like a key requirement.\n\nInstead we can connect just to a single Cloud Storage file, either by:\ni. replace previous prices with latest prices\nii. store previous prices in GCS if required to be retained","timestamp":"1671286080.0","upvote_count":"1","comment_id":"748099"},{"comment_id":"744499","poster":"DGames","content":"Selected Answer: B\nB is most inexpensive approach.","upvote_count":"1","timestamp":"1670970300.0"},{"upvote_count":"2","comment_id":"741571","timestamp":"1670752080.0","content":"Selected Answer: B\nThe technical requirement is having frequently access info to join with other BQ data, as cheap as possible. B fits perfectly.\nCorner cases for external data sources:\n â€¢ Avoiding duplicate data in BigQuery storage\n â€¢ Queries that do not have strong performance requirements\n â€¢ Small amount of frequently changing data to join with other tables in BigQuery\nhttps://cloud.google.com/blog/products/gcp/accessing-external-federated-data-sources-with-bigquerys-data-access-layer","poster":"odacir"},{"timestamp":"1669038960.0","content":"Selected Answer: D\nI would say D, regional Google Cloud Storage bucket - cheap.\nA - not cheap\nB - NoSQL database for your web and mobile applications\nC - Federated queries let you send a query statement to Cloud Spanner or Cloud SQL databases\nAnd we need to combine data in DQ with data from bucket","poster":"assU2","upvote_count":"1","comment_id":"723568"},{"content":"according to this : \nhttps://cloud.google.com/bigquery/docs/external-data-sources\nFederated queries don't work with Cloud Storage.\nhow can it be B ?","upvote_count":"2","poster":"MisuLava","comment_id":"703284","comments":[{"upvote_count":"1","comments":[{"poster":"gudiking","upvote_count":"1","timestamp":"1668626940.0","content":"It seems to me that they do: https://cloud.google.com/bigquery/docs/external-data-cloud-storage","comment_id":"719954"}],"timestamp":"1667683320.0","content":"Correct, it cannot be B because BQ federated queries only work with Cloud SQL or Spanner","comment_id":"712008","poster":"cloudmon"}],"timestamp":"1666636680.0"},{"upvote_count":"2","poster":"ducc","content":"Selected Answer: B\nI voted for B","timestamp":"1661724420.0","comment_id":"653177"},{"upvote_count":"1","poster":"FrankT2L","content":"Selected Answer: C\nThe average prices of these goods are updated every 30 minutes\n--> No Cloud Storage","timestamp":"1655305080.0","comment_id":"616806"},{"upvote_count":"2","timestamp":"1654789440.0","comment_id":"614073","poster":"paulino_gauna","content":"Selected Answer: C\nneed to udpate the data"},{"comment_id":"608086","content":"Selected Answer: B\nAnswer: B is correct because regional storage is cheaper than BigQuery storage.","poster":"Yad_datatonic","timestamp":"1653656220.0","upvote_count":"1"},{"upvote_count":"4","timestamp":"1651795260.0","content":"you cannot use federated queries with google cloud storage. how can B would be correct answer?","comments":[{"upvote_count":"1","timestamp":"1665560160.0","poster":"Julionga","comment_id":"692816","content":"It says federated data source and not federated queries \nAn external data source (also known as a federated data source) is a data source that you can query directly even though the data is not stored in BigQuery. Instead of loading or streaming the data, you create a table that references the external data source."}],"comment_id":"597485","poster":"Vip777"},{"poster":"Arkon88","upvote_count":"1","content":"Selected Answer: B\nB is correct. \n\nwe need it as cheaer as possible so cloud storage + external tables. Data wil be overwritten every 30 min\n\nhttps://cloud.google.com/blog/products/gcp/accessing-external-federated-data-sources-with-bigquerys-data-access-layer","timestamp":"1646388960.0","comment_id":"560685"},{"content":"Selected Answer: B\n@MaxNRG","comment_id":"523683","poster":"sraakesh95","timestamp":"1642187100.0","upvote_count":"2"},{"poster":"hendrixlives","timestamp":"1639724820.0","content":"Selected Answer: B\nB is correct. This is one of the use cases for external (federated) data sources in BigQuery:\n\"Small amount of frequently changing data to join with other tables in BigQuery\".\n\nhttps://cloud.google.com/blog/products/gcp/accessing-external-federated-data-sources-with-bigquerys-data-access-layer","upvote_count":"4","comment_id":"503421"},{"content":"Selected Answer: B\nB is correct because regional storage is cheaper than BigQuery storage.\nA is not correct because it is not the cheapest way of accomplishing this task.\nC, D are not correct because it is not the least expensive option. Using Dataflow to query BigQuery adds unnecessary cost to the deployment and will cost more than using BigQuery natively.","poster":"MaxNRG","upvote_count":"2","timestamp":"1638090660.0","comment_id":"489009"},{"content":"Selected Answer: B\nyes is B, as wrote @funtoosh","poster":"StefanoG","upvote_count":"1","comment_id":"487481","timestamp":"1637940780.0"},{"poster":"gcp_k","timestamp":"1635380160.0","upvote_count":"3","comment_id":"468917","content":"Isn't federated queries can be run with Cloud Spanner or Cloud Sql? External data sources are BigTable, GCS and Drive. How come B is the correct answer?","comments":[{"upvote_count":"1","comment_id":"490000","content":"B would be the right answer only if in the text of the question they were referring to \"External Data Source\", which is not the case. Hence B is excluded.","poster":"maurodipa","timestamp":"1638199260.0","comments":[{"poster":"maurodipa","timestamp":"1639594140.0","upvote_count":"1","comment_id":"502420","content":"However, as per google docs on BigQery, it states that in this case the best option is to update the table on GCS. So, if in answer B it was called external table instead of federated data source, the correct answer would have been B. \n\nAs per google docs on BigQuery:\nUse cases for external data sources include:\nLoading and cleaning your data in one pass by querying the data from an external data source (a location external to BigQuery) and writing the cleaned result into BigQuery storage.\nHaving a small amount of frequently changing data that you join with other tables. As an external data source, the frequently changing data does not need to be reloaded every time it is updated."}]}]},{"poster":"anji007","content":"Ans: B\nC& D: Makes solution costly by involving Dataflow (and Datastore).\nA: partition for just 100 most common goods may not be correct choice, also Cloud Storage is cheaper than BigQuery Storage.","comment_id":"462766","upvote_count":"1","timestamp":"1634322720.0"},{"content":"Answer is D.\nRegional bucket, combining with data flow is cheaper.\nB is incorrect as federated queries has more processing cost.","comment_id":"458342","comments":[{"comment_id":"475047","poster":"Crudgey","content":"Federated queries are free as you're already paying for storage.\n\nThey're more taxing but they're still free","upvote_count":"1","timestamp":"1636488360.0"}],"upvote_count":"1","timestamp":"1633535700.0","poster":"ManojT"},{"content":"Correct B\nAs per google docs on BigQuery:\nUse cases for external data sources include:\n\nLoading and cleaning your data in one pass by querying the data from an external data source (a location external to BigQuery) and writing the cleaned result into BigQuery storage.\nHaving a small amount of frequently changing data that you join with other tables. As an external data source, the frequently changing data does not need to be reloaded every time it is updated.","upvote_count":"9","timestamp":"1615596480.0","poster":"BhupiSG","comment_id":"309303"},{"timestamp":"1615395540.0","content":"B:\nhttps://cloud.google.com/bigquery/external-data-sources","poster":"daghayeghi","upvote_count":"2","comment_id":"307321"},{"content":"Answer C:\nBecause we need to \"update\" our data we couldn't use data storage for storing data, then the only choice would be Datastore.","upvote_count":"2","timestamp":"1613347500.0","poster":"daghayeghi","comment_id":"290569"},{"content":"B is right, go for it","comment_id":"288138","timestamp":"1613032500.0","poster":"Sush12","upvote_count":"2"},{"upvote_count":"2","content":"Correct B","poster":"naga","comment_id":"285665","timestamp":"1612719600.0"},{"poster":"JohnHD","comments":[{"comment_id":"238824","upvote_count":"3","comments":[{"comment_id":"265415","content":"If we store the data in BigQuery it uses the columnar format and compression techniques. Would it not reduce the amount of data scanned and query time and hence cost when compared to querying on a file in GCS via Federated queries?","poster":"adigabp","upvote_count":"2","timestamp":"1610444340.0"}],"content":"A is right, however B is cheaper and that is the main request.","timestamp":"1607476980.0","poster":"HectorLeon2099"},{"poster":"Deepakd","timestamp":"1648999920.0","content":"Google's recommended practice is to use federated queries when the underlying data is frequently updated. (30 minutes in this case)","upvote_count":"1","comment_id":"580356"}],"timestamp":"1604164080.0","upvote_count":"2","comment_id":"210012","content":"Answer A: It is the cheapest because it does not incur Datastore & Dataflow expenses. A direct upload is performed without using Datastorage."},{"upvote_count":"4","content":"B is Correct","comment_id":"161149","timestamp":"1597792980.0","poster":"haroldbenites"},{"comments":[{"timestamp":"1600098180.0","comment_id":"179384","poster":"IsaB","upvote_count":"8","content":"Yes, very misleading. Always look for the right answer based on comments, never based on the 'correct' answer indicated."}],"comment_id":"149668","content":"ok..so if the answer is B ,and they suggest C as the correct answer why should we be misled...?","upvote_count":"5","timestamp":"1596450120.0","poster":"TUTUNOMT"},{"timestamp":"1584815460.0","upvote_count":"5","comment_id":"66634","poster":"Rajokkiyam","content":"answer B"},{"timestamp":"1584774060.0","content":"Answer: B","upvote_count":"6","comment_id":"66419","poster":"[Removed]"}],"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/16253-exam-professional-data-engineer-topic-1-question-46/"},{"id":"BQQPS11CYp62HTN34lf0","answers_community":["B (41%)","A (41%)","D (19%)"],"discussion":[{"poster":"jvg637","timestamp":"1584904320.0","comment_id":"67066","comments":[{"timestamp":"1631608740.0","comment_id":"444423","content":"BigQuery is a datawarehouse, not a transactional db. You need to store transactional data as a requirement.","comments":[{"poster":"alecuba16","content":"Biquery Supports transactions:\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/transactions\n, but indeed is not a good DB for OLTP.\n\nBut I would said or CloudSQL or BigQuery","timestamp":"1657979940.0","comment_id":"632192","upvote_count":"4"},{"poster":"alexmirmao","timestamp":"1633596060.0","content":"In my opinion transactional data doesnt mean transactions they could be grouped so there is no need to write register by register.","upvote_count":"8","comments":[{"upvote_count":"5","comment_id":"463261","timestamp":"1634413020.0","poster":"yoshik","content":"In other questions they talk about 'transactional log data' when referring to past transactions, but you could be right, agree. In that case ok A BigQuery. Nevertheless, the question is formulated ambiguously."}],"comment_id":"458615"}],"upvote_count":"28","poster":"yoshik"}],"upvote_count":"61","content":"You want to optimize the data schema + Machine Learning --> Bigquery. So A"},{"comment_id":"66423","timestamp":"1584776160.0","upvote_count":"26","poster":"[Removed]","content":"Answer: Should be D - Datastore","comments":[{"upvote_count":"20","comments":[{"timestamp":"1638617760.0","poster":"BigQuery","upvote_count":"7","comment_id":"493673","content":"BQML is there. But, In question do they want to do ML on BQ?? Its saying just ML Based Company."}],"comment_id":"219088","timestamp":"1605353700.0","poster":"GeeBeeEl","content":"There is SQLML with BigQuery, you know that?\nYou cannot optimize a schema in datastore, it is a NoSQL document database built for automatic scaling, high performance, and ease of application development. It does not work based on schemas!"},{"content":"It was also a difficult one for Chat GPT, it did give different answers each time I inquiry more about the question. After a few iterations, we also agreed on \"D\" :) - because;\n\nIn the context of a food ordering service, storing data about what a user likes and doesn't like to eat can potentially involve a varied and dynamic set of data. Some users might have a long list of food preferences, while others might have only a few. Some users might update their likes and dislikes frequently, while others rarely or never. This kind of data is a good match for a NoSQL database like Datastore, which can easily accommodate such variations.","timestamp":"1686135180.0","poster":"cetanx","upvote_count":"5","comment_id":"917126"}]},{"timestamp":"1738405800.0","poster":"cqrm3n","comment_id":"1349827","upvote_count":"2","content":"Selected Answer: B\nThe answer should be Cloud SQL because it is a relational database suitable for transactional data.\n\nBigQuery is for analytics and querying - not suitable for transactional workload.\nBigtable is for unstructured and time series data.\nDatastore is a nosql document database for semi structured data."},{"content":"Selected Answer: A\nFor a machine learning-based food ordering service that requires optimised storage of transactional data, Google Cloud BigQuery is a suitable choice","poster":"Yad_datatonic","upvote_count":"1","timestamp":"1737543480.0","comment_id":"1344714"},{"upvote_count":"1","comment_id":"1342439","content":"Selected Answer: A\nithin Google Cloud, the database that most readily allows for data schema optimization is BigQuery; it provides features like schema auto-detection, columnar storage, and the ability to manually define your schema to tailor it for efficient querying and analysis of large datasets.","poster":"grshankar9","timestamp":"1737183300.0"},{"comment_id":"1337505","upvote_count":"3","content":"Selected Answer: B\nCloud SQL is the best choice for your application as it provides relational database management and is optimized for storing transactional data with SQL querying capabilities. It is well-suited for managing user profiles, account information, and orders, ensuring data integrity, and supporting complex queries necessary for the food ordering service.","timestamp":"1736241960.0","poster":"manikolbe"},{"upvote_count":"1","poster":"Ronn27","timestamp":"1735843560.0","content":"Selected Answer: B\nUse BigQuery for analyzing aggregated data (e.g., predicting food trends or training ML models).\nUse Cloud Bigtable for large-scale real-time recommendation engines if needed in the future.\nUse Firestore for dynamic, semi-structured data with real-time updates if you need flexibility over transactional consistency.\nCloud SQL strikes the right balance for this use case due to its support for structured data, transactions, and easy integration with other GCP services.\nSo B. CloudSQL is the right answer","comment_id":"1335711"},{"timestamp":"1734705240.0","comment_id":"1329504","content":"Selected Answer: B\nCloud SQL can store transactional data not Big Query. Big Query is an analytical service.","poster":"sravi1200","upvote_count":"1"},{"timestamp":"1734586020.0","content":"Selected Answer: A\nEasy implement data schema + Machine Learning model in Big Query","comment_id":"1328861","poster":"DGames","upvote_count":"1"},{"content":"Selected Answer: B\ndatabase will be used to storage all transactional data.... I think that you need a relational database for that, then federated tables to bigquery to analysis.","upvote_count":"1","poster":"julydev82","timestamp":"1733756760.0","comment_id":"1324109"},{"comment_id":"1312975","timestamp":"1731741960.0","poster":"decipher9","content":"For a machine learning-based food ordering service that needs to store transactional data, Cloud SQL is the most suitable option. Here's why:\n\nCloud SQL is a fully-managed relational database service that supports transactional workloads, making it ideal for storing user profiles, account information, and order details.\nIt provides strong consistency and supports complex queries, which are essential for managing and retrieving transactional data efficiently.\nWhile BigQuery is excellent for large-scale data analysis, it is not optimized for transactional data storage.\nCloud Bigtable is designed for high-throughput and low-latency workloads but lacks the transactional capabilities needed for this use case.\nCloud Datastore is a NoSQL database that supports transactions but is generally less powerful than a relational database for complex transactional schemas12.\nSo, the best choice for your needs is B. Cloud SQL.","upvote_count":"1"},{"poster":"SamuelTsch","content":"Selected Answer: B\nNo idea why so many people go to A. But as transactional data, I think B is correct.","comment_id":"1301614","timestamp":"1729606200.0","upvote_count":"1"},{"comment_id":"1287620","upvote_count":"1","timestamp":"1726993320.0","poster":"baimus","content":"Selected Answer: D\nThe details of the information definitely look suited to noSql to me, so that means C or D. Datastore is designed for this sort of thing - transactional nosql for an App. I took the question to mean \"the machine learning app already exists\" so the fact bigquery allows ML isn't relevant. It would be a leap to assume that the ML is done in Bigquery (I have a current Google ML pro cert, and this wouldn't say bigquery to me from that cert)"},{"upvote_count":"1","timestamp":"1724756160.0","comment_id":"1273384","content":"Selected Answer: B\nCloud SQL is a fully-managed relational database service that supports MySQL, PostgreSQL, and SQL Server. It is well-suited for transactional workloads, allowing you to store structured data with relationships between different entities, such as users, orders, and profiles.","poster":"Nittin"},{"upvote_count":"5","timestamp":"1716139080.0","comment_id":"1213896","poster":"39405bb","content":"he best answer for this scenario is B. Cloud SQL. Here's why:\n\nRelational Data: The information you need to store (user profile, account information, order information) is highly structured and relational. Cloud SQL, being a relational database service, is designed to handle this type of data efficiently.\nTransactional Workloads: Food ordering involves transactional operations (placing orders, updating user preferences, etc.). Cloud SQL is optimized for transactional workloads, ensuring data consistency and integrity.\nEase of Use: Cloud SQL is a managed service, meaning Google handles maintenance, updates, and backups, making it easier to manage than some other options.\nIntegration with Machine Learning: Cloud SQL can easily integrate with other Google Cloud Platform products like BigQuery and Vertex AI, which are crucial for machine learning tasks."},{"upvote_count":"1","comment_id":"1173158","content":"Selected Answer: A\nBigQuery is a fully managed, serverless data warehouse that enables scalable analysis of large datasets. It is designed to handle large volumes of data and support complex queries, making it suitable for storing transactional data and performing analytics. With BigQuery, you can optimize your data schema and easily scale as your data grows. Additionally, BigQuery integrates well with other Google Cloud Platform services, including machine learning services, enabling you to build advanced analytics and predictive models on your transactional data.","poster":"I__SHA1234567","timestamp":"1710396240.0"},{"poster":"philli1011","content":"C\nIt says that the database will be used to store the transactions data. BigQuery is not usually characterized as a data storage system. Also a databased is used for storing transactional Data not a Data Wharehouse.","comment_id":"1134218","timestamp":"1706455500.0","upvote_count":"1"},{"content":"My choice is C\nIt says \"DataBase Schema\" not DataWharehouse Schema. It didn't mention if the ML is to be done in the DataBase or not, it just states that a database is to be created.","upvote_count":"1","poster":"philli1011","comment_id":"1134212","timestamp":"1706455260.0"},{"comments":[{"content":"Hello Camaro, when u going to take the exam ? am appearing on 23rd Dec and keen to know if this set is still valid ?","timestamp":"1702980240.0","poster":"LaxmanTiwari","comment_id":"1100505","upvote_count":"1"}],"poster":"Camaro","content":"I asked ChatGPT this question. \nIt first answered Datastore.\nI said the question asks us to optimize the data schema and Datastore has no schema.\nThen it answered CloudSQL.\nI said the question asks about Machine learning aspect.\nThen it answered Bigquery.\nI said its a food ordering service and must need low latency\nThen it answered Bigtable.\nGPT is clearly not a good tool to use for the prep please avoid it. Its flawed currently that is DEC2023!","timestamp":"1702655160.0","upvote_count":"3","comment_id":"1097458"},{"timestamp":"1699486380.0","poster":"rocky48","comment_id":"1066027","upvote_count":"1","content":"Selected Answer: A\nA. BigQuery -> Most Probably the answer because for analytic it should automatically be big query. But my question is why not others. I've used BigQuery and I know that it allows schema optimization as well as one of it's feature is built in machine learning.\nMore here:https://cloud.google.com/bigquery/docs/introduction\n\nB. Cloud SQL -> I am not being about to find the doc for cloud sql where it talks about being able to optimize the schema and if it can be used for machine learning applications.\n\nC. Cloud Bigtable -> Possible answer because there's schema and can be used for machine learning application.\nDoc: https://cloud.google.com/bigtable/docs/overview\n\nD. Cloud Datastore -> Wrong because Datastore doesnâ€™t have a schema.\nDoc: https://cloud.google.com/datastore/docs/concepts/overview"},{"poster":"A_Nasser","upvote_count":"1","content":"Selected Answer: D\nThe right answer is D because Datastore is transactional, scalable, and can deliver an output to ML.","comment_id":"1018850","timestamp":"1695822720.0"},{"content":"Selected Answer: B\nAlthough you could argue with the formulation of the question, I did also read that this is about a transactional database which BigQuery is not. Thus I would go with Cloud SQL.","comment_id":"987460","poster":"Mark_86","timestamp":"1692710640.0","upvote_count":"1"},{"timestamp":"1691253660.0","content":"Initially I also thought it would be D, but then I re-read & it mentions the whole question is about Data Analytics, hence it has to be A... BigQuery","poster":"alihabib","upvote_count":"1","comment_id":"973180"},{"timestamp":"1690818540.0","poster":"NeoNitin","comment_id":"968261","content":"A sahi hai","upvote_count":"1"},{"comment_id":"967615","upvote_count":"1","content":"Selected Answer: B\nto store all the transactional data of the product","poster":"nescafe7","timestamp":"1690763040.0"},{"upvote_count":"2","content":"Selected Answer: A\nBigQuery is right.\n\nCloud SQL: Not convenient to build ML models on top of it. Also it is a relational database. So what does optimize schema mean here? Imo it means make it more performant. BigQuery de-normalized tables can be more performant than Cloud SQL imposing relational constraints and doing expensive joins.\nCloud BigTable: Don't see why we should use BigTable to store such data and spin up a cluster if we don't want to do serving for an app that requires really high throughput and low latency. Nothing about latency is mentioned in the description so nothing that points to BigTable.\nCloud Datastore: I don't know enough about Datastore, but I have never seen it been mentioned together with ML in my experience with GCP and preparing for the exam.","poster":"Mathew106","timestamp":"1689945060.0","comment_id":"958522"},{"poster":"KC_go_reply","comment_id":"956304","timestamp":"1689747300.0","upvote_count":"3","content":"Selected Answer: D\nA: Just because it says 'machine learning based service' and BigQuery has BQML doesn't mean we want to carry out machine learning in the database itself.\nB: The only aspect that speaks for Cloud SQL is 'transactional data'. However, they don't state explicitly that the database here should be used for the transactions.\nC: No real reason here to use Bigtable, which is very expensive.\n\nD: Datastore would make sense. Information such as user food preferences and profile information are likely semi-structured, thus fitting for a document-like data model. Also, if we want transactions, Datastore also supports that."},{"timestamp":"1689675840.0","poster":"autumn2005","content":"Selected Answer: A\nBigQuery is the best choice","comment_id":"955226","upvote_count":"1"},{"upvote_count":"1","comment_id":"903313","poster":"vaga1","timestamp":"1684678140.0","content":"Selected Answer: A\nCloud SQL is never the best choice for analytics"},{"poster":"cchen8181","comment_id":"900463","content":"Selected Answer: B\nI would go with B: Cloud SQL.\n\nJust because the data will eventually be used for machine learning doesn't mean we need to store data in a database that has built in ML capability like BigQuery. I think since it is transactional data, we are looking for an OLTP solution instead of OLAP, so I would go with Cloud SQL over BigQuery.","timestamp":"1684351860.0","upvote_count":"4"},{"comment_id":"889904","timestamp":"1683276300.0","content":"question formulated badly. \nexcluding \"machine learning\", SQL fits. \nBut with \"ML\", have to choose A. Otherwise SQL, and adding a machine learning tool to it (exporting data from SQL to 3rd party tools). Bad question and answers","upvote_count":"2","poster":"Oleksandr0501"},{"comment_id":"880826","poster":"Jarek7","upvote_count":"3","timestamp":"1682453580.0","content":"Selected Answer: B\nI'm confused with this one. None fits perfect, each fits somehow. \nFirst of all I think we have definitely a relational data here. This one just seems to me as the most clear requirement here - relations are given in the question, so I'd drop Datastore and Bigtable.\nThen I think the key here is that we need to store transactional data. Yes, it means only that it is data about payments, but if it is payment data we just need transactions to be sure that it is accurate - we need to rollback transaction if the payment fails. AFAIK we can use Cloud SQL as a source for BQ, and then use it for ML, so, I'd go for B) Cloud SQL.\nIf the transactions are not priority, then A would be best option."},{"timestamp":"1681244640.0","comments":[{"timestamp":"1689747480.0","poster":"KC_go_reply","content":"Wrong. Datastore is for semi-structured data, and our case does not necessarily imply fully structured data (user preferences, profiles etc.)","upvote_count":"1","comment_id":"956309"}],"poster":"Adswerve","content":"Selected Answer: B\nB Cloud SQL, because it's transactional db\nA is wrong - BQ is not used for transactional load.\nD is wrong - Datastore is used for unstructured data, whereas our use case implies structured data","comment_id":"867658","upvote_count":"3"},{"comment_id":"845939","content":"Transactional DB -> B","timestamp":"1679401860.0","poster":"Tatus","upvote_count":"1"},{"upvote_count":"2","comment_id":"824947","timestamp":"1677596280.0","poster":"tibuenoc","content":"Selected Answer: B\nanswer B\n\nThe database will be used to store all the transactional data of the product. what we need is the database only for store transactional data, not for analysis and ML. so the answer should be \"the database that stores transactional data\", which means, Cloud SQL. if you want to analyze or do ML you just specify Cloud SQL as a federated data source."},{"upvote_count":"3","content":"1. Strong Schema. the table should be normalized\n2. Transaction DB\nThe conclusion is absolutely B. Cloud SQL..","timestamp":"1677079020.0","comment_id":"817970","poster":"jin0"},{"comment_id":"798672","timestamp":"1675582980.0","poster":"Tarek_74","content":"Selected Answer: B\nTransactions so B cloud Sql","upvote_count":"3"},{"comment_id":"791882","content":"ANSWER IS 'D'\nBigtable is optimized for high volumes of data and analytics,igtable is indexed by a single Row Key\nDatastore is optimized to serve high-value transactional data to applications","poster":"vraj_007","upvote_count":"1","comments":[{"timestamp":"1688461080.0","content":"The collected data are transactional (OLTP) and are used to train a ML model (structured data). D is wrong, I think Cloud SQL is the only possible answer.","comment_id":"942551","poster":"euro202","upvote_count":"1"}],"timestamp":"1675013220.0"},{"content":"Selected Answer: D\nD. Datastore\nCloud Datastore is a NoSQL document database service that is well-suited for storing structured and semi-structured data, such as the user profile, user account information, and order information described in the question. Cloud Datastore can provide several options to optimize the data schema, such as denormalization, partitioning and sharding","timestamp":"1674737520.0","comment_id":"788728","upvote_count":"1","poster":"PolyMoe","comments":[{"timestamp":"1675191960.0","poster":"andye12","comments":[{"upvote_count":"1","timestamp":"1689746820.0","comment_id":"956293","poster":"KC_go_reply","content":"How does it state 'very structured'? A user can like and dislike X, Y amount of things. Doesn't fit into relation schema does it?"},{"comment_id":"917134","content":"Here is a Chat GPT answer after a few iterations of the choices. In the beginning it also chose Cloud SQL but user profile seems to be an important criteria. Here its answer;\n\nConsidering the requirements and the discussion we've had:\n\n- Storing user profiles, including likes and dislikes\n- Storing user account information\n- Storing order information (when, from where, and to whom)\nAs well as the fact that this data is going to be used for a machine learning-based food ordering service, which may need to handle a considerable amount of semi-structured data and will benefit from flexible schema evolution, my suggestion would lean towards Google Cloud Datastore (Option D).","upvote_count":"1","timestamp":"1686135720.0","poster":"cetanx"}],"upvote_count":"4","comment_id":"794523","content":"do not just copy the answer from chatGPT. This is very structured data, so no need for datastore"}]},{"comment_id":"779215","poster":"samdhimal","comments":[{"poster":"samdhimal","comment_id":"779236","content":"So I am pretty sure D is wrong for sure because I am looking at the doc and it's saying that Datastore doesnt have schema. Hence, this option is of no use.","comments":[{"poster":"samdhimal","comment_id":"779237","upvote_count":"2","content":"But what about others?\nA. BigQuery -> Most Probably the answer because for analytic it should automatically be big query. But my question is why not others. I've used BigQuery and I know that it allows schema optimization as well as one of it's feature is built in machine learning.\nMore here:https://cloud.google.com/bigquery/docs/introduction\n\nB. Cloud SQL -> I am not being about to find the doc for cloud sql where it talks about being able to optimize the schema and if it can be used for machine learning applications.\n\nC. Cloud Bigtable -> Possible answer because there's schema and can be used for machine learning application.\nDoc: https://cloud.google.com/bigtable/docs/overview\n\nD. Cloud Datastore -> Wrong because Datastore doesnâ€™t have a schema.\nDoc: https://cloud.google.com/datastore/docs/concepts/overview\n\nLet's discuss this only based on docs. Please make your your reply contains supporting google doc links. Let's get to the bottom of this.","timestamp":"1673982120.0"}],"upvote_count":"2","timestamp":"1673982060.0"}],"upvote_count":"3","content":"I am Seriously super confused about this one. People are literally just saying A,B,C,D.","timestamp":"1673980920.0"},{"comment_id":"774010","content":"If you want to optimize the data schema for a transactional database on the Google Cloud Platform, you should use Cloud Spanner... but that's not a choice...\n\nThen it would make Cloud SQL the answer. I'm on the fence between BQ and Cloud SQL\n\nDefinitely not BigTable - that NoSQL.... Datastore is also NoSQL... not needed","timestamp":"1673575260.0","poster":"desertlotus1211","upvote_count":"2"},{"upvote_count":"2","poster":"Nirca","timestamp":"1672741680.0","comment_id":"764421","content":"Selected Answer: A\nYou need Analytics. A"},{"content":"Answer is C: ChatGPT says it is BigTable and the explanation makes sense to me as it can be used for storing transactional data and be used for ML models.","poster":"SelvaJG","comments":[{"poster":"lucaluca1982","timestamp":"1679629440.0","upvote_count":"1","content":"just tried and it said B","comment_id":"848919"}],"timestamp":"1672528620.0","upvote_count":"1","comment_id":"762978"},{"timestamp":"1671492840.0","content":"Selected Answer: A\nIt should be BQ. Transactional data term probably is to mislead the user. BQ tables when used as a DwH do consist a lot of transactions sourced from different source systems designed as fact tables. We are not talking about ingestion here rather its more about consumption/serving (ML). So although Cloud SQL is good for OLTP, this is still a analytics use case.\nAnswer should be A (BigQuery)","upvote_count":"2","poster":"Krish6488","comment_id":"750327"},{"comment_id":"746871","content":"Selected Answer: A\nBigquery stores transactional data as a fact table,support schema optimization which is not possible with datastore (not suggestable), So i support option A","timestamp":"1671171900.0","poster":"Asheesh1909","upvote_count":"1"},{"content":"Selected Answer: A\nStraight Answer A - Big Query, for ML and transaction capability (work as data warehouse)\nEasy to built ML model using BQML to forecast what same would like to eat.","poster":"DGames","comment_id":"744503","timestamp":"1670970780.0","upvote_count":"1"},{"content":"Selected Answer: B\nOptimizing for transactional data, not for machine learning.","timestamp":"1670763900.0","upvote_count":"3","comment_id":"741715","poster":"lukas_xls"},{"content":"Selected Answer: A\nAs NicolasN says. The answer is A.","timestamp":"1670752260.0","upvote_count":"1","comment_id":"741575","poster":"odacir"},{"timestamp":"1669756020.0","upvote_count":"6","comment_id":"730853","poster":"NicolasN","content":"Selected Answer: A\nThe answer comes from the official \"Best practices for implementing machine learning on Google Cloud\". \nðŸ”— https://cloud.google.com/architecture/ml-on-gcp-best-practices#avoid-storing-data-in-block-storage\n\nThe interesting points:\nðŸ”¸Store tabular data in BigQuery / Store image, video, audio and unstructured data on Cloud Storage\nðŸ”¸Avoid storing data in block storage, like Network File Systems or on virtual machine (VM) hard disks\nðŸ”¸Similarly, avoid reading data directly from databases like Cloud SQL. \nðŸ”¸Instead, store data in BigQuery and Cloud Storage."},{"upvote_count":"2","poster":"nkit","content":"Selected Answer: A\nMachine learning -> BQ\n(the word \"transactional database\" is used to confuse you","comment_id":"724387","timestamp":"1669129080.0"},{"timestamp":"1668824340.0","content":"Nobody asked you to use BQML. They only said machine learning based product.","upvote_count":"1","poster":"ovokpus","comment_id":"721722"},{"comment_id":"719357","poster":"Jay_Krish","content":"Selected Answer: A\nHow could it be D - Datastore whilst it is a NoSQL. The incoming data seems to be structured. It's either A or B but A looks more like an answer to me. \nA - Big Query","upvote_count":"1","timestamp":"1668578280.0"},{"comment_id":"685108","poster":"Chavoz","comments":[{"upvote_count":"1","poster":"Chavoz","comment_id":"685109","timestamp":"1664751540.0","content":"Extracted from the \"Modernizing Data Lakes and Data Warehouses\" Google Cloud course: \n\n\"Nested columns can be understood as a form of repeated field. It preserves the\nrelational qualities of the original data and schema while enabling columnar and\nparallel processing of the repeated nested fields. It is the best alternative for data that\nalready has a relational pattern to it. Turning the relation into a nested or repeated\nfield improves BigQuery performance.\n\nNested and repeated fields help BigQuery work with data sourced in relational\ndatabases.\n\nLook for nested and repeated fields whenever BigQuery is used in a hybrid solution in\nconjunction with traditional databases.\""},{"comment_id":"686967","content":"Have you taken GCP Professional Data Engineer exam?","upvote_count":"1","poster":"varsha_7890","timestamp":"1664980080.0"}],"timestamp":"1664751360.0","content":"Selected Answer: A\nRather than \"transactions\", the key here is optimize the schema. So A, because in Big Query you can denormalize the tables (the flattened (non-relational) organization makes queries more efficient because they can be processed in parallel using columnar processing), and also use nested and repeated fields.","upvote_count":"2"},{"poster":"ducc","upvote_count":"1","content":"Selected Answer: A\nI vote for A.\nIt is \"stored\" transactions, not processing it.\nAnd the question is about ML -> BigQuery ML","timestamp":"1661724540.0","comment_id":"653180"},{"content":"B is the correct answer.\n\"The database will be used to store all the transactional data of the product.\" and BQ is too expensive for that only.\nDesigning the ML can be done on BQ witg federated data source CloudSQL, but that is different step.","poster":"MisuLava","comment_id":"652301","upvote_count":"1","timestamp":"1661536980.0"},{"upvote_count":"3","poster":"OhBoyV2","timestamp":"1660646220.0","content":"Selected Answer: B\nThe Answer is B. CloudSQL is an OLTP database.","comment_id":"647592"},{"timestamp":"1657184040.0","comments":[{"content":"BigQuery is not optimized for OLTP, but it can do transactions: https://cloud.google.com/bigquery/docs/reference/standard-sql/transactions\n\nBut the point is that ML is not specified as either inside the DB or outside, so we don't know if they plan to use ML inside google cloud, if it is and there are not a lot of transactions BQ could be OK, but as you indicated is better to use CloudSQL","upvote_count":"1","timestamp":"1657980120.0","comment_id":"632193","poster":"alecuba16"}],"comment_id":"628276","content":"Answer is B ( cloud SQL). Reasons are:\n1. Bigquery is not for OLTP i.e transactional data.\n2. Cloud Datastore is no sql and meant for web developments. Here we need it for ML analytics hence cloud sql is best.","poster":"thapliyal","upvote_count":"2"},{"content":"Selected Answer: B\nAS Cloud SQL is best way to store the transactional data.","poster":"KundanK973","comment_id":"625555","upvote_count":"3","timestamp":"1656645840.0"},{"upvote_count":"2","content":"The question states that you are working on an ML based food ordering service. Its a service so it will be a continuous operation. Meaning that you will not work on just a batch of data, it will be continuous transactional data that will be used for their ML service.\n\nThe only product that should be used for transactional data from the options is Cloud SQL, and you can optimize schema.","comment_id":"612460","timestamp":"1654537620.0","poster":"AmirN"},{"poster":"Yad_datatonic","upvote_count":"1","comment_id":"608089","timestamp":"1653656880.0","content":"Selected Answer: A\nThey just want to store transactional data and BQ has schema optimization."},{"comment_id":"590065","content":"Selected Answer: A\nDatastore is NoSQL hence Schemaless, so goodluck optimising a schema with it. BQ allows both schema optimisation and ML\nhttps://cloud.google.com/datastore/docs/concepts/overview#:~:text=Unlike%20traditional%20relational%20databases%20which%20enforce%20a%20schema%2C%20Datastore%20is%20schemaless.","poster":"NR22","upvote_count":"2","timestamp":"1650638640.0"},{"comment_id":"589879","poster":"Tlab97271","content":"Database schema is for relational databases. Datastore is NoSQL, so the answer must be A: BigQuery","upvote_count":"1","timestamp":"1650621360.0"},{"poster":"pthyadi","content":"Selected Answer: A\nMachine Learning+ Schema Optimisation , and it is not used for OLTP they will load Transactions data into a storage","timestamp":"1650439200.0","comment_id":"588481","upvote_count":"3"},{"content":"If Datastore then why not C? isn't CloudSQL is best for storing transactional data?","upvote_count":"2","comment_id":"585037","timestamp":"1649831580.0","poster":"Pinko1497"},{"content":"Selected Answer: A\nVote for A as we have 2 requirements that correlate with BQ\n1) ML \n2) Shema optimisation (arrays)\n\nTrainsactional data used here to make us think, we are not going to make transactions we are going to use it for analysis. so all for BQ","comment_id":"560691","upvote_count":"2","poster":"Arkon88","timestamp":"1646389500.0"},{"content":"Selected Answer: A\nFirebase is a documental DB so can have a schema but \"Cloud Datastore\" not.\nThe question says it needs to \"store\" transactional data, not doing transaction operations and the data will be used for ML. For all these reasones BigQuery is the best choice.","upvote_count":"2","comments":[{"content":"in such questions google is pushing indirectly to bq. \nnormally, a predictive service requires a persistance, ml pipeline and a service layer. the trick in such questions are, somehow, it creates a perception that you will store data and do ml on db side :) that's why the answer bq which is not the best way to do ml, or not the best cheapest way or not best practise way !!","upvote_count":"1","timestamp":"1644556800.0","comment_id":"545069","poster":"Tanzu"}],"poster":"BigDataBB","timestamp":"1643792460.0","comment_id":"538480"},{"timestamp":"1643195820.0","poster":"ZIMARAKI","comment_id":"532819","upvote_count":"1","content":"Selected Answer: D\nI would go for D"},{"timestamp":"1642956840.0","comment_id":"530690","poster":"rbeeraka","content":"Selected Answer: B\nits because of schema","upvote_count":"1"},{"poster":"Deepakd","content":"Here the requirement is to store the transactional data that is used for a ML application . This is different from a DB which is used in a transaction processing(OLTP) system. If the requirement has been to store data to be used by a transaction processing system , Cloud SQL could have been the answer. But the ask is data will be used by a ML Model. Also all data warehouses store transaction data only , that is why fact tables are designed for. I will go with BQ. Schema optimization is not possible in NO SQL Darabases , so C& D are ruled out.","timestamp":"1642911720.0","comment_id":"530245","upvote_count":"6"},{"comment_id":"523693","poster":"sraakesh95","timestamp":"1642188120.0","content":"Selected Answer: D\nD: Transactional DB frequent schema changes. Daatastore can be made as a part of an ML pipeline. Also, cheaper than BQ with all the requirements specified\nNot A: BigQuery is used for OLAP querying, ML could be just to confuse user","upvote_count":"2"},{"content":"Selected Answer: D\nthere is a similar question in PCA exam, and for storing user profiles and ML operations Datastore. \n\nhttps://cloud.google.com/architecture/building-scalable-web-apps-with-cloud-datastore\nSee Datastore usecases:\n\n- User profiles to customize a user's experience based on their past activities and preferences. With Datastore's flexible schema, you can evolve the structure of user profiles over timeâ€”for example, by adding new properties to support new features in your application. Schema changes happen with no downtime, and performance doesn't degrade even as the number of users grows.\n\n- Datastore as part of a machine learning pipeline:\nhttps://cloud.google.com/architecture/building-scalable-web-apps-with-cloud-datastore#integration-with-other-gcp-products","poster":"medeis_jar","upvote_count":"7","timestamp":"1641305940.0","comment_id":"516700","comments":[{"poster":"MaxNRG","content":"Agreed with D: Datastore","timestamp":"1642869300.0","comment_id":"529976","upvote_count":"1"}]},{"content":"Selected Answer: B\nHere \"Machine learning-based\" is for describing the type of the product. Something like Google is for Ml based searching. The question should not be concluded to that point. The most important thing is to need to store transactional info which has mentioned in the point form. Datastore and CloudSQL are transactional DBS. Then need to choose which one from those two. The next point is Schema. Schema is related to CloudSQL. \n\nThe answer is B.","comment_id":"509463","upvote_count":"2","poster":"Bhawantha","timestamp":"1640503140.0"},{"poster":"Bhawantha","comment_id":"509454","timestamp":"1640501160.0","content":"Selected Answer: B\nSince this is transactional.","upvote_count":"2"},{"content":"It's our homely Big Query :D\nQues says: \"designing the database schema for a machine learning-based food ordering service that will predict what users want to eat\", and \"just storing transactional data\" not behaving like a transactional database\". On top of it, performing ML-so BQ can easily do it","poster":"kishanu","comment_id":"505639","timestamp":"1640026740.0","upvote_count":"2"},{"comments":[{"timestamp":"1639596420.0","comment_id":"502446","upvote_count":"1","poster":"maurodipa","content":"Sorry, typo: Answer is B!"}],"upvote_count":"1","poster":"maurodipa","comment_id":"502431","timestamp":"1639594920.0","content":"Answer is A: key word here is \"optimize the data schema\", which is an action you do only in SQL db. In NoSQL db you do index optimization. \nhttps://www.databasejournal.com/features/mysql/understanding-schema-optimization-in-mysql.html\n\nhttps://www.mongodb.com/nosql-explained/data-modeling"},{"poster":"BigQuery","content":"Selected Answer: A\nSAY MY NAME!","upvote_count":"2","comment_id":"493675","timestamp":"1638617820.0","comments":[{"upvote_count":"1","comment_id":"493676","comments":[{"poster":"BigQuery","comment_id":"494932","timestamp":"1638769980.0","content":"UPON FURTHER CONSIDERATION BIGQUERY IS CORRECT","upvote_count":"2"}],"poster":"BigQuery","timestamp":"1638617880.0","content":"But Guys I think Option D is correct. Since It States They want to make best use of Schema. and there are 3 JSON Like Structure the app needs to store. Remember they just asking for storage solution."}]},{"timestamp":"1638095820.0","poster":"MaxNRG","upvote_count":"3","content":"Selected Answer: D\nIt can be D:\nhttps://cloud.google.com/architecture/building-scalable-web-apps-with-cloud-datastore\nSee usecases.\n\nUser profiles to customize a user's experience based on their past activities and preferences. With Datastore's flexible schema, you can evolve the structure of user profiles over timeâ€”for example, by adding new properties to support new features in your application. Schema changes happen with no downtime, and performance doesn't degrade even as the number of users grows.\nyou can use Datastore as part of a machine learning pipeline:\nhttps://cloud.google.com/architecture/building-scalable-web-apps-with-cloud-datastore#integration-with-other-gcp-products","comment_id":"489060","comments":[{"comments":[{"comment_id":"581711","poster":"tavva_prudhvi","upvote_count":"1","timestamp":"1649234400.0","content":"Why are you obsessed with Datastore with ML? They just mentioned ML as the company type, that's it! We have to focus on schema, transactional data!"}],"comment_id":"489064","timestamp":"1638096240.0","upvote_count":"1","content":"That's a tricky one. A and D are both possible.\nBut I would with D","poster":"MaxNRG"}]},{"upvote_count":"1","timestamp":"1638091560.0","poster":"MaxNRG","content":"Selected Answer: A\nStructured and analytical hence A BigQuery is correct.\nhttps://cloud.google.com/architecture/ml-on-structured-data-analysis-prep-1\nhttps://towardsdatascience.com/machine-learning-for-data-analysts-bigquery-ml-b60ef05e43c2","comments":[{"content":"where in the question says \"Structured and analytical\"? So, many people without knowing data related terms commenting and changing peoples mind.","poster":"BigQuery","timestamp":"1638617640.0","upvote_count":"2","comment_id":"493670"}],"comment_id":"489014"},{"poster":"Negro","comment_id":"478922","content":"A: It says you need to store transactional data. You can optimize the schema in BigQuery. Then, as you want to do ML, you are going to do data explor, transf to build the analytical table. That analytical table could be input to a BQML model.\nCloud SQL could work, but con GCP I think BigQuery is the right answer.","upvote_count":"2","timestamp":"1637001060.0"},{"timestamp":"1635986220.0","content":"A. \nIt says it needs to \"store\" transactional data, not doing transaction operations. Also, data will be used for ML, so A-BigQuery is the best choice.","poster":"JayZeeLee","comment_id":"472335","upvote_count":"4","comments":[{"poster":"exnaniantwort","upvote_count":"1","comment_id":"530221","timestamp":"1642905960.0","content":"Data warehouse is meant to do mostly read (80%) and less write (20%). \nOLTP instead does 80% write, 20% read, which means what you said \"store\" transational data.\nDefinitely A is false."}]},{"timestamp":"1634323440.0","upvote_count":"2","content":"Ans: A","comment_id":"462767","poster":"anji007"},{"poster":"MrXBasit","timestamp":"1632583500.0","comment_id":"451486","upvote_count":"4","content":"The answer is D because we need to store transactional data. Datastore is a highly scalable NoSQL database for your applications. Datastore automatically handles sharding and replication, providing you with a highly available and durable database that scales automatically to handle your applications' load. Datastore provides a myriad of capabilities such as ACID transactions, SQL-like queries, indexes, and much more."},{"content":"B\n> The database will be used to store all the transactional data of the product.\nwhat we need is the database only for store transactional data, not for analysis and ML.\nso the answer should be \"the database that stores transactional data\", which means, Cloud SQL.\nif you want to analyze or do ML you just specify Cloud SQL as a federated data source.\n\nA: it's good for analysis but it costs too much to input/output data frequently.\nC: BigTable is not good for transactional data.\nD: okay datastore supports transactions, but it is weaker than RDB, and also, in this case, the data schema has already defined , you should use RDB.","poster":"kubosuke","comment_id":"443202","timestamp":"1631405220.0","upvote_count":"4"},{"poster":"Yashwanth0818","content":"Answer: Cloud SQL\nQuestion is about designing Database not about ML task.We can have database in Cloud SQL as it is transactional.Requirement is database should be compatible for ML services so we can access this database in BQ for ML services.So, Cloud SQL is correct.","comments":[{"comment_id":"469834","timestamp":"1635521400.0","poster":"squishy_fishy","upvote_count":"2","content":"You need to serve the data for Machine Learning. You can't do that with Cloud SQL. A is the correct answer."}],"upvote_count":"5","comment_id":"402434","timestamp":"1625807580.0"},{"poster":"awssp12345","timestamp":"1625418420.0","upvote_count":"3","content":"A simple google search for \"GCP database for ML\" results in BQ. Wasn't inclined but changed my mind from D to A.","comment_id":"398530"},{"upvote_count":"2","content":"Vote for 'A'","timestamp":"1624806840.0","poster":"sumanshu","comment_id":"392169"},{"poster":"gcper","timestamp":"1617030600.0","comment_id":"323578","comments":[{"timestamp":"1621725600.0","content":"Storing transactional data is different than processing transactions in real time. BQ is perfect for doing analysis of transactional data.","poster":"Jphix","upvote_count":"4","comment_id":"363983"}],"content":"B\n\nI think the initial sentences are there to trick the test taker... They go on to say \"The database will be used to store all the transactional data of the product. You want to optimize the data schema.\"\n\nNote: store transactional data. BQ isn't a transactional database. Datastore is schemaless document NoSQL database and BigTable is rowkey NoSQL database.","upvote_count":"9"},{"upvote_count":"4","poster":"daghayeghi","timestamp":"1615398720.0","content":"A:\nYou want to optimize the data schema + Machine Learning --> Bigquery.\nYou cannot optimize a schema in datastore, it is a NoSQL document database built for automatic scaling, high performance, and ease of application development. It does not work based on schemas!","comment_id":"307352"},{"upvote_count":"4","content":"Correct A","timestamp":"1612719720.0","comment_id":"285667","poster":"naga"},{"upvote_count":"1","content":"Option-C: Refer this guide: https://stackoverflow.com/questions/30085326/google-cloud-bigtable-vs-google-cloud-datastore","timestamp":"1610421060.0","poster":"StelSen","comment_id":"265268"},{"upvote_count":"4","poster":"SureshKotla","timestamp":"1600753860.0","content":"B - User profile data, transaction information is best stored in Cloud SQL. Moreover the ML component is not necessarily GCP etc.. It can be a simple Python ML program which can use Cloud SQL. I believe the focus is on database.","comment_id":"184216"},{"comment_id":"161151","timestamp":"1597793100.0","content":"A is correct","upvote_count":"3","poster":"haroldbenites"},{"content":"Answer A is correct; Google best practice - Transactional + ML = Bigquery.","timestamp":"1594448940.0","poster":"AJKumar","comment_id":"131922","upvote_count":"3"},{"comments":[{"poster":"Rajuuu","content":"CloudSQL database does not support ML serviceâ€¦You nee a BigQuery for the ML service.","upvote_count":"1","comment_id":"137048","timestamp":"1594979220.0"}],"content":"I think it is B: \"food ordering service\", \"store all the transactional data of the product.\" so it should be transactional database: Cloud SQL","comment_id":"114900","timestamp":"1592672580.0","upvote_count":"2","poster":"ch3n6"},{"upvote_count":"9","comments":[{"poster":"GeeBeeEl","comment_id":"219071","content":"store transactions (item sold) is same as store a database transaction!!!","timestamp":"1605352200.0","upvote_count":"1"}],"comment_id":"86038","timestamp":"1589024280.0","poster":"vindahake","content":"BiqQuery -the use of word transactional data is misleading and I think refers to store transactions (items sold) and not database transactions"},{"timestamp":"1587227400.0","content":"A; ML should be BQ; stored all transactional data (not use for transactional) so should BQ","comment_id":"76122","comments":[],"poster":"itche_scratche","upvote_count":"9"},{"poster":"taepyung","comments":[{"upvote_count":"3","poster":"taepyung","comments":[{"upvote_count":"2","poster":"GeeBeeEl","timestamp":"1605352260.0","content":"Good that you are thinking more taepyung, Do you have a link to back this up?","comment_id":"219074"}],"timestamp":"1592436720.0","content":"I change my choice after thinking more.\nMy choice is BQ\nI agree with itche & vindahake","comment_id":"112801"}],"upvote_count":"2","timestamp":"1587193380.0","comment_id":"75961","content":"Answer: C\n- BigQuery is not for transaction\n- Data Store is schemaless (remember that this question metioned that user wants to optimize scheme)\n\nSo, my choice is Bigtable\n- transactional & scheme"},{"upvote_count":"7","content":"Is datastore, the key words here are â€œtransactional data of the productâ€ and â€œoptimize the data schemaâ€ Datastore is the document database perfect for this.","comments":[{"content":"I think key word is : \"You are designing the database schema for a machine learning-based food\" - i.e. we are designing a DB for ML","upvote_count":"1","comment_id":"392171","timestamp":"1624807020.0","poster":"sumanshu"}],"comment_id":"68016","timestamp":"1585101540.0","poster":"digvijay"},{"upvote_count":"7","comment_id":"66638","timestamp":"1584815700.0","poster":"Rajokkiyam","content":"If we are planning to Store and Do M/L on the database then BQ is the default Choice.\nElse, CloudSQL(its simple transactional data). DataStore(NoSQL/SchemaLess) is not a choice because the requirement has already defined the definite schema."},{"timestamp":"1584417000.0","content":"Why not D? BigQuery is ideal for storing the data for analysis but for the service to make prediction, data should be from some sort of ODS or transaction database which Cloud DataStore is ideal in this case.","upvote_count":"6","poster":"rickywck","comment_id":"65002"}],"question_images":[],"unix_timestamp":1584417000,"exam_id":10,"answer_description":"","topic":"1","question_text":"You are designing the database schema for a machine learning-based food ordering service that will predict what users want to eat. Here is some of the information you need to store:\nâœ‘ The user profile: What the user likes and doesn't like to eat\nâœ‘ The user account information: Name, address, preferred meal times\nâœ‘ The order information: When orders are made, from where, to whom\nThe database will be used to store all the transactional data of the product. You want to optimize the data schema. Which Google Cloud Platform product should you use?","timestamp":"2020-03-17 04:50:00","question_id":262,"answer_ET":"A","answer_images":[],"answer":"B","url":"https://www.examtopics.com/discussions/google/view/16820-exam-professional-data-engineer-topic-1-question-47/","isMC":true,"choices":{"A":"BigQuery","B":"Cloud SQL","C":"Cloud Bigtable","D":"Cloud Datastore"}},{"id":"JEYp2jROpvETS1OLywgF","discussion":[{"timestamp":"1632746100.0","comment_id":"68619","poster":"[Removed]","content":"Answer: C\nDescription: Bigquery understands UTF-8 encoding anything other than that will result in data issues with schema","upvote_count":"26"},{"upvote_count":"17","timestamp":"1676883780.0","comment_id":"427943","content":"Answer : C :\n\" If you don't specify an encoding, or if you specify UTF-8 encoding when the CSV file is not UTF-8 encoded, BigQuery attempts to convert the data to UTF-8. Generally, your data will be loaded successfully, but it may not match byte-for-byte what you expect.\"\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#details_of_loading_csv_data","poster":"YAS007"},{"timestamp":"1721237460.0","content":"SITUATION:\n- Your company is loading comma-separated values (CSV) files into Google BigQuery. \n- Data is fully imported successfully. \nPROBLEM:\n- Imported data is not matching byte-to-byte to the source file. Reason?","comments":[{"timestamp":"1721237520.0","content":"A. The CSV data loaded in BigQuery is not flagged as CSV.\nSince BigQuery support multiple formats it could be that maybe avro or json was selected. \nBut the file import was successful hence csv was selected. Either manually or it was left as is since the default file type is csv. Lastly, this is WRONG.\nB. The CSV data has invalid rows that were skipped on import.\n-> Since the data was successfully imported there were no invalid rows. Hence, This is wrong answer too.","poster":"samdhimal","comment_id":"779265","upvote_count":"2","comments":[{"content":"C. The CSV data loaded in BigQuery is not using BigQuery's default encoding.\n-> \"BigQuery supports UTF-8 encoding for both nested or repeated and flat data. BigQuery supports ISO-8859-1 encoding for flat data only for CSV files.\"\nSource: https://cloud.google.com/bigquery/docs/loading-data\nDefault BQ Encoding: UTF-8\nThis is probably the correct answer because if the csv file encoding was not UTF-8 and instead it was ISO-8859-1 then we would have to tell bigquery that orelse it will assume it is UTF-8. Hence, Imported data is not matching byte-to-byte to the source file. CORRECT ANSWER!","timestamp":"1721237520.0","poster":"samdhimal","upvote_count":"2","comments":[{"timestamp":"1721237520.0","content":"D. The CSV data has not gone through an ETL phase before loading into BigQuery.\n-> ETL means Extract, Transform and Load and this is actually very important content for Cloud Data Engineers. Look into it if interested! But getting back to the topic: ETL is usually required when the source format and target format are different. You need to extract source file and the transform it before loading the data to fit the target. This is also not a viable option. Also Data is imported successfully and the question doesn't mention anything regarding ETL.","upvote_count":"2","comment_id":"779268","poster":"samdhimal"}],"comment_id":"779267"}]}],"comment_id":"779261","poster":"samdhimal","upvote_count":"2"},{"content":"Selected Answer: C\nA is not correct because if another data format other than CSV was selected then the data would not import successfully.\nB is not correct because the data was fully imported meaning no rows were skipped.\nC is correct because this is the only situation that would cause successful import.\nD is not correct because whether the data has been previously transformed will not affect whether the source file will match the BigQuery table.","poster":"medeis_jar","comment_id":"516707","upvote_count":"6","timestamp":"1688473500.0"},{"comments":[{"timestamp":"1718169300.0","poster":"NicolasN","content":"Exactlyâ¬†\nThe updated link (Dec. 2022) and the quote:\nðŸ”— https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#encoding\n\"If you don't specify an encoding, or if you specify UTF-8 encoding when the CSV file is not UTF-8 encoded, BigQuery attempts to convert the data to UTF-8. Generally, your data will be loaded successfully, but it may not match byte-for-byte what you expect.\"","comment_id":"742497","upvote_count":"1"}],"content":"Selected Answer: C\nC is correct because this is the only situation that would cause successful import.\nA is not correct because if another data format other than CSV was selected then the data would not import successfully.\nB is not correct because the data was fully imported meaning no rows were skipped.\nD is not correct because whether the data has been previously transformed will not affect whether the source file will match the BigQuery table.\nhttps://cloud.google.com/bigquery/docs/loading-data#loading_encoded_data","comment_id":"489015","poster":"MaxNRG","timestamp":"1685258880.0","upvote_count":"2"},{"upvote_count":"3","timestamp":"1681585020.0","content":"Ans: C","poster":"anji007","comment_id":"462773"},{"content":"Vote for 'C'","poster":"sumanshu","timestamp":"1672162440.0","comments":[{"comment_id":"401983","poster":"sumanshu","upvote_count":"2","content":"A is not correct because if another data format other than CSV was selected then the data would not import successfully.\nB is not correct because the data was fully imported meaning no rows were skipped.\nC is correct because this is the only situation that would cause successful import.\nD is not correct because whether the data has been previously transformed will not affect whether the source file will match the BigQuery table.","timestamp":"1673195280.0"}],"upvote_count":"3","comment_id":"392184"},{"poster":"naga","content":"Correct C","timestamp":"1659887040.0","upvote_count":"2","comment_id":"285670"},{"upvote_count":"3","timestamp":"1645233960.0","content":"C is correct","comment_id":"161152","poster":"haroldbenites"},{"comment_id":"160332","poster":"saurabh1805","timestamp":"1645135800.0","upvote_count":"6","content":"C is correct answer, Refer below link for more informaiton.\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#details_of_loading_csv_data"},{"timestamp":"1632202560.0","comment_id":"66424","content":"Answer: C","upvote_count":"10","poster":"[Removed]"}],"answer_images":[],"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/17082-exam-professional-data-engineer-topic-1-question-48/","answer_ET":"C","answers_community":["C (100%)"],"question_text":"Your company is loading comma-separated values (CSV) files into Google BigQuery. The data is fully imported successfully; however, the imported data is not matching byte-to-byte to the source file. What is the most likely cause of this problem?","choices":{"B":"The CSV data has invalid rows that were skipped on import.","A":"The CSV data loaded in BigQuery is not flagged as CSV.","D":"The CSV data has not gone through an ETL phase before loading into BigQuery.","C":"The CSV data loaded in BigQuery is not using BigQuery's default encoding."},"question_id":263,"topic":"1","exam_id":10,"answer":"C","question_images":[],"answer_description":"","timestamp":"2020-03-21 08:36:00","unix_timestamp":1584776160},{"id":"NlYQMWFDCRBXMvXQyyGx","topic":"1","answer":"CD","answer_ET":"CD","url":"https://www.examtopics.com/discussions/google/view/17083-exam-professional-data-engineer-topic-1-question-49/","unix_timestamp":1584776880,"discussion":[{"timestamp":"1608166620.0","upvote_count":"55","content":"E cannot be: Transfer Service is recommended for 300mbps or faster\nhttps://cloud.google.com/storage-transfer/docs/on-prem-overview\n\nBandwidth is not an issue, so B is not an answer\n\nCloud Storage loading gets better throughput the larger the files are. Therefore making them smaller with compression does not seem a solution. -m option to do parallel work is recommended. Therefore A is not and C is an answer.\nhttps://medium.com/@duhroach/optimizing-google-cloud-storage-small-file-upload-performance-ad26530201dc\n\nThat leaves D as the other option. It is true you cannot user tar directly with gsutil, but you can load the tar file to Cloud Storage, move the file to a Compute Engine instance with Linux, use tar to split files and copy them back to Cloud Storage. Batching many files in a larger tar will improve Cloud Storage throughput.\n\nSo, given the alternatives, I think answer is CD","comments":[{"content":"This should be the correct answer.","comment_id":"398555","timestamp":"1625420340.0","poster":"awssp12345","upvote_count":"3"},{"upvote_count":"1","poster":"musumusu","comment_id":"819826","comments":[{"timestamp":"1677627960.0","upvote_count":"2","content":"normally the solutions are Google Cloud Services based, as it's a vendor exam","poster":"Booqq","comment_id":"825392"},{"comment_id":"880839","content":"They have 20.000 files 4kb each per hour, so bandwith needed for it is far below 1mbps. 50mbps is enough to upload all day generated data in about 5 minutes.","upvote_count":"2","poster":"Jarek7","timestamp":"1682455020.0"}],"content":"50mbps is so slow, why you think bandwidth is ok! For parallel upload you need good internet ?","timestamp":"1677190860.0"},{"comment_id":"459264","upvote_count":"4","content":"D is incorrect. gsutil with -m option uses multiprocessing/multithreading. It means it will copy the file in parallel. The benefit of multiprocessing/multithreading is significantly high when working with large number of files, instead of file size. The important point of multiprocessing/multithreading is sending multiple files in parallel. Hence file size doesn't give impact to gsutil with -m option. Gsutil with -m option doesn't split a big file into multiple chunks and transfer it in parallel. So in my opinion the answer is A and C.","poster":"vholti","timestamp":"1633702140.0","comments":[{"content":"Here is the docs which support my opinion: https://cloud.google.com/storage/docs/gsutil/addlhelp/TopLevelCommandLineOptions","comments":[{"poster":"Mathew106","timestamp":"1690191240.0","content":"We have small files of 4KB and no issues with bandwidth. It's not an issue that -m does not split files. Our problem is with total volume.","comment_id":"961413","upvote_count":"1"}],"poster":"vholti","upvote_count":"3","timestamp":"1633702380.0","comment_id":"459266"},{"upvote_count":"2","comment_id":"958535","content":"As far as I understand compression is not something we want here because bandwidth is not an issue and compressed files will need to be decompressed on the cloud. On top of that if we want to load those files later in BigQuery to create the report we know that we cannot load compressed csv files in parallel.\n\ngsutil makes the most sense because it will be used to load all new files in parallel.\n\nI answered D as well because I thought that none of the others made sense and D is the only one that mentions creating the bucket on GCS and perhaps migrating data that is missed during the update in the architecture.\n\nSo D to create the bucket, C to update the process and move the data to the bucket, then D to move any lost data during the update.","comments":[{"upvote_count":"1","content":"Typo, I meant E in my post. C and E, not C and D.","timestamp":"1690191360.0","poster":"Mathew106","comment_id":"961417"}],"poster":"Mathew106","timestamp":"1689945600.0"}]}],"poster":"Toto2020","comment_id":"246085"},{"timestamp":"1584776880.0","comments":[{"comment_id":"219379","content":"support this with a link....","poster":"GeeBeeEl","upvote_count":"3","comments":[{"poster":"gcppde","comments":[{"upvote_count":"1","poster":"tavva_prudhvi","content":"This link does support for C, but what about A? any supported links?","comment_id":"596835","timestamp":"1651670400.0"},{"poster":"BhupiSG","upvote_count":"8","comment_id":"309312","content":"Thank you! From this doc:\nFollow these rules of thumb when deciding whether to use gsutil or Storage Transfer Service:\n\nTransfer scenario Recommendation\nTransferring from another cloud storage provider Use Storage Transfer Service.\nTransferring less than 1 TB from on-premises Use gsutil.\nTransferring more than 1 TB from on-premises Use Transfer service for on-premises data.\nTransferring less than 1 TB from another Cloud Storage region Use gsutil.\nTransferring more than 1 TB from another Cloud Storage region Use Storage Transfer Service.","timestamp":"1615597260.0"}],"timestamp":"1614288120.0","upvote_count":"1","content":"Here you go: https://cloud.google.com/storage-transfer/docs/overview#gsutil","comment_id":"299396"}],"timestamp":"1605393480.0"}],"content":"Should be AC","comment_id":"66427","poster":"[Removed]","upvote_count":"35"},{"upvote_count":"2","timestamp":"1722459540.0","comment_id":"1259099","poster":"iooj","content":"Selected Answer: CD\nC - because gsutil is recommended for transferring less than 1 TB from on-premises \nC excludes E; \nbandwidth is not a problem due to a simple math, so we exclude B;\n4 KB file is compressed enough, so we exclude A;\nD - works fine because even with -m flag we can send tars in parallel."},{"poster":"zohra-khouy.f","upvote_count":"1","content":"Selected Answer: AC\nAC is the answer","comment_id":"1166622","timestamp":"1709656920.0"},{"upvote_count":"2","poster":"spicebits","timestamp":"1699269360.0","comment_id":"1063766","content":"How can C and E be the answer? They are solving the same problem with different approaches. If you pick C then E can not be an answer. If you pick E then C can not be an answer. This question also seems a bit dated because of gcloud storage cli which is much more performant than gsutil. I would pick C&D as the combination makes the most sense given the choices."},{"upvote_count":"1","comment_id":"916672","content":"@hendrixlives arguments are correct. The approach between the resources in use and how to optimize the ingestion must be balanced.","poster":"Maurilio_Cardoso","timestamp":"1686089820.0"},{"comment_id":"896453","upvote_count":"2","timestamp":"1683962100.0","content":"Selected Answer: CD\nC is correct without an doubt\nI was in doubt between D and E \nA and B does not seems correct because it states that the bandwidth is not fully utilized .\nNow D and E \nIf the bandwidth was higher the E would be good\nD even if it seems that will not make difference because tar files does not have compression transmit one file instead of 1000 is significantly faster so I would choose \nC and D","poster":"Kiroo"},{"comment_id":"889907","poster":"Oleksandr0501","upvote_count":"1","content":"Selected Answer: CD\nCD , i guess. Liked explanation in discussion. \nchanging from BC to CD","timestamp":"1683276600.0"},{"upvote_count":"2","content":"Guys. need a help. anyone appeared for the exam very recently (Apr 2023)? preparing all the questions from here, would be enough?","comment_id":"882016","poster":"Kart87","timestamp":"1682542080.0"},{"comments":[{"comment_id":"1044688","upvote_count":"1","poster":"patiwwb","content":"Yes the 2 are excluding each other. So it's AC","timestamp":"1697434980.0"}],"upvote_count":"3","timestamp":"1682457000.0","poster":"Jarek7","comment_id":"880862","content":"Selected Answer: AC\nIt seems that Google would like AC. A is not neccessary - it doesnt make a significant change - small files do not compress well and bandwith is so big that file size is not an issue - the isue is 0,2s latency. The biggest benefit is that we can simply enable compression from gsutils parameters, it will not add any implementation complexity. \nFor me C solo is ok and D solo might be even better, but more complex. C and D cannot be mixed - they exclude each other. C is more simple and uses Google service so it seems to be desired answer. And it makes sense if they want us to select 2 actions we have to make - If we go for C we can also get some benefit from A, if we go for D there is no other answer we can select and it is much more complex in implementation than AC(which is by far good enough)."},{"content":"Answer: B&C \nA: files are 4kb, no need for compression\nB: more files to be transmitted per unit time with 100mbps or get 5g network (~200 mbps)\nC: gsutil parallel ingestion will reduce time \nD: TAR is not a good compression and slower in transfer even slower than csv. speed is 50mbps so don't go with it. \nE: Storage Transfer service needs good internet and used for large size of data and for on premises storage, this one is regular ingestion.","timestamp":"1676369700.0","upvote_count":"3","comment_id":"808267","poster":"musumusu"},{"comments":[{"timestamp":"1675844820.0","content":"Why not option D?\nOption D, which involves assembling 1000 files into a TAR file and then transmitting it, may not be an effective solution for the current situation. While TAR archives can help reduce the number of files that need to be transmitted, disassembling the TAR archive in the cloud after receiving it could increase the time required to process the data. This could make it difficult to meet the goal of making reports with data from the previous day available by 10:00 a.m. each day.\n\nFurthermore, compressing the TAR archive could increase the time required to create the archive, and may not provide a significant improvement in terms of transfer time, as the individual CSV files are already small in size. This makes it less effective compared to the other options of parallel transfers and data compression.","upvote_count":"1","comment_id":"801781","poster":"manigcp","comments":[{"poster":"Jarek7","upvote_count":"1","comments":[{"comment_id":"880854","poster":"Jarek7","content":"OK. AC seems to be rright as we can simply enable the compression by gsutil options.","upvote_count":"1","timestamp":"1682456400.0"}],"content":"II wouldnt agree, the main issue here is latency 0,2s and 20000 files per hour - it is even beyond possible transfer without paralelisation or file merging. Compression and sending 1000 files at once resolves the issue. Just as option C. But they don't make any sense together. I think they exclude D because of additional complexity - compression and then decompression is much more difficult than using gsutil. Thus we go for C. If we need one more then only A makes some sense, but I wouldn't go for it. We have enough bandwith for this size of file. We just need get rid of latency, by paralelization.","timestamp":"1682456100.0","comment_id":"880849"}]}],"upvote_count":"1","poster":"manigcp","content":"-- From ChatGPT --\n\nB. Redesign the data ingestion process to use gsutil tool to send the CSV les to a storage bucket in parallel.\nA. Introduce data compression for each file to increase the rate of file transfer.\n\nReasoning:\nB. Redesigning the data ingestion process to use gsutil tool to send the CSV files to a storage bucket in parallel will help improve the rate at which the data is transferred to the cloud. This is because gsutil allows for parallel transfers, thereby utilizing the available bandwidth more efficiently and reducing the time required to transfer the data.\n\nA. Introducing data compression for each file will also help improve the rate of file transfer. This is because compressed data takes up less space and can be transferred faster, thereby reducing the time required to transfer the data.","comment_id":"801780","timestamp":"1675844760.0"},{"content":"Selected Answer: CD\nhttps://cloud.google.com/storage/docs/parallel-composite-uploads","timestamp":"1669417320.0","comment_id":"727126","upvote_count":"2","poster":"Leeeeee"},{"comment_id":"590415","content":"A is incorrect as rate of file transfer is not an issue, system is not able to handle current load itself, compression will make it even faster","timestamp":"1650691620.0","poster":"abhineet1313","upvote_count":"1"},{"comment_id":"588923","poster":"alecuba16","upvote_count":"1","comments":[{"comment_id":"616541","timestamp":"1655270040.0","content":"E is wrong, as Bandwidth already low, so storage Transfer service will not help here","poster":"tavva_prudhvi","upvote_count":"2"}],"timestamp":"1650483120.0","content":"Selected Answer: DE\nMultiple small files transfer is a bad practice. You should always use some aggregation strategy like tar or zip multiple files. A is discarded because talks about compressing a single file. B is discarded because the bandwidth is not the problem.\n\nOption C could be , but multi threading has a limit. Then the best option is D or use some google on prem mirroring service like E."},{"content":"E is wrong Google Cloud Storage Transfer Service (online) != Transfer Appliance(on-premise)","poster":"Jojo9400","comment_id":"573138","upvote_count":"1","timestamp":"1647972600.0"},{"timestamp":"1647405120.0","comment_id":"568779","upvote_count":"1","poster":"OmJanmeda","content":"Selected Answer: CD\nCD is correct option"},{"poster":"Tanzu","upvote_count":"3","comment_id":"545738","content":"20k files * 24 hours = 480k files x2 * 4kilobyte= 3.8gb everyday and must be processed in 10 hours\n\n- either C or E (not both)= total size is less than 1tb per day, so C (gsutil) is the right tool\n- in gsutil, the process can be paralleled (so you can utilize bandwidth throughput) and using rsync it is also possible to compress (to increase the transfer rate). and you do not need to decompress at target (to decrease process time at target such as untar or decompress). So A is better than E.!\n\nthe following cmd does the job.\ngsutil -m rsync -az sourceDir gs://targetBucket","timestamp":"1644659040.0"},{"content":"Selected Answer: AC\nA - would minimize over all size during seasonal times\nC - gcp standard for skinny files","comment_id":"530705","upvote_count":"1","poster":"rbeeraka","timestamp":"1642957860.0"},{"upvote_count":"1","content":"A,C\nA - would minimize over all size during seasonal times\nC - gcp standard for skinny files","timestamp":"1642957380.0","comment_id":"530696","poster":"rbeeraka"},{"poster":"sraakesh95","comment_id":"523695","timestamp":"1642188360.0","content":"Selected Answer: CD\n@Toto2020","upvote_count":"1"},{"content":"Selected Answer: CD\nas explained by hendrixlives","poster":"medeis_jar","comment_id":"516714","upvote_count":"1","timestamp":"1641306600.0"},{"poster":"hendrixlives","comment_id":"503436","content":"Selected Answer: CD\nCD is correct.\nSee: https://jbrojbrojbro.medium.com/parallel-uploads-for-smaller-files-387ff86afc74 \n\nA: size is small enough that compressing each file will not help (indeed, it may even add overhead).\n\nB: bandwidth is not a problem, no need to increase.\n\nC: Parallel uploading the files with -m will increase speed in general.\n \nD: many individual small files are a problem, since each file adds overhead to the processing and upload to GCS, and the upload sped of GCS is proportional to the size. If we pack all the small files in a bigger single TAR, it will improve the overall performance. \n\nE: Storage Transfer Service is intended to move 100s of TB, and requires a 300Mbps connection as minimum (the doc even states that if your connection is less than 300Mbps is better to use gsutil). See: https://cloud.google.com/storage-transfer/docs/on-prem-overview#requirements","comments":[{"content":"dont see 300 limit mentioned anywhere, it says you can limit the bandwidth based on your isp https://cloud.google.com/storage-transfer/docs/managing-on-prem-jobs#control-bandwidth","poster":"abhineet1313","comment_id":"590416","upvote_count":"1","timestamp":"1650691800.0"}],"timestamp":"1639726200.0","upvote_count":"9"},{"timestamp":"1638092340.0","content":"Selected Answer: CD\ncompression individual files wonâ€™t help and increase bandwidth neither since there is no shortage there.\nC looks good. sending files in parallel problem solved\nD also looks good, since weâ€™ll save many connections","comment_id":"489024","poster":"MaxNRG","upvote_count":"4"},{"timestamp":"1636369980.0","comment_id":"474236","upvote_count":"1","poster":"Untamables","content":"A, D are correct. Sending tar.gz is efficient. B, C and E are not effective because the network latency is so high."},{"content":"The current architecture - 20,000 files per hour of approx 4kB files = 80MB of data every hour Stored in Local SFTP -->> Cloud SFTP -->> Cloud Storage.\nThe possible scenario in the next 3 months 40,000 files per hour of approx 4kB files = 160MB of data every hour Stored in Local SFTP -->> Cloud SFTP -->> Cloud Storage.\nAny step that eliminates our use of the SFTP servers and get us straight to Cloud Storage would modify our architecture for the better. \nOur network is okay.\nS3 Compatible Storage endpoints are any Object Storage Endpoints deployed locally.\nUsing the -m flag in gsutil employs multithreading to our copying operations.","upvote_count":"1","timestamp":"1634976060.0","comment_id":"466471","poster":"mbkim"},{"poster":"anji007","timestamp":"1634324640.0","comment_id":"462775","upvote_count":"8","content":"Ans: C & D.\nA: Compressing, reduces the file size hence bandwidth utilization will comedown, but as per the question Bandwidth utilization is already low. So doesnt make difference.\nB: Not required.\nE: Size is fairly lesser than 1TB, so gsutil suffice for this problem."},{"upvote_count":"2","content":"Correct answer is C & E","poster":"sandipk91","timestamp":"1628348820.0","comment_id":"421263","comments":[{"comment_id":"440006","upvote_count":"2","content":"You mentioned 'E' as an option. What is S3 compatible storage end point and how is that used?","timestamp":"1630887420.0","poster":"Ral17"}]},{"content":"\"This design is barely able to keep up with the current volume, even though the bandwidth utilization is rather low.\"\n\nIt is clear that the problem here is not the transfer rate but the number of files, so A is discarded. If we TAR the small files into one big file and use gsutil we'll solve the problem.\n\nC and D","timestamp":"1627215360.0","upvote_count":"8","comment_id":"413886","comments":[{"content":"same as you. we need to choose the answer that improves bandwidth utilization.","poster":"kubosuke","upvote_count":"2","timestamp":"1631406240.0","comment_id":"443204"}],"poster":"MrCastro"},{"timestamp":"1625231400.0","upvote_count":"7","content":"CD seems Correct.\nA - Files are already small. Compression not required\nB - Not recommended always\nC - gsutil -m serves this purpose to transfer multiple files with less bandwidth (50mbps <300mbps)\nD - Can TAR the csv files and transfer to VM and unTAR in VM\nE - Storage Transfer not recommended as bandwidth is limited to 50mbps and total files size is less than TB.","comment_id":"396900","poster":"navemula"},{"poster":"sumanshu","upvote_count":"1","comment_id":"392213","timestamp":"1624809900.0","content":"Vote for 'AC'\n\nB - wrong (we need to provide solution without changing internet speed :P)\nD - if we TAR 1000 files, its okay, but Volume is getting increased continously..How we define the number ?\nE - Bandwidth already low, so storage Transfer service will not help here.","comments":[{"upvote_count":"2","comment_id":"443944","timestamp":"1631533320.0","poster":"StefanoG","content":"The questin talks about tar fileS, in other words, a tar file every 1000 log files. And to optimize it, as wrote in this official doc: https://cloud.google.com/storage/docs/request-rate\n\"If your request rate is less than 1000 write requests per second or 5000 read requests per second, then no ramp-up is needed\"\nSo tar files avoid the ramp up"}]},{"upvote_count":"2","comment_id":"307362","content":"C & E.\nAB & D care about data transfered, and there isn't any problem with the bandwith, problem is about sevice. Then only C and E change the service that are correct answers.","timestamp":"1615399980.0","poster":"daghayeghi"},{"upvote_count":"3","timestamp":"1612721520.0","comment_id":"285696","content":"Correct CE","poster":"naga"},{"comment_id":"181551","upvote_count":"7","comments":[{"comments":[{"poster":"MaxNRG","upvote_count":"1","comment_id":"489067","timestamp":"1638096900.0","content":"\"Avoid breaking a transfer into smaller chunks if possible and instead upload the entire content in a single chunk.\""}],"poster":"GeeBeeEl","comment_id":"219402","timestamp":"1605395640.0","content":"This is best practices, there is no mention of tar....... https://cloud.google.com/storage/docs/best-practices","upvote_count":"2"}],"content":"Answer should be C &D. Use Google Storage Bucket instead of sftp solution currently deployed make use of gsutil command to upload file in parallel. Combine small files in large (TAR) file as per recommended best practices.","poster":"SteelWarrior","timestamp":"1600424220.0"},{"poster":"Tanmoyk","comment_id":"170737","content":"C D seems more valid, as per Google's recommendation small files must be merged before transfer it to GCP. After transferring the files to FTP server that can be decompress and can be sent to process","upvote_count":"4","timestamp":"1598872440.0"},{"timestamp":"1595881140.0","comments":[{"poster":"retep007","comment_id":"453560","content":"Completely wrong assumption. What do you mean it doesn't support tar? Cloud Storage is a blob storage it stores anything you give it.\n\nCD is indeed correct answer","upvote_count":"1","timestamp":"1632850740.0"}],"content":"CD is most suitable option but GCP does not support TAR files. So I have to vote for AC","comment_id":"145254","poster":"sh2020","upvote_count":"4"},{"upvote_count":"4","timestamp":"1593925800.0","poster":"Rajuuu","content":"AC is the correct answer.\nOther options are not in scope of the solution req.","comment_id":"126616"},{"comment_id":"124150","content":"CD. E is recommended to transfer over 1TB data.\nTransfer scenario Recommendation\nTransferring from another cloud storage provider Use Storage Transfer Service\nTransferring less than 1 TB from on-premises Use gsutil\nTransferring more than 1 TB from on-premises Use Transfer service for on-premises data","upvote_count":"7","poster":"norwayping","timestamp":"1593599220.0"},{"comment_id":"118252","timestamp":"1592988360.0","comments":[{"upvote_count":"2","comments":[{"upvote_count":"1","comment_id":"219400","poster":"GeeBeeEl","content":"Do you have a link to back that up?","timestamp":"1605395520.0"}],"poster":"gvaf4","comment_id":"118253","timestamp":"1592988480.0","content":"GCS does not support TAR file, so D is excluded"}],"poster":"gvaf4","upvote_count":"2","content":"should be C & E. \nA,B is wrong due to low bandwidth utilization"},{"upvote_count":"15","timestamp":"1592704200.0","comments":[],"comment_id":"115156","poster":"ch3n6","content":"C, D\nC: use gsutil to upload in parallel, improve upload speed. \nD: tar many small files into one: more efficient for network transfer. 4kB is too small for TCP connection. see: tcp slow start\nWhy not A: \" bandwidth utilization is rather low.\". \nWhy not B: same as A. Why not E: transfer service it not for on-premises."},{"comment_id":"100393","timestamp":"1591054680.0","upvote_count":"12","content":"CD - and compress files and use gsutil -m instead of sftp","poster":"serg3d"},{"poster":"digvijay","timestamp":"1585102260.0","upvote_count":"1","comment_id":"68019","content":"selected C & E"},{"upvote_count":"3","timestamp":"1585101720.0","content":"Answer should be BC...File size is less than 4KB so the compression will not help in this case .so option A will be discarded .","comment_id":"68018","poster":"digvijay","comments":[{"poster":"GeeBeeEl","upvote_count":"1","comment_id":"219398","timestamp":"1605395400.0","content":"So why go with option B??"}]},{"comment_id":"67068","upvote_count":"2","comments":[{"content":"S3-compatible storage endpoint in your network doesn't make any sense here wrt the question.\nE - doesn't work","comment_id":"67430","poster":"[Removed]","timestamp":"1585020660.0","upvote_count":"10"}],"poster":"jvg637","timestamp":"1584904980.0","content":"Should be C & E. \nAB & D care about data transfered, and there isn't any problem with the bandwith"}],"timestamp":"2020-03-21 08:48:00","answers_community":["CD (79%)","AC (17%)","3%"],"question_text":"Your company produces 20,000 files every hour. Each data file is formatted as a comma separated values (CSV) file that is less than 4 KB. All files must be ingested on Google Cloud Platform before they can be processed. Your company site has a 200 ms latency to Google Cloud, and your Internet connection bandwidth is limited as 50 Mbps. You currently deploy a secure FTP (SFTP) server on a virtual machine in Google Compute Engine as the data ingestion point. A local SFTP client runs on a dedicated machine to transmit the CSV files as is. The goal is to make reports with data from the previous day available to the executives by 10:00 a.m. each day. This design is barely able to keep up with the current volume, even though the bandwidth utilization is rather low.\nYou are told that due to seasonality, your company expects the number of files to double for the next three months. Which two actions should you take? (Choose two.)","question_images":[],"choices":{"D":"Assemble 1,000 files into a tape archive (TAR) file. Transmit the TAR files instead, and disassemble the CSV files in the cloud upon receiving them.","A":"Introduce data compression for each file to increase the rate file of file transfer.","B":"Contact your internet service provider (ISP) to increase your maximum bandwidth to at least 100 Mbps.","C":"Redesign the data ingestion process to use gsutil tool to send the CSV files to a storage bucket in parallel.","E":"Create an S3-compatible storage endpoint in your network, and use Google Cloud Storage Transfer Service to transfer on-premises data to the designated storage bucket."},"exam_id":10,"answer_description":"","answer_images":[],"question_id":264,"isMC":true},{"id":"f6yybjrc8Sht1pz2yxWz","question_id":265,"url":"https://www.examtopics.com/discussions/google/view/16637-exam-professional-data-engineer-topic-1-question-5/","isMC":true,"exam_id":10,"topic":"1","answers_community":["D (100%)"],"choices":{"A":"Use federated data sources, and check data in the SQL query.","B":"Enable BigQuery monitoring in Google Stackdriver and create an alert.","D":"Run a Google Cloud Dataflow batch pipeline to import the data into BigQuery, and push errors to another dead-letter table for analysis.","C":"Import the data into BigQuery using the gcloud CLI and set max_bad_records to 0."},"question_text":"An external customer provides you with a daily dump of data from their database. The data flows into Google Cloud Storage GCS as comma-separated values\n(CSV) files. You want to analyze this data in Google BigQuery, but the data could have rows that are formatted incorrectly or corrupted. How should you build this pipeline?","answer":"D","discussion":[{"upvote_count":"14","poster":"Radhika7983","comment_id":"213167","timestamp":"1604545980.0","content":"The answer is D. An ETL pipeline will be implemented for this scenario. Check out handling invalid inputs in cloud data flow\n\nhttps://cloud.google.com/blog/products/gcp/handling-invalid-inputs-in-dataflow\n\nParDos . . . and donâ€™ts: handling invalid inputs in Dataflow using Side Outputs as a â€œDead Letterâ€ file","comments":[{"upvote_count":"5","comment_id":"734899","content":"The sources you've provided cannot be accessed. Here is an updated best practice. https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#use_dead_letter_queues","poster":"jkhong","timestamp":"1670137860.0","comments":[{"poster":"nadavw","content":"https://cloud.google.com/dataflow/docs/guides/write-to-bigquery#:~:text=It%27s%20a%20good%20practice%20to%20send%20the%20errors%20to%20a%20dead%2Dletter%20queue%20or%20table%2C%20for%20later%20processing.%20For%20more%20information%20about%20this%20pattern%2C%20see%20BigQueryIO%20dead%20letter%20pattern.\n\nIt's a good practice to send the errors to a dead-letter queue or table, for later processing. For more information about this pattern, see BigQueryIO dead letter pattern.","upvote_count":"1","timestamp":"1723360380.0","comment_id":"1263850"}]}]},{"upvote_count":"5","content":"Disagree a bit here. Could well be A. In one Coursera video course (https://www.coursera.org/learn/batch-data-pipelines-gcp/lecture/SkDus/how-to-carry-out-operations-in-bigquery), they do have a video about when to just use an SQL query to find wrong data without creating a Dataflow pipeline. The question says \"SQL\" as a language, not Cloud SQL as a service. Federated Sources is great because you can federate a CSV file in GCS with BigQuery. From the video: \"In this section, we'll take a look at exactly how BigQuery can help with some of those data quality issues we just described. Let's start with validity, what do we mean by invalid? It can mean things like corrupted data maybe data that is missing a timestamp\"","poster":"fire558787","timestamp":"1629123900.0","comments":[{"poster":"kelvinksau","content":"https://cloud.google.com/bigquery/external-data-sources\nUse cases for external data sources include:\n\nFor ETL workloads, loading and cleaning your data in one pass and writing the cleaned result into BigQuery storage.\nJoining BigQuery tables with frequently changing data from an external data source. By querying the external data source directly, you don't need to reload the data into BigQuery storage every time it changes.","timestamp":"1630149540.0","comment_id":"433798","upvote_count":"2"}],"comment_id":"425842"},{"comment_id":"1060874","timestamp":"1727154300.0","poster":"rocky48","upvote_count":"3","content":"Selected Answer: D\nOption A is incorrect because federated data sources do not provide any data validation or cleaning capabilities and you'll have to do it on the SQL query, which could slow down the performance.\n\nOption B is incorrect because Stackdriver monitoring can only monitor the performance of the pipeline, but it can't handle corrupted or incorrectly formatted data.\n\nOption C is incorrect because using gcloud CLI and setting max_bad_records to 0 will ignore the corrupted or incorrectly formatted data and continue the load process, this will lead to incorrect analysis.\n\nAnswer D. Run a Google Cloud Dataflow batch pipeline to import the data into BigQuery, and push errors to another dead-letter table for analysis."},{"comment_id":"1050467","timestamp":"1727154300.0","poster":"rtcpost","upvote_count":"2","content":"Selected Answer: D\nGoogle Cloud Dataflow allows you to create a data pipeline that can preprocess and transform data before loading it into BigQuery. This approach will enable you to handle problematic rows, push them to a dead-letter table for later analysis, and load the valid data into BigQuery.\n\nOption A (using federated data sources and checking data in the SQL query) can be used but doesn't directly address the issue of handling corrupted or incorrectly formatted rows.\n\nOptions B and C are not the best choices for handling data quality and error issues. Enabling monitoring and setting max_bad_records to 0 in BigQuery may help identify errors but won't store the problematic rows for further analysis, and it might prevent loading any data with issues, which may not be ideal."},{"content":"D. Run a Google Cloud Dataflow batch pipeline to import the data into BigQuery, and push errors to another dead-letter table for analysis.\n\nBy running a Cloud Dataflow pipeline to import the data, you can perform data validation, cleaning and transformation before it gets loaded into BigQuery. Dataflow allows you to handle corrupted or incorrectly formatted rows by pushing them to another dead-letter table for analysis. This way, you can ensure that only clean and correctly formatted data is loaded into BigQuery for analysis.","timestamp":"1727154300.0","comment_id":"784800","upvote_count":"2","comments":[{"timestamp":"1674431640.0","comments":[{"poster":"hamza101","content":"for Option C i think when setting max_bad_records to 0 this will prevent the loading to be achieved since the condition will cut off the loading if we have at least 1 corrupted row","upvote_count":"2","timestamp":"1690296540.0","comment_id":"962828"}],"poster":"samdhimal","content":"Option A is incorrect because federated data sources do not provide any data validation or cleaning capabilities and you'll have to do it on the SQL query, which could slow down the performance.\n\nOption B is incorrect because Stackdriver monitoring can only monitor the performance of the pipeline, but it can't handle corrupted or incorrectly formatted data.\n\nOption C is incorrect because using gcloud CLI and setting max_bad_records to 0 will ignore the corrupted or incorrectly formatted data and continue the load process, this will lead to incorrect analysis.","comment_id":"784801","upvote_count":"5"}],"poster":"samdhimal"},{"poster":"RT_G","upvote_count":"1","content":"Selected Answer: D\nAll other options only alert or error out bad data. As the question requires, option D sends bad data to the dead letter table for further analysis while valid data is loaded to the table","timestamp":"1727154240.0","comment_id":"1065062"},{"upvote_count":"1","content":"Selected Answer: D\nAgreed: D","timestamp":"1684502340.0","poster":"vaga1","comment_id":"901961"},{"poster":"odiez3","upvote_count":"1","content":"D because you need Transform the data","comment_id":"849552","timestamp":"1679683200.0"},{"content":"Selected Answer: D\nD. The question is asking pipeline, then letâ€™s build a pipeline.","timestamp":"1676511180.0","upvote_count":"3","comment_id":"810164","poster":"Morock"},{"timestamp":"1665937980.0","upvote_count":"1","comment_id":"696382","poster":"Besss","content":"Selected Answer: D\nAgreed: D"},{"poster":"Dip1994","timestamp":"1659497700.0","comment_id":"641546","content":"The correct answer is D","upvote_count":"1"},{"upvote_count":"1","timestamp":"1646209980.0","comment_id":"559271","content":"Selected Answer: D\nCorrect - D (as we need to create Pipeline) which possible via 'D'","poster":"Arkon88"},{"timestamp":"1636307460.0","content":"Looks like D, with C you will not import anything, stackdriver alerts will not help you with this and with federated resources you wonâ€™t know what happened with those bad records. D is the most complete one.\nhttps://cloud.google.com/blog/products/gcp/handling-invalid-inputs-in-dataflow","poster":"MaxNRG","upvote_count":"3","comment_id":"473992"},{"poster":"anji007","content":"Ans: D","upvote_count":"1","timestamp":"1634215200.0","comment_id":"462016"},{"timestamp":"1632641460.0","comment_id":"451707","poster":"nickozz","upvote_count":"1","content":"D seems to be correct. explained here how combined wth Pub/Sub, this can be achieved. https://cloud.google.com/pubsub/docs/handling-failures"},{"timestamp":"1629697680.0","poster":"kubosuke","content":"D\nthis structure is known as \"dead letter pattern\".\n\n\nhttps://medium.com/@rako/error-rate-monitoring-in-dead-letter-pattern-using-apache-beam-193fd03f8970","comment_id":"429689","upvote_count":"2"},{"upvote_count":"1","comment_id":"371497","timestamp":"1622513820.0","poster":"amityleo","content":"D seems the correct answer: https://cloud.google.com/bigquery/docs/loading-data#dataflow"},{"comments":[{"content":"A. Use federated data sources, and check data in the SQL query. - WRONG (Because we are changing source itself, i.e. SQL, MySQL, PstgresSQL) instead of correcting the problem\nB. Enable BigQuery monitoring in Google Stackdriver and create an alert. (WRONG - Because setting and creating an alert will not solve the corrupted data problem)\nC. Import the data into BigQuery using the gcloud CLI and set max_bad_records to 0. (wrong - here we are saying set max_bad_records = 0 (i.e let's load all bad records into bi-query)\nD. Run a Google Cloud Dataflow batch pipeline to import the data into BigQuery, and push errors to another dead-letter table for analysis. (CORRECT - Dataflow is used for this pupose only i.e transform the data and dead letter queue pupose is to write any invalid records - so that it can be analyzed later (rather than ignoring))","upvote_count":"5","poster":"sumanshu","timestamp":"1624392120.0","comments":[{"comment_id":"400763","poster":"sumanshu","timestamp":"1625653800.0","content":"Correct - D (as we need to create Pipeline) which possible via 'D'","upvote_count":"1"},{"content":"SQL u can write on Bigquery also. Not necessary MySQL etc","upvote_count":"1","timestamp":"1635447300.0","comment_id":"469433","poster":"SonuKhan1"}],"comment_id":"388257"}],"upvote_count":"1","content":"Correct : C","poster":"lbhhoya82","timestamp":"1616646120.0","comment_id":"319771"},{"content":"D is correct","timestamp":"1614520200.0","upvote_count":"2","poster":"sid091","comment_id":"300809"},{"comment_id":"284918","poster":"naga","upvote_count":"3","content":"Correct D","timestamp":"1612625940.0"},{"comment_id":"161849","timestamp":"1597879020.0","poster":"Andrabi","content":"Agree with D","upvote_count":"2"},{"comment_id":"161676","upvote_count":"2","content":"D\nParDos . . . and donâ€™ts: handling invalid inputs in Dataflow using Side Outputs as a â€œDead Letterâ€ file","poster":"atnafu2020","timestamp":"1597855560.0"},{"timestamp":"1597685460.0","upvote_count":"3","content":"it should be D:https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv","poster":"Ravivarma4786","comment_id":"160234"},{"comment_id":"159779","poster":"PRABHUKKARTHI","content":"Answer is D","timestamp":"1597654200.0","upvote_count":"3"},{"upvote_count":"4","content":"Answer D. This usual method followed in general ETL process.","poster":"AshokC","timestamp":"1594689420.0","comment_id":"134484"},{"poster":"jagadamba","content":"agree with D","comment_id":"121924","upvote_count":"3","timestamp":"1593354840.0"}],"timestamp":"2020-03-15 08:14:00","question_images":[],"unix_timestamp":1584256440,"answer_ET":"D","answer_description":"","answer_images":[]}],"exam":{"isImplemented":true,"numberOfQuestions":319,"name":"Professional Data Engineer","isBeta":false,"provider":"Google","isMCOnly":true,"lastUpdated":"15 Feb 2025","id":10},"currentPage":53},"__N_SSP":true}