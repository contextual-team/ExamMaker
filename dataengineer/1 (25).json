{"pageProps":{"questions":[{"id":"i4qWumswCeeHv2cGgS6I","unix_timestamp":1703925240,"isMC":true,"answer_images":[],"timestamp":"2023-12-30 09:34:00","url":"https://www.examtopics.com/discussions/google/view/129858-exam-professional-data-engineer-topic-1-question-211/","question_images":[],"answers_community":["A (67%)","C (29%)","5%"],"exam_id":10,"question_text":"You are using BigQuery with a multi-region dataset that includes a table with the daily sales volumes. This table is updated multiple times per day. You need to protect your sales table in case of regional failures with a recovery point objective (RPO) of less than 24 hours, while keeping costs to a minimum. What should you do?","answer_description":"","question_id":126,"discussion":[{"upvote_count":"15","timestamp":"1704651960.0","comments":[{"content":"Based on the information provided and the need to avoid data loss in the case of a hard regional failure in BigQuery, which could result in the destruction of all data in that region, the focus should be on creating backups in a geographically distinct region. Considering this scenario, the most suitable option would be Option A\n\nHere's why this option is the most appropriate:","poster":"MaxNRG","timestamp":"1704652020.0","upvote_count":"5","comments":[{"poster":"MaxNRG","upvote_count":"3","content":"• Cross-Region Backup: Exporting the data to a Google Cloud Storage bucket that is either dual or multi-regional ensures that your backups are stored in a different geographic location. This is critical for protecting against hard regional failures.\n• Data Durability: Cloud Storage provides high durability for stored data, making it a reliable option for backups in the case of regional disasters.\n• Cost-Effectiveness: While there are costs associated with storage and data transfer, this method can be more cost-effective compared to maintaining active replicas of the data in multiple regions, especially if the data is large.","comments":[{"timestamp":"1704652080.0","content":"• Flexibility and Automation: The export process can be automated and scheduled to occur daily, aligning with your RPO of less than 24 hours. This ensures that the most recent data is always backed up.\n• Recovery Process: In the event of a hard regional failure, the data can be restored from the Cloud Storage backup to another operational BigQuery region, ensuring continuity of operations.","comments":[{"timestamp":"1704652140.0","content":"The other options, while viable in certain scenarios, do not provide the same level of protection against a hard regional failure:\n• Option B (Copy to Backup Region) and Option D (Modify ETL to Load into Backup Region) do not address the possibility of a hard regional failure adequately, as they do not necessarily imply storing data in a geographically distinct region.\n• Option C (BigQuery Snapshot) is useful for point-in-time recovery but does not inherently protect against hard regional failures since the snapshots are within the same BigQuery service.\nFocusing on a robust disaster recovery strategy is crucial. Option A provides a balance between ensuring data availability in the event of a regional disaster and managing costs, aligning with best practices for data management in the cloud.","upvote_count":"4","comment_id":"1116083","poster":"MaxNRG"}],"upvote_count":"3","comment_id":"1116082","poster":"MaxNRG"}],"comment_id":"1116081","timestamp":"1704652080.0"}],"comment_id":"1116080"}],"content":"Selected Answer: A\nWhy not C:\n\nA table snapshot must be in the same region, and under the same organization, as its base table.\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#limitations","poster":"MaxNRG","comment_id":"1116079"},{"poster":"raaad","comment_id":"1112217","timestamp":"1704226380.0","content":"Selected Answer: C\nOption C provides cost-effective way. \n- BigQuery table snapshots are a feature that allows you to capture the state of a table at a particular point in time. \n- Snapshots are incremental, so they only store the data that has changed, making them more cost-effective than full table copies. \n- In the event of a regional failure, you can quickly restore the table from a snapshot.","upvote_count":"7"},{"comment_id":"1305753","poster":"ToiToi","upvote_count":"2","content":"Selected Answer: A\nWhy other options are not as suitable:\n\nB (Copy of the dataset): Copying the entire dataset daily is more expensive and less efficient than exporting just the table data.\nC (BigQuery snapshot): snapshots are within the same region and won't protect against a regional outage.\nD (Modify ETL job): This adds complexity to your ETL process and might not be the most efficient or cost-effective way to achieve your RPO.","timestamp":"1730459700.0"},{"timestamp":"1724946000.0","content":"A : BQ snapshot should be in same region, and if so the region fails so does the snapshot.","poster":"Priyal19","upvote_count":"1","comment_id":"1274591"},{"comment_id":"1268502","upvote_count":"1","content":"Selected Answer: C\nWhy Option A is not suitable: Restoring data from Option A would require reloading it back into BigQuery, which is time-consuming. This process cannot guarantee a recovery point objective (RPO) of less than 24 hours.","timestamp":"1724054580.0","poster":"viciousjpjp"},{"content":"Believe me all questions were from Exam topic all were there yesterday in exam. But yes dont go with starting questions mainly focus questions after 200 and latest questions are at last page.","poster":"meh_33","upvote_count":"2","comment_id":"1264468","timestamp":"1723437660.0"},{"poster":"meh_33","content":"Selected Answer: C\nC seems correct and Raaad also saying same.","upvote_count":"1","timestamp":"1723302240.0","comment_id":"1263558"},{"timestamp":"1713864960.0","comment_id":"1200627","upvote_count":"1","poster":"ostora","content":"Selected Answer: C\nit is c"},{"comments":[{"content":"At the beginning I also thought that \"C\" is a correct answer, but futher I found documentation https://cloud.google.com/bigquery/docs/locations. According this documentation \n\"Selecting a multi-region location does not provide cross-region replication or regional redundancy, so there is no increase in dataset availability in the event of a regional outage. Data is stored in a single region within the geographic location.\n\nData located in the EU multi-region is only stored in the europe-west1 (Belgium) or europe-west4 (Netherlands) data centers\"\nSo, multi-region dataset just means locating data inside one of US regions, hence snapshot also will be stored in the same region what means that answer C is not correct","comment_id":"1172509","timestamp":"1710330780.0","poster":"Sergei_B","upvote_count":"2"}],"comment_id":"1157366","poster":"pandeyspecial","upvote_count":"2","content":"Selected Answer: C\nC. Schedule a daily BigQuery snapshot of the table.\n\nHere's why:\n\nCost-effective: BigQuery snapshots are significantly cheaper than daily exports to Cloud Storage or copying the entire dataset to a backup region. They offer point-in-time backups with minimal storage costs.\nFast recovery: Snapshots can be restored quickly, meeting your RPO requirement of less than 24 hours.\nMulti-regional: By default, BigQuery snapshots are automatically stored in a different region from the source data, ensuring redundancy and disaster recovery.","timestamp":"1708711680.0"},{"upvote_count":"1","content":"Selected Answer: A\nA. Schedule a daily export of the table to a Cloud Storage dual or multi-region bucket.","poster":"JyoGCP","comment_id":"1151154","timestamp":"1708017900.0"},{"poster":"GCP001","comment_id":"1126041","timestamp":"1705594920.0","upvote_count":"2","content":"Selected Answer: A\nOption A. Check the ref for regional loss -\nhttps://cloud.google.com/bigquery/docs/reliability-intro#scenario_loss_of_region"},{"comment_id":"1123218","timestamp":"1705310640.0","content":"Selected Answer: A\nA. Schedule a daily export of the table to a Cloud Storage dual or multi-region bucket.","upvote_count":"1","poster":"datapassionate"},{"comment_id":"1121422","content":"Selected Answer: A\nA: MaxNRG and Helinia cleared the reasons very well","upvote_count":"2","poster":"Matt_108","timestamp":"1705135140.0"},{"comments":[{"poster":"MaxNRG","comment_id":"1116060","comments":[{"timestamp":"1704651720.0","upvote_count":"1","content":"I agree, It's A:\nA table snapshot must be in the same region, and under the same organization, as its base table.\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#limitations","comment_id":"1116076","poster":"MaxNRG"}],"upvote_count":"1","timestamp":"1704651000.0","content":"Option A (Export to Cloud Storage): While exporting to Cloud Storage is a viable backup strategy, it can be more expensive and less efficient than using snapshots, especially if the table is large and updated frequently."},{"timestamp":"1704635460.0","content":"Why not C: \n\"BigQuery also supports the ability to snapshot tables. With this feature you can explicitly backup data within the same region for longer than the 7 day time travel window. A snapshot is purely a metadata operation and results in no additional storage bytes. While this can add protection against accidental deletion, it does not increase the durability of the data.\"\n\nhttps://cloud.google.com/bigquery/docs/reliability-intro#scenario_accidental_deletion_or_data_corruption","poster":"Helinia","comment_id":"1115561","upvote_count":"3"}],"timestamp":"1704594180.0","poster":"Helinia","content":"Selected Answer: A\n\"BigQuery does not offer durability or availability in the extraordinarily unlikely and unprecedented event of physical region loss. This is true for both \"regions and multi-region\" configurations. Hence maintaining durability and availability under such a scenario requires customer planning.\"\n\n\"To avoid data loss in the face of destructive regional loss, you need to back up data to another geographic location. For example, you could periodically export a snapshot of your data to Google Cloud Storage in another geographically distinct region.\"\n\nRef: https://cloud.google.com/bigquery/docs/reliability-intro#scenario_loss_of_region","comment_id":"1115559","upvote_count":"5"},{"comments":[{"timestamp":"1704424800.0","comment_id":"1114231","upvote_count":"3","poster":"qq589539483084gfrgrgfr","content":"https://cloud.google.com/bigquery/docs/reliability-intro"}],"comment_id":"1114230","timestamp":"1704424740.0","content":"Option A","poster":"qq589539483084gfrgrgfr","upvote_count":"3"},{"comment_id":"1109534","poster":"e70ea9e","upvote_count":"2","content":"Selected Answer: D\nAutomatically replicates data to a backup region upon each update, ensuring an RPO of less than 24 hours, even with multiple daily updates.","timestamp":"1703925240.0","comments":[{"comment_id":"1112218","content":"Option D:\nDoubles the write load and storage costs since you are maintaining two live datasets.","upvote_count":"4","poster":"raaad","timestamp":"1704226500.0"}]}],"topic":"1","answer":"A","answer_ET":"A","choices":{"D":"Modify ETL job to load the data into both the current and another backup region.","C":"Schedule a daily BigQuery snapshot of the table.","B":"Schedule a daily copy of the dataset to a backup region.","A":"Schedule a daily export of the table to a Cloud Storage dual or multi-region bucket."}},{"id":"1ECq2jTPXReeF3rcap5e","answer_description":"","discussion":[{"poster":"MaxNRG","comment_id":"1116098","content":"Selected Answer: B\nThe best approach would be to check if there is a firewall rule allowing traffic on TCP ports 12345 and 12346 for the Dataflow network tag.\nDataflow uses TCP ports 12345 and 12346 for communication between worker nodes. Using network tags and associated firewall rules is a Google-recommended security practice for controlling access between Compute Engine instances like Dataflow workers.\n\nSo the key things to check would be:\n\n1. Ensure your Dataflow pipeline is using the Dataflow network tag on the worker nodes. This tag is applied by default unless overridden.\n2. Check if there is a firewall rule allowing TCP 12345 and 12346 ingress and egress traffic for instances with the Dataflow network tag. If not, add the rule.\n\nOptions A, C and D relate to other networking aspects but do not directly address the Google recommended practice of using network tags and firewall rules.","upvote_count":"9","timestamp":"1720370760.0"},{"timestamp":"1723735620.0","comment_id":"1151156","content":"Selected Answer: B\nB. Determine whether there is a firewall rule set to allow traffic on TCP ports 12345 and 12346 for the Dataflow network tag.","poster":"JyoGCP","upvote_count":"1"},{"poster":"Matt_108","upvote_count":"1","comment_id":"1121447","content":"Selected Answer: B\nB, check if there is a firewall rule allowing traffic on TCP ports 12345 and 12346 for the Dataflow network tag.","timestamp":"1720855080.0"},{"content":"Selected Answer: B\nBecause network tags are used and Dataflow uses TCP ports 12345 and 12346 as stated on\nhttps://cloud.google.com/dataflow/docs/guides/routes-firewall","comment_id":"1115676","poster":"Smakyel79","timestamp":"1720337280.0","upvote_count":"3"},{"comment_id":"1112248","timestamp":"1719946800.0","content":"Selected Answer: B\nThis option focuses directly on ensuring that the firewall rules are set up correctly for the network tags used by Dataflow worker nodes. It specifically addresses the potential issue of worker nodes not being able to communicate due to restrictive firewall rules blocking the necessary ports.","poster":"raaad","upvote_count":"3"},{"upvote_count":"2","comment_id":"1109536","timestamp":"1719729360.0","poster":"e70ea9e","content":"Selected Answer: B\nFocus on Network Tags:\n\nAdheres to the recommended practice of using network tags for firewall configuration, enhancing security and flexibility.\nAvoids targeting specific subnets, which can be less secure and harder to manage."}],"question_images":[],"question_id":127,"exam_id":10,"url":"https://www.examtopics.com/discussions/google/view/129859-exam-professional-data-engineer-topic-1-question-212/","isMC":true,"topic":"1","unix_timestamp":1703925360,"answers_community":["B (100%)"],"choices":{"A":"Determine whether your Dataflow pipeline has a custom network tag set.","D":"Determine whether your Dataflow pipeline is deployed with the external IP address option enabled.","B":"Determine whether there is a firewall rule set to allow traffic on TCP ports 12345 and 12346 for the Dataflow network tag.","C":"Determine whether there is a firewall rule set to allow traffic on TCP ports 12345 and 12346 on the subnet used by Dataflow workers."},"timestamp":"2023-12-30 09:36:00","question_text":"You are troubleshooting your Dataflow pipeline that processes data from Cloud Storage to BigQuery. You have discovered that the Dataflow worker nodes cannot communicate with one another. Your networking team relies on Google Cloud network tags to define firewall rules. You need to identify the issue while following Google-recommended networking security practices. What should you do?","answer_ET":"B","answer":"B","answer_images":[]},{"id":"VIgOsRrJOpvciTZhHG9q","question_id":128,"answer":"A","answer_images":[],"choices":{"A":"Cluster the table by country and username fields.","B":"Cluster the table by country field, and partition by username field.","C":"Partition the table by country and username fields.","D":"Partition the table by _PARTITIONTIME."},"question_images":["https://img.examtopics.com/professional-data-engineer/image3.png"],"answer_ET":"A","answers_community":["A (95%)","5%"],"topic":"1","answer_description":"","timestamp":"2023-12-30 09:38:00","discussion":[{"poster":"Ryannn23","timestamp":"1738162800.0","comment_id":"1348611","content":"Selected Answer: A\nPartition on String is not available in BQ - excludes B and C\nPartition by ingest time not useful as the query is filtering on 2 other columns - excludes D\nCorrect answer A: cluster on both fields.","upvote_count":"1"},{"upvote_count":"1","timestamp":"1725722820.0","poster":"niujo","comments":[{"upvote_count":"1","comment_id":"1336056","poster":"b3e59c2","content":"Because our query filtering is relating to country and user, and nothing to do with time. A partition by time will provide no performance increase in this case.","timestamp":"1735915980.0"}],"comment_id":"1168221","content":"Why not D? if u partition by date and int going to be the best option??"},{"comment_id":"1151159","content":"Selected Answer: A\nIf country is represented by an integer code, then partition by country and cluster by username would be a better solution. As country code is a string, available best solution is \"A. Cluster the table by country and username fields.\"","upvote_count":"3","poster":"JyoGCP","timestamp":"1723735860.0"},{"upvote_count":"4","timestamp":"1721030940.0","content":"Selected Answer: A\nCorrect answer: A. Cluster the table by country and username fields.\n\nWhy not B and C - > Intiger is required for partitioning \nhttps://cloud.google.com/bigquery/docs/partitioned-tables#integer_range","poster":"datapassionate","comment_id":"1123235"},{"comments":[{"poster":"SanjeevRoy91","timestamp":"1726791840.0","comment_id":"1177881","content":"Is not mandatory to have partitioning for clustering?","upvote_count":"1"}],"poster":"Matt_108","timestamp":"1720856520.0","comment_id":"1121460","content":"Selected Answer: A\nA: the fields are both strings, which are not supported for partitioning. Moreover, the fields are regularly used in filters, which is where clustering really improves performance","upvote_count":"3"},{"poster":"Takshashila","comment_id":"1116912","content":"Selected Answer: B\nClustering can also be done after partiton?","comments":[{"upvote_count":"2","timestamp":"1725777180.0","poster":"chambg","comment_id":"1168661","content":"Yes but the partition is done on username field which has 10 million values. Since a BQ table can only have 4000 it is not suitable"}],"upvote_count":"1","timestamp":"1720458000.0"},{"poster":"raaad","content":"Selected Answer: A\n- Clustering organizes the data based on the specified columns (in this case, country_name and username). \n- When a query filters on these columns, BigQuery can efficiently scan only the relevant parts of the table","timestamp":"1719947340.0","comment_id":"1112251","upvote_count":"4"},{"poster":"e70ea9e","timestamp":"1719729480.0","comment_id":"1109537","upvote_count":"3","content":"Selected Answer: A\ncountry and username --> cluster"}],"unix_timestamp":1703925480,"url":"https://www.examtopics.com/discussions/google/view/129860-exam-professional-data-engineer-topic-1-question-213/","exam_id":10,"question_text":"Your company's customer_order table in BigQuery stores the order history for 10 million customers, with a table size of 10 PB. You need to create a dashboard for the support team to view the order history. The dashboard has two filters, country_name and username. Both are string data types in the BigQuery table. When a filter is applied, the dashboard fetches the order history from the table and displays the query results. However, the dashboard is slow to show the results when applying the filters to the following query:\n\n//IMG//\n\n\nHow should you redesign the BigQuery table to support faster access?","isMC":true},{"id":"ugE4EFpOIcpX37gVxhVE","answer_ET":"B","answer":"B","answer_images":[],"exam_id":10,"question_text":"You have a Standard Tier Memorystore for Redis instance deployed in a production environment. You need to simulate a Redis instance failover in the most accurate disaster recovery situation, and ensure that the failover has no impact on production data. What should you do?","question_images":[],"answers_community":["B (86%)","10%"],"answer_description":"","unix_timestamp":1703925540,"timestamp":"2023-12-30 09:39:00","topic":"1","isMC":true,"discussion":[{"content":"Selected Answer: B\nThe best option is B - Create a Standard Tier Memorystore for Redis instance in a development environment. Initiate a manual failover by using the force-data-loss data protection mode.\nThe key points are:\n• The failover should be tested in a separate development environment, not production, to avoid impacting real data.\n• The force-data-loss mode will simulate a full failover and restart, which is the most accurate test of disaster recovery.\n• Limited-data-loss mode only fails over reads which does not fully test write capabilities.\n• Increasing replicas in production and failing over (C) risks losing real production data.\n• Failing over production (D) also risks impacting real data and traffic.\nSo option B isolates the test from production and uses the most rigorous failover mode to fully validate disaster recovery capabilities.","poster":"MaxNRG","upvote_count":"13","timestamp":"1704655080.0","comment_id":"1116120"},{"timestamp":"1730214660.0","content":"Selected Answer: B\nThe question says \"no impact on production data\" Thus, the best practice is about simulating in a different environment. force-data-loss mode covers the most accurate disaster recovery situation. (https://cloud.google.com/memorystore/docs/redis/about-manual-failover)","upvote_count":"1","poster":"SamuelTsch","comment_id":"1304507"},{"timestamp":"1729489320.0","content":"D:\n\"A standard tier Memorystore for Redis instance uses a replica node to back up the primary node. A normal failover occurs when the primary node becomes unhealthy, causing the replica to be designated as the new primary. A manual failover differs from a normal failover because you initiate it yourself.\"\nThe limited-data-loss mode minimizes data loss by verifying that the difference in data between the primary and replica is below 30 MB before initiating the failover. The offset on the primary is incremented for each byte of data that must be synchronized to its replicas.","poster":"mi_yulai","upvote_count":"1","comment_id":"1300766"},{"comment_id":"1275371","content":"Selected Answer: A\nWe are trying to simulate the disaster recovery on a redis Instance and we want minimum data loss.\nTherefore, Option A - create a test Standard Tier Memorystore for Redis instance in Dev Environment and use the limited data loss data protection mode, seems to be the correct option here.","upvote_count":"1","poster":"mayankazyour","timestamp":"1725079260.0"},{"upvote_count":"1","timestamp":"1720545180.0","poster":"anyone_99","content":"D seems correct. We are required to simulate and not test in a different environment.\n\"How data protection modes work\nThe limited-data-loss mode minimizes data loss by verifying that the difference in data between the primary and replica is below 30 MB before initiating the failover. The offset on the primary is incremented for each byte of data that must be synchronized to its replicas. In the limited-data-loss mode, the failover will abort if the greatest offset delta between the primary and each replica is 30MB or greater. If you can tolerate more data loss and want to aggressively execute the failover, try setting the data protection mode to force-data-loss.\n\nThe force-data-loss mode employs a chain of failover strategies to aggressively execute the failover. It does not check the offset delta between the primary and replicas before initiating the failover; you can potentially lose more than 30MB of data changes.\"","comment_id":"1245005"},{"upvote_count":"2","poster":"tibuenoc","timestamp":"1705485360.0","content":"Selected Answer: B\nhttps://cloud.google.com/memorystore/docs/redis/about-manual-failover","comment_id":"1124865"},{"content":"Selected Answer: B\nB. Create a Standard Tier Memorystore for Redis instance in a development environment. Initiate a manual failover by using the force-data-loss data protection mode","timestamp":"1705313760.0","poster":"datapassionate","upvote_count":"1","comment_id":"1123238"},{"poster":"Matt_108","upvote_count":"1","content":"Selected Answer: B\nBest option is B - no impact on production env and forces a full failover","timestamp":"1705140000.0","comment_id":"1121480"},{"poster":"raaad","content":"Selected Answer: C\nIncreasing the number of replicas in a Redis instance in a production environment means that we will have additional copies of the same data and thats why failover will not impact the production data","upvote_count":"1","comment_id":"1112355","timestamp":"1704238680.0","comments":[{"timestamp":"1704655140.0","upvote_count":"1","poster":"MaxNRG","comment_id":"1116122","content":"\"no impact on production data\" - not C nor D"}]},{"comment_id":"1109539","content":"Selected Answer: C\nSeparate Development Environment:\n\nIsolates testing from production, preventing any impact on live data or services.\nProvides a safe and controlled environment for simulating failover scenarios.","upvote_count":"1","timestamp":"1703925540.0","poster":"e70ea9e"}],"url":"https://www.examtopics.com/discussions/google/view/129861-exam-professional-data-engineer-topic-1-question-214/","question_id":129,"choices":{"A":"Create a Standard Tier Memorystore for Redis instance in the development environment. Initiate a manual failover by using the limited-data-loss data protection mode.","D":"Initiate a manual failover by using the limited-data-loss data protection mode to the Memorystore for Redis instance in the production environment.","C":"Increase one replica to Redis instance in production environment. Initiate a manual failover by using the force-data-loss data protection mode.","B":"Create a Standard Tier Memorystore for Redis instance in a development environment. Initiate a manual failover by using the force-data-loss data protection mode."}},{"id":"CqirfJMzfSKJuS74Dbk2","answer_ET":"C","question_id":130,"discussion":[{"content":"Selected Answer: C\nAnalytics Hub","upvote_count":"2","comment_id":"1152525","poster":"JyoGCP","timestamp":"1723889160.0"},{"timestamp":"1720041840.0","poster":"raaad","content":"Selected Answer: C\n- Create a copy of the necessary tables into a new dataset that doesn't use CMEK, ensuring the data is accessible without requiring the partner to have access to the encryption key.\n- Analytics Hub can then be used to share this data securely and efficiently with the partner organization, maintaining control and governance over the shared data.","comment_id":"1113213","upvote_count":"3"},{"content":"Selected Answer: C\nPreserves Key Confidentiality:\n\nAvoids sharing your CMEK with the partner, upholding key security and control.","poster":"e70ea9e","timestamp":"1719729600.0","upvote_count":"2","comment_id":"1109540"}],"exam_id":10,"answers_community":["C (100%)"],"choices":{"C":"Copy the tables you need to share to a dataset without CMEKs. Create an Analytics Hub listing for this dataset.","D":"Create an authorized view that contains the CMEK to decrypt the data when accessed.","A":"Provide the partner organization a copy of your CMEKs to decrypt the data.","B":"Export the tables to parquet files to a Cloud Storage bucket and grant the storageinsights.viewer role on the bucket to the partner organization."},"answer_images":[],"answer":"C","url":"https://www.examtopics.com/discussions/google/view/129862-exam-professional-data-engineer-topic-1-question-215/","question_images":[],"question_text":"You are administering a BigQuery dataset that uses a customer-managed encryption key (CMEK). You need to share the dataset with a partner organization that does not have access to your CMEK. What should you do?","unix_timestamp":1703925600,"topic":"1","timestamp":"2023-12-30 09:40:00","isMC":true,"answer_description":""}],"exam":{"isImplemented":true,"isMCOnly":true,"isBeta":false,"name":"Professional Data Engineer","id":10,"numberOfQuestions":319,"provider":"Google","lastUpdated":"15 Feb 2025"},"currentPage":26},"__N_SSP":true}