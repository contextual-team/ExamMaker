{"pageProps":{"questions":[{"id":"1Xi3TRcVgRb6HyaQXyTj","question_images":[],"choices":{"D":"You expect future mutations to have similar features to the mutated samples in the database.","E":"You already have labels for which samples are mutated and which are normal in the database.","C":"You expect future mutations to have different features from the mutated samples in the database.","B":"There are roughly equal occurrences of both normal and mutated samples in the database.","A":"There are very few occurrences of mutations relative to normal samples."},"answer_ET":"AD","exam_id":10,"answer_images":[],"question_text":"You want to use a database of information about tissue samples to classify future tissue samples as either normal or mutated. You are evaluating an unsupervised anomaly detection method for classifying the tissue samples. Which two characteristic support this method? (Choose two.)","url":"https://www.examtopics.com/discussions/google/view/16281-exam-professional-data-engineer-topic-1-question-14/","answers_community":["AD (47%)","AC (46%)","7%"],"answer":"AD","topic":"1","timestamp":"2020-03-11 18:24:00","question_id":46,"isMC":true,"answer_description":"","discussion":[{"poster":"jvg637","comments":[{"upvote_count":"1","content":"as per chatGPT, it can be different (C) - thats how unsupervised anomaly detection works - as long as they are different than \"normal' tissues , they would be detected","comment_id":"1326075","timestamp":"1734084180.0","poster":"AmitK121981"},{"comment_id":"494902","content":"Guys its A & C.\nAnomaly detection has two basic assumptions: \n->Anomalies only occur very rarely in the data. (a)\n->Their features differ from the normal instances significantly. (c)\n\nlink -> https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1#:~:text=Unsupervised%20Anomaly%20Detection%20for%20Univariate%20%26%20Multivariate%20Data.&text=Anomaly%20detection%20has%20two%20basic,from%20the%20normal%20instances%20significantly.","poster":"BigQuery","comments":[{"comment_id":"502480","upvote_count":"27","content":"I don't agree on C. Anomaly detection assumes \"Their features differ from the NORMAL INSTANCES significantly\" and in the C option you have: \n\"You expect future mutations to have different features from the MUTATED SAMPLES IN THE DATABASE\". \n\nIMHO Answer D fits better: \"D. You expect future mutations to have similar features to the mutated samples in the database.\" - in other words: Expect future anomalies to be similar to the anomalies that we already have in database","poster":"szefco","timestamp":"1639601940.0"}],"timestamp":"1638766500.0","upvote_count":"22"}],"upvote_count":"73","comment_id":"64229","timestamp":"1584271680.0","content":"I think that AD makes more sense. D is the explanation you gave. In the rest, A makes more sense, in any anomaly detection algorithm it is assumed a priori that you have much more \"normal\" samples than mutated ones, so that you can model normal patterns and detect patterns that are \"off\" that normal pattern. For that you will always need the no. of normal samples to be much bigger than the no. of mutated samples."},{"content":"A instead of B:\n\"anomaly detection (also outlier detection[1]) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data","timestamp":"1583947440.0","poster":"jvg637","comment_id":"62579","upvote_count":"21"},{"comment_id":"1346613","upvote_count":"2","timestamp":"1737832320.0","poster":"LP_PDE","content":"Selected Answer: AC\nAC. Answer C. Unsupervised anomaly detection methods are particularly useful when you don't have labeled examples of the anomalies you're trying to detect. Why not D, if future mutations are similar to existing ones, a supervised model trained on labeled examples of known mutations would likely be more accurate in classifying new samples."},{"timestamp":"1737180780.0","comment_id":"1342431","upvote_count":"1","poster":"cqrm3n","content":"Selected Answer: AC\nThe answer should be A and C.\n\nUnsupervised anomaly detection is useful when labels are unavailable, or when anomalies are rare and distinct.\n\nHence A is definitely correct because anomaly detection excels when anomalies are rare compared to normal data.\n\nI think C is correct because by adding new mutation data that is similar to the existing mutation data, the model will learn in a broader sense on what constitutes to 'mutation', and it leads to a better generalization. If the new data is too similar to the existing mutation data (answer D), the model might overfit to those specific examples. However, the new data should still share some fundamental characteristic to the existing data so that the model can recognize them as belonging to the same anomaly category."},{"poster":"kumar34","content":"Selected Answer: AC\nI think it's A&C. For an anomaly detection model, the ratio of normal vs abnormal is expected to be high. 'C' because the model is expected to be adaptive meaning the model detects the abnormal features that can be different from the abnormal features currently being trained on.","timestamp":"1735081620.0","comment_id":"1331265","upvote_count":"1"},{"poster":"SamuelTsch","comments":[{"poster":"SamuelTsch","content":"I correct my answer. AD should be better. Unsupervised method is usually used for grouping the data. So, if the future mutations have similar features to the mutated samples, our trained model should group it into anomalies even though no label exists.","upvote_count":"1","timestamp":"1729531500.0","comment_id":"1301201"}],"timestamp":"1729530240.0","upvote_count":"1","content":"Selected Answer: AC\nThe keyword is unsupervised anomaly detection. So A is correct. We think and should ensure the majority of data represents 'normal'. Unsupervised methods are good for detecting unknown patterns. Thus C could be correct.","comment_id":"1301180"},{"timestamp":"1727155320.0","comment_id":"503327","content":"Selected Answer: AD\nAD: to use unsupervised anomaly detection the anomalies a) must be rare b) they must differ from the NORMAL. So...\nA: mutated samples must be scarce compared to normal tissue.\nD: yes, we expect the future mutated samples to have similar features to the mutated samples currently in the database.\nWhy not C? If I train my model with mutated samples with specific characteristics, I do not expect it to find different mutations. In the future, when new mutations appear, I would retrain my model including those new samples.","poster":"hendrixlives","upvote_count":"4"},{"upvote_count":"4","poster":"MaxNRG","content":"Selected Answer: AD\nAnomaly detection has two basic assumptions:\n*Anomalies only occur very rarely in the data.\n*Their features differ from the normal instances significantly.\nAnomaly detection involves identifying rare data instances (anomalies) that come from a different class or distribution than the majority (which are simply called “normal” instances). Given a training set of only normal data, the semi-supervised anomaly detection task is to identify anomalies in the future. Good solutions to this task have applications in fraud and intrusion detection.\nThe unsupervised anomaly detection task is different: Given unlabeled, mostly-normal data, identify the anomalies among them.\nhttps://www.science.gov/topicpages/u/unsupervised+anomaly+detection\nA because “Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal”, B is for Supervised anomaly detection https://en.wikipedia.org/wiki/Anomaly_detection","comment_id":"528611","timestamp":"1727155320.0"},{"content":"Selected Answer: AD\nA - anomaly detection is used for detecting rare events, meaning it is expected that there are much less of those than of normal ones.\nD - you expect the future mutations to be similar to the mutations you already have, so that you can detect them (pattern recognition)","upvote_count":"2","poster":"gudiking","comment_id":"719602","timestamp":"1727155320.0"},{"upvote_count":"4","content":"Selected Answer: AD\nA makes sense\n\nC and D compares future mutations to mutated samples in database\n\nThe question is pretty badly worded… If we were to run a full unsupervised anomaly detection over the entire dataset, C and D will be true, since some future mutations may be similar to current mutations and some will be significantly different to current mutations.\n\nThe question is suggesting \"labelling\" tissue samples using unsupervised anomaly detection, and subsequently using the labels with a supervised algorithm to classify future samples. If this interpretation of the question is correct, then D makes sense","comment_id":"741533","timestamp":"1727155320.0","poster":"jkhong"},{"timestamp":"1727155320.0","comment_id":"768244","upvote_count":"2","content":"Selected Answer: AD\nThe answer should be AD.\n\nA, anomaly should have a little amount, if there are many samples then we should do classification instead, because unsupervised will give a lot of false positive.\n\nD, the future anomaly should be of the same distribution as present anomaly! or else our anomaly detection will not be generalize to the future feature.","poster":"korntewin"},{"comment_id":"799159","content":"A. There are very few occurrences of mutations relative to normal samples. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying rare events or anomalies in large amounts of data. By training the algorithm on the normal tissue samples in the database, it can then identify new tissue samples that have different features from the normal samples and classify them as mutated.\n\nD. You expect future mutations to have similar features to the mutated samples in the database. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying patterns or anomalies in the data. By training the algorithm on the mutated tissue samples in the database, it can then identify new tissue samples that have similar features and classify them as mutated.","timestamp":"1727155320.0","upvote_count":"2","poster":"samdhimal"},{"content":"Selected Answer: AD\nD should be correct. You expect future samples will correlate with the training samples. That's the whole point of learning procedure. If you do not expect that they have similar features, then why would you use features in the training samples in the first place? A is also correct, since anomaly labels would be seen rarely.","poster":"azmiozgen","comment_id":"949537","timestamp":"1727155260.0","upvote_count":"5"},{"poster":"rocky48","comment_id":"1062185","timestamp":"1727155260.0","upvote_count":"2","content":"Selected Answer: AD\nA. There are very few occurrences of mutations relative to normal samples. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying rare events or anomalies in large amounts of data. By training the algorithm on the normal tissue samples in the database, it can then identify new tissue samples that have different features from the normal samples and classify them as mutated.\n\nD. You expect future mutations to have similar features to the mutated samples in the database. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying patterns or anomalies in the data. By training the algorithm on the mutated tissue samples in the database, it can then identify new tissue samples that have similar features and classify them as mutated."},{"content":"Selected Answer: AC\nA. There are very few occurrences of mutations relative to normal samples.\n\nAnomaly detection is well-suited for situations where anomalies (in this case, mutations) are rare compared to the normal cases. When the dataset is highly imbalanced, with far fewer mutated samples than normal samples, anomaly detection can be used to identify these rare cases as outliers or anomalies.\nC. You expect future mutations to have different features from the mutated samples in the database.\n\nUnsupervised anomaly detection works under the assumption that anomalies (mutations) will differ significantly from the majority of the data (normal samples). If future mutations are expected to exhibit different features, this method can help detect those anomalies as deviations from the normal samples.","comment_id":"1262377","upvote_count":"2","timestamp":"1723101240.0","poster":"Nittin"},{"comment_id":"1258323","content":"Selected Answer: AC\nA. There are very few occurrences of mutations relative to normal samples.\nAnomaly detection is particularly useful in scenarios where anomalies (mutations, in this case) are rare compared to normal instances. This aligns with the nature of anomaly detection, which focuses on identifying rare events that deviate significantly from the majority (normal) data.\n\nC. You expect future mutations to have different features from the mutated samples in the database.\n\nUnsupervised anomaly detection methods do not rely on prior knowledge of anomalies. They work on the assumption that anomalies will be different from normal instances in a significant way. If future mutations have different features from known mutations, it supports using an unsupervised method as it can detect novel anomalies not seen during training","upvote_count":"3","timestamp":"1722361020.0","poster":"iooj"},{"poster":"Roulle","upvote_count":"1","content":"That's A and D. The aim of unsupervised classification of anomalies is to identify sub-groups with characteristics in common that may resemble anomalies. So, when a new mutation appears, we can determine whether it shares characteristics with previously discovered anomaly subgroups. If this mutation is an anomaly and has very different characteristics from our detected anomaly subgroup, it is likely to be associated with an incorrect group.","timestamp":"1720529460.0","comment_id":"1244923"},{"comment_id":"1225194","content":"Selected Answer: AC\nA. There are very few occurrences of mutations relative to normal samples.\n\nUnsupervised anomaly detection is particularly useful in situations where anomalies (mutations) are rare compared to the normal instances. This characteristic aligns well with unsupervised methods that can detect outliers or rare events in a dataset dominated by normal samples.\nC. You expect future mutations to have different features from the mutated samples in the database.\n\nAnomaly detection methods are effective when future anomalies do not follow the same patterns as the known anomalies. These methods aim to identify instances that significantly deviate from the norm, which suits the scenario where future mutations might exhibit different characteristics from those currently known.","timestamp":"1717656060.0","poster":"pandey_0307","upvote_count":"1"},{"poster":"tdum76000","content":"Selected Answer: AC\nAs A is a good answer, i'd like to give my point of view on the second right answer. I initially thought D was the correct one, as you would normally train your model to detect mutations seen in the training dataset. But the goal of unsupervised learning is to detect unidentified patterns. If you were sure the mutations would always look the same, you'd rather use supervised learning and labels the \"normal\" and \"mutated\" tissues, which would result in better performances in my point of view.","comment_id":"1100658","timestamp":"1702992720.0","upvote_count":"2"},{"content":"Selected Answer: AC\nUnsupervised anomaly detection is best for scenarios without labels or when the anomalies are unknown or ever-changing","upvote_count":"2","poster":"spicebits","timestamp":"1699561560.0","comment_id":"1066709"},{"upvote_count":"2","comment_id":"1057010","timestamp":"1698599580.0","poster":"axantroff","content":"Selected Answer: AC\nAC; might also be interesting - https://towardsdatascience.com/unsupervised-learning-for-anomaly-detection-44c55a96b8c1 as comments below"},{"comment_id":"1027029","poster":"imran79","content":"For unsupervised anomaly detection to be effective, it works best when anomalies (or mutations in this case) are rare compared to normal instances. Moreover, if future mutations are expected to have different features from those currently in the database, an unsupervised method would be beneficial since it doesn't rely on previously seen patterns of mutations.\n\nThe two characteristics that support the use of an unsupervised anomaly detection method in this scenario are:\n\nA. There are very few occurrences of mutations relative to normal samples.\nC. You expect future mutations to have different features from the mutated samples in the database.","upvote_count":"2","timestamp":"1696646700.0"},{"upvote_count":"1","content":"A is definitely true. Next comes the tricky difference between C & D. We can in fact even use supervised learning for case D where future mutations are similar to mutations in the training sample given that we have enough samples in the anomalous class then over-sample the anomalous class and under-sample the other class. Therefore I am inclined to choose C instead of D.","poster":"gaurav0480","comment_id":"985859","timestamp":"1692541680.0"},{"content":"Selected Answer: AC\nA & C\n100% sure, as you would only use unsupervised learning if you cannot supervise your algorithm. The other answers imply that you have enough, expectedly similar mutations to supervise on.","timestamp":"1690354440.0","poster":"Mark_86","upvote_count":"1","comment_id":"963466"},{"timestamp":"1690181100.0","poster":"Mathew106","comment_id":"961236","upvote_count":"1","content":"Answers B and C are both dumb, sorry to say. There are different approaches to anomaly detection. Some expect different features from the training dataset anomalies and some don't. If you cluster the training data and assign an anomaly label to any data point in an anomaly cluster then you expect them to have similar features. If you disregard the anomaly clusters and you simply set a rule saying \"a data point is an anomaly if it it's further away from X than the clusters 1,2,3 with healthy tissues, then you don't care about having similar features, as long as they are not similar to the healthy tissues."},{"timestamp":"1684347420.0","content":"Selected Answer: AC\nI would choose A and C.\n\nNot B because mutations should be rare.\nNot D because mutations can be unpredictable and if D were true it would point to supervised learning.\nNot E since it would point to supervised learning.","upvote_count":"2","poster":"cchen8181","comment_id":"900434"},{"poster":"despee","timestamp":"1682984940.0","content":"Selected Answer: AC\nGuys if there are equals in the DB, it becomes a classification problem not an anomaly detection.","comment_id":"886826","upvote_count":"3"},{"upvote_count":"3","timestamp":"1679073660.0","comment_id":"842162","content":"Selected Answer: AC\nThe question is more about why *unsupervised* *anomaly detection*.\nA explains the *anomaly detection*\nC explains why *unsupervised*\n\nIf the mutations were like the database you could simply do supervised learning.","poster":"juliobs","comments":[{"content":"I think it should be D instead of C, because for a good clustering you want the intra cluster distance to be low --> that would imply you want similar mutations.","poster":"shabfat","timestamp":"1679141820.0","upvote_count":"2","comment_id":"842751"}]},{"upvote_count":"4","content":"Selected Answer: AD\nD. You expect future mutations to have similar features to the MUTATED samples in the database.","poster":"charline","comment_id":"839653","timestamp":"1678867500.0"},{"timestamp":"1678509720.0","upvote_count":"4","poster":"bha11111","comment_id":"835660","content":"Selected Answer: AD\nA and D makes more sense"},{"comment_id":"828131","timestamp":"1677861900.0","upvote_count":"3","content":"AC. C rather than D because if the relation between mutation and features does not change, then there is no reason to use unsupervised learning. E.g., consider logistic regression vs clustering algorithm based on L2 norm. Logistic regression will pick up only those features, which are associated with a mutation in your particular sample. If a feature, not associated with a mutation is a training sample, is 5 sigmas outside its mean, logistic regression will not detect anything. On the other hand, clustering will pick up any observation where any feature is 5 sigmas outside its mean.","poster":"Aaronn14"},{"content":"Right Answer is C & D:\nUnsupervised learning can only justify the patten of data and make custer of them. \nFor example, It can make \n1 cluster of Apple, \n1 cluster of mango, \nNow it can predict the Apple or mango (similar feature)\nor Graphes in new cluster (different feature)\n\nOption A: It doesn't matter there are few occurance or more to make the cluster. If there is only one Apple and Mango it will still make two cluster.","upvote_count":"2","poster":"musumusu","comment_id":"819091","timestamp":"1677150120.0"},{"comment_id":"807317","timestamp":"1676287020.0","poster":"charline","content":"Selected Answer: AD\nthe future mutation should be similar to the tested mutations","comments":[{"timestamp":"1677154800.0","comment_id":"819169","poster":"Zhaas","content":"No, for ML 2 classes should be different","upvote_count":"1"}],"upvote_count":"3"},{"poster":"certmonkey","timestamp":"1675963440.0","comment_id":"803524","upvote_count":"6","content":"Selected Answer: AC\nAnomaly detection unsupervised learning\nThe objective of Unsupervised Anomaly Detection is to detect previously unseen rare objects or events without any prior knowledge about these. The only information available is that the percentage of anomalies in the dataset is small, usually less than 1%.\n\nFrom: https://paperswithcode.com/task/unsupervised-anomaly-detection#:~:text=The%20objective%20of%20Unsupervised%20Anomaly,%2C%20usually%20less%20than%201%25.\n\nSo it is A & C"},{"timestamp":"1674315240.0","comment_id":"783490","poster":"dconesoko","upvote_count":"2","content":"Selected Answer: AC\nas i explained earlier"},{"upvote_count":"1","timestamp":"1674315180.0","comment_id":"783488","content":"My two cents, we can not be very sure about how future mutations looks like, because thats a biological blind process, thus all we can do is to able to identity what is normal and no matter how the tissue mutates , so far as its different from the normal sample then its tagged as an anomaly. thus i vote for AC","poster":"dconesoko"},{"timestamp":"1672049340.0","upvote_count":"2","poster":"Kyr0","comment_id":"757339","content":"Selected Answer: AD\nAD for me"},{"timestamp":"1671574200.0","upvote_count":"2","comment_id":"751584","poster":"dragolemguty","content":"Selected Answer: AD\nAD more sense"},{"upvote_count":"2","poster":"Nirca","content":"Selected Answer: AC\nSupervised and unsupervised learning have one key difference. Supervised learning uses labeled datasets, whereas unsupervised learning uses unlabeled datasets. By “labeled” we mean that the data is already tagged with the right answer.","timestamp":"1670775960.0","comment_id":"741886"},{"poster":"hauhau","content":"Selected Answer: AD\nC : if the similarity differ in future from your sample, how do you predict?","upvote_count":"4","timestamp":"1669715040.0","comment_id":"730175"},{"timestamp":"1669395480.0","upvote_count":"2","comment_id":"726964","content":"Selected Answer: AC\nLet's understand Unsupervised anomaly \n\nUnsupervised anomaly Detection involves an unlabeled dataset. It assumes that the majority data points in the unlabeled dataset are “normal” and it looks for data points that differs from the “normal” data points.\n\nthe option A and C only supports Unsupervised anomaly Detection method","poster":"dish11dish"},{"content":"Selected Answer: AD\nAD for sure","timestamp":"1668414960.0","poster":"thisisapen25","upvote_count":"2","comment_id":"717810"},{"timestamp":"1665760500.0","content":"Selected Answer: AD\nAD for sure","upvote_count":"2","poster":"smileypun","comment_id":"694871"},{"poster":"devaid","content":"Selected Answer: AD\nAD absolutely.","comment_id":"688672","upvote_count":"2","timestamp":"1665151320.0"},{"content":"Selected Answer: AD\nAD for sure","poster":"Parth_P","upvote_count":"2","comment_id":"686991","timestamp":"1664982660.0"},{"timestamp":"1664873040.0","upvote_count":"2","poster":"max_c","content":"Selected Answer: AD\nD as future mutations having same features as present ones just means patterns recognition","comment_id":"686041"},{"poster":"nkunwar","comment_id":"681402","upvote_count":"1","timestamp":"1664341860.0","content":"Its A and C: Just envision a scenario in which we have unclassified data points (tissues - normal or mutated with specific features). Suppose we could run a data mining or clustering technique we may find scenario in which certain data element/s is/are well off from rest of the cluster. In that case, it may be due to certain new features may have popped in a few mutated tissues or specific feature values that may have gone haywire, both cases can be classified as OUTLIERS and this is where I feel A &C are correct."},{"upvote_count":"6","content":"Selected Answer: AC\nI would go for AC here.\n\nAbout C: We have to keep in mind that we are doing __unsupervised__ anomaly detection to classify the tissue samples. This means that we do not consider the labels that are present in the database, but we are trying to identify what tissue samples are normal and which other tissue samples that deviate from the normal ones, e.g. the mutated tissue samples. The normal tissue samples should share the same set of features, whereas the mutated tissue samples (i.e. the anomalies) do not share them. However, one mutated tissue sample may not be exactly comparable to another mutated tissue sample as they may have different features. The only thing the mutated tissues have in common with each other is that they deviate from the normal ones. Therefore, the D option appears to me less correct in this sense as it assumes that mutated tissue samples may share the same set of features, which is not really the case in the context of unsupervised anomaly detection.","comment_id":"672551","timestamp":"1663517760.0","poster":"SMASL"},{"comment_id":"650604","content":"Selected Answer: AC\nFrom wikipedia and other sources\nI think it is AC\nAnomalies are instances or collections of data that occur very rarely in the data set and whose features differ significantly from most of the data.\nAnomalies are instances or collections of data that occur very rarely in the data set and whose features differ significantly from most of the data.\n\n\nhttps://en.wikipedia.org/wiki/Anomaly_detection","poster":"ducc","timestamp":"1661233560.0","upvote_count":"2"},{"poster":"Dhamsl","content":"BC makes sense. Unsupervised model means we do not have mutated data to be referred for future expectation. Also, to classify data not in the time series, it is better to have equal amount of mutated data for accurate and clear results","upvote_count":"1","timestamp":"1661203260.0","comment_id":"650433"},{"upvote_count":"1","timestamp":"1660617600.0","content":"I went into this thinking classification, and came out of it thinking anomaly detection so thank you all!\nAnomaly detection has two basic assumptions: you have much more normal data than abnormal data, and their features vary from the normal data significantly.","poster":"rowan_","comment_id":"647421"},{"comment_id":"639088","poster":"Pime13","timestamp":"1659082980.0","upvote_count":"3","content":"Selected Answer: AC\nhttps://paperswithcode.com/task/unsupervised-anomaly-detection#:~:text=The%20objective%20of%20Unsupervised%20Anomaly,%2C%20usually%20less%20than%201%25.\n\nThe objective of Unsupervised Anomaly Detection is to detect previously unseen rare objects or events without any prior knowledge about these. The only information available is that the percentage of anomalies in the dataset is small, usually less than 1%."},{"comment_id":"619006","content":"Selected Answer: AD\nI would go with A and D","timestamp":"1655704440.0","poster":"akash78","upvote_count":"2"},{"poster":"NickNtaken","content":"Selected Answer: AC\nD is not correct because if the future differences are the same as current, supervised learning algorithm is a better choice, instead of clustering.","comment_id":"609352","timestamp":"1653929280.0","upvote_count":"2"},{"comment_id":"598905","upvote_count":"3","content":"Selected Answer: BD\nTo me this seems like a classic binary classification problem. You want to classify a tissue sample either as normal or as a anomaly. For that, the training sample must be of equal size for each class (so that there isn't created a bias to either class - B). For a sample to be recognized, it has to be somewhat similar to the normal or anomaly samples already in the dataset - D.","timestamp":"1652078340.0","poster":"Rmf29"},{"comment_id":"585646","content":"I will go with AD","poster":"ip_7","timestamp":"1649928720.0","upvote_count":"1"},{"comment_id":"580992","content":"Unsupervised Anomaly Detection: This method does require any training data and instead assumes two things about the data ie Only a small percentage of data is anomalous and Any anomaly is statistically different from the normal samples. Based on the above assumptions, the data is then clustered using a similarity measure and the data points which are far off from the cluster are considered to be anomalies.\n\nGoing by the above statement the mutated samples should be few in nature to observe the anamolies. Option A seems correct","poster":"jagan79","comments":[{"comment_id":"588260","poster":"msaqib934","content":"As you said, measure based on similarity, but it does not mean this should be fewer, can be greater or equal. So i think B is correct rather A.","timestamp":"1650381900.0","upvote_count":"1"}],"upvote_count":"1","timestamp":"1649124000.0"},{"comment_id":"535556","timestamp":"1643470980.0","comments":[{"comment_id":"581305","poster":"tavva_prudhvi","timestamp":"1649168400.0","content":"But, C is completely different, so you say the future mutations are gonna be different with the current mutations in the database?","upvote_count":"1"}],"upvote_count":"3","content":"Selected Answer: AC\ni agree with Anomaly detection has two basic assumptions:\n->Anomalies only occur very rarely in the data. (a)\n->Their features differ from the normal instances significantly. (c)","poster":"fraloca"},{"comment_id":"531758","content":"Selected Answer: AD\nDefinitely AD","poster":"animesh54","timestamp":"1643073420.0","upvote_count":"2"},{"upvote_count":"6","comment_id":"523021","poster":"sraakesh95","timestamp":"1642100820.0","content":"Selected Answer: BD\n1. B: \nAs the algorithm to be considered is unsupervised and the problem is classification, clustering would be a natural first pick.\nNow, with that in mind, any clustering algorithm requires a huge disparity within it's feature metrics to get seggregated (in this case a binary classification of mutated or normal)\n\nTo ensure that the separation is clear in the vector space (irrespective of the number of features considered), we need to ensure that there is ample multi-dimensional disparity in the \"normal\" and \"mutated\" tissue data samples.\n\nTo ensure this disparity, the it is better if there are roughly equal occurrences of both mutated and normal samples\n\nD: \nIf the new mutations have similar features to the already existing mutations, it will ensure that will fall in the same category as the existing mutations, thereby making clear distinction between the normal and the mutated tissues.\n\nsupporting link: https://developers.google.com/machine-learning/clustering/clustering-algorithms"},{"comment_id":"523020","poster":"sraakesh95","upvote_count":"2","timestamp":"1642100700.0","content":"1. B: \nAs the algorithm to be considered is unsupervised and the problem is classification, clustering would be a natural first pick.\nNow, with that in mind, any clustering algorithm requires a huge disparity within it's feature metrics to get seggregated (in this case a binary classification of mutated or normal)\n\nTo ensure that the separation is clear in the vector space (irrespective of the number of features considered), we need to ensure that there is ample multi-dimensional disparity in the \"normal\" and \"mutated\" tissue data samples.\n\nTo ensure this disparity, the it is better if there are roughly equal occurrences of both mutated and normal samples\n\nD: \nIf the new mutations have similar features to the already existing mutations, it will ensure that will fall in the same category as the existing mutations, thereby making clear distinction between the normal and the mutated tissues.\n\nsupporting link: https://developers.google.com/machine-learning/clustering/clustering-algorithms"},{"comment_id":"516460","timestamp":"1641291840.0","poster":"medeis_jar","content":"Selected Answer: AD\nAD: to use unsupervised anomaly detection the anomalies a) must be rare b) they must differ from the NORMAL","upvote_count":"2"},{"content":"Selected Answer: AD\nI think the answer is AD","upvote_count":"3","comment_id":"495841","poster":"popmoi","timestamp":"1638870060.0"},{"poster":"rfmartinezv","comment_id":"493729","timestamp":"1638624780.0","upvote_count":"1","content":"Selected Answer: AC\nA & C loooks to be correct."},{"comment_id":"493544","timestamp":"1638600420.0","upvote_count":"2","poster":"BigQuery","content":"Selected Answer: AC\nAnomaly detection has two basic assumptions:\nAnomalies only occur very rarely in the data.\nTheir features differ from the normal instances significantly."},{"content":"Selected Answer: AC\nA and C","upvote_count":"1","comment_id":"491072","timestamp":"1638317040.0","poster":"dbsuppt"},{"upvote_count":"1","poster":"StefanoG","timestamp":"1637918580.0","content":"Selected Answer: AC\nI agree with what @maxNRG wrote","comment_id":"487216"},{"timestamp":"1636394220.0","content":"Anomaly detection has two basic assumptions:\n*Anomalies only occur very rarely in the data.\n*Their features differ from the normal instances significantly.\nAnomaly detection involves identifying rare data instances (anomalies) that come from a different class or distribution than the majority (which are simply called “normal” instances). Given a training set of only normal data, the semi-supervised anomaly detection task is to identify anomalies in the future. Good solutions to this task have applications in fraud and intrusion detection.\nThe unsupervised anomaly detection task is different: Given unlabeled, mostly-normal data, identify the anomalies among them.\nhttps://www.science.gov/topicpages/u/unsupervised+anomaly+detection\nA because “Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal”, B is for Supervised anomaly detection https://en.wikipedia.org/wiki/Anomaly_detection","comment_id":"474422","poster":"MaxNRG","comments":[{"timestamp":"1642698900.0","content":"A and D","upvote_count":"1","comment_id":"528610","poster":"MaxNRG"}],"upvote_count":"1"},{"comment_id":"457772","poster":"anji007","comments":[{"upvote_count":"1","comment_id":"569873","timestamp":"1647531180.0","poster":"godot","content":"C says: \"you expect *future* mutations to have different features from the MUTATED SAMPLES IN THE DATABASE\" that would make any AI model wrong."}],"timestamp":"1633446600.0","content":"Ans: A and C\n\nAnomaly detection has two basic assumptions:\nAnomalies only occur very rarely in the data.\nTheir features differ from the normal instances significantly.","upvote_count":"4"},{"timestamp":"1632422880.0","poster":"MrXBasit","upvote_count":"4","content":"A and C is correct","comment_id":"450475"},{"content":"The whole point of doing anomaly detection instead of classification is that you do not know what kind of anomalies are possible beforehand. In anomaly detection, we try to find the distribution of the normal samples and whatever do not fit in that distribution are detected as anamolies.","timestamp":"1629742620.0","comment_id":"430244","poster":"asksathvik","upvote_count":"3"},{"timestamp":"1629207180.0","upvote_count":"4","content":"correct answer is A & D","poster":"sandipk91","comment_id":"426355"},{"content":"AD makes more sense as anomalies are fairly rare events and any detection model that we create is based on the assumption that the incoming dataset will have similar features to that used in training.","timestamp":"1625321640.0","comment_id":"397619","poster":"Tiku_91","upvote_count":"2"},{"comment_id":"390032","upvote_count":"8","content":"Vote for A & C","poster":"sumanshu","timestamp":"1624581960.0"},{"poster":"userd83","comment_id":"327999","timestamp":"1617543000.0","content":"ad - future mutations may not have exact same features as mutated samples in the database but we can expect it to have similar traits that that can be leveraged for detection","upvote_count":"2"},{"timestamp":"1617285300.0","comment_id":"325909","content":"AC. It can't be D. If you are using an unsupervised method it means you don't have labels.","comments":[{"content":"D is about features not labels, I would go for AD","comments":[{"content":"To piggy back on this, I believe the point of D, \"You expect future mutations to have similar features to the mutated samples in the database\" is that you are expecting that the anomalies detected in the testing batch are still going to be valid in future batches. If you don't expect the anomalies to have similar features later then your method is kind of pointless.","comment_id":"365774","timestamp":"1621877940.0","poster":"thesysops","upvote_count":"2"}],"poster":"larnaud","timestamp":"1619447280.0","comment_id":"343334","upvote_count":"3"}],"poster":"evertonllins","upvote_count":"5"},{"content":"I would go for A and D.\nB is incorrect as it's unsupervised learning and not a classification algorithm so a balanced sample is not a priority in this case\nC is incorrect. It totally makes no sense as the predicted mutations have different characteristics from the current mutations in the dataset\nE is incorrect as it's unsupervised learning we're talking to, not supervised learning","poster":"trannammai","comment_id":"325631","comments":[{"upvote_count":"1","comment_id":"438536","poster":"StefanoG","timestamp":"1630674840.0","content":"ML work with distribution of the data, so data out of the \"right\" range are anomalies and is not necessare that the new anomalous data go in the same region where are the anomalous data of the test.\nSo i think that the right answer is A, C"}],"upvote_count":"2","timestamp":"1617264660.0"},{"content":"A and C\n\n\"today anomalies are known to have two important characteristics:\n1. Anomalies are different from the norm with respect to their features\n2. They are rare in a dataset compared to normal instances.\"","comment_id":"310238","timestamp":"1615695060.0","upvote_count":"2","comments":[{"poster":"yoshik","content":"you create the model to identify witch features values map to a mutated/not mutated tissue. you use that map to classify future unseen tissues. if you expect that those unseen tissues have different features then you can't use your actual map, hence no sense making a model. Then D is right. what about it?","comment_id":"451722","timestamp":"1632643920.0","upvote_count":"1"}],"poster":"mario_ordinola"},{"content":"A:\nUnsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set.\nhttps://en.wikipedia.org/wiki/Anomaly_detection\nC:\nanomalies are data points that are few and different.","poster":"daghayeghi","comment_id":"305950","timestamp":"1615235640.0","upvote_count":"5"},{"content":"I feel AD is correct and BC is wrong","upvote_count":"4","timestamp":"1612889400.0","poster":"naga","comment_id":"286984"},{"comment_id":"284954","upvote_count":"2","timestamp":"1612629120.0","poster":"naga","content":"Correct BC"},{"poster":"apnu","content":"A,D is the correct answer.","comment_id":"261559","upvote_count":"5","timestamp":"1609997700.0"},{"poster":"awadheshk","content":"AC","timestamp":"1609904820.0","comment_id":"260739","upvote_count":"5"},{"content":"AD\nA - https://en.wikipedia.org/wiki/Anomaly_detection (url provided by arnabbis4u )\nD - When ever ML related work is doing our features remains same, their value may change. That what anomaly detection has to be done. Take the eg of JOB application , if we have age and experience , if age = 25 and 15 years of experience. Of its own both values are valid but when it comes to together its an anomaly .","comment_id":"213873","upvote_count":"3","timestamp":"1604640600.0","poster":"DeepakKhattar"},{"comment_id":"211882","poster":"Govind10","content":"A and C. A characteristic of anamoly detection and C - Since training set contains more of normal cases - anything outside that is anamoly. in real life, they need to be different from normal and doesn't matter if they are similar to what is classed as anamoly","timestamp":"1604402820.0","upvote_count":"5"},{"poster":"ajay1709","upvote_count":"4","comment_id":"192376","timestamp":"1601736720.0","content":"A and C"},{"poster":"PRABHUKKARTHI","comment_id":"159957","timestamp":"1597665960.0","content":"This Question is quite confusing. Can the author please recheck the answers and provide the explanation for the correct answer ?","upvote_count":"7"},{"upvote_count":"6","comment_id":"143054","timestamp":"1595639400.0","comments":[{"comment_id":"188929","comments":[{"upvote_count":"1","comment_id":"189309","timestamp":"1601332080.0","poster":"GregDT","content":"After re-reading the various source links on anomaly detection, I think that A and D are the correct answers. The source article above makes this statement \"Their features differ from the normal instances significantly\". This is not the same as answer C: \"You expect future mutations to have different features from the mutated samples in the database\""}],"upvote_count":"2","poster":"GregDT","timestamp":"1601284980.0","content":"Please review the link posted above. This does support answers A and C."}],"content":"A,C","poster":"RP123"},{"timestamp":"1594120140.0","comment_id":"128849","upvote_count":"2","poster":"saurabhsingh92","content":"B&C \nB - Model will be able to learn better about the classification\nC - Unsupervised model, expect feature that can't be labeled or new"},{"timestamp":"1589834040.0","content":"Correct AD. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set.\n\nhttps://en.wikipedia.org/wiki/Anomaly_detection","comment_id":"91620","poster":"arnabbis4u","upvote_count":"2"},{"timestamp":"1586457300.0","upvote_count":"11","comment_id":"72725","content":"A,C.\nA - The intent is to perform anomaly detection. Mutated samples are generally a very small fraction, compared to the normal samples.\nC - Mutations by definition do not follow a set pattern, as such we may expect to see future samples with different features.","poster":"Ganshank","comments":[{"upvote_count":"2","poster":"dambilwa","comment_id":"120130","content":"I have also reached the same conclusion. Option A & C","timestamp":"1593140940.0"},{"comment_id":"128530","timestamp":"1594089060.0","content":"Mutations ARE different, but how they are expressed is likely in a limited range. At the DNA level they may be uniques, but physiologically they start 'Acting and looking like cancer cells', and we already have those in the database","upvote_count":"1","poster":"droogie"}]},{"comment_id":"71330","content":"AD : Mutation of cells is not a common phenomena it's very rare phenomena like Cancer tissues are mutated. So Chances of getting Mutated samples is very low.\nSo getting enough labelled dataset for prediction is not practical so must go for unsupervised method rather than classification. So answer A is correct.\nNo Matter what machine learning and AI is pattern based techonology so future samples should show similar traits as earlier dataset for it to work. So answer D should be correct.","upvote_count":"10","poster":"MauryaSushil","timestamp":"1586067780.0"}],"unix_timestamp":1583947440},{"id":"PUfmrQ5IxCD6XiyRIU3g","answer_ET":"C","discussion":[{"content":"Selected Answer: C\nAccording to the documentation:\nUse a Universally Unique Identifier (UUID)\nYou can use a Universally Unique Identifier (UUID) as defined by RFC 4122 as the primary key. Version 4 UUID is recommended, because it uses random values in the bit sequence. Version 1 UUID stores the timestamp in the high order bits and is not recommended.\n\nhttps://cloud.google.com/spanner/docs/schema-design","timestamp":"1678722360.0","poster":"Remi2021","comments":[{"content":"Agree with C","upvote_count":"1","poster":"AzureDP900","timestamp":"1688137380.0","comment_id":"762729"}],"comment_id":"668094","upvote_count":"9"},{"poster":"barnac1es","upvote_count":"4","timestamp":"1711255380.0","comment_id":"1015439","content":"Selected Answer: C\nFor a transaction table in Cloud Spanner that stores product sales data, from a performance perspective, it is generally recommended to choose a primary key that allows for even distribution of data across nodes and minimizes hotspots. Therefore, option C, which suggests using a random universally unique identifier number (version 4 UUID), is the preferred choice."},{"comment_id":"985590","timestamp":"1708418580.0","upvote_count":"1","poster":"arien_chen","content":"Selected Answer: C\nFor a RDB I would choice D.\n\nBut for Google Spanner, Google says: \nhttps://cloud.google.com/spanner/docs/schema-and-data-model#:~:text=monotonically%20increasing%20integer"},{"upvote_count":"2","comment_id":"893157","timestamp":"1699546080.0","poster":"vaga1","content":"Selected Answer: C\nB might work if you say timestamp instead than epoch. PK of sales should contain the exact purchase date or timestamp, not the time when the transaction was processed. I personally associate the term epoch in this context to the process timestamp instead than the purchase timestamp."},{"comments":[{"timestamp":"1732413240.0","comment_id":"1217116","upvote_count":"1","content":"Agreed. Additionally, using the product name can lead to unbalanced distribution if some products are sold more frequently than others.","poster":"NickNtaken"}],"timestamp":"1694583720.0","content":"Selected Answer: C\nB may cause error if same product ID came at the same time (same id + same epoch)\nSo C is the correct answer here","poster":"midgoo","upvote_count":"2","comment_id":"837731"},{"upvote_count":"2","content":"Selected Answer: C\nA and D are invalid because they monotonically increases.\nB would work, but in terms of pure performance UUID 4 is the fastest because it virtually will not cause hotspots","comment_id":"750832","poster":"jkhong","timestamp":"1687255740.0"},{"comment_id":"739033","upvote_count":"3","poster":"odacir","timestamp":"1686220380.0","content":"Selected Answer: C\nA and D are not valid, because they monotonically increase.\nC avoid hotspots for sure, but It's nor relate with querys. So for writing performance it's perfect that the reason for chose this:  “You need to create a new transaction table in Cloud Spanner that stores product sales data”. They only ask you to store product data, its a writing ops.\nIf the question had spoken about query the info or hard performance read, the best option would be B, because it has the balance of writing/reading best practices.\nThere are a few disadvantages to using a UUID:\n\n They are slightly large, using 16 bytes or more. Other options for primary keys don't use this much storage.\n They carry no information about the record. For example, a primary key of SingerId and AlbumId has an inherent meaning, while a UUID does not.\n You lose locality between records that are related, which is why using a UUID eliminates hotspots.\n\n\nhttps://cloud.google.com/spanner/docs/schema-design#uuid_primary_key"},{"upvote_count":"1","content":"Selected Answer: C\nC. A random universally unique identifier number (version 4 UUID)\n\nFrom https://cloud.google.com/spanner/docs/schema-and-data-model\n\n\nThere are techniques that can spread the load across multiple servers and avoid hotspots:\n\nHash the key and store it in a column. Use the hash column (or the hash column and the unique key columns together) as the primary key.\nSwap the order of the columns in the primary key.\nUse a Universally Unique Identifier (UUID). Version 4 UUID is recommended, because it uses random values in the high-order bits. Don't use a UUID algorithm (such as version 1 UUID) that stores the timestamp in the high order bits.\nBit-reverse sequential values.","timestamp":"1678018140.0","poster":"YorelNation","comment_id":"660028"},{"comments":[{"comment_id":"1334445","timestamp":"1735597020.0","upvote_count":"1","poster":"LP_PDE","content":"Potential Skew: If there are a limited number of product names, this could still lead to uneven data distribution and potential hotspots.\nIncreased Key Size: Concatenating strings can result in larger primary keys, which can slightly impact storage and performance."}],"upvote_count":"2","poster":"jsree236","comment_id":"659916","content":"Selected Answer: B\nAnswer should be B as in all the other options hotspotting is possible. According to proper schema design guideline.. \nSchema design best practice #1: Do not choose a column whose value monotonically increases or decreases as the first key part for a high write rate table.\n\nSupporting link: \nhttps://cloud.google.com/spanner/docs/schema-design#primary-key-prevent-hotspots","timestamp":"1678011780.0"}],"topic":"1","answers_community":["C (92%)","8%"],"answer_images":[],"question_id":47,"exam_id":10,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/80270-exam-professional-data-engineer-topic-1-question-140/","timestamp":"2022-09-05 10:23:00","answer_description":"","unix_timestamp":1662366180,"answer":"C","choices":{"C":"A random universally unique identifier number (version 4 UUID)","D":"The original order identification number from the sales system, which is a monotonically increasing integer","B":"A concatenation of the product name and the current epoch time","A":"The current epoch time"},"question_images":[],"question_text":"You need to create a new transaction table in Cloud Spanner that stores product sales data. You are deciding what to use as a primary key. From a performance perspective, which strategy should you choose?"},{"id":"8vaiQXGCxzw9bma13zuq","answer_description":"","answer_ET":"D","url":"https://www.examtopics.com/discussions/google/view/17225-exam-professional-data-engineer-topic-1-question-141/","discussion":[{"content":"Answer D is correct. Aggregated log sink will create a single sink for all projects, the destination can be a google cloud storage, pub/sub topic, bigquery table or a cloud logging bucket. without aggregated sink this will be required to be done for each project individually which will be cumbersome.\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks","comment_id":"186071","upvote_count":"30","timestamp":"1616589600.0","comments":[{"comment_id":"762730","content":"D is right","upvote_count":"1","timestamp":"1688137500.0","poster":"AzureDP900"}],"poster":"SteelWarrior"},{"poster":"[Removed]","comments":[{"timestamp":"1631474580.0","poster":"daghayeghi","comment_id":"309190","content":"https://cloud.google.com/iam/docs/job-functions/auditing#scenario_operational_monitoring","upvote_count":"3"},{"poster":"Rajuuu","comment_id":"133655","upvote_count":"3","timestamp":"1610521020.0","content":"The above link shows BigQuery as a sink for aggregated exports and not Cloud Storage."}],"comment_id":"68134","timestamp":"1601035440.0","content":"Correct: D\nhttps://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors","upvote_count":"12"},{"poster":"pbtpratik","content":"D is the correct ans","timestamp":"1727326980.0","comment_id":"1183091","upvote_count":"1"},{"poster":"barnac1es","content":"Selected Answer: D\nD. \nHere's why this option is recommended:\nAggregated Export Sink: By using an aggregated export sink, you can consolidate data access logs from multiple projects into a single location. This simplifies log management and retention policies.\nNewly Created Project for Audit Logs: Creating a dedicated project for audit logs allows you to centralize access control and manage logs separately from individual Data Analyst projects.\nAccess Restriction: By restricting access to the project containing the exported logs, you ensure that only authorized audit personnel have access to the logs while preventing Data Analysts from accessing them.","comment_id":"1015440","upvote_count":"1","timestamp":"1711255620.0"},{"content":"Selected Answer: D\nTo create the Log Router, at step 3 to define the logs (Source), we can include logs from many projects (aggregated)","upvote_count":"1","timestamp":"1694584320.0","poster":"midgoo","comment_id":"837735"},{"upvote_count":"2","comment_id":"732641","timestamp":"1685618040.0","content":"Selected Answer: D\nD is the answer.\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks\nAggregated sinks combine and route log entries from the Google Cloud resources contained by an organization or folder. For instance, you might aggregate and route audit log entries from all the folders contained by an organization to a Cloud Storage bucket.","poster":"zellck"},{"content":"D is correct","poster":"dffffff","timestamp":"1666927740.0","comment_id":"593500","upvote_count":"1"},{"content":"Selected Answer: D\nD: https://cloud.google.com/logging/docs/export/aggregated_exports\nYou can create an aggregated export sink that can export log entries from all the projects, folders, and billing accounts of an organization. As an example, you might use this feature to export audit log entries from an organization's projects to a central location.","poster":"MaxNRG","timestamp":"1657382940.0","upvote_count":"4","comment_id":"520387"},{"timestamp":"1648769700.0","upvote_count":"4","content":"The auditor needs to audit data analyst's behaviors (how they access multiple projects in BQ ). So, the key is, multiple projects. According to Google doc project-level sinks:\nhttps://cloud.google.com/logging/docs/export/configure_export_v2\nHowever, the Cloud Console can only create or view sinks in Cloud projects. To create sinks in organizations, folders, or billing accounts using the gcloud command-line tool or Cloud Logging API, see Aggregated sinks.\n\nObviously, the auditor needs to check all projects accessed by data analyst which is not project-level, a higher level like folder or organization level, this can only be done via the aggregate sink.\n\nSo D is the answer.","comment_id":"455183","poster":"Chelseajcole"},{"comment_id":"398275","timestamp":"1641303600.0","upvote_count":"6","content":"A - eliminated , because logs needs to be retained for 6 months (So, some storage require)\nB - eliminated, because if we store in same project then, Data Analyst can also access (But in question it's mention, ONLY audit personnel needs access)\nC - Wrong (No need to restrict project as well as logs separately) - wording does not look okay.\nD - Correct (If we restrict the project, then all resources get restricted)\n\nVote for D","poster":"sumanshu","comments":[{"upvote_count":"2","comments":[{"timestamp":"1658853840.0","poster":"at99","upvote_count":"1","content":"Sinks are different from Aggregate Sinks, refer https://cloud.google.com/logging/docs/export/configure_export_v2#api","comment_id":"533150"}],"timestamp":"1641308760.0","comment_id":"398350","poster":"sumanshu","content":"Option 'C' - I guess said - restrict access to the project with the exported logs. (i.e. restrict access of that project from where we took logs) - If I am not wrong... Thus it's INCORRECT"}]},{"content":"what is the difference between C and D? I think it's same.","comments":[{"poster":"FP77","content":"I think the key difference is that D talks about aggregated sinks.","comment_id":"982755","upvote_count":"1","timestamp":"1708107240.0"}],"poster":"septiandy","upvote_count":"3","comment_id":"341007","timestamp":"1634901900.0"},{"upvote_count":"3","poster":"haroldbenites","comment_id":"163549","timestamp":"1613999820.0","content":"D is correct"},{"comment_id":"160971","upvote_count":"3","timestamp":"1613668440.0","content":"D is correct answer, refer below link for more information.","poster":"saurabh1805"},{"upvote_count":"5","content":"Ans : D\n Aggregated Exports, which allows you to set up a sink at the Cloud IAM organization or folder level, and export logs from all the projects inside the organization or folder.","timestamp":"1610382660.0","poster":"VishalB","comment_id":"132157"},{"poster":"[Removed]","content":"Answer D","upvote_count":"4","timestamp":"1600754280.0","comment_id":"66859"}],"unix_timestamp":1584863880,"isMC":true,"choices":{"C":"Export the data access logs via a project-level export sink to a Cloud Storage bucket in a newly created projects for audit logs. Restrict access to the project with the exported logs.","B":"Export the data access logs via a project-level export sink to a Cloud Storage bucket in the Data Analysts' projects. Restrict access to the Cloud Storage bucket.","A":"Enable data access logs in each Data Analyst's project. Restrict access to Stackdriver Logging via Cloud IAM roles.","D":"Export the data access logs via an aggregated export sink to a Cloud Storage bucket in a newly created project for audit logs. Restrict access to the project that contains the exported logs."},"question_images":[],"topic":"1","answer":"D","answers_community":["D (100%)"],"question_id":48,"exam_id":10,"question_text":"Data Analysts in your company have the Cloud IAM Owner role assigned to them in their projects to allow them to work with multiple GCP products in their projects. Your organization requires that all BigQuery data access logs be retained for 6 months. You need to ensure that only audit personnel in your company can access the data access logs for all projects. What should you do?","answer_images":[],"timestamp":"2020-03-22 08:58:00"},{"id":"Axig8Dqt5sHR9dwdEuS1","discussion":[{"comment_id":"1183092","upvote_count":"1","poster":"pbtpratik","content":"B is correct answer","timestamp":"1727327040.0"},{"content":"Selected Answer: B\nViewing project and reservation slot usage in Stackdriver Monitoring\nInformation is available from the \"Slots Allocated\" metric in Stackdriver Monitoring. This metric information includes a per-reservation and per-job breakdown of slot usage. The information can also be visualized by using the custom charts metric explorer.\nhttps://cloud.google.com/bigquery/docs/reservations-monitoring\nhttps://cloud.google.com/monitoring/api/metrics_gcp","poster":"MaxNRG","timestamp":"1718714460.0","upvote_count":"2","comments":[{"content":"this is stols/allocated, not slots/allocated_for_project","timestamp":"1737907980.0","poster":"keisoes","comment_id":"1346995","upvote_count":"1"}],"comment_id":"1099794"},{"content":"Selected Answer: B\nThe slots/allocated_for_project metric provides information about the number of slots allocated to each project. It directly reflects the slot usage, making it a relevant and accurate metric for monitoring slot allocation within each project.\n\nOptions A, C, and D involve log exports and custom metrics, but they may not be as straightforward or provide the same level of detail as the built-in metric slots/allocated_for_project:","comment_id":"1015443","timestamp":"1711255860.0","poster":"barnac1es","upvote_count":"3"},{"upvote_count":"2","timestamp":"1710938580.0","content":"The naming is quite misleading in this case, but it actually seems from the documentation that slots/allocated_for_project indicates the \"slots used by project\", in which case answer B is correct: https://cloud.google.com/monitoring/api/metrics_gcp#:~:text=slots/allocated_for_project%20GA%0ASlots%20used%20by%20project","comment_id":"1012220","poster":"ckanaar"},{"content":"Selected Answer: D\nB slots/allocated_for_project will give you the total number of slots allocated to each project, but it will not tell you how many slots are actually being used.\n\nThe purpose to monitor 'slot usgae' is for billing. 'slot/allocated' means nothing.\nOption D is better than B.\n\nAnd, the question mention 'Each analytics team in organization', so it should be 'organization level'.","poster":"arien_chen","timestamp":"1708423620.0","comment_id":"985634","upvote_count":"1"},{"timestamp":"1695522300.0","content":"Selected Answer: D\nIf 'usage' = how the slots are being used, D is the corret answer\nIf 'usage' = how the slots are being allocated, B is the correct answer\n\nI think in this question, usage = how the slots are being used","upvote_count":"1","poster":"midgoo","comment_id":"848944"},{"poster":"musumusu","timestamp":"1692300900.0","content":"Answer B, \nWhy not D, aggregated log export is good but it will generate all the details which is large in size and costly too. you dont need all the information. It can break data privacy. so look for B because this much is asked only. Normally, i make such errors alot.","comment_id":"812403","upvote_count":"1"},{"comment_id":"751867","content":"Selected Answer: B\nThe correct answer is B. You should create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project.\n\nThis metric represents the number of BigQuery slots allocated for a project. By creating a Cloud Monitoring dashboard based on this metric, you can monitor the slot usage within each project in your organization. This will allow each team to monitor their own slot usage and ensure that they are not exceeding their allocated quota.\n\nOption A is incorrect because the query/scanned_bytes metric represents the number of bytes scanned by BigQuery queries, not the slot usage.\n\nOption C is incorrect because it involves creating a log export for each project and using a custom metric based on the totalSlotMs field. While this may be a valid way to monitor slot usage, it is more complex than simply using the slots/allocated_for_project metric.\n\nOption D is also incorrect because it involves creating an aggregated log export at the organization level, which is not necessary for monitoring slot usage within individual projects.","upvote_count":"4","timestamp":"1687314060.0","poster":"saurabhsingh4k"},{"upvote_count":"2","poster":"dn_mohammed_data","content":"vote for B","timestamp":"1679489280.0","comment_id":"675993"},{"comment_id":"667600","content":"B ,the another is related to the question as well.\nhttps://cloud.google.com/bigquery/docs/reservations-monitoring#viewing-slot-usage","upvote_count":"4","poster":"John_Pongthorn","timestamp":"1678682940.0"},{"comments":[{"poster":"AzureDP900","upvote_count":"1","content":"B. Create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project","timestamp":"1688137560.0","comment_id":"762731"}],"content":"Selected Answer: B\nB the below is related to the question.\nhttps://cloud.google.com/blog/topics/developers-practitioners/monitoring-bigquery-reservations-and-slot-utilization-information_schema","comment_id":"667597","upvote_count":"4","poster":"John_Pongthorn","timestamp":"1678682580.0"}],"answer":"B","url":"https://www.examtopics.com/discussions/google/view/81914-exam-professional-data-engineer-topic-1-question-142/","unix_timestamp":1663036980,"timestamp":"2022-09-13 04:43:00","exam_id":10,"question_images":[],"answer_ET":"B","isMC":true,"answers_community":["B (87%)","13%"],"choices":{"C":"Create a log export for each project, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring dashboard based on the custom metric","B":"Create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project","A":"Create a Cloud Monitoring dashboard based on the BigQuery metric query/scanned_bytes","D":"Create an aggregated log export at the organization level, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring dashboard based on the custom metric"},"answer_description":"","question_id":49,"question_text":"Each analytics team in your organization is running BigQuery jobs in their own projects. You want to enable each team to monitor slot usage within their projects.\nWhat should you do?","topic":"1","answer_images":[]},{"id":"PbCqdieszJfr5Alz7ffg","question_images":[],"answers_community":["D (80%)","A (20%)"],"url":"https://www.examtopics.com/discussions/google/view/79678-exam-professional-data-engineer-topic-1-question-143/","answer_ET":"D","isMC":true,"topic":"1","question_text":"You are operating a streaming Cloud Dataflow pipeline. Your engineers have a new version of the pipeline with a different windowing algorithm and triggering strategy. You want to update the running pipeline with the new version. You want to ensure that no data is lost during the update. What should you do?","timestamp":"2022-09-03 06:44:00","answer_description":"","discussion":[{"timestamp":"1686221280.0","content":"Selected Answer: D\nIt's D. → Your engineers have a new version of the pipeline with a different windowing algorithm and triggering strategy. \nNew version is mayor changes. Stop and drain and then launch the new code is a lot is the safer way. \nWe recommend that you attempt only smaller changes to your pipeline's windowing, such as changing the duration of fixed- or sliding-time windows. Making major changes to windowing or triggers, like changing the windowing algorithm, might have unpredictable results on your pipeline output.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing","upvote_count":"15","comment_id":"739061","poster":"odacir","comments":[{"upvote_count":"2","timestamp":"1686760860.0","content":"Since updating the job as in A does a compatibility check, wouldn't you want to try that first? Then if the compatibility check fails then you proceed to drain current pipeline and then launch new pipeline (Answer D)?\n\nAs in A would be correct answer, then if compatibility check fails, you proceed to D. \n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#CCheck","comment_id":"745375","comments":[{"poster":"ckanaar","timestamp":"1711034280.0","upvote_count":"1","comment_id":"1013102","content":"You're right in your reasoning, but since the documentation specifically uses this example for stopping and draining, it's safe to assume that the compatibility check will always fail with these adjustments. Therefore, we can go straight to D. \n\nFurthermore, answer A doesn't state: \"Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to the existing name, if the compatibility check fails, THEN proceed to stopping the pipeline with the drain option\", so in itself it is not the right answer if the check fails."}],"poster":"maggieee"}]},{"timestamp":"1719745260.0","content":"Selected Answer: D\nD seems the right way to go","comment_id":"1109734","poster":"patitonav","upvote_count":"1"},{"poster":"TVH_Data_Engineer","upvote_count":"1","content":"Selected Answer: D\nOption A is the first approach to try, as it allows for an in-flight update with minimal disruption. However, if the changes in the new version of the pipeline are not compatible with an in-flight update (due to significant changes in windowing or triggering), then option D should be used. The Drain option ensures a graceful shutdown of the existing pipeline, reducing the risk of data loss, and then a new job can be started with the updated code.","timestamp":"1719203580.0","comment_id":"1104475"},{"content":"Selected Answer: D\nA is not an option as \"You want to ensure that no data is lost during the update. \": \nMaking major changes to windowing or triggers, like changing the windowing algorithm, might have unpredictable results on your pipeline output.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#change_windowing","poster":"MaxNRG","comment_id":"1099828","timestamp":"1718718000.0","upvote_count":"1"},{"poster":"barnac1es","upvote_count":"1","content":"Selected Answer: D\nDrain Option: The \"Drain\" option allows the existing Dataflow job to complete processing of any in-flight data before stopping the job. This ensures that no data is lost during the transition to the new version.\nCreate a New Job: After draining the existing job, you create a new Cloud Dataflow job with the updated code. This new job starts fresh and continues processing data from where the old job left off.\n\nOption A (updating the inflight pipeline with the --update option) may not guarantee no data loss, as the update could disrupt the existing job's operation and potentially cause data loss.\n\nOption B (updating the inflight pipeline with the --update option and a new job name) is similar to option A and may not provide data loss guarantees.\n\nOption C (stopping the pipeline with the Cancel option and creating a new job) will abruptly stop the existing job without draining, potentially leading to data loss.","timestamp":"1711256760.0","comment_id":"1015447"},{"comment_id":"963660","timestamp":"1706275680.0","upvote_count":"1","content":"Look D after seeing some docs. please check the below link https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline","poster":"knith66"},{"poster":"vamgcp","upvote_count":"1","content":"Selected Answer: D\nI will go with option D - If you want to minimize the impact of the update, then option A is the best option. However, if you are not concerned about a temporary interruption in processing, then option D is also a valid option. Option Pros Cons\nA Does not stop the pipeline, so no data is lost. Requires you to create a new version of the pipeline.\nB Creates a new job with the updated code, so you do not have to update the running pipeline. Can lead to data loss if the new job does not process all of the data that was in the running pipeline.\nC Stops the pipeline and drains any data that is currently in flight, so no data is lost. Causes a temporary interruption in processing.","comment_id":"963297","timestamp":"1706245860.0"},{"upvote_count":"3","poster":"midgoo","content":"Selected Answer: D\nA is not recommeded for major changes in pipeline.","timestamp":"1694587320.0","comment_id":"837774"},{"upvote_count":"1","comment_id":"812424","timestamp":"1692301740.0","comments":[{"timestamp":"1692899700.0","comment_id":"820886","poster":"musumusu","content":"Answer D: as per latest documents 02/2023 google has removed update flag.","upvote_count":"3"}],"content":"Answer A:\n```gcloud dataflow jobs update <JOB_ID> --update <GCS_PATH_TO_UPDATED_PIPELINE> --region <REGION>```\n--update flag does not miss any data and you can execute this command even yourpipeline is running. Its safe any fast, you can continuously make some change and update this command. no problem.\nStop and Drain, is required when you want to test the pipeline and stop it without losing the data.","poster":"musumusu"},{"content":"Selected Answer: D\nagree with odacir","poster":"jkhong","comment_id":"753198","upvote_count":"4","timestamp":"1687423920.0"},{"poster":"hauhau","comment_id":"734224","timestamp":"1685763480.0","content":"Selected Answer: A\nvote A\nD: drain doesn't mention about update dataflow job just stop and preserve data\nA: replace existing job and preserve data\n(When you update your job, the Dataflow service performs a compatibility check between your currently-running job and your potential replacement job. The compatibility check ensures that things like intermediate state information and buffered data can be transferred from your prior job to your replacement job.)\n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline","upvote_count":"2"},{"comments":[{"content":"Are mayor changes. It's not safe to update. I vote D.","poster":"odacir","timestamp":"1686221340.0","upvote_count":"1","comment_id":"739063"}],"poster":"zellck","comment_id":"732634","upvote_count":"1","timestamp":"1685617680.0","content":"Selected Answer: A\nA is the answer.\n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Launching\nTo update your job, launch a new job to replace the ongoing job. When you launch your replacement job, set the following pipeline options to perform the update process in addition to the job's regular options:\n- Pass the --update option.\n- Set the --jobName option in PipelineOptions to the same name as the job you want to update."},{"timestamp":"1685074320.0","content":"D\nA-is not because The Dataflow service retains the job name, but runs the replacement job with an updated Job ID. \nDescription:\nWhen you update a job on the Dataflow service, you replace the existing job with a new job that runs your updated pipeline code. The Dataflow service retains the job name, but runs the replacement job with an updated Job ID. This process can cause downtime while the existing job stops, the compatibility check runs, and the new job starts.'\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python:~:text=When%20you%20update%20a,has%20the%20following%20transforms%3A\nD is correct\nDrain ->clone -> update -> run","upvote_count":"1","poster":"Atnafu","comment_id":"727307","comments":[{"comment_id":"727324","timestamp":"1685075520.0","poster":"Atnafu","upvote_count":"1","content":"Changed my mind to A\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python_2:~:text=Set%20the%20%2D%2Djob_name,%2D%2Dtransform_name_mapping%20option."}]},{"content":"Selected Answer: D\nChanging windowing algorithm may break the pipeline.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing","timestamp":"1684938000.0","poster":"drunk_goat82","comment_id":"726036","upvote_count":"3"},{"poster":"ovokpus","timestamp":"1684813980.0","content":"Selected Answer: A\nNo, do not drain the current job.","comment_id":"724918","upvote_count":"1"},{"poster":"dish11dish","timestamp":"1684555500.0","content":"Selected Answer: D\nin this scenario pipline is streaming pipline with windowing algorithm and triggering strategy changes to new one without loss of data,so better to go with Drain option as it fullfile all precondition described in scenario which is :-\n1.streaming\n2.code changes with windowing algorithm and triggering strategy to new way\n3.no loss of data during update \n\nReferances:-\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain\nDrain a job. This method applies only to streaming pipelines. Draining a job enables the Dataflow service to finish processing the buffered data while simultaneously ceasing the ingestion of new data. For more information, see Draining a job.","upvote_count":"1","comment_id":"722420","comments":[{"upvote_count":"1","content":"If the pipeline was batch then ans would been A","poster":"dish11dish","timestamp":"1684555680.0","comment_id":"722422"}]},{"upvote_count":"3","comment_id":"713132","timestamp":"1683465300.0","content":"D: They want to preserve data and updates might not be predictable.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing","poster":"Mcloudgirl"},{"comment_id":"712665","content":"Selected Answer: A\nIt's A (https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#UpdateMechanics)\nD would stop the pipeline, leading to loss of new data that would be sent into the pipeline before you start the new pipeline.","poster":"cloudmon","upvote_count":"3","timestamp":"1683401940.0"},{"timestamp":"1682525160.0","poster":"Azlijaffar","upvote_count":"3","comment_id":"704848","content":"The google documentation did mention that if the windowing or triggering algorithm changes, you might have unpredictable results. This question did mention change in windowing and triggering algorithm. \nSo A might lead to unpredictable results. \nSafe bet is D. \nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline"},{"content":"Selected Answer: D\nD of course. Also you can only update for minor changes on windowing/triggering. Question say a different strategy.","comment_id":"696605","poster":"devaid","upvote_count":"4","timestamp":"1681682220.0"},{"timestamp":"1681379460.0","poster":"josrojgra","comments":[{"poster":"Duckjai","upvote_count":"1","content":"Agree. It is A","comment_id":"694577","timestamp":"1681457460.0"}],"content":"Selected Answer: A\nThe answer is A.\nThe question says \"update\", and on the documentation (https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#UpdateMechanics) says explicitly on the second paragraph this: \nThe replacement job preserves any intermediate state data from the prior job, as well as any buffered data records or metadata currently \"in-flight\" from the prior job. For example, some records in your pipeline might be buffered while waiting for a window to resolve.\n\nSo the update process prevent data loss.","comment_id":"693778","upvote_count":"4"},{"timestamp":"1679489400.0","poster":"dn_mohammed_data","upvote_count":"2","comment_id":"675994","content":"D : drain to ensure that there is no data loss during the code update"},{"upvote_count":"2","content":"Selected Answer: D\ndrain for graceful shutdown","poster":"MounicaN","timestamp":"1678559400.0","comment_id":"666277"},{"content":"Selected Answer: D\nDrain option is the correct one","timestamp":"1678089540.0","upvote_count":"3","poster":"sanjithj","comment_id":"660858"},{"content":"Drain is the only option","poster":"damaldon","upvote_count":"1","timestamp":"1677849240.0","comment_id":"658372"},{"upvote_count":"3","poster":"ducc","timestamp":"1677825840.0","content":"Selected Answer: D\nIf you want to prevent data loss as you bring down your streaming pipelines, the best option is to drain your job.","comment_id":"658073"}],"answer":"D","answer_images":[],"choices":{"C":"Stop the Cloud Dataflow pipeline with the Cancel option. Create a new Cloud Dataflow job with the updated code","A":"Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to the existing job name","B":"Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to a new unique job name","D":"Stop the Cloud Dataflow pipeline with the Drain option. Create a new Cloud Dataflow job with the updated code"},"exam_id":10,"unix_timestamp":1662180240,"question_id":50}],"exam":{"isMCOnly":true,"id":10,"name":"Professional Data Engineer","provider":"Google","isBeta":false,"numberOfQuestions":319,"isImplemented":true,"lastUpdated":"15 Feb 2025"},"currentPage":10},"__N_SSP":true}