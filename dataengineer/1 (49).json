{"pageProps":{"questions":[{"id":"GTx21c1E4HpsGw3bW51d","answer_images":[],"exam_id":10,"isMC":true,"answer_description":"","url":"https://www.examtopics.com/discussions/google/view/17052-exam-professional-data-engineer-topic-1-question-32/","topic":"1","question_images":[],"question_id":246,"answers_community":["A (100%)"],"discussion":[{"poster":"IsaB","content":"I hate it when I read the question, than I think oh easy and I KNOW the answer, then I look at the choices and the answer I thought of is just not there at all... and I realize I absolutely have no idea :'D","comment_id":"179287","timestamp":"1600086720.0","upvote_count":"53"},{"upvote_count":"23","content":"Correct A","poster":"[Removed]","comments":[{"upvote_count":"20","timestamp":"1584710640.0","content":"https://cloud.google.com/bigtable/docs/performance#troubleshooting\nIf you find that you're reading and writing only a small number of rows, you might need to redesign your schema so that reads and writes are more evenly distributed.","comment_id":"66270","poster":"[Removed]"}],"comment_id":"66269","timestamp":"1584710100.0"},{"content":"Believe me all questions were from Exam topic all were there yesterday in exam. But yes dont go with starting questions mainly focus questions after 200 and latest questions are at last page.","poster":"meh_33","comment_id":"1264466","upvote_count":"3","timestamp":"1723437480.0"},{"upvote_count":"1","poster":"09878d5","content":"Selected Answer: A\nB is a Lie\nC and D are actually not recommended \nA is correct as it will help in even distribution of load and avoid hotspots","comment_id":"1255191","timestamp":"1721934120.0"},{"upvote_count":"3","content":"Improving performance in Google Cloud Bigtable involves optimizing the schema design to distribute the load efficiently across the clusters. Given the scenario, the best option would be:\n\nA. Redefine the schema by evenly distributing reads and writes across the row space of the table.\n\nExplanation:\n\nDistributing reads and writes evenly across the row space helps prevent hotspots and ensures that the load is spread evenly, avoiding performance bottlenecks.\nGoogle Cloud Bigtable's performance is influenced by how well the data is distributed across the tablet servers, and evenly distributing the load can lead to better performance.\nThis approach aligns with best practices for designing scalable and performant Bigtable schemas.","timestamp":"1701701160.0","comment_id":"1087723","poster":"JOKKUNO"},{"content":"Selected Answer: A\nThe comment from hilel_eth totally makes sense to me. I would go with A","timestamp":"1700584320.0","upvote_count":"1","comment_id":"1076453","poster":"axantroff"},{"comments":[{"timestamp":"1702290300.0","content":"HEY DID U CLEAR THE EXAM","poster":"roty","comment_id":"1093371","upvote_count":"2"}],"content":"Guys, how relevant are these questions, as of Aug 14, 2023 Could we clear the PDE exam with these set of questions?","comment_id":"980413","timestamp":"1691983380.0","upvote_count":"7","poster":"hkris909"},{"upvote_count":"1","poster":"FP77","comment_id":"966263","content":"Selected Answer: A\nA is the only one that makes sense and is correct","timestamp":"1690621980.0"},{"content":"I understand why it could be A. But why not B also? Is it because of the typo saying BigDate instead of BigTable?","comment_id":"961315","timestamp":"1690186080.0","poster":"Mathew106","upvote_count":"1"},{"comment_id":"866660","timestamp":"1681165740.0","content":"Selected Answer: A\nA to avoid hot-spotting https://cloud.google.com/bigtable/docs/schema-design","poster":"Adswerve","upvote_count":"2"},{"comment_id":"755903","timestamp":"1671992580.0","poster":"Brillianttyagi","upvote_count":"6","content":"Selected Answer: A\nA - \nMake sure you're reading and writing many different rows in your table. Bigtable performs best when reads and writes are evenly distributed throughout your table, which helps Bigtable distribute the workload across all of the nodes in your cluster. If reads and writes cannot be spread across all of your Bigtable nodes, performance will suffer.\n\nhttps://cloud.google.com/bigtable/docs/performance#troubleshooting"},{"timestamp":"1670709600.0","poster":"hilel_eth","content":"Selected Answer: A\nA good way to improve read and write performance in a database system like Google Cloud Bigtable is to redefine the schema of the table so that reads and writes are evenly distributed across the row space of the table. This can help reduce bottlenecks in processing capacity and improve efficiency in table management. In addition, by evenly distributing read and write operations, it can prevent the accumulation of operations in one part of the table, which can improve the overall performance of the system.","comment_id":"741262","upvote_count":"3"},{"timestamp":"1659093420.0","comment_id":"639158","upvote_count":"4","content":"Selected Answer: A\nhttps://cloud.google.com/bigtable/docs/keyvis-overview#what-is-keyvis\n\nTo accomplish these goals, Key Visualizer can help you complete the following tasks:\n\nCheck whether your reads or writes are creating hotspots on specific rows","poster":"Pime13"},{"content":"Selected Answer: A\nA is correct\nhttps://cloud.google.com/bigtable/docs/performance#troubleshooting\n\nIf you find that you're reading and writing only a small number of rows, you might need to redesign your schema so that reads and writes are more evenly distributed.","poster":"Arkon88","timestamp":"1646322180.0","upvote_count":"3","comment_id":"560182"},{"content":"correct answer -> Redefine the schema by evenly distributing reads and writes across the row space of the table.\n\nMake sure you're reading and writing many different rows in your table. Bigtable performs best when reads and writes are evenly distributed throughout your table, which helps Bigtable distribute the workload across all of the nodes in your cluster. If reads and writes cannot be spread across all of your Bigtable nodes, performance will suffer.\nIf you find that you're reading and writing only a small number of rows, you might need to redesign your schema so that reads and writes are more evenly distributed.\n\nReference: https://cloud.google.com/bigtable/docs/performance#troubleshooting","upvote_count":"2","timestamp":"1642983660.0","poster":"samdhimal","comment_id":"530925"},{"timestamp":"1636900380.0","upvote_count":"11","content":"A as the schema needs to be redesigned to distribute the reads and writes evenly across each table.\nRefer GCP documentation - Bigtable Performance:\nhttps://cloud.google.com/bigtable/docs/performance\nThe table's schema is not designed correctly. To get good performance from Cloud Bigtable, it's essential to design a schema that makes it possible to distribute reads and writes evenly across each table. See Designing Your Schema for more information. \nhttps://cloud.google.com/bigtable/docs/schema-design\nOption B is wrong as increasing the size of cluster would increase the cost.\nOption C is wrong as single row key for frequently updated identifiers reduces performance\nOption D is wrong as sequential IDs would degrade the performance.\nA safer approach is to use a reversed version of the user's numeric ID, which spreads traffic more evenly across all of the nodes for your Cloud Bigtable table.","comment_id":"478168","poster":"MaxNRG"},{"content":"Ans: A","timestamp":"1634053920.0","poster":"anji007","upvote_count":"1","comment_id":"461142"},{"comment_id":"401888","poster":"sumanshu","timestamp":"1625745780.0","content":"Vote for A","upvote_count":"3"},{"content":"Correct is A: https://cloud.google.com/bigtable/docs/performance#troubleshooting\n\nMake sure you're reading and writing many different rows in your table. Bigtable performs best when reads and writes are evenly distributed throughout your table, which helps Bigtable distribute the workload across all of the nodes in your cluster. If reads and writes cannot be spread across all of your Bigtable nodes, performance will suffer.","upvote_count":"4","timestamp":"1625395260.0","comment_id":"398223","poster":"timolo"},{"poster":"naga","content":"Correct A","upvote_count":"4","comment_id":"285611","timestamp":"1612713600.0"},{"timestamp":"1605595320.0","upvote_count":"6","comment_id":"220858","content":"This pretty straightforward. https://cloud.google.com/bigtable/docs/performance#troubleshooting\nOne of the troubleshoot performance issue is Make sure you're reading and writing many different rows in your table. That is Cloud Bigtable performs best when reads and writes are evenly distributed throughout your table, which helps Cloud Bigtable distribute the workload across all of the nodes in your cluster.","poster":"Radhika7983"},{"timestamp":"1605227940.0","comment_id":"218199","poster":"snamburi3","content":"A\nmentioned in the below post \"Ideally, both reads and writes should be distributed evenly across the row space of the table.\"\nhttps://cloud.google.com/bigtable/docs/schema-design\nhttps://cloud.google.com/bigtable/docs/schema-design","upvote_count":"4"},{"comment_id":"161949","content":"A\nThe table's schema is not designed correctly. To get good performance from Cloud Bigtable, it's essential to design a schema that makes it possible to distribute reads and writes evenly across each table. See Designing Your Schema for more information.","upvote_count":"5","timestamp":"1597898280.0","poster":"atnafu2020"},{"timestamp":"1593873000.0","poster":"dg63","upvote_count":"2","content":"In my opinion D is that correct answer as hotspot issue may be happening. \nQuestion does not indicate that application is doing frequent read and updates to limited number of rows.","comment_id":"126213"},{"comment_id":"68565","content":"Answer: A\nDescription: Cloud Bigtable performs best when reads and writes are evenly distributed throughout your table, which helps Cloud Bigtable distribute the workload across all of the nodes in your cluster. If reads and writes cannot be spread across all of your Cloud Bigtable nodes, performance will suffer.\nIf you find that you're reading and writing only a small number of rows, you might need to redesign your schema so that reads and writes are more evenly distributed.","upvote_count":"9","timestamp":"1585303860.0","poster":"[Removed]"}],"timestamp":"2020-03-20 14:15:00","question_text":"Your company is running their first dynamic campaign, serving different offers by analyzing real-time data during the holiday season. The data scientists are collecting terabytes of data that rapidly grows every hour during their 30-day campaign. They are using Google Cloud Dataflow to preprocess the data and collect the feature (signals) data that is needed for the machine learning model in Google Cloud Bigtable. The team is observing suboptimal performance with reads and writes of their initial load of 10 TB of data. They want to improve this performance while minimizing cost. What should they do?","answer_ET":"A","unix_timestamp":1584710100,"answer":"A","choices":{"B":"The performance issue should be resolved over time as the site of the BigDate cluster is increased.","C":"Redesign the schema to use a single row key to identify values that need to be updated frequently in the cluster.","D":"Redesign the schema to use row keys based on numeric IDs that increase sequentially per user viewing the offers.","A":"Redefine the schema by evenly distributing reads and writes across the row space of the table."}},{"id":"tGafLadUjI70GAhc2eb2","discussion":[{"timestamp":"1616849400.0","comments":[{"comments":[{"comment_id":"442804","content":"messages sent successfully to Topic, but not Subscription.\nin this case, if Dataflow cannot handle messages correctly it might not return acknowledgments to the Pub/Sub, and these errors can be seen from Monitoring.\nhttps://cloud.google.com/pubsub/docs/monitoring#monitoring_exp","upvote_count":"12","poster":"kubosuke","timestamp":"1662868980.0","comments":[{"poster":"Tanzu","comment_id":"532366","upvote_count":"1","timestamp":"1674675360.0","comments":[{"poster":"jkhong","content":"PubSub doesn't forward from subscriber to subscription. A topic sends it over to subscription first, then to subscriber","comment_id":"738082","timestamp":"1701964860.0","upvote_count":"2"}],"content":"to be more precise, first to publisher, \n- then forwards to topic, and persistance for a while\n- then forwards to subscribe, \n- then to subscription..\n- then acknowledgement happens\n\nso in every steps, there is possibly for errors."}]}],"timestamp":"1636764480.0","poster":"snamburi3","upvote_count":"10","content":"All messages are being published to Cloud Pub/Sub successfully. so Stackdriver might not help.","comment_id":"218203"},{"comment_id":"68583","poster":"[Removed]","upvote_count":"12","content":"this will help us understand the reason, when we know that the data is not reaching subscriber then there is no point in checking it with dummy data","timestamp":"1616849460.0"},{"timestamp":"1626129960.0","poster":"tprashanth","upvote_count":"25","comments":[{"upvote_count":"1","comment_id":"738084","content":"Please refer to this PubSub specific Monitoring metrics https://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog","poster":"jkhong","timestamp":"1701964920.0"},{"comments":[{"content":"Exactly!","timestamp":"1686767820.0","comment_id":"616354","poster":"ritinhabb","upvote_count":"1"}],"poster":"mikey007","upvote_count":"2","content":"https://cloud.google.com/pubsub/docs/monitoring","comment_id":"142010","timestamp":"1627043820.0"}],"comment_id":"133299","content":"B.\nStack driver monitoring is for performance, not logging of missing data."}],"upvote_count":"35","content":"Answer: C\nDescription: Stackdriver can be used to check the error like number of unack messages, publisher pushing messages faster","poster":"[Removed]","comment_id":"68581"},{"content":"Should be B","poster":"[Removed]","comments":[{"poster":"[Removed]","content":"confused with D as well.","timestamp":"1616252280.0","comments":[{"timestamp":"1674676140.0","upvote_count":"3","content":"push or pull is about how target will handle the messages. pull mode gives flexibility when to get messages , so considering if target (or client) is slow, then it can make predictable choices.\n\ndataflow is serverless. so if you need to awake it when necessary, you should use push mechanism. or leverage cloud composer/airflow and listen to pub/sub to trigger the dataflow.","comment_id":"532373","poster":"Tanzu"},{"poster":"jvg637","content":"pushing is only for a https endpoint. So Dataflow just can pull messages","upvote_count":"4","timestamp":"1616448180.0","comment_id":"67093"},{"content":"Push or Pull guarantees the message to be delivered at-least once. So it doesn't make any difference.","comment_id":"66627","upvote_count":"6","timestamp":"1616350980.0","poster":"Rajokkiyam","comments":[{"comment_id":"308575","upvote_count":"3","content":"Push needs Https endpoint","poster":"gopinath_k","timestamp":"1647063300.0"}]}],"comment_id":"66286","upvote_count":"1"}],"comment_id":"66282","timestamp":"1616251320.0","upvote_count":"25"},{"comment_id":"1050562","comments":[{"poster":"ruben82","content":"You must know what kind of data causes errors. I think, the first step is to get erroneous data and then test with sample of it.","upvote_count":"1","timestamp":"1730371800.0","comment_id":"1058697"}],"content":"Selected Answer: B\nB. Run a fixed dataset through the Cloud Dataflow pipeline and analyze the output.\n* By running a fixed dataset through the Cloud Dataflow pipeline, you can determine if the problem lies within the data processing stage. This allows you to identify any issues with data transformation, filtering, or processing in your pipeline.\n* Analyzing the output from this fixed dataset will help you isolate the problem and confirm whether it's related to data processing or the dashboard application.","upvote_count":"1","timestamp":"1729600500.0","poster":"rtcpost"},{"upvote_count":"1","content":"B. Run a fixed dataset through the Cloud Dataflow pipeline and analyze the output. If this results in the expected output, then the problem might be with the dashboard application (Option A), and that should be checked next.","timestamp":"1728290040.0","comment_id":"1027204","poster":"imran79"},{"upvote_count":"3","comment_id":"919136","timestamp":"1717925220.0","content":"Selected Answer: B\n\"...to find the missing messages\"\nUp to that remark, Monitoring was a valid option as well. But missing messages cannot be found with monitoring.\nIt is simply not possible to find the exact missing message. I read this remark as a test if you know what is, and what isn't possible with monitoring.","poster":"WillemHendr"},{"upvote_count":"2","comments":[{"content":"B is not the next step. The next step is between pub/sub ands dataflow(C). B will not help with it at all. However it could show the issue if it is the pipeline or the view. But also it could not show it - you have no idea why some messages are not shown, so most probably it wouldnt get you any info. Definitely next step is to chek if the issue is between pubsub and dataflow. Then you coud go with B.","upvote_count":"1","poster":"Jarek7","timestamp":"1714669560.0","comment_id":"887718"}],"timestamp":"1712834160.0","poster":"izekc","content":"Selected Answer: B\nhere is to determine next step. Not better way to optimize the workload. So B is the correct next step","comment_id":"867236"},{"comments":[{"upvote_count":"1","timestamp":"1714669860.0","comment_id":"887722","content":"It could be the issue. But C would reveal it if this is the real issue - if you will not check stackdriver, you cannot be sure if you really resolved the issue, as even if it seems to be working properly after switch to pull you cannot be sure if it is because of some other temporal factor.","poster":"Jarek7"}],"comment_id":"866662","content":"Selected Answer: D\nPull subscription is the correct one. Push subscription means Dataflow cannot keep up with the topic.","timestamp":"1712788620.0","poster":"Adswerve","upvote_count":"1"},{"upvote_count":"4","poster":"midgoo","comment_id":"818701","timestamp":"1708656240.0","content":"Selected Answer: B\nIf the Dataflow does not have the expected output, it is either wrong at the input or at the pipelines. The chance that the issue is at the input (PubSub) is very low. For this case, it is likely the pipelines got some mistakes (e.g. JSON parsing failed). So we should follow B to debug the pipelines (using snapshot as test dataset for example)"},{"content":"Selected Answer: B\nThe most efficient solution would be to run a fixed dataset through the Cloud Dataflow pipeline and analyze the output (Option B). This will allow you to determine if the issue is with the pipeline or with the dashboard application. By analyzing the output, you can see if the messages are being processed correctly and determine if there are any discrepancies or missing messages. If the issue is with the pipeline, you can then debug and make any necessary updates to ensure that all messages are processed correctly. If the issue is with the dashboard application, you can then focus on resolving that issue. This approach allows you to isolate and identify the root cause of the missing messages in a controlled and efficient manner.","poster":"ploer","upvote_count":"7","timestamp":"1706959260.0","comment_id":"796981"},{"poster":"Lestrang","upvote_count":"7","timestamp":"1706538060.0","content":"Selected Answer: B\nI've just skimmed over the Stackdriver docs, yes guys, it helps you check the number and age of messages that were not received/acknowledged, excellent, hurray.\n\nSo first off, c will not give us the missing messages, it will give us the count and age.\nthat means that c is inherently incorrect. \n\nAdditionally, will knowledge of the number of messages make resolving the problem any easier? No, it is just confirming what we already know.\n\nMeanwhile, approach B, will allow us to see HOW and WHY it is missing some messages, which is the step that proceeds the fix.","comment_id":"791706"},{"comments":[{"timestamp":"1714670340.0","comment_id":"887737","poster":"Jarek7","upvote_count":"2","comments":[{"comment_id":"1076471","content":"Exactly. Sometimes it is total garbage","timestamp":"1732208100.0","upvote_count":"1","poster":"axantroff"}],"content":"I'm tired with these responses about what chatGPT says. Most probably you've used the free 3.5 version which is absolute disaster regarding being all knowing oracle. BTW in this case I wouldn't believe even GPT4. It is a difficult question that needs a specific knowledge and experience which might be not available in the GPT training data. You cannot use any GPT up to 4 as an argument in such cases."},{"poster":"Lestrang","timestamp":"1706538240.0","comment_id":"791710","upvote_count":"4","content":"I provided it with the question as input but added the metrics available in Stackdriver, here is the response:\n\nB. Run a fixed dataset through the Cloud Dataflow pipeline and analyze the output.\n\nIf messages are being published successfully to Cloud Pub/Sub but are missing in the dashboard, the issue is likely to be with the Cloud Dataflow pipeline that processes the messages. To find the root cause of the problem, you should run a fixed dataset through the pipeline and analyze the output. This will allow you to see if the pipeline is correctly processing all messages, and identify any processing errors that might be causing messages to be lost. The output can be compared to the expected results to identify any discrepancies and resolve the issue."}],"comment_id":"788617","content":"Selected Answer: C\nHere is ChatGPT answer : \nIt's always a good practice to start by checking the logs and monitoring tools to see if there is any indication of an issue with the messages being published to Cloud Pub/Sub. In this case, you should use Google Stackdriver Monitoring to investigate if the missing messages have been published or not. You can also run a fixed dataset through the Cloud Dataflow pipeline to see if the pipeline is processing the messages correctly. If there is no issue found on the Cloud Pub/Sub and Cloud Dataflow, then you can check the dashboard application to see if it is not displaying the messages correctly. As a last resort, you can switch Cloud Dataflow to pull messages from Cloud Pub/Sub instead of Cloud Pub/Sub pushing messages to Cloud Dataflow.","poster":"PolyMoe","timestamp":"1706265480.0","upvote_count":"2"},{"upvote_count":"4","timestamp":"1705598280.0","content":"The question is really not asking for a solution to the problem, per se - but more of what would the next step in the situation to triage the issue.... \n\nAnswer would be B over D. Answer D would be the recommended solution IF the question asked to rectify/fixed the issue.\n\nThoughts?","comment_id":"780245","poster":"desertlotus1211","comments":[{"timestamp":"1712834220.0","upvote_count":"1","poster":"izekc","content":"Agree with u","comment_id":"867237"}]},{"upvote_count":"3","content":"Selected Answer: D\nD. Dataflow must PULL the data to process it in real-time. Missing messages in the dashboard, means that the Pub/Sub to Dataflow was misconfigured as PUSH.","poster":"Prakzz","comments":[{"timestamp":"1705764420.0","upvote_count":"1","poster":"hasoweh","content":"Pull will lead to latency as new data will not be streamed upon arrival, but instead will only be passed on when Dataflow makes a pull request. So if data comes in at time 0:01 but pull requests are only happening every 10 seconds, we have 9 second delay. Push will automatically push the data to any subscribers as soon as the data comes, and thus is closer to real-time.","comment_id":"782411"}],"timestamp":"1703035440.0","comment_id":"750380"},{"timestamp":"1703025540.0","poster":"Krish6488","content":"Selected Answer: B\nTo me, B sounds more logical for the below reason.\nOption C would have been ideal because any debugging starts with checking the logs, however the option says, check stackdriver for missing messages. Had it been, check stackdriver to figure out the number of undelivered messages, C would have been more suitable. Given the slight bit of dodginess in option c, I would go with B","upvote_count":"3","comment_id":"750297"},{"comment_id":"744170","upvote_count":"1","poster":"Nirca","timestamp":"1702479780.0","content":"Selected Answer: B\nWhy checking Pub/Sub again when this is already verified to be fine according to the question. Shouldn't you be checking the next stage in the flow which is Dataflow? \nOption - B"},{"poster":"DGames","upvote_count":"1","comment_id":"743505","content":"Selected Answer: B\nAnswer - B. Because already we know message is missing so better to test with fixed dataset and check code .","timestamp":"1702433880.0"},{"comment_id":"730800","upvote_count":"1","timestamp":"1701287580.0","poster":"Atnafu","content":"C\nhttps://cloud.google.com/pubsub/docs/monitoring#:~:text=the%20specific%20metrics.-,Monitor%20message%20backlog,information%20about%20this%20metric%2C%20see%20the%20relevant%20section%20of%20this%20document.,-Create%20alerting%20policies"},{"upvote_count":"5","poster":"Jay_Krish","comments":[{"content":"+1, Sound and concise reasoning","poster":"NicolasN","comment_id":"742604","timestamp":"1702373460.0","upvote_count":"1"}],"content":"Selected Answer: B\nWhy checking Pub/Sub again when this is already verified to be fine according to the question. Shouldn't you be checking the next stage in the flow which is Dataflow? Option - B","comment_id":"722205","timestamp":"1700422980.0"},{"upvote_count":"1","comment_id":"643967","timestamp":"1691474940.0","poster":"soume","content":"Answer : C\nBased to the documentation, with can monitor the ack message from the subscription (subscription/ack_latencies) https://cloud.google.com/monitoring/api/metrics_gcp#gcp-pubsub"},{"poster":"Rahul95","timestamp":"1690008300.0","content":"Answer B, You can't se the missing records using stackdriver you can only see logs.","upvote_count":"2","comment_id":"635036"},{"poster":"thapliyal","upvote_count":"1","content":"Ansewer is C. Just read this google doc https://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog","timestamp":"1687787340.0","comment_id":"622598"},{"timestamp":"1680533880.0","comment_id":"580343","poster":"Deepakd","content":"Answer is B.\nStackdriver will tell the count of messages not delivered. It wont tell why they are not being delivered. To understand the RCA , the job needs to be executed with fixed set of data.","upvote_count":"2"},{"content":"Answer:B\nMonitoring is used for metrics. Metrics wont tell us what the missing data is.","timestamp":"1678678860.0","poster":"popuri_10","comment_id":"566520","upvote_count":"1"},{"upvote_count":"3","poster":"Arkon88","timestamp":"1677859020.0","comment_id":"560192","content":"Selected Answer: C\nC as mentioned in https://cloud.google.com/pubsub/docs/monitoring\n\nMonitoring the backlog\nTo ensure that your subscribers are keeping up with the flow of messages, create a dashboard that shows the following backlog metrics, aggregated by resource, for all your subscriptions:\n\nsubscription/num_undelivered_messages to see the number of unacknowledged messages\nsubscription/oldest_unacked_message_age to see the age of the oldest unacknowledged message in the subscription's backlog"},{"content":"Selected Answer: B\nStack driver will tell us about performance but not the details of missing data.","timestamp":"1677622560.0","comment_id":"558388","upvote_count":"1","poster":"Prathamesh2612"},{"timestamp":"1676205780.0","comment_id":"545838","content":"Answer is B Run a fixed dataset through the Cloud Dataflow pipeline and analyze the output.\n\nStack driver could tell us about performance but not logging of missing data.\nReference:\nhttps://cloud.google.com/pubsub/docs/monitoring","poster":"VishalBule","upvote_count":"1"},{"comment_id":"523140","comments":[{"comment_id":"771074","timestamp":"1704863880.0","content":"You can only see the number of unack message instead of which message is unacknowledged","poster":"ler_mp","upvote_count":"1"}],"poster":"sraakesh95","content":"Selected Answer: C\nReference: https://cloud.google.com/pubsub/docs/monitoring\nQuoted text: \nMonitoring the backlog\nTo ensure that your subscribers are keeping up with the flow of messages, create a dashboard that shows the following backlog metrics, aggregated by resource, for all your subscriptions:\n\nsubscription/num_undelivered_messages to see the number of unacknowledged messages\nsubscription/oldest_unacked_message_age to see the age of the oldest unacknowledged message in the subscription's backlog","upvote_count":"3","timestamp":"1673645160.0"},{"upvote_count":"9","comment_id":"478173","poster":"MaxNRG","content":"B as the issue can be debugged by running a fixed dataset and checking the output.\nRefer GCP documentation - Dataflow logging:\nhttps://cloud.google.com/dataflow/docs/guides/logging\nA is wrong as the Dashboard uses data provided by Dataflow, the input source for Dashboard seems to be the issue\nC is wrong as Monitoring will not help find missing messages in Cloud Pub/Sub\nD is wrong as Dataflow cannot be configured as Push endpoint with Cloud Pub/Sub.","timestamp":"1668437340.0"},{"content":"B is correct.","upvote_count":"1","comment_id":"469242","timestamp":"1666958880.0","poster":"Biju1","comments":[{"content":"The question is \"What should you do next?\".\nB can be the last step, after checking with Stackdrive Monitoring that all messages are received by the subscriber (option C)","upvote_count":"2","comment_id":"470300","poster":"sergio6","timestamp":"1667151000.0","comments":[{"timestamp":"1700149620.0","poster":"wan2three","content":"this should be the right logic of this Q","comment_id":"719806","upvote_count":"1"}]}]},{"timestamp":"1665589920.0","comment_id":"461141","poster":"anji007","upvote_count":"1","content":"Ans: B"},{"comment_id":"441895","timestamp":"1662718080.0","content":"What I would really do is go to the DataFlow job and check that there are no errors happening there","comments":[{"timestamp":"1662869040.0","upvote_count":"1","poster":"kubosuke","content":"lol I'll do same as you","comment_id":"442805"}],"upvote_count":"6","poster":"michaelkhan3"},{"content":"Vote for 'B'","poster":"sumanshu","upvote_count":"2","comment_id":"391588","timestamp":"1656281940.0"},{"timestamp":"1654819800.0","upvote_count":"3","comment_id":"378604","poster":"koupayio","content":"I vote B:\nC won't help when: message sent from Pub/Sub to Dataflow right, but messed inside Dataflow PTransform"},{"poster":"naga","comment_id":"285612","timestamp":"1644249720.0","upvote_count":"3","content":"Correct B"},{"content":"B as per Linux academy","poster":"tikna","timestamp":"1641831960.0","upvote_count":"7","comment_id":"264116"},{"upvote_count":"11","timestamp":"1641530220.0","comment_id":"261535","content":"Guys,Its confusing with so many discussion and the actual suggested answers being wrong, did anyone take an exam recently and if you think you got this question right, can you mark the confirmed answer","poster":"dakk"},{"comment_id":"244138","content":"B is correct.","upvote_count":"3","timestamp":"1639525560.0","poster":"NamitSehgal"},{"upvote_count":"5","comment_id":"220921","content":"Cannot be D and A. In pub sub monitoring https://cloud.google.com/pubsub/docs/monitoring\nwe can check for the number of undelivered messages and oldest unacked message age. However, this will not tell us what those messages are which are not going. Will just give the count. Hence I believe B is the answer using which we will know what any why those messages are not flowing through cloud data flow.","timestamp":"1637139240.0","poster":"Radhika7983"},{"poster":"Tanmoyk","content":"B is correct as per Linux Academy","timestamp":"1633583220.0","comment_id":"194866","upvote_count":"7"},{"poster":"KennneK","upvote_count":"7","comments":[{"timestamp":"1662869280.0","content":"oh cool agree with you if we just monitor them it doesn't resolve the issue","poster":"kubosuke","comment_id":"442809","upvote_count":"1"}],"content":"Prefer B.\nC is possible but you can imagine that's really inefficient. If you can find the missed message, which means you've already known the cause.\nI chose B because it is trying to repulicate the issue, and the actual cause should be the message is not in the format that Dataflow pipeline expected.","timestamp":"1630590840.0","comment_id":"172082"},{"comment_id":"162308","timestamp":"1629471300.0","content":"B' is correct.\nStack driver could tell us about performance but not missing info","upvote_count":"4","poster":"atnafu2020"},{"content":"B Correct.\nC Incorect. Stackdriver monitoring is for monitoring infraestructure.\nhttps://cloud.google.com/migrate/compute-engine/docs/4.8/how-to/monitoring/using-stackdriver-monitoring","poster":"haroldbenites","timestamp":"1629326760.0","upvote_count":"3","comment_id":"161134"},{"upvote_count":"2","comment_id":"133982","poster":"AshokC","timestamp":"1626184500.0","content":"C is correct"},{"timestamp":"1621614600.0","comment_id":"93525","comments":[{"content":"In Question it's mentioned - all messages are being published to Cloud Pub/Sub successfully.\nSo, I don't think it will make any impact to recheck pub/sub...Issue is in pipeline..So B is correct","comment_id":"401891","timestamp":"1657281960.0","upvote_count":"2","poster":"sumanshu"}],"content":"Answer C","upvote_count":"1","poster":"arnabbis4u"}],"question_images":[],"answer_description":"","unix_timestamp":1584715320,"answer_images":[],"answers_community":["B (74%)","C (17%)","9%"],"exam_id":10,"topic":"1","question_id":247,"answer_ET":"B","url":"https://www.examtopics.com/discussions/google/view/17054-exam-professional-data-engineer-topic-1-question-33/","answer":"B","timestamp":"2020-03-20 15:42:00","isMC":true,"choices":{"D":"Switch Cloud Dataflow to pull messages from Cloud Pub/Sub instead of Cloud Pub/Sub pushing messages to Cloud Dataflow.","B":"Run a fixed dataset through the Cloud Dataflow pipeline and analyze the output.","C":"Use Google Stackdriver Monitoring on Cloud Pub/Sub to find the missing messages.","A":"Check the dashboard application to see if it is not displaying correctly."},"question_text":"Your software uses a simple JSON format for all messages. These messages are published to Google Cloud Pub/Sub, then processed with Google Cloud\nDataflow to create a real-time dashboard for the CFO. During testing, you notice that some messages are missing in the dashboard. You check the logs, and all messages are being published to Cloud Pub/Sub successfully. What should you do next?"},{"id":"ibfILIIiKL7btUJ1H1Hc","discussion":[{"content":"Selected Answer: C\nC. Store the common data encoded as Avro in Google Cloud Storage.\n\nThis approach allows for interoperability between BigQuery and Hadoop/Spark as Avro is a commonly used data serialization format that can be read by both systems. Data stored in Google Cloud Storage can be accessed by both BigQuery and Dataproc, providing a bridge between the two environments. Additionally, you can set up data transformation pipelines in Dataproc to work with this data.","comment_id":"1050790","poster":"rtcpost","timestamp":"1697988000.0","upvote_count":"6"},{"poster":"iooj","comment_id":"1259026","timestamp":"1722448440.0","content":"Selected Answer: C\nin BigQuery we can use BigLake tables based on Avro for historical data, and Spark stored procedures","upvote_count":"1"},{"timestamp":"1718310480.0","content":"Data lake,fully managed, data analytics. Stores structured and unstructured data are keywords,so answer is GCS, OPTION C","upvote_count":"1","comment_id":"1230087","poster":"dhvanil"},{"comment_id":"1087727","timestamp":"1701701940.0","upvote_count":"4","poster":"JOKKUNO","content":"Given the scenario described for Flowlogistic's requirements and technical environment, the most suitable option for storing common data that is used by both Google BigQuery and Apache Hadoop/Spark workloads is:\n\nC. Store the common data encoded as Avro in Google Cloud Storage."},{"poster":"nescafe7","content":"Selected Answer: D\nTo simplify the question, Apache Hadoop and Spark workloads that cannot be moved to BigQuery can be handled by DataProc. So the correct answer is D.","comment_id":"967572","timestamp":"1690757820.0","upvote_count":"3"},{"poster":"Mathew106","timestamp":"1690186320.0","content":"Selected Answer: B\nB is the right answer. Common data will lie in BigQuery but will be accessible via the views with SQL in Hadoop workloads.","upvote_count":"2","comment_id":"961322"},{"content":"Selected Answer: B\nC should be the correct answer. However, please note that Google just released the BigQuery Connector for Hadoop, so if they ask the same question today, B will be the correct answer.\nA could be correct too, but I cannot see why it has to be partitioned","poster":"midgoo","timestamp":"1677132660.0","upvote_count":"4","comments":[{"content":"If you check the https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery, it unloads the BQ data to GCS, utilizes it, and then deletes it from the GCS. Storing common data twice (at BQ and GCS) will not be the best option compared to 'C' (using GCS as the main common dataset).","upvote_count":"3","comment_id":"942687","timestamp":"1688470260.0","poster":"res3"}],"comment_id":"818845"},{"upvote_count":"1","content":"Selected Answer: C\nI would vote for C as it can be used for analysis with Bigquery. Furthermore, Hadoop workload can also be transferred to dataproc connected to GCS.","timestamp":"1673140320.0","poster":"korntewin","comment_id":"769007"},{"poster":"DGames","timestamp":"1670898660.0","comment_id":"743516","upvote_count":"1","content":"Selected Answer: B\nAnswer B look ok , because in question they want to store common data which can use by both workload, and using big query and primary analytical tool that would be best option and easy to analysis common data."},{"comments":[{"content":"I thought you can now store unstructured data in BigQuery via the object tables announced during Google NEXT 2022... If that's possib;e, does that make B a better choice?","comment_id":"763233","timestamp":"1672585380.0","upvote_count":"1","poster":"tunstila"}],"comment_id":"731620","content":"\"Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data\" - BigQuery cant take unstructured data so A and B are out.\nStoring data in HDFS storage is never recommended unless latency is a requirement, so D is out.\n\nThat leaves us with GCS. Answer is C","timestamp":"1669819680.0","upvote_count":"3","poster":"kelvintoys93"},{"comment_id":"721297","poster":"drunk_goat82","content":"Selected Answer: C\nBigQuery can use federated queries to connect to the avro data in GCS while running spark jobs on it. If you duplicate the date you have to manage both data sets.","upvote_count":"2","timestamp":"1668777960.0"},{"content":"A\nThey wanted BigQuery. And connector is all you need to perform Hadoop or spark. Hadoop migration can be done using dataproc.","upvote_count":"1","timestamp":"1668615180.0","comments":[{"comment_id":"719823","upvote_count":"2","content":"Also apparently they want all data at one place and want bigQ","timestamp":"1668615240.0","poster":"wan2three"}],"poster":"wan2three","comment_id":"719822"},{"content":"Selected Answer: C\nC as it can be used as an external table from BigQuery and with the Cloud Storage Connector it can be used by the Spark workloads (running in Dataproc)","comment_id":"719733","upvote_count":"1","poster":"gudiking","timestamp":"1668608880.0"},{"comment_id":"718021","timestamp":"1668436200.0","content":"Selected Answer: C\nC, as both capable of AVRO, but the customer does not know what they want to do with the data yet.","upvote_count":"1","poster":"solar_maker"},{"comment_id":"710912","content":"Selected Answer: D\nIn Technical requirements it Was clearly mentioned that they need to Migrate existing Hadoop Cluster for which Data Proc Cluster is a replacement.","upvote_count":"1","poster":"Leelas","timestamp":"1667537760.0"},{"poster":"vishal0202","timestamp":"1663760940.0","comment_id":"675089","content":"C is ans...avro data can be accessed by spark as well","upvote_count":"4"},{"content":"Selected Answer: C\nThe answer is C","comment_id":"657882","timestamp":"1662158640.0","poster":"ducc","upvote_count":"3"}],"question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0001900008.png"],"topic":"1","answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/79622-exam-professional-data-engineer-topic-1-question-34/","question_text":"Flowlogistic Case Study -\n\nCompany Overview -\nFlowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.\n\nCompany Background -\nThe company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.\n\nSolution Concept -\nFlowlogistic wants to implement two concepts using the cloud:\n✑ Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads\n✑ Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.\n\nExisting Technical Environment -\nFlowlogistic architecture resides in a single data center:\n✑ Databases\n8 physical servers in 2 clusters\n- SQL Server `\" user data, inventory, static data\n3 physical servers\n- Cassandra `\" metadata, tracking messages\n10 Kafka servers `\" tracking message aggregation and batch insert\n✑ Application servers `\" customer front end, middleware for order/customs\n60 virtual machines across 20 physical servers\n- Tomcat `\" Java services\n- Nginx `\" static content\n- Batch servers\n✑ Storage appliances\n- iSCSI for virtual machine (VM) hosts\n- Fibre Channel storage area network (FC SAN) `\" SQL server storage\n- Network-attached storage (NAS) image storage, logs, backups\n✑ 10 Apache Hadoop /Spark servers\n- Core Data Lake\n- Data analysis workloads\n✑ 20 miscellaneous servers\n- Jenkins, monitoring, bastion hosts,\n\nBusiness Requirements -\nBuild a reliable and reproducible environment with scaled panty of production.\n//IMG//\n\n✑ Aggregate data in a centralized Data Lake for analysis\n✑ Use historical data to perform predictive analytics on future shipments\n✑ Accurately track every shipment worldwide using proprietary technology\n✑ Improve business agility and speed of innovation through rapid provisioning of new resources\n✑ Analyze and optimize architecture for performance in the cloud\n✑ Migrate fully to the cloud if all other requirements are met\n\nTechnical Requirements -\n✑ Handle both streaming and batch data\n✑ Migrate existing Hadoop workloads\n✑ Ensure architecture is scalable and elastic to meet the changing demands of the company.\n✑ Use managed services whenever possible\n✑ Encrypt data flight and at rest\n✑ Connect a VPN between the production data center and cloud environment\n\nSEO Statement -\nWe have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.\nWe need to organize our information so we can more easily understand where our customers are and what they are shipping.\n\nCTO Statement -\nIT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.\n\nCFO Statement -\nPart of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.\nFlowlogistic wants to use Google BigQuery as their primary analysis system, but they still have Apache Hadoop and Spark workloads that they cannot move to\nBigQuery. Flowlogistic does not know how to store the data that is common to both workloads. What should they do?","answer":"C","isMC":true,"unix_timestamp":1662158640,"choices":{"D":"Store he common data in the HDFS storage for a Google Cloud Dataproc cluster.","B":"Store the common data in BigQuery and expose authorized views.","A":"Store the common data in BigQuery as partitioned tables.","C":"Store the common data encoded as Avro in Google Cloud Storage."},"exam_id":10,"answer_description":"","answer_ET":"C","question_id":248,"answers_community":["C (58%)","B (27%)","D (15%)"],"timestamp":"2022-09-03 00:44:00"},{"id":"vHQ1NO4S39hAVMtYVG9T","answer_ET":"A","exam_id":10,"answer_description":"","answers_community":["A (90%)","10%"],"question_text":"Flowlogistic Case Study -\n\nCompany Overview -\nFlowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.\n\nCompany Background -\nThe company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.\n\nSolution Concept -\nFlowlogistic wants to implement two concepts using the cloud:\n✑ Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads\n✑ Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.\n\nExisting Technical Environment -\nFlowlogistic architecture resides in a single data center:\n✑ Databases\n8 physical servers in 2 clusters\n- SQL Server `\" user data, inventory, static data\n3 physical servers\n- Cassandra `\" metadata, tracking messages\n10 Kafka servers `\" tracking message aggregation and batch insert\n✑ Application servers `\" customer front end, middleware for order/customs\n60 virtual machines across 20 physical servers\n- Tomcat `\" Java services\n- Nginx `\" static content\n- Batch servers\n✑ Storage appliances\n- iSCSI for virtual machine (VM) hosts\n- Fibre Channel storage area network (FC SAN) `\" SQL server storage\n- Network-attached storage (NAS) image storage, logs, backups\n✑ 10 Apache Hadoop /Spark servers\n- Core Data Lake\n- Data analysis workloads\n✑ 20 miscellaneous servers\n- Jenkins, monitoring, bastion hosts,\n\nBusiness Requirements -\n✑ Build a reliable and reproducible environment with scaled panty of production.\n✑ Aggregate data in a centralized Data Lake for analysis\n✑ Use historical data to perform predictive analytics on future shipments\n✑ Accurately track every shipment worldwide using proprietary technology\n✑ Improve business agility and speed of innovation through rapid provisioning of new resources\n✑ Analyze and optimize architecture for performance in the cloud\n✑ Migrate fully to the cloud if all other requirements are met\n\nTechnical Requirements -\n✑ Handle both streaming and batch data\n✑ Migrate existing Hadoop workloads\n✑ Ensure architecture is scalable and elastic to meet the changing demands of the company.\n✑ Use managed services whenever possible\n✑ Encrypt data flight and at rest\n✑ Connect a VPN between the production data center and cloud environment\n\nSEO Statement -\nWe have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.\nWe need to organize our information so we can more easily understand where our customers are and what they are shipping.\n\nCTO Statement -\nIT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.\n\nCFO Statement -\nPart of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.\nFlowlogistic's management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system.\nYou need to build a new system on Google Cloud Platform (GCP) that will feed the proprietary tracking software. The system must be able to ingest data from a variety of global sources, process and query in real-time, and store the data reliably. Which combination of GCP products should you choose?","discussion":[{"comment_id":"64253","timestamp":"1600165080.0","poster":"jvg637","upvote_count":"38","content":"I would say A.\nI think Pub/Sub can't directly send data to Cloud SQL."},{"content":"Answer: A","comment_id":"66291","timestamp":"1600608780.0","upvote_count":"15","poster":"[Removed]"},{"upvote_count":"1","comment_id":"1212586","poster":"billalltf","content":"Selected Answer: A\nA is right answer","timestamp":"1731799020.0"},{"comment_id":"1087729","content":"Given the requirements for ingesting data from global sources, processing and querying in real-time, and storing the data reliably for the real-time inventory tracking system, the most suitable combination of Google Cloud Platform (GCP) products is:\nA. Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage\nExplanation:\nCloud Pub/Sub: It is a messaging service that allows you to asynchronously send and receive messages between independent applications.\nCloud Dataflow: It can handle both streaming and batch data, making it suitable for real-time processing of data from various sources.\nCloud Storage: Cloud Storage can be used to store the processed and analyzed data reliably. It provides scalable, durable, and globally accessible object storage, making it suitable for storing large volumes of data.","timestamp":"1717506180.0","poster":"JOKKUNO","upvote_count":"1"},{"poster":"rtcpost","upvote_count":"2","comment_id":"1050791","timestamp":"1713799260.0","content":"Selected Answer: A\nA. Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage\n\nHere's why this combination is suitable:\n\nCloud Pub/Sub: It is used for ingesting real-time data from various global sources. It's a messaging service that can handle large volumes of data and is highly scalable.\n\nCloud Dataflow: It's a stream and batch data processing service that allows you to process and analyze the data in real-time. It can take data from Pub/Sub and perform transformations or aggregations as needed.\n\nCloud Storage: It provides reliable storage for the data. You can store the processed data in Cloud Storage for further analysis, and it is a scalable and durable storage solution.\n\nOption B is not ideal because Local SSDs are not a suitable storage option for persisting data that needs to be reliably stored. Option C includes Cloud SQL, which is not typically used for ingesting and processing real-time data. Option D includes Cloud Load Balancing, which is not relevant to the use case of ingesting and processing data for the inventory tracking system."},{"timestamp":"1706280000.0","upvote_count":"1","poster":"Vipul1600","content":"Since Cloud SQL is fully managed service & Dataflow is serverless hence we should opt for dataflow as it is thumb rule for google that we should choose serverless product over fully managed service.","comment_id":"963715"},{"content":"Selected Answer: A\nThe technical requirements mention that the pipeline should handle both streaming and batch data. The solution should include DataFlow and not Cloud SQL. the answer is A.","upvote_count":"2","comment_id":"955329","timestamp":"1705585260.0","poster":"Mathew106"},{"comment_id":"808309","poster":"niketd","timestamp":"1692005220.0","content":"Selected Answer: A\nPub/Sub to scale streaming data, Dataflow to processes both structured and unstructured data and cloud storage to store common data","upvote_count":"1"},{"content":"Selected Answer: A\nOption B. Cloud Pub/Sub, Cloud Dataflow, and Local SSD is not a good option as Local SSD is not a scalable solution and could not handle large amount of data\nOption C. Cloud Pub/Sub, Cloud SQL, and Cloud Storage is not a good option as Cloud SQL is a relational database and is not suitable for real-time processing and querying large amounts of data\nOption D. Cloud Load Balancing, Cloud Dataflow, and Cloud Storage is not a good option as Cloud Load Balancing is used for distributing traffic across multiple instances, it doesn't handle data processing and storage.","timestamp":"1690361400.0","comment_id":"788628","upvote_count":"1","poster":"PolyMoe"},{"timestamp":"1690361340.0","comment_id":"788625","upvote_count":"1","poster":"PolyMoe","content":"Selected Answer: A\nA. Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage is the best combination of GCP products for the use case described.\nCloud Pub/Sub can be used to ingest data from a variety of global sources, as it allows for easy integration with external systems through its publish-subscribe messaging model.\nCloud Dataflow can be used to process and query the data in real-time, as it is a fully managed service for creating data pipelines that can handle both batch and streaming data.\nCloud Storage can be used to store the data reliably, as it is a fully managed object storage service that can handle large amounts of data and is highly durable and available."},{"timestamp":"1689580380.0","content":"Selected Answer: A\nAnswer is A. Cloud Dataflow for batch + streaming, Cloud Pub/Sub for streaming ingestion, Cloud Storage for long term data storage.","poster":"jkh_goh","upvote_count":"1","comment_id":"778764"},{"poster":"Jay_Krish","comment_id":"722208","content":"Are scenario based questions still in the latest exam?? Are these still relevant?","upvote_count":"2","timestamp":"1684518780.0"},{"poster":"kastuarr","upvote_count":"2","timestamp":"1681842780.0","content":"Selected Answer: C\nExisting inventory data is in SQL, data ingested from Kafka will need to update inventory at some point. Existence of SQL in current estate indicates SQL must be present in the Cloud estate","comment_id":"698441"},{"content":"This site make me feel that it intends to make users to be involved in discussion by suggesting wrong answer","upvote_count":"9","timestamp":"1678834260.0","comment_id":"669324","poster":"Dhamsl"},{"timestamp":"1676092020.0","poster":"Megmang","comment_id":"645242","upvote_count":"3","content":"Selected Answer: A\nAnswer is clearly option A."},{"timestamp":"1673649240.0","content":"why are there so many incorrect answers? it's so hard to study this way","poster":"[Removed]","comment_id":"631059","upvote_count":"6"},{"poster":"ratnesh99","content":"Answer A : because Cloud Sql not suitable for Global","timestamp":"1672999020.0","comment_id":"627783","upvote_count":"1"},{"poster":"CedricLP","comment_id":"585156","upvote_count":"2","timestamp":"1665658980.0","content":"Selected Answer: A\nOnly A can manage a lot's of data.\nTarget is Cloud Storage (obviously not SSD)\nInput is Pub/Sub to replace Kafka\nCloud SQL + Storage has no sense in this context"},{"comment_id":"560219","poster":"Arkon88","upvote_count":"4","content":"Selected Answer: A\nA. Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage\nas explained by JayZeeLee :\nB is incorrect, because local SSD wouldn't satisfy the needs.\nC is incorrect, because one of the requirements is 'Global', Cloud SQL is well suited for regional applications. Cloud Spanner is a better suit in that regard.\nD is incorrect, because Load Balancer is for web traffic, not messages.","timestamp":"1662215820.0"},{"comment_id":"534887","timestamp":"1659025560.0","poster":"kped21","content":"I should be A","upvote_count":"1"},{"content":"Selected Answer: A\nas explained by JayZeeLee","upvote_count":"1","timestamp":"1656929280.0","poster":"medeis_jar","comment_id":"516553"},{"content":"Its A\nIngest Data from Global Resouces - Pub/Sub\nProcess data in real-time - Dataflow\nStore data reliably - Cloud Storage","timestamp":"1655621520.0","comment_id":"504733","poster":"kishanu","upvote_count":"1"},{"timestamp":"1651530060.0","content":"A. \nB is incorrect, because local SSD wouldn't satisfy the needs. \nC is incorrect, because one of the requirements is 'Global', Cloud SQL is well suited for regional applications. Cloud Spanner is a better suit in that regard.\nD is incorrect, because Load Balancer is for web traffic, not messages.","poster":"JayZeeLee","comment_id":"471861","upvote_count":"2"},{"content":"Ans C : They need at least 1 relational DB (cloud SQL) since they have inventory and customer data on SQL server","timestamp":"1651006020.0","upvote_count":"1","comment_id":"468253","poster":"SonuKhan1"},{"poster":"anji007","comment_id":"461138","timestamp":"1649778660.0","upvote_count":"1","content":"Ans: A"},{"timestamp":"1648382700.0","content":"a would say A, since we must stream real-time data , so we use Pub/sub + dataProc, and we have structured and not structured data, so we use GCS","upvote_count":"3","poster":"Ysance_AGS","comment_id":"452393"},{"timestamp":"1646831160.0","comments":[{"timestamp":"1654330500.0","poster":"BigQuery","upvote_count":"2","comments":[{"timestamp":"1654468260.0","content":"yo lmk how it is","poster":"sup12345","upvote_count":"1","comment_id":"494759"}],"comment_id":"493638","content":"bro all the questions are like this I feel completely off having exam scheduled in 2 days with these type of answers."}],"poster":"michaelkhan3","content":"What???\nHow does the data get from Pub/Sub to CloudSQL? What is the point of Cloud storage if you are going to store the data in CloudSQL?\nWho comes up with these answers?","upvote_count":"6","comment_id":"441921"},{"upvote_count":"4","comment_id":"391598","content":"Vote for 'A'\n\nBut, it looks, Flowlogistic Case Study now removed from syllabus of PDE\n\nhttps://cloud.google.com/certification/guides/data-engineer","poster":"sumanshu","timestamp":"1640565900.0"},{"timestamp":"1636620480.0","upvote_count":"2","poster":"jasper_430","comment_id":"354405","content":"I think it should be A since it requires process and query in \"real-time\", which should use DataFlow instead of CloudSQL."},{"comment_id":"294383","timestamp":"1629381360.0","poster":"Enam","upvote_count":"3","content":"A, pubsub/dataflow and cloud storage. Cloud storage is preferred for staging solution"},{"comment_id":"285620","poster":"naga","content":"Correct C","timestamp":"1628346300.0","upvote_count":"1"},{"content":"Do we really have this kind of questions? It would take 10 mins to read and understand tthe question itself","comment_id":"274547","timestamp":"1627034760.0","upvote_count":"5","poster":"[Removed]"},{"upvote_count":"3","comment_id":"252190","poster":"muraric","content":"I like the answer A. I doubt if pub/sub can send data directly to CloudSQL","timestamp":"1624634040.0"},{"poster":"Wee99","content":"C as \"process and query\", Cloud SQL for query","comment_id":"223610","upvote_count":"2","timestamp":"1621509900.0"},{"poster":"Radhika7983","comment_id":"220965","upvote_count":"5","comments":[{"timestamp":"1646527380.0","comment_id":"439971","poster":"Ral17","content":"What about query in real time?","upvote_count":"1"}],"content":"The answer is A. Cloud pub sub for real time processing, cloud data flow for real time streaming data processing and loading data into cloud storage.","timestamp":"1621239960.0"},{"timestamp":"1617351960.0","comment_id":"191486","upvote_count":"4","content":"Its A,https://cloud.google.com/blog/products/data-analytics/streaming-analytics-now-simpler-more-cost-effective-cloud-dataflow","poster":"TNT87"},{"poster":"SteelWarrior","timestamp":"1616058180.0","content":"Answer should be A. \n. Ingest data from variety of sources -> Cloud Pub/Sub\n. Real-time processing - Cloud DataFlow\n. Store Reliably - Google Cloud Storage","comment_id":"181442","upvote_count":"7"},{"content":"Answer should be A - Pub/Sub -> Dataflow for real time processing requirements. https://codelabs.developers.google.com/codelabs/cpb104-pubsub/#0","upvote_count":"4","timestamp":"1608099780.0","comment_id":"111303","poster":"jai_examtopics"},{"content":"From the given options, C is the only one that satisfy \"query in real-time\". And you can use cloud functions to ingest into cloud sql directly from pub/sub","comment_id":"93599","poster":"willbot","timestamp":"1605998880.0","upvote_count":"3"},{"timestamp":"1603035420.0","upvote_count":"7","comment_id":"76111","poster":"itche_scratche","content":"A; pub/sub->dataflow (where you can do real-time analytic)->BQ/GCS"}],"answer":"A","answer_images":[],"unix_timestamp":1584274680,"question_images":[],"url":"https://www.examtopics.com/discussions/google/view/16658-exam-professional-data-engineer-topic-1-question-35/","topic":"1","question_id":249,"isMC":true,"timestamp":"2020-03-15 13:18:00","choices":{"B":"Cloud Pub/Sub, Cloud Dataflow, and Local SSD","C":"Cloud Pub/Sub, Cloud SQL, and Cloud Storage","A":"Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage","D":"Cloud Load Balancing, Cloud Dataflow, and Cloud Storage"}},{"id":"GupSd4Es2XnMgc2vc4R3","answer_description":"","isMC":true,"answer_ET":"C","exam_id":10,"timestamp":"2020-03-20 16:36:00","unix_timestamp":1584718560,"choices":{"C":"Create a view on the table to present to the virtualization tool.","A":"Export the data into a Google Sheet for virtualization.","B":"Create an additional table with only the necessary columns.","D":"Create identity and access management (IAM) roles on the appropriate columns, so only they appear in a query."},"question_id":250,"answer_images":[],"url":"https://www.examtopics.com/discussions/google/view/17058-exam-professional-data-engineer-topic-1-question-36/","question_text":"Flowlogistic Case Study -\n\nCompany Overview -\nFlowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.\n\nCompany Background -\nThe company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.\n\nSolution Concept -\nFlowlogistic wants to implement two concepts using the cloud:\nUse their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads\n//IMG//\n\n✑ Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.\n\nExisting Technical Environment -\nFlowlogistic architecture resides in a single data center:\n✑ Databases\n8 physical servers in 2 clusters\n- SQL Server `\" user data, inventory, static data\n3 physical servers\n- Cassandra `\" metadata, tracking messages\n10 Kafka servers `\" tracking message aggregation and batch insert\n✑ Application servers `\" customer front end, middleware for order/customs\n60 virtual machines across 20 physical servers\n- Tomcat `\" Java services\n- Nginx `\" static content\n- Batch servers\n✑ Storage appliances\n- iSCSI for virtual machine (VM) hosts\n- Fibre Channel storage area network (FC SAN) `\" SQL server storage\n- Network-attached storage (NAS) image storage, logs, backups\n✑ 10 Apache Hadoop /Spark servers\n- Core Data Lake\n- Data analysis workloads\n✑ 20 miscellaneous servers\n- Jenkins, monitoring, bastion hosts,\n\nBusiness Requirements -\n✑ Build a reliable and reproducible environment with scaled panty of production.\n✑ Aggregate data in a centralized Data Lake for analysis\n✑ Use historical data to perform predictive analytics on future shipments\n✑ Accurately track every shipment worldwide using proprietary technology\n✑ Improve business agility and speed of innovation through rapid provisioning of new resources\n✑ Analyze and optimize architecture for performance in the cloud\n✑ Migrate fully to the cloud if all other requirements are met\n\nTechnical Requirements -\nHandle both streaming and batch data\n//IMG//\n\n✑ Migrate existing Hadoop workloads\n✑ Ensure architecture is scalable and elastic to meet the changing demands of the company.\n✑ Use managed services whenever possible\n✑ Encrypt data flight and at rest\n✑ Connect a VPN between the production data center and cloud environment\n\nSEO Statement -\nWe have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.\nWe need to organize our information so we can more easily understand where our customers are and what they are shipping.\n\nCTO Statement -\nIT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.\n\nCFO Statement -\nPart of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.\nFlowlogistic's CEO wants to gain rapid insight into their customer base so his sales team can be better informed in the field. This team is not very technical, so they've purchased a visualization tool to simplify the creation of BigQuery reports. However, they've been overwhelmed by all the data in the table, and are spending a lot of money on queries trying to find the data they need. You want to solve their problem in the most cost-effective way. What should you do?","answers_community":["C (70%)","B (30%)"],"answer":"C","discussion":[{"upvote_count":"39","content":"Answer: C","poster":"[Removed]","comment_id":"66292","timestamp":"1584718560.0"},{"timestamp":"1605610260.0","poster":"Radhika7983","comments":[{"content":"I don't think so because in question they worried about spending money for query but, using view could not make money safe because logical view scan all of the data in the table. so, for saving money for query then Answer B is more suitable","upvote_count":"4","poster":"jin0","comments":[{"timestamp":"1715929680.0","comment_id":"1212732","upvote_count":"2","poster":"mark1223jkh","content":"The point is reducing the number of columns, not caching. Yes, it will query the table, but with only the necessary columns the view has."}],"timestamp":"1676976300.0","comment_id":"816454"}],"content":"Answer is C. A logical view can be created with only the required columns which is required for visualization. B is not the right option as you will create a table and make it static. What happens when the original data is updated. This new table will not have the latest data and hence view is the best possible option here.","upvote_count":"25","comment_id":"220982"},{"upvote_count":"1","content":"Selected Answer: C\nAnswer is C:\n\nA BigQuery view solves this problem by:\n✅ Simplifying the dataset → Shows only the relevant columns/data.\n✅ Improving query efficiency → Reduces query costs by filtering unnecessary data.\n✅ Enhancing usability → Sales teams get only the insights they need without dealing with complex queries.\n✅ Reducing costs → Since views don’t store data separately, they don’t incur additional storage costs.","poster":"cqrm3n","comment_id":"1349797","timestamp":"1738401180.0"},{"upvote_count":"3","content":"Selected Answer: C\nAnswer C contains both requirements - minimal columns by creating view of the table and visualization tool for report","timestamp":"1729539000.0","comment_id":"1301234","poster":"SamuelTsch"},{"upvote_count":"2","poster":"rocky48","timestamp":"1699417200.0","comment_id":"1065312","content":"Selected Answer: C\nAnswer: C"},{"poster":"rtcpost","timestamp":"1697988180.0","upvote_count":"4","content":"Selected Answer: C\nC. Create a view on the table to present to the virtualization tool.\n\nCreating a view in BigQuery allows you to define a virtual table that is a subset of the original data, containing only the necessary columns or filtered data that the sales team requires for their reports. This approach is cost-effective because it doesn't involve exporting data to external tools or creating additional tables, and it ensures that the sales team is working with the specific data they need without running expensive queries on the full dataset. It simplifies the data for non-technical users while keeping the data in BigQuery, which is a powerful and cost-efficient data warehousing solution.\n\nOptions A (exporting to Google Sheet) and B (creating an additional table) might introduce data redundancy and maintenance overhead, and they don't provide the same level of control and security as creating a view. Option D (IAM roles) doesn't address the issue of simplifying the data for the sales team; it's more focused on access control.","comment_id":"1050793"},{"content":"Selected Answer: C\nC. You won't pay for storage for the view, and it will only include the necessary columns. Even if we assume that we don't talk about a materialized view, a logical view query can use the cache as much as a table query. So a new table does not have any benefit over a view, even if the view is logical.","upvote_count":"2","timestamp":"1690187160.0","comment_id":"961331","poster":"Mathew106"},{"comment_id":"876101","poster":"abi01a","timestamp":"1682039520.0","content":"The answer is C","upvote_count":"2"},{"poster":"kplam","comment_id":"874341","content":"Answer is C","timestamp":"1681888620.0","upvote_count":"3"},{"timestamp":"1679894760.0","upvote_count":"1","poster":"lucaluca1982","content":"Selected Answer: B\nB it is more cost-effective and efficient approach to handle reports","comment_id":"851746"},{"poster":"bha11111","upvote_count":"3","timestamp":"1678516500.0","comment_id":"835701","content":"Selected Answer: C\nC is correct"},{"content":"C —— view is better than another table to keep data consistent","timestamp":"1678101240.0","comment_id":"830743","poster":"Booqq","upvote_count":"3"},{"upvote_count":"2","poster":"JJJJim","comment_id":"802183","timestamp":"1675869960.0","content":"Selected Answer: C\nAnswer is C, creating view tables can easy and flexible to do the most cost-effetive way."},{"upvote_count":"2","timestamp":"1674660540.0","content":"Selected Answer: C\nThe appropriate solution is C, creating a view on the table, by selecting the relevant columns only (and not by creating another, static, table)","comment_id":"787810","poster":"PolyMoe"},{"poster":"dconesoko","upvote_count":"1","timestamp":"1674319980.0","comment_id":"783562","content":"Selected Answer: B\nProviding that the question was explicit in B and D about the selection of the appropriate columns it quite intriguing that it did not mention the selection of the appropriate column for the view. We can definitely build a view which might just present the same data or something much complex, thus i vote for B"},{"timestamp":"1674019560.0","content":"C is the correct answer","comment_id":"779629","poster":"GCPpro","upvote_count":"1"},{"timestamp":"1673221320.0","comment_id":"769910","upvote_count":"1","content":"Selected Answer: C\nI mean C","poster":"Isaga"},{"poster":"noonting","comment_id":"766705","comments":[{"upvote_count":"1","timestamp":"1673338080.0","comment_id":"771180","poster":"ler_mp","content":"But you would want a logical view for this, not materialized view"}],"content":"Selected Answer: B\nThe answer is B. Because it is not specified as a materialized view","timestamp":"1672928520.0","upvote_count":"1"},{"comment_id":"758500","upvote_count":"2","timestamp":"1672145220.0","content":"A materialized view would be the best choice since it contains real-time streaming data and is also cost-effective since it only changes the delta from query updates. This is possible by using smart caching.","poster":"Rodolfo_Marcos"},{"comment_id":"739938","poster":"lukacs16","content":"The correct answer is C. It should be B if there wouldn't for the real-time tracking requirement.","upvote_count":"3","timestamp":"1670575620.0"},{"comments":[{"poster":"Brillianttyagi","content":"How do you insert new data in table(what for the visual for new data).","comment_id":"757163","upvote_count":"1","timestamp":"1672039440.0"}],"timestamp":"1669767780.0","upvote_count":"1","content":"Selected Answer: B\nB\nNot view for C because view will be billed every query if it is not data studio for caching and it will costly if view is complex","comment_id":"730949","poster":"hauhau"},{"poster":"Dan137","comments":[{"timestamp":"1689699000.0","upvote_count":"1","content":"Google materialized views and you will find there are also materialized views. However, I do believe that the answer should mention \"materialized\" and not just view.","poster":"Mathew106","comment_id":"955654"}],"timestamp":"1661915340.0","content":"Selected Answer: B\nI go with B becase according to views documentation: https://cloud.google.com/bigquery/docs/views-intro#view_pricing \"BigQuery's views are logical views, not materialized views. Because views are not materialized, the query that defines the view is run each time the view is queried. Queries are billed according to the total amount of data in all table fields referenced directly or indirectly by the top-level query. For more information, see query pricing.\"","upvote_count":"6","comment_id":"654659"},{"upvote_count":"2","timestamp":"1655128500.0","content":"Selected Answer: C\nC. Create a view on the table to present to the VISUALIZATION tool.\n\n\"virtualization\" is one of several typoes....","poster":"rr4444","comment_id":"615813"},{"upvote_count":"1","poster":"sw52099","content":"Since the primary problem is the team are overwhelmed by all the data, so the solution should be reducing the number of columns. C. creating a view doesn't reduce the number of columns.","comment_id":"599973","timestamp":"1652257020.0"},{"upvote_count":"4","timestamp":"1649847960.0","comment_id":"585159","poster":"CedricLP","content":"Selected Answer: C\nAs already said :\nA. Data is too big to be virtualized using Google Sheets.\nB. Creating the new table won't be able to keep up with the new data inserts.\nC. This will help to select appropriate columns as well as will be able to deal with new data inserts.\nD. You cannot restrict access to selected columns using IAM. Views should be used to do that.","comments":[{"comment_id":"598937","poster":"stefanop","upvote_count":"1","content":"It is possible to set column-level policies to enable only specific users (or group of users) to access (https://cloud.google.com/bigquery/docs/column-level-security-intro). So I think option D will be the best one.","timestamp":"1652083440.0"}]},{"content":"Selected Answer: B\nEveryone said C, but... it's a virtualization tool??","comment_id":"580948","upvote_count":"1","poster":"devric","timestamp":"1649115120.0"},{"upvote_count":"2","timestamp":"1634054760.0","content":"Ans: C","comment_id":"461155","poster":"anji007"},{"content":"Answer: B\nA. Data is too big to be virtualized using Google Sheets.\nB. Creating the new table won't be able to keep up with the new data inserts.\nC. This will help to select appropriate columns as well as will be able to deal with new data inserts.\nD. You cannot restrict access to selected columns using IAM. Views should be used to do that.","comment_id":"403332","upvote_count":"4","timestamp":"1625920080.0","poster":"humza","comments":[{"content":"so answer is C","timestamp":"1632498780.0","upvote_count":"3","comment_id":"450970","poster":"vintop95"}]},{"content":"Why not D??","timestamp":"1625335380.0","upvote_count":"1","poster":"awssp12345","comment_id":"397777","comments":[{"upvote_count":"1","timestamp":"1625335860.0","content":"will not work because they don't know what data they need. So cannot assign access to just few columns.","poster":"awssp12345","comment_id":"397781"}]},{"timestamp":"1624781580.0","comment_id":"391878","upvote_count":"1","poster":"sumanshu","content":"Vote for C"},{"timestamp":"1615325640.0","comment_id":"306693","content":"I understand from the question that they want to eliminate query and it's strange. because we need query for extracting our result, and non of the choice wouldn't suggest a way that make query simpler. but with existing answers, C would be more near.","poster":"daghayeghi","upvote_count":"3"},{"poster":"naga","upvote_count":"3","content":"Correct C","comment_id":"285625","timestamp":"1612715640.0"},{"content":"Storing a few columns extra compared to get the advantage of caching versus no extra storagecost but require compute for every single query.\nI'd say it's more cost effective to store the columns in a new table and make use of caching","upvote_count":"1","comments":[{"comment_id":"185031","timestamp":"1600839780.0","poster":"Diqtator","upvote_count":"1","content":"Hmm, might incur a problem with keeping the new table up to date and force another load job.\nI don't like C because it calls data studio a virtualization tool instead of visualization tool.."}],"timestamp":"1600684320.0","comment_id":"183583","poster":"Diqtator"},{"poster":"atnafu2020","timestamp":"1597936980.0","comment_id":"162323","upvote_count":"3","content":"C\nis correct"},{"poster":"haroldbenites","content":"C. Correct. With the view is possible seleect only that fileds that you need.","upvote_count":"2","timestamp":"1597791420.0","comment_id":"161136"},{"comment_id":"147364","upvote_count":"2","poster":"yoRob","content":"Makes sense. B you would have to deal with making the new table consistent and you have extra space. C does the Job, more efficiently with less effort","timestamp":"1596109440.0"}],"topic":"1","question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0002300001.png","https://www.examtopics.com/assets/media/exam-media/04341/0002400014.png"]}],"exam":{"lastUpdated":"15 Feb 2025","numberOfQuestions":319,"isImplemented":true,"id":10,"isBeta":false,"isMCOnly":true,"provider":"Google","name":"Professional Data Engineer"},"currentPage":50},"__N_SSP":true}