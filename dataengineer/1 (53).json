{"pageProps":{"questions":[{"id":"odfmfIYhvWsajCK1rm1k","choices":{"A":"Redis","D":"MongoDB","F":"HDFS with Hive","B":"HBase","E":"Cassandra","C":"MySQL"},"exam_id":10,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/16661-exam-professional-data-engineer-topic-1-question-50/","answer_ET":"BDE","question_id":266,"answer_description":"","topic":"1","answer":"BDE","unix_timestamp":1584277140,"question_images":[],"discussion":[{"upvote_count":"39","comments":[{"comment_id":"453474","content":"Redis is also NoSQL","upvote_count":"2","comments":[{"comments":[{"content":"Memorystore, Google's managed Redis service is. But OS Redis is not. Though it is hard to find a 100GB RAM machine","comment_id":"1012986","upvote_count":"1","timestamp":"1726914300.0","poster":"ckanaar"}],"comment_id":"459269","content":"Redis is limited to 1 TB capacity quota per region. So it doesn't satisfy the requirement.\nhttps://cloud.google.com/memorystore/docs/redis/quotas","poster":"vholti","timestamp":"1665238800.0","upvote_count":"3"}],"timestamp":"1664376720.0","poster":"sergio6"}],"timestamp":"1615813140.0","content":"BDE. Hive is not for NoSQL","comment_id":"64269","poster":"jvg637"},{"poster":"awssp12345","comments":[{"poster":"[Removed]","upvote_count":"1","timestamp":"1708443240.0","content":"HDFS is. Hadoop Distributed File System. HDFS is storage and HIVE is for processing.","comment_id":"815459"}],"content":"Answer is BDE - \nA. Redis - Redis is an in-memory non-relational key-value store. Redis is a great choice for implementing a highly available in-memory cache to decrease data access latency, increase throughput, and ease the load off your relational or NoSQL database and application. Since the question does not ask cache, A is discarded.\nB. HBase - Meets reqs\nC. MySQL - they do not need ACID, so not needed.\nD. MongoDB - Meets reqs\nE. Cassandra - Apache Cassandra is an open source NoSQL distributed database trusted by thousands of companies for scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.\nF. HDFS with Hive - Hive allows users to read, write, and manage petabytes of data using SQL. Hive is built on top of Apache Hadoop, which is an open-source framework used to efficiently store and process large datasets. As a result, Hive is closely integrated with Hadoop, and is designed to work quickly on petabytes of data. HIVE IS NOT A DATABSE.","comment_id":"398565","timestamp":"1656957540.0","upvote_count":"32"},{"comment_id":"1329511","content":"Selected Answer: BDE\nOption A: Redis cannot handle large scale data it is NOSQL db to store small amount of key value pairs, \nOption B: HBase NOSQL db built on Hadoop does not support ACID Properties. Correct answer\nOption C: Mysql Does not store telemetry IOT data. Mysql is a relational database structured data only stored.\nOption D, E: NOSQL Databases, Option F: HDFS with hive used for batch processing not real time streaming data.\nOption","poster":"sravi1200","timestamp":"1734706140.0","upvote_count":"1"},{"timestamp":"1707906660.0","upvote_count":"1","content":"BDE\nFaster Database are NoSql db than SQL, Cassandra is the fastest one in market now than Hbase and then others, in given list MongoBD","poster":"musumusu","comment_id":"808280"},{"content":"\"Which three databases meet your requirements? \"\nHive is not a database server.\nHBase, Mongo and Cassandra are and meet the criteria.\nBDE is the right answer","timestamp":"1693073340.0","poster":"MisuLava","upvote_count":"1","comment_id":"652304"},{"poster":"sraakesh95","upvote_count":"1","timestamp":"1673724900.0","content":"Selected Answer: BDE\n@hendrixlives","comment_id":"523699"},{"poster":"medeis_jar","timestamp":"1672842720.0","content":"Selected Answer: BDE\nas explained by hendrixlives","upvote_count":"1","comment_id":"516718"},{"upvote_count":"14","poster":"hendrixlives","content":"Selected Answer: BDE\nBDE:\nA. Redis is a key-value store (and in many cases used as in-memory and non persistent cache). It is not designed for \"100TB per year\" of highly available storage.\nB. HBase is similar to Google Bigtable, fits the requirements perfectly: highly available, scalable and with very low latency.\nC. MySQL is a relational DB, designed precisely for ACID transactions and not for the stated requirements. Also, growth may be an issue.\nD. MongoDB is a document-db used for high volume data and maintains currently used data in RAM, so performance is usually really good. Should also fit the requirements well.\nE. Cassandra is designed precisely for highly available massive datasets, and a fine tuned cluster may offer low latency in reads. Fits the requirements.\nF. HDFS with Hive is great for OLAP and data-warehouse scenarios, allowing to solve map-reduce problems using an SQL subset, but the latency is usually really high (we may talk about seconds, not milliseconds, when obtaining results), so this does not complies with the requirements.","comment_id":"503477","timestamp":"1671266160.0"},{"poster":"MaxNRG","upvote_count":"1","comments":[{"content":"Latency in Hive is usually quite high, and one of the requirements is \"low latency\"","timestamp":"1671264060.0","upvote_count":"2","comments":[{"upvote_count":"1","content":"good point!","poster":"MaxNRG","timestamp":"1674405660.0","comment_id":"529979"},{"poster":"MaxNRG","content":"agreed on BDE","upvote_count":"2","comment_id":"532296","timestamp":"1674667620.0"}],"comment_id":"503460","poster":"hendrixlives"}],"comment_id":"489048","content":"Selected Answer: BEF\nVery strange question, seems outdated and irrelevant to me as it doesn't contain any GCP products :)\n\nAnyway, I would choose BEF.\nRedis is in-memory key value, not good\nHBase yes, excelent case for linear growth and a column-oriented database\nmysql not good, too big and no need for transactionality\nMongodb, document db with flexible schema ??\nYes Cassandra, good use case\nApache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis. \nhttps://www.wikiwand.com/en/Apache_Hive","timestamp":"1669630140.0"},{"comment_id":"462779","content":"Ans: B, D and E","timestamp":"1665861360.0","upvote_count":"2","poster":"anji007"},{"content":"vote for BDE","upvote_count":"2","comment_id":"392208","timestamp":"1656345540.0","poster":"sumanshu"},{"poster":"BhupiSG","comment_id":"309318","upvote_count":"2","timestamp":"1647133680.0","comments":[{"timestamp":"1650055140.0","upvote_count":"5","poster":"Manue","content":"Hive is not for low latency queries. It is for analytics.","comment_id":"336586"}],"content":"BEF\nB: HBASE is based upon BigTable\nE: Cassandra is low latency columnar distributed database like BigTable\nF: HDFS is low latency distributed file system and Hive will help with running the queries"},{"comment_id":"307377","content":"BDE:\nThese are NoSQL DB, Hive is not for NoSQL.","poster":"daghayeghi","timestamp":"1646937720.0","upvote_count":"2"},{"content":"The answer is ADE, the statement says they require a NoSQL with high availability and low latency, they do not require consistency. \nC. it is not NoSQL.\nF. it is not NoSQL.\nB. it is NoSQL but focused on strong consistency and based on HDFS, you need HDFS for Hbase.\nTherefore the answer is ADE","poster":"Rayleigh","comment_id":"297224","upvote_count":"1","timestamp":"1645601460.0"},{"content":"BDF:\nRedis and Cassandra have only Rowkey and couldn't be indexed, and MySQL isn't NoSQL, Then B D and E is correct answer.","comment_id":"290613","poster":"daghayeghi","upvote_count":"1","timestamp":"1644889920.0"},{"poster":"naga","timestamp":"1644256500.0","comment_id":"285684","upvote_count":"3","content":"Correct BDE"},{"comment_id":"255432","upvote_count":"3","poster":"apnu","timestamp":"1640849820.0","content":"it should be BDE because Hive is a sql based datawarehouse , it is not a nosql DB"},{"content":"I agree with HaroldBenites and I like your answer. I did some research and yes, you cannot query HBase by individual fields. See https://opensource.com/article/19/8/apache-hive-vs-apache-hbase you cannot query by row 00001 or 00002 etc but not by the field!!! wow","timestamp":"1636957200.0","comment_id":"219536","poster":"GeeBeeEl","upvote_count":"2"},{"poster":"Tanmoyk","timestamp":"1630408560.0","comment_id":"170743","content":"B D E seems most relevant here.","upvote_count":"2"},{"content":"ADE are NoSQL.\nB is NoSQL but doesn´t efficient for run queires by individual fileds. It is by rowkey","poster":"haroldbenites","timestamp":"1629330540.0","upvote_count":"3","comments":[{"content":"Hbase is a wide column database, like Cassandra. If it's not efficient for run queries by individual fields, Cassandra is also not. So the E would also be wrong.","poster":"sergio6","timestamp":"1664376300.0","comment_id":"453473","upvote_count":"1"}],"comment_id":"161158"},{"content":"BDE is correct answer","upvote_count":"3","comment_id":"160334","timestamp":"1629231240.0","poster":"saurabh1805"},{"timestamp":"1624241700.0","upvote_count":"4","poster":"ch3n6","comment_id":"115165","content":"BDE: https://db-engines.com/en/system/Cassandra%3BHive"},{"upvote_count":"6","poster":"[Removed]","timestamp":"1616856900.0","comment_id":"68622","content":"Answer: B, D, E\nDescription: These are NoSQL DB"},{"upvote_count":"9","content":"i would choose : BDE","comment_id":"66428","poster":"[Removed]","timestamp":"1616313300.0"},{"content":"I would pick BEF. MongoDB is not an ideal bigData storage but a NoSQL instead.","comment_id":"65006","upvote_count":"2","timestamp":"1615953480.0","poster":"rickywck"}],"answers_community":["BDE (94%)","6%"],"timestamp":"2020-03-15 13:59:00","question_text":"You are choosing a NoSQL database to handle telemetry data submitted from millions of Internet-of-Things (IoT) devices. The volume of data is growing at 100\nTB per year, and each data entry has about 100 attributes. The data processing pipeline does not require atomicity, consistency, isolation, and durability (ACID).\nHowever, high availability and low latency are required.\nYou need to analyze the data by querying against individual fields. Which three databases meet your requirements? (Choose three.)","answer_images":[]},{"id":"qVwioClv782HCHT17hhe","question_id":267,"answer_images":[],"isMC":true,"exam_id":10,"unix_timestamp":1584091680,"question_images":[],"answer_description":"","question_text":"You are training a spam classifier. You notice that you are overfitting the training data. Which three actions can you take to resolve this problem? (Choose three.)","choices":{"D":"Use a larger set of features","C":"Use a smaller set of features","A":"Get more training examples","F":"Decrease the regularization parameters","E":"Increase the regularization parameters","B":"Reduce the number of training examples"},"url":"https://www.examtopics.com/discussions/google/view/16468-exam-professional-data-engineer-topic-1-question-51/","answer":"ACE","discussion":[{"upvote_count":"68","content":"it should be ACE","timestamp":"1599982080.0","comment_id":"63454","poster":"madhu1171"},{"comment_id":"66431","poster":"[Removed]","timestamp":"1600668240.0","content":"Should be ACE","upvote_count":"19","comments":[{"content":"prevent overfitting: less variables, regularisation, early ending on the training","timestamp":"1600668360.0","comment_id":"66432","upvote_count":"14","poster":"[Removed]"}]},{"comment_id":"1096468","poster":"TVH_Data_Engineer","timestamp":"1718362200.0","content":"Selected Answer: ACE\nTo address the problem of overfitting in training a spam classifier, you should consider the following three actions:\n\nA. Get more training examples:\n\nWhy: More training examples can help the model generalize better to unseen data. A larger dataset typically reduces the chance of overfitting, as the model has more varied examples to learn from.\nC. Use a smaller set of features:\n\nWhy: Reducing the number of features can help prevent the model from learning noise in the data. Overfitting often occurs when the model is too complex for the amount of data available, and having too many features can contribute to this complexity.\nE. Increase the regularization parameters:\n\nWhy: Regularization techniques (like L1 or L2 regularization) add a penalty to the model for complexity. Increasing the regularization parameter will strengthen this penalty, encouraging the model to be simpler and thus reducing overfitting.","upvote_count":"5"},{"upvote_count":"2","poster":"Mathew106","timestamp":"1705850700.0","content":"Selected Answer: ACE\n100% ACE\n\nWe need more data because less data induces overfitting. We need less features to make the problem simpler to learn and not promote learning a very complex function for thousands of features that might not apply to the test data. We also need to use regularization to keep the weights constrained.","comment_id":"958543"},{"upvote_count":"1","poster":"theseawillclaim","timestamp":"1705523100.0","content":"Selected Answer: ACE\nDefinitely ACE. \nMore training data and less variables can prevent the model from being too picky or specific.","comment_id":"954523"},{"poster":"jin0","upvote_count":"1","timestamp":"1692767280.0","comment_id":"818897","content":"? why A is answer? even though 'more training example' not 'more dataset example'. I understand that there is dataset same and there is only change the size of training examples size. in this case there are valid and test example should be reduced. isn't it?"},{"comment_id":"774731","poster":"desertlotus1211","content":"Collect more training data: This will help the model generalize better and reduce overfitting.\n\nUse regularization techniques: Techniques such as L1 and L2 regularization can be applied to the model's weights to prevent them from becoming too large and causing overfitting.\n\nUse early stopping: This involves monitoring the performance of the model on a validation set during training, and stopping the training when the performance on the validation set starts to degrade. This helps to prevent the model from becoming too complex and overfitting the training data.","timestamp":"1689262500.0","comments":[{"comment_id":"774736","poster":"desertlotus1211","upvote_count":"1","content":"Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily.\n\nA & C are correct... the third one --- not sure on","timestamp":"1689262740.0"}],"upvote_count":"1"},{"comment_id":"770710","timestamp":"1688917500.0","poster":"RoshanAshraf","upvote_count":"1","content":"Selected Answer: ACE\nA -The training data is causing the overfiting for the testing data, so addition of training data will solve this.\nC - Larger sets will cause overfitting, so we have to use smaller sets or reduce features\nE - Increase the regularization is a method for solving the Overfitting model"},{"comment_id":"766087","timestamp":"1688497320.0","content":"Answers are;\nA. Get more training examples\nC. Use a smaller set of features\nE. Increase the regularization parameters\n\nPrevent overfitting: less variables, regularisation, early ending on the training\n\nReference:\nhttps://cloud.google.com/bigquery-ml/docs/preventing-overfitting","poster":"AzureDP900","upvote_count":"3"},{"comment_id":"744634","content":"Selected Answer: ADE\nAnswer ADE","poster":"DGames","timestamp":"1686701940.0","upvote_count":"1"},{"upvote_count":"1","timestamp":"1682434560.0","comment_id":"703964","poster":"MisuLava","content":"Selected Answer: ACE\n100% sure ACE\n\nhttps://elitedatascience.com/overfitting-in-machine-learning"},{"timestamp":"1677442440.0","upvote_count":"1","poster":"MisuLava","comment_id":"652308","content":"Answer is : ACE\nhttps://www.ibm.com/cloud/learn/overfitting#:~:text=Overfitting%20is%20a%20concept%20in,unseen%20data%2C%20defeating%20its%20purpose."},{"timestamp":"1676396460.0","poster":"Noahz110","content":"Selected Answer: ACE\nim vote for ACE","upvote_count":"1","comment_id":"646818"},{"poster":"Dip1994","upvote_count":"1","comment_id":"642542","timestamp":"1675534320.0","content":"It should be ACE"},{"comment_id":"523706","content":"Selected Answer: ACE\n@medeis_jar","upvote_count":"1","timestamp":"1657820940.0","poster":"sraakesh95"},{"upvote_count":"4","poster":"medeis_jar","comment_id":"516724","timestamp":"1656938160.0","content":"Selected Answer: ACE\nAs MaxNRG wrote:\nThe tools to prevent overfitting: less variables, regularization, early ending on the training.\n\n- Adding more training data will increase the complexity of the training set and help with the variance problem.\n- Reducing the feature set will ameliorate the overfitting and help with the variance problem.\n- Increasing the regularization parameter will reduce overfitting and help with the variance problem."},{"timestamp":"1655571060.0","content":"Selected Answer: ACE\nACE\n\nThe tools to prevent overfitting: less variables, regularization, early ending on the training…\nOverfitting means that the classifier knows too well the data and fails to generalize. We should use a smaller number of features to help the classifier generalize, and more examples so that it can have more variety.\nThe gap in errors between training and test suggests a high variance problem in which the algorithm has overfit the training set.\n- Adding more training data will increase the complexity of the training set and help with the variance problem.\n- Reducing the feature set will ameliorate the overfitting and help with the variance problem.\n- Increasing the regularization parameter will reduce overfitting and help with the variance problem.\nhttps://github.com/mGalarnyk/datasciencecoursera/blob/master/Stanford_Machine_Learning/Week6/AdviceQuiz.md","comment_id":"504402","upvote_count":"4","poster":"MaxNRG"},{"comment_id":"504376","poster":"MaxNRG","timestamp":"1655568900.0","content":"ACE\n\nThe tools to prevent overfitting: less variables, regularization, early ending on the training…\nOverfitting means that the classifier knows too well the data and fails to generalize. We should use a smaller number of features to help the classifier generalize, and more examples so that it can have more variety.\nThe gap in errors between training and test suggests a high variance problem in which the algorithm has overfit the training set. \n- Adding more training data will increase the complexity of the training set and help with the variance problem. \n- Reducing the feature set will ameliorate the overfitting and help with the variance problem.\n- Increasing the regularization parameter will reduce overfitting and help with the variance problem.\nhttps://github.com/mGalarnyk/datasciencecoursera/blob/master/Stanford_Machine_Learning/Week6/AdviceQuiz.md","upvote_count":"2"},{"poster":"JG123","upvote_count":"5","comment_id":"487263","content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: ACE","timestamp":"1653554700.0"},{"comment_id":"462780","poster":"anji007","content":"Ans: A, C and E.\nTo avoid overfitting use more training samples, with lesser features and use regularization.","timestamp":"1650050340.0","upvote_count":"3"},{"poster":"sumanshu","content":"Vote for ACE","timestamp":"1641659700.0","upvote_count":"1","comment_id":"401985"},{"comment_id":"355302","upvote_count":"2","timestamp":"1636703700.0","poster":"jasper_430","content":"no doubt in ACE"},{"comment_id":"307388","timestamp":"1631294220.0","poster":"daghayeghi","upvote_count":"4","content":"ACE:\nhttps://towardsdatascience.com/deep-learning-3-more-on-cnns-handling-overfitting-2bd5d99abe5d\nhttps://cloud.google.com/bigquery-ml/docs/preventing-overfitting"},{"poster":"daghayeghi","upvote_count":"2","timestamp":"1629076140.0","content":"ACE, Reference:\nhttps://cloud.google.com/bigquery-ml/docs/preventing-overfitting","comment_id":"291445"},{"timestamp":"1628353740.0","content":"Correct ADF","poster":"naga","upvote_count":"1","comment_id":"285719"},{"upvote_count":"4","timestamp":"1628246460.0","comment_id":"284807","content":"ACE: Reason: Overfitting results in noisy model and the predictions wont be with good accuracy. Feed more apt training samples for a better model. More number of features implies more dimensionality which would also result in poor performance.","poster":"shilpa"},{"timestamp":"1627090980.0","poster":"[Removed]","content":"Obviously ACE","comment_id":"274974","upvote_count":"2"},{"timestamp":"1625623680.0","comment_id":"261524","content":"It's ACE, why ADF answer is given by portal","poster":"AnilKr","upvote_count":"2"},{"poster":"Nileshk611","upvote_count":"2","content":"It's ACE","comment_id":"217468","timestamp":"1620749940.0"},{"comments":[{"timestamp":"1620148560.0","comment_id":"212968","poster":"ave4000","comments":[{"upvote_count":"1","timestamp":"1620148560.0","comment_id":"212969","content":"i mean its good but you guys knew that already xD","poster":"ave4000"}],"upvote_count":"1","content":"my bad"}],"content":"You are all wrong. You reduce overfitting by increasing the alfa/lambda (regularization coefficients), not the other way.","comment_id":"212967","timestamp":"1620148500.0","upvote_count":"1","poster":"ave4000"},{"content":"agreed with ACE","upvote_count":"2","comment_id":"194054","timestamp":"1617692280.0","poster":"Darlee"},{"timestamp":"1616070540.0","upvote_count":"2","poster":"SteelWarrior","comment_id":"181555","content":"Answer should be ACE."},{"upvote_count":"2","poster":"Tanmoyk","timestamp":"1614518280.0","content":"A C E are most suitable option","comment_id":"170744"},{"content":"ACE is Correct","comment_id":"161159","upvote_count":"2","poster":"haroldbenites","timestamp":"1613699460.0"},{"upvote_count":"2","content":"ACE seems to be correct one, Refer below link for more information.\nhttps://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/\n\nhttps://elitedatascience.com/overfitting-in-machine-learning","timestamp":"1613600460.0","comment_id":"160338","poster":"saurabh1805"},{"comment_id":"145862","content":"https://stackoverflow.com/questions/37776333/why-too-many-features-cause-over-fitting#:~:text=Overfitting%20means%20your%20model%20does,allow%20polynomials%20to%20degree%20100.","upvote_count":"3","timestamp":"1611850620.0","poster":"mikey007"},{"comment_id":"139857","poster":"atnafu2020","upvote_count":"2","timestamp":"1611185820.0","content":"ACE\nis correct answer"},{"poster":"Rajokkiyam","timestamp":"1600711680.0","comment_id":"66661","upvote_count":"6","content":"Answer: ACE"}],"answers_community":["ACE (95%)","5%"],"timestamp":"2020-03-13 10:28:00","answer_ET":"ACE","topic":"1"},{"id":"cdFutPuLuXgLTWW2Givb","choices":{"B":"Grant the Project Owner role to a service account, and run the job with it","D":"Use a user account with the Project Viewer role on the Cloud Dataproc cluster to read the batch files and write to BigQuery","C":"Use a service account with the ability to read the batch files and to write to BigQuery","A":"Restrict the Google Cloud Storage bucket so only you can see the files"},"exam_id":10,"isMC":true,"url":"https://www.examtopics.com/discussions/google/view/16822-exam-professional-data-engineer-topic-1-question-52/","answer_ET":"C","question_id":268,"answer_description":"","topic":"1","answer":"C","unix_timestamp":1584417660,"discussion":[{"poster":"digvijay","comment_id":"68022","content":"A is wrong, if only I can see the bucket no automation is possible, besides, also needs launch the dataproc job\nB is too much, does not follow the security best practices\nC has one point missing…you need to submit dataproc jobs.\nIn D viewer role will not be able to submit dataproc jobs, the rest is ok\n\nThus….the only one that would work is B! BUT this service account has too many permissions. Should have dataproc editor, write big query and read from bucket","comments":[{"comment_id":"453569","upvote_count":"5","content":"C doesn't need permission to submit dataproc jobs, it's workload SA. Job can be submitted by any other identity","poster":"retep007","timestamp":"1664387220.0"},{"comment_id":"123573","upvote_count":"15","content":"Hence - Contextually, Option [C] looks to be the right fit","poster":"dambilwa","timestamp":"1625064840.0"}],"upvote_count":"34","timestamp":"1616638500.0"},{"upvote_count":"31","comment_id":"65007","poster":"rickywck","content":"Should be C","timestamp":"1615953660.0"},{"upvote_count":"1","timestamp":"1721814960.0","comment_id":"961438","poster":"Mathew106","content":"Selected Answer: B\nWe need permissions for submitting dataproc jobs and writing to BigQuery. Project Owner will fix all of that even though it's not a good solution. The rest won't work at all."},{"comment_id":"869024","timestamp":"1712976900.0","poster":"Adswerve","upvote_count":"4","content":"Selected Answer: C\nC\nProject Owner is too much, violates the principle of least privilege"},{"timestamp":"1706274420.0","content":"Selected Answer: C\nC. Use a service account with the ability to read the batch files and to write to BigQuery\n\nIt is best practice to use service accounts with the least privilege necessary to perform a specific task when automating jobs. In this case, the job needs to read the batch files from Cloud Storage and write the results to BigQuery. Therefore, you should create a service account with the ability to read from the Cloud Storage bucket and write to BigQuery, and use that service account to run the job.","comment_id":"788744","poster":"PolyMoe","upvote_count":"3"},{"comment_id":"766378","upvote_count":"1","timestamp":"1704442980.0","content":"Selected Answer: B\nB works for the given requirement","poster":"Mkumar43"},{"timestamp":"1703068380.0","poster":"Krish6488","content":"Least privilege principle. Option C. job can be submitted or triggered using a Cron or a composer which uses another SA with different set of privileges","upvote_count":"2","comment_id":"750763"},{"content":"Selected Answer: B\nB because we need to run job .. option C mentioned permission about read and write nothing mention to run the job . In case project owner to service account it’s similar just running job and doing rest of tasks read and writing as well.","timestamp":"1702521300.0","comment_id":"744655","poster":"DGames","upvote_count":"2"},{"poster":"ThomasChoy","comment_id":"590321","upvote_count":"2","content":"Selected Answer: C\nThe answer is C because Service Account is the best way to access the BigQuery API if your application can run jobs associated with service credentials rather than an end-user's credentials, such as a batch processing pipeline.\nhttps://cloud.google.com/bigquery/docs/authentication","timestamp":"1682214180.0"},{"poster":"Bhawantha","timestamp":"1673514900.0","comment_id":"522020","upvote_count":"4","content":"Selected Answer: C\nData owners cant create jobs or queries. -> B out \nWe need service Account -> D out\nAccess only granting me does not solve the problem -> A out \nThe answer is C. ( Minimum rights to perform the job)"},{"timestamp":"1672843260.0","upvote_count":"1","comment_id":"516728","poster":"medeis_jar","content":"Selected Answer: C\n\"taking nightly batch files containing non-public information from Google Cloud Storage, processing them with a Spark Scala job on a Google Cloud\nDataproc cluster, and depositing the results into Google BigQuery\""},{"content":"C should be okay,since he is already a project owner, I guess compute service account created will have access to run the jobs","timestamp":"1672252020.0","poster":"prasanna77","comment_id":"511457","upvote_count":"1"},{"timestamp":"1671389820.0","upvote_count":"1","content":"Selected Answer: C\nC,\nProject Owner role to a service account - is too much","poster":"MaxNRG","comment_id":"504405"},{"content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: C","upvote_count":"6","comment_id":"487267","poster":"JG123","timestamp":"1669459860.0"},{"content":"Ans: C\nSee this: https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2","upvote_count":"3","timestamp":"1665862200.0","poster":"anji007","comment_id":"462781"},{"comment_id":"442108","poster":"Blobby","upvote_count":"4","timestamp":"1662745260.0","content":"C as service account invoked to read the data into GCS and write to BQ once transformed via Data Proc. Assumes DataProc can inherit SA authorisation to perform transform and propagate. \nB seems to violate key IAM principle enforcing least privilege;\nhttps://cloud.google.com/iam/docs/recommender-overview"},{"timestamp":"1656363900.0","comment_id":"392357","upvote_count":"4","content":"Vote for 'C\"","comments":[{"upvote_count":"3","comment_id":"402133","timestamp":"1657303560.0","poster":"sumanshu","content":"Vote for B, (though it's too much access) - But C has one accessing missing (i.e Dataproc job execution) Thus B is correct"}],"poster":"sumanshu"},{"upvote_count":"2","poster":"kino2020","comment_id":"342270","timestamp":"1650848700.0","content":"If the answer is \"Workflow using Cloud Schedule\" and the question is about cost effectiveness, then it's Workflow using Cloud Schedule compared to \"Cloud Composer\".\nHowever, since the only answer is \"workflow template\", scheduling is not possible.\nSo the answer is C's \"Cloud Composer\".\nhttps://cloud.google.com/dataproc/docs/tutorials/workflow-scheduler"},{"comment_id":"309326","content":"C\nB seems to much","poster":"BhupiSG","upvote_count":"2","timestamp":"1647134700.0"},{"comment_id":"307399","timestamp":"1646943060.0","upvote_count":"2","content":"A is wrong, if only I can see the bucket no automation is possible, besides, also needs launch the dataproc job\nB is too much, does not follow the security best practices\nC has one point missing…you need to submit dataproc jobs.\nIn D viewer role will not be able to submit dataproc jobs, the rest is ok\n\nThus….the only one that would work is B! BUT this service account has too many permissions. Should have dataproc editor, write big query and read from bucket","poster":"daghayeghi"},{"comments":[{"comment_id":"193386","upvote_count":"3","poster":"kino2020","comments":[{"upvote_count":"2","comment_id":"425455","poster":"safiyu","content":"I am not sure about this. act as role on service account should be assigned to the user trying to impersonate this service account.. not the service account itself. C is right for this I suppose","timestamp":"1660596060.0"}],"content":"This question was created at least around 2019. So when this question comes up, it is a qualification for which we don't know the result of the answer, so when it does, call the examiner and inquire course.","timestamp":"1633414080.0"}],"timestamp":"1633413780.0","poster":"kino2020","upvote_count":"1","content":"We used to be able to do it, but now it's right that we can't.\n---------------------------------------------\nSecurity requirement beginning August 3, 2020: Dataproc users are required to have service account ActAs permission to deploy Dataproc resources, for example, to create clusters and submit jobs. See Managing service account impersonation for detailed information.\nOpt-in for existing Dataproc users: Existing Dataproc users as of August 3, 2020 can opt in to this security requirement (see Securing Dataproc, Dataflow, and Cloud Data Fusion).\n--------------------------------------------------\nhttps://cloud.google.com/dataproc/docs/concepts/iam/iam#permissions","comment_id":"193383"},{"comment_id":"189478","content":"A, C and D don't mention permission for the Dataproc job. The answer is B. Also because the user is executing jobs manually as the Project Owner, if the service account is not a Project Owner too, there could be permission issues with the resources.","timestamp":"1632896700.0","upvote_count":"6","poster":"alek6dj"},{"comment_id":"183847","timestamp":"1632244440.0","poster":"Diqtator","content":"C doesn't give permissions to run all the jobs needed. B should be correct.","upvote_count":"3"},{"upvote_count":"1","poster":"haroldbenites","comment_id":"161162","timestamp":"1629331500.0","content":"C is Correct"},{"timestamp":"1626783120.0","comment_id":"139479","content":"Correct Answer : C\nExplanation:-This option is correct as the best practice is to use a service account with least privilege.\n\nPractices - Service Accounts\nA service account is a special type of Google account intended to represent a non-human user that needs to authenticate and be authorized to access data in Google APIs.\n\nTypically, service accounts are used in scenarios such as -\n• Running workloads on virtual machines (VMs).","upvote_count":"4","poster":"VishalB"},{"poster":"Rajuuu","comment_id":"131175","timestamp":"1625895960.0","content":"C provides the minimum privilege to execute the job","upvote_count":"3"},{"poster":"AJKumar","content":"Question requirement is to automate the job, Service account is best for this case, Correct C.","comment_id":"120767","timestamp":"1624730400.0","upvote_count":"2"},{"comments":[{"upvote_count":"2","timestamp":"1628424900.0","comments":[{"timestamp":"1633288560.0","poster":"dabrat","comment_id":"192506","upvote_count":"2","content":"dataproc.jobs.create permits the submission of Dataproc jobs to Dataproc clusters in the containing project"}],"content":"Why would it be impossible with \"C\"?","comment_id":"153019","poster":"lgdantas"}],"content":"With C it's impossible to run a job. So B","comment_id":"67062","upvote_count":"4","poster":"jvg637","timestamp":"1616439300.0"},{"comment_id":"66662","timestamp":"1616357340.0","upvote_count":"3","content":"Should be C","poster":"Rajokkiyam"},{"upvote_count":"4","comments":[{"poster":"Rajokkiyam","content":"Given Project owner role It might provide additional privileges to the ServiceAccounts liek deleting the files etc. Basic Priv needed for this type of action is to read source file and write into Bquery.","comment_id":"66663","upvote_count":"5","timestamp":"1616357520.0"}],"timestamp":"1616314140.0","content":"Should be B","poster":"[Removed]","comment_id":"66433"}],"question_images":[],"answers_community":["C (79%)","B (21%)"],"timestamp":"2020-03-17 05:01:00","question_text":"You are implementing security best practices on your data pipeline. Currently, you are manually executing jobs as the Project Owner. You want to automate these jobs by taking nightly batch files containing non-public information from Google Cloud Storage, processing them with a Spark Scala job on a Google Cloud\nDataproc cluster, and depositing the results into Google BigQuery.\nHow should you securely run this workload?","answer_images":[]},{"id":"NxKXqjNkdoqwbDJhwtOi","isMC":true,"exam_id":10,"question_text":"You are using Google BigQuery as your data warehouse. Your users report that the following simple query is running very slowly, no matter when they run the query:\nSELECT country, state, city FROM [myproject:mydataset.mytable] GROUP BY country\nYou check the query plan for the query and see the following output in the Read section of Stage:1:\n//IMG//\n\nWhat is the most likely cause of the delay for this query?","answer_ET":"D","answer_description":"","answer_images":[],"unix_timestamp":1584779520,"discussion":[{"timestamp":"1584779520.0","comment_id":"66435","content":"Should be D","poster":"[Removed]","upvote_count":"26"},{"content":"D; Purple is reading, Blue is writing. so majority is reading.","timestamp":"1587229980.0","comments":[{"poster":"squishy_fishy","comment_id":"469841","upvote_count":"1","timestamp":"1635522840.0","content":"I have been looking for the color code descriptions for a while. Thank you!"}],"comment_id":"76126","poster":"itche_scratche","upvote_count":"25"},{"comment_id":"1259105","content":"Selected Answer: D\nD - stands for Data Skew\nThe Read section of the query plan shows a heavy concentration of processing in one area (as indicated by the pink bar being much longer than the purple bar). This typically indicates data skew, where the majority of the data is processed by a small subset of nodes.","poster":"iooj","timestamp":"1722460560.0","upvote_count":"1"},{"upvote_count":"2","content":"Selected Answer: D\nThe most likely cause of the delay for this query is option D. Most rows in the [myproject:mydataset.mytable] table have the same value in the country column, causing data skew.\n\nGroup by queries in BigQuery can run slowly when there is significant data skew on the grouped columns. Since the query is grouping by country, if most rows have the same country value, all that data will need to be shuffled to a single reducer to perform the aggregation. This can cause a data skew slowdown.\n\nOptions A and B might cause general slowness but are unlikely to affect this specific grouping query. Option C could also cause some slowness but not to the degree that heavy data skew on the grouped column could. So D is the most likely root cause. Optimizing the data distribution to reduce skew on the grouped column would likely speed up this query.","poster":"MaxNRG","comment_id":"1098164","timestamp":"1702730460.0"},{"comment_id":"1094998","timestamp":"1702418940.0","content":"Data skew is when one or some partitions have significantly more data compared to other partitions. Data-skew is usually the result of operations that require re-partitioning the data, mostly join and grouping ( GroupBy ) operations. So D.","upvote_count":"1","poster":"JOKKUNO"},{"content":"Selected Answer: D\nD. Most rows in the [myproject:mydataset.mytable] table have the same value in the country column, causing data skew\n\nData skew occurs when one or more values in a column have a disproportionately large number of rows compared to other values in that column. This can cause performance issues when running queries that group by that column, like the one in the question. In this case, if most of the rows in the [myproject:mydataset.mytable] table have the same value in the country column, then the query will need to process a large number of rows with that value, which can cause significant delay.","upvote_count":"4","poster":"PolyMoe","timestamp":"1674739140.0","comment_id":"788752"},{"poster":"AzureDP900","content":"D is right","upvote_count":"3","comment_id":"765686","timestamp":"1672838940.0"},{"upvote_count":"2","timestamp":"1671533460.0","comment_id":"750771","content":"Selected Answer: D\ndata skewing causing imbalance in data distribution across slots. It also causes errors if the group by column has NULLS. Since option C does not call out the Group by column, D is a closer answer contextually","poster":"Krish6488"},{"poster":"Jasar","upvote_count":"3","timestamp":"1668515700.0","content":"Selected Answer: A\nA is the best option becouse the color bar show the high number of reads and i think its not a skew becouse biguery was build to compute the data fast","comment_id":"718719"},{"comment_id":"659901","upvote_count":"11","poster":"arpitagrawal","timestamp":"1662365100.0","content":"The query would throw the error because you're using a group by clause on country but not aggregating city or state."},{"timestamp":"1661538060.0","comment_id":"652312","poster":"MisuLava","upvote_count":"3","content":"Selected Answer: D\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns"},{"timestamp":"1651416360.0","comment_id":"595627","poster":"Paul_Oprea","content":"BTW, how is the query even syntactically valid? It has non aggregated columns in the SELECT part of the query. That query will not run in the first place, unless I'm missing something.","upvote_count":"18"},{"upvote_count":"1","poster":"Arkon88","comment_id":"560874","timestamp":"1646409360.0","content":"Selected Answer: D\nD\nImage says that average(dark) and maximum(light) have difference in few times, this it is a skew\n\nhttps://cloud.google.com/bigquery/query-plan-explanation\nThe color indicators show the relative timings for all steps across all stages. For example, the COMPUTE step of Stage 00 shows a bar whose shaded fraction is 21/30 since 30ms is the maximum time spent in a single step of any stage. The parallel input information shows that each stage required only a single worker, so there's no variance between average and slowest timings."},{"timestamp":"1642190100.0","comment_id":"523712","upvote_count":"3","poster":"sraakesh95","content":"Selected Answer: D\nhttps://medium.com/slalom-build/using-bigquery-execution-plans-to-improve-query-performance-af141b0cc33d"},{"timestamp":"1642190040.0","content":"https://medium.com/slalom-build/using-bigquery-execution-plans-to-improve-query-performance-af141b0cc33d","upvote_count":"1","comment_id":"523711","poster":"sraakesh95"},{"upvote_count":"5","comment_id":"516730","content":"Selected Answer: D\nD\nColors: Purple is reading, Blue is writing. so the majority is reading.\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns","poster":"medeis_jar","timestamp":"1641307500.0"},{"poster":"morpho4444","comment_id":"490308","content":"If you read this https://medium.com/slalom-build/using-bigquery-execution-plans-to-improve-query-performance-af141b0cc33d C can't be right because the skewness happen when the column you use for grouping contains lots of null values, here C mentions columns that aren't part of the grouping clause. \n\nD, that's not how data get skewed, it gets skewed due to null values.\n\nA is the only answer here.","comments":[{"content":"A Cant be answer. Since users whenever running queries facing the problems.","upvote_count":"1","poster":"BigQuery","timestamp":"1638622020.0","comment_id":"493698"}],"upvote_count":"1","timestamp":"1638234300.0"},{"content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: D","comment_id":"487272","timestamp":"1637924220.0","upvote_count":"5","poster":"JG123"},{"poster":"anji007","upvote_count":"2","content":"Ans: D\nData is skewed.","timestamp":"1634326560.0","comment_id":"462783"},{"timestamp":"1625401980.0","upvote_count":"9","poster":"timolo","comment_id":"398314","comments":[{"timestamp":"1633703520.0","poster":"vholti","comment_id":"459273","content":"The query from question doesn't have partition filter (no WHERE clause). So regardless there is data skew, it's not the root cause of the low performance in this case. Option A is more reasonable I think.","upvote_count":"1"}],"content":"Correct is D: https://cloud.google.com/bigquery/docs/best-practices-performance-patterns\n\nPartition skew, sometimes called data skew, is when data is partitioned into very unequally sized partitions. This creates an imbalance in the amount of data sent between slots. You can't share partitions between slots, so if one partition is especially large, it can slow down, or even crash the slot that processes the oversized partition."},{"upvote_count":"3","comment_id":"392360","content":"Vote for 'D\"","poster":"sumanshu","timestamp":"1624828200.0"},{"comment_id":"309333","content":"Correct A\nThe query plan heat diagram shows about 25% time spent in reading. 75% in other wait, compute and write. This is an indication of constraint on the BigQuery computing slots.\nWhy not B: This is a small table. No need to partition.\nWhy not C: NULL values should not slow down computing\nWhy not D: Data skew is not likely. Even if the data is skewed, the total data is not large. So, computing should still be fast.","poster":"BhupiSG","timestamp":"1615600080.0","upvote_count":"5","comments":[{"poster":"H_S","upvote_count":"3","comment_id":"363784","timestamp":"1621698360.0","content":"o matter when they run the query!! can't be always overwelmed, i'll go with d"}]},{"timestamp":"1608474660.0","upvote_count":"7","content":"Option D - ref https://cloud.google.com/bigquery/docs/best-practices-performance-patterns#data_skew\n\nBest practice: If your query processes keys that are heavily skewed to a few values, filter your data as early as possible.","comment_id":"248726","poster":"cosmidumi"},{"content":"Partitions should not cause this issue so B is not correct.","upvote_count":"1","comment_id":"207060","poster":"Surjit24","timestamp":"1603804200.0"},{"poster":"GHN74","comment_id":"170535","upvote_count":"7","timestamp":"1598848020.0","content":"B is incorrect as the question clearly states “whenever they run the query” which means its a constant problem not adhoc. Option B is talking about a specific situation which means when (supposedly) fewer users have an active connection query should run faster but the question rules it out saying you always face performance problem.\n\nIn this case we have a Design issue hence option D sounds correct."},{"poster":"haroldbenites","comment_id":"161163","content":"D Is correct","timestamp":"1597795620.0","upvote_count":"4"},{"content":"User are using to many concurrent queries","comment_id":"152817","upvote_count":"1","poster":"AnujSingh","timestamp":"1596854640.0"},{"comment_id":"119075","content":"Option [D] - is correct. Ruling out option [B] because for a partitioned table, there should have been a wildcard character in TABLE_NAME or WHERE clause. Query mentioned in the question doesn't depict this.","poster":"dambilwa","upvote_count":"2","timestamp":"1593057420.0"},{"timestamp":"1585322400.0","comment_id":"68628","poster":"[Removed]","content":"Answer: D","upvote_count":"3"},{"upvote_count":"3","poster":"jvg637","content":"This chart show that the query reading part takes the most time. So B. Dataskew occurs in the processing time (orange color). If you examine the query explain plan and see a significant difference between avg and max compute times, your data is probably skewed.\n\nTo avoid performance issues that result from data skew:\n\nUse an approximate aggregate function such as APPROX_TOP_COUNT to determine if the data is skewed.\nFilter your data as early as possible.\nUnbalanced joins\nData skew can also appear when you use JOIN clauses. Because BigQuery shuffles data on each side of the join, all data with the same join key goes to the same shard. This shuffling can overload the slot.\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns#data_skew","comment_id":"67063","timestamp":"1584903600.0"}],"answer":"D","timestamp":"2020-03-21 09:32:00","choices":{"D":"Most rows in the [myproject:mydataset.mytable] table have the same value in the country column, causing data skew","B":"The [myproject:mydataset.mytable] table has too many partitions","A":"Users are running too many concurrent queries in the system","C":"Either the state or the city columns in the [myproject:mydataset.mytable] table have too many NULL values"},"url":"https://www.examtopics.com/discussions/google/view/17085-exam-professional-data-engineer-topic-1-question-53/","topic":"1","question_images":["https://www.examtopics.com/assets/media/exam-media/04341/0004200001.jpg"],"answers_community":["D (88%)","13%"],"question_id":269},{"id":"alLCLgWR4cZbbhq2AX9O","unix_timestamp":1584285420,"isMC":true,"timestamp":"2020-03-15 16:17:00","question_text":"Your globally distributed auction application allows users to bid on items. Occasionally, users place identical bids at nearly identical times, and different application servers process those bids. Each bid event contains the item, amount, user, and timestamp. You want to collate those bid events into a single location in real time to determine which user bid first. What should you do?","answer_images":[],"answer":"D","answer_ET":"D","exam_id":10,"question_images":[],"answer_description":"","answers_community":["D (53%)","B (46%)","1%"],"discussion":[{"content":"I'd go with B: real-time is requested, and the only scenario for real time (in the 4 presented) is the use of pub/sub with push.","poster":"jvg637","comments":[{"timestamp":"1644826620.0","comment_id":"546971","content":"B. \n- for realtime pub/sub push is critical. pull creates latency. (eliminates D)\n- process by event-time, not by process -time (eliminates D)","poster":"Tanzu","comments":[{"timestamp":"1648223220.0","comment_id":"575145","poster":"godot","content":"no push avail: https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub#streaming-pull-migration","upvote_count":"1"},{"content":"The dataflow is designed for realtime processing. and this case should be needed to use dataflow because there is no option to order the data if not using dataflow. So D is answer I think","poster":"jin0","timestamp":"1677150840.0","upvote_count":"1","comment_id":"819103"}],"upvote_count":"6"},{"content":"Agree with B","poster":"AzureDP900","comment_id":"765696","timestamp":"1672839300.0","upvote_count":"1"},{"comment_id":"337790","content":"i would go with option B, Cause option D states \"Give the bid for each item to the user in the bid event that is processed first\" . The requirement is to get the first bid based on event time not processed first in dataflow.","upvote_count":"28","poster":"[Removed]","timestamp":"1618690860.0"},{"content":"This approach is not ideal because it requires a custom endpoint to write the bid event information into Cloud SQL. This adds additional complexity and potential points of failure to the architecture, as well as adding latency to the processing of bid events, since the data must be written to both Pub/Sub and Cloud SQL. Additionally, it can be more challenging to ensure that bid events are processed in the order they were received, since the data is being written to multiple databases. Finally, using a single database to store bid events could limit scalability and availability, and can also result in slow query performance.","comment_id":"797811","upvote_count":"3","timestamp":"1675505700.0","poster":"donbigi"},{"comment_id":"395451","content":"Yep, Pub/Sub doesn't have FIFO yet, B is the one that keeps the right order","poster":"ralf_cc","timestamp":"1625110980.0","upvote_count":"7","comments":[{"content":"it is not a queue, and that is not a issue :)","comment_id":"546972","poster":"Tanzu","upvote_count":"3","timestamp":"1644826680.0","comments":[{"content":"in a distributed environment, you can not handle this problem wit a queue by the way !","upvote_count":"3","timestamp":"1644826740.0","poster":"Tanzu","comment_id":"546974"}]}]}],"upvote_count":"65","timestamp":"1584285420.0","comment_id":"64343"},{"poster":"Ganshank","comment_id":"73139","upvote_count":"34","timestamp":"1586564340.0","comments":[{"timestamp":"1679452260.0","comment_id":"846601","poster":"unnamed12355","upvote_count":"4","content":"D isnt correct, Pub/sub can send messages out of order, it is no guaranty that the event with lowest timestamp will be processed first\nB is correct"},{"poster":"Tanzu","upvote_count":"2","timestamp":"1644826980.0","content":"Yeap, that's why B is the right one. It has pub/sub push, more real time than pub/sub pull. You need to aware at some point , something has to be pulled which adds a latency.","comment_id":"546975"}],"content":"D\nThe need is to collate the messages in real-time. We need to de-dupe the messages based on timestamp of when the event occurred. This can be done by publishing ot Pub-Sub and consuming via Dataflow."},{"comment_id":"1337511","content":"Selected Answer: B\nprocess by event-time, not by process -time (eliminates D","upvote_count":"2","poster":"manikolbe","timestamp":"1736243280.0"},{"poster":"Ronn27","upvote_count":"1","content":"Selected Answer: D\nWriting directly to Cloud SQL in real time can cause bottlenecks as Cloud SQL is not designed for high-frequency, low-latency writes from multiple sources.\n\nAnswer D is right as Dataflow and Pubsub has the realtime capability","comment_id":"1336410","timestamp":"1736002560.0"},{"comment_id":"1328890","poster":"DGames","content":"Selected Answer: D\nBid event time and Pull Subscription is important part.","timestamp":"1734594120.0","upvote_count":"1"},{"poster":"baimus","upvote_count":"1","timestamp":"1727012400.0","comment_id":"1287738","content":"Selected Answer: D\nIt feels like it depends what's actually in the dataflow pipeline. D I believe is the answer they intend, even if messages are pulled out of order."},{"comment_id":"1249925","content":"Selected Answer: D\nWhile using Cloud Pub/Sub for real-time event streaming is a good choice, pushing events to a custom endpoint that writes to Cloud SQL introduces additional complexity.\nCustom endpoints need to be maintained, and the process of writing to Cloud SQL might not be as efficient as using a purpose-built data processing service.","poster":"manel_bhs","timestamp":"1721242920.0","upvote_count":"1"},{"timestamp":"1720003980.0","comment_id":"1241345","upvote_count":"1","poster":"Snnnnneee","content":"Selected Answer: B\nIn D the user gets it where the data is ingested first. That can be wrong for a global auction solution"},{"content":"Selected Answer: D\nThis is the most suitable solution for the requirements. Google Cloud Pub/Sub can handle high throughput and low-latency data ingestion. Coupled with Google Cloud Dataflow, which can process data streams in real time, this setup allows for immediate processing of bid events. Dataflow can also handle ordering and timestamp extraction, crucial for determining which bid came first. This architecture supports scalability and real-time analytics, which are essential for a global auction system.","comment_id":"1207260","upvote_count":"2","poster":"yassoraa88","timestamp":"1714985880.0"},{"comment_id":"1207134","poster":"teka112233","upvote_count":"3","content":"Selected Answer: D\nthe Answer should be D for the following \nReal-time Processing\nCentralized Processing\nWinner Determination\nalso, B is unsuitable as While Pub/Sub can ingest data, Cloud SQL is a relational database not designed for real-time processing at this scale. Maintaining a custom endpoint adds complexity.","timestamp":"1714963440.0"},{"timestamp":"1710405420.0","upvote_count":"3","comment_id":"1173244","poster":"I__SHA1234567","content":"Selected Answer: D\nGoogle Cloud Pub/Sub is a scalable and reliable messaging service that can handle high volumes of data and deliver messages in real-time. By having each application server publish bid events to Cloud Pub/Sub, you ensure that all bid events are collected centrally.\n\nUsing Cloud Dataflow with a pull subscription allows you to process the bid events in real-time. Cloud Dataflow provides a managed service for stream and batch processing, and it can handle the real-time processing requirements efficiently.\n\nBy processing the bid events with Cloud Dataflow, you can determine which user bid first by applying the appropriate logic within your Dataflow pipeline. This approach ensures scalability, reliability, and real-time processing capabilities, making it suitable for handling bid events from multiple application servers."},{"content":"B should be the answer, because it writes the bid into Cloud SQL to a distributed system. This way the customer know if they get the bid or not, immediately.\nAlso, push requests are faster than pull requests, hence they are better for realtime experience.","upvote_count":"1","timestamp":"1706633280.0","comment_id":"1135962","poster":"philli1011"},{"comment_id":"1104557","timestamp":"1703416380.0","upvote_count":"1","poster":"arpana_naa","content":"Selected Answer: D\npub/sub for entry time stamp + event time \ndataflow for processing and dataflow is better for real time"},{"poster":"Nandababy","comment_id":"1091747","upvote_count":"1","content":"To accurately determine who bid first in a globally distributed auction application, utilizing a push mechanism instead of a pull mechanism is generally considered the more reliable approach. \nB should be correct answer.","timestamp":"1702119180.0"},{"content":"Selected Answer: B\nkey words is \"single location in real time\"","comment_id":"1076307","timestamp":"1700572860.0","upvote_count":"2","poster":"Zepopo"},{"content":"Selected Answer: D\nAnswer : D\nWe need to de-dupe the messages based on timestamp of when the event occurred. This can be done by publishing ot Pub-Sub and consuming via Dataflow.\nD sounds like a complete answer. B does not.","poster":"rocky48","timestamp":"1699908060.0","upvote_count":"2","comment_id":"1069736"},{"comment_id":"1039120","poster":"Nivea007","timestamp":"1696906800.0","content":"D. Have each application server write the bid events to Google Cloud Pub/Sub as they occur. Use a pull subscription to pull the bid events using Google Cloud.\nThis approach leverages Google Cloud Pub/Sub for real-time data ingestion and Google Cloud Dataflow for real-time data processing, ensuring that bids are processed as they occur, which aligns with real-time requirements.\n\nIt's not B because there is a step involving a custom endpoint that writes data into Cloud SQL. This additional step could introduce some latency, and it's important to ensure that the custom endpoint and Cloud SQL database can handle the real-time load effectively.","comments":[{"content":"But D treats the bids according to the processed time. We need to consider event time that's why B is the right answer.","poster":"patiwwb","upvote_count":"1","comment_id":"1044690","timestamp":"1697435460.0"}],"upvote_count":"1"},{"content":"D. Have each application server write the bid events to Google Cloud Pub/Sub as they occur. Use a pull subscription to pull the bid events using Google Cloud Dataflow. Give the bid for each item to the user in the bid event that is processed first.","comment_id":"1027731","timestamp":"1696742100.0","poster":"imran79","upvote_count":"1"},{"upvote_count":"2","comment_id":"1023632","content":"Selected Answer: B\nB. Have each application server write the bid events to Cloud Pub/Sub as they occur. Push the events from Cloud Pub/Sub to a custom endpoint that writes the bid event information into Cloud SQL. is correct","poster":"Nirca","timestamp":"1696311900.0"},{"content":"Correct Answer is B. option D is based on processing first and not based on event first. so option D cannot be right answer","timestamp":"1695409920.0","comment_id":"1014424","upvote_count":"2","poster":"DeepakVenkatachalam"},{"poster":"np717","content":"Selected Answer: D\nD is the best solution because it is both real-time and scalable. Google Cloud Dataflow can process the bid events in the order in which they occurred and give the bid for each item to the user in the bid event that is processed first.","upvote_count":"1","comment_id":"992764","timestamp":"1693282680.0"},{"content":"B.here is why\nOption B is like using special messaging balloons that quickly carry all the bids to a special spot. From there, a super-fast friend checks them and tells us who bid first. This way, we find out quickly!\n\nOption D is like having all the bids sent to a special magic box that quickly sends them to a smart computer friend. This friend looks at the bids right away and tells us who should get the toy based on who bid first.\nIn conclusion, Option B (Cloud Pub/Sub and Cloud SQL) or Option D (Google Cloud Pub/Sub and Cloud Dataflow) are the most suitable choices for real-time processing of bids and determining the first bidder. They offer efficient, scalable, and real-time solutions for handling bid events in a globally distributed auction application. The final decision would depend on factors such as the specific requirements, infrastructure, and expertise of the development team.","comments":[{"comment_id":"971524","content":"Option B (Cloud Pub/Sub and Cloud SQL):\n\nAdvantage: Cloud Pub/Sub can handle real-time data streaming, making it a good choice for quickly receiving bids. Storing bid events in Cloud SQL ensures they are easily accessible and can be analyzed in real-time.\nDisadvantage: It might require some additional setup and configuration to connect Cloud Pub/Sub to a custom endpoint and Cloud SQL.\n\nOption D (Google Cloud Pub/Sub and Cloud Dataflow):\n\nAdvantage: Cloud Pub/Sub can handle real-time data streaming, similar to Option B. Using Cloud Dataflow can process the bid events quickly and efficiently.\nDisadvantage: It might require some additional setup and configuration for Cloud Dataflow.","timestamp":"1691104800.0","poster":"NeoNitin","upvote_count":"1"}],"comment_id":"971523","timestamp":"1691104680.0","upvote_count":"1","poster":"NeoNitin"},{"upvote_count":"1","content":"B is correct, As push provides near-real time","timestamp":"1690183560.0","poster":"knith66","comment_id":"961271"},{"upvote_count":"1","content":"A tough call between B and D. What favors B is that push is the right pattern for a real-time scenario. Also, the timestamp is already contained in the bid event. D gives the bid to the bid event that is processed first. This is not chronologically correct, as Dataflow has not guarantee to process the events in order. Overall, B makes more sense, as it respects the given timestamp and the processing itself will have lower latency.","timestamp":"1687924860.0","poster":"KC_go_reply","comment_id":"936062"},{"timestamp":"1684352760.0","comment_id":"900477","content":"Selected Answer: B\nI would choose B.\n\nD is not correct due to giving the bid to the first event processed, there is no guarantee the bids will be processed in the right order when using Pub/Sub.\n\nB allows storing to a central location as requested, and Cloud SQL is a transactional database that can handle the kind of real time OLTP processing described here.","poster":"cchen8181","upvote_count":"1"},{"content":"Selected Answer: D\nOption D is the correct solution.\n\nHaving each application server write the bid events to Google Cloud Pub/Sub as they occur allows for real-time collation of the bid events into a single location. Using a pull subscription to pull the bid events using Google Cloud Dataflow ensures that the events are processed in a single order and that the user who bid first is identified. This approach is scalable and provides low-latency processing, making it suitable for a globally distributed auction application.\n\nOption B also suggests using Cloud Pub/Sub, but it then pushes the events to a custom endpoint that writes to Cloud SQL. This approach can add additional latency and complexity, and may not be as scalable as using Cloud Dataflow directly with Pub/Sub.","upvote_count":"1","comment_id":"879494","poster":"Oleksandr0501","timestamp":"1682350020.0"},{"content":"Selected Answer: B\nFrom architecture perspective and high burst transactions. B is not a good option.\nI will go with D","poster":"izekc","timestamp":"1681246980.0","upvote_count":"1","comment_id":"867676"},{"upvote_count":"1","comment_id":"864206","content":"ChatGPT went with D","timestamp":"1680898020.0","poster":"ZSoulaimane"},{"content":"Selected Answer: B\n• Push = lower latency, more-real-time\n• Pull is ideal for large volume of messages, and uses batch delivery","upvote_count":"1","comment_id":"839779","timestamp":"1678876260.0","poster":"tibuenoc"},{"comment_id":"826518","poster":"lucaluca1982","upvote_count":"3","content":"Option D: Pull allows to keep the sequence of the bid","timestamp":"1677735000.0"},{"comment_id":"808292","content":"Anser is D:\nmany users are confused here in PUSH and PULL from pub/sub. PUSH is used when you want monitor data quickly on dashboard or in reports for analysis. PULL is used when data is big, and duplicate (like same amount of bids in this case is possible) which is collected securely and processed at a given time. It doesn't mean pub/sub did not receive data in real time. Here we need to be careful in data collection and all bid time record than showing it on dashboard. PUSH is not a good approach here. \nOption B: is more expensive and less compatible with Pub/Sub, you need to handle same amount bids as the occured.. i dont see any real time solution here.","timestamp":"1676372160.0","upvote_count":"4","poster":"musumusu"},{"timestamp":"1675505760.0","poster":"donbigi","comment_id":"797815","upvote_count":"5","content":"Selected Answer: D\nThis approach provides a scalable and highly available solution for processing bid events in real time. By using Cloud Pub/Sub, the application servers can publish the bid events as they occur, and the data can be pulled from the Pub/Sub topic by a single Cloud Dataflow pipeline. This ensures that bid events are not duplicated and can be processed in the order they are received. Additionally, Cloud Dataflow provides a flexible and scalable mechanism for processing large amounts of real-time data, which makes it well-suited for this use case."},{"upvote_count":"3","comment_id":"797387","poster":"r9765345","content":"Selected Answer: D\n2 minutes in to this video by Google explains why the answer is D:\nhttps://www.youtube.com/watch?v=KObJkda4ZfY","timestamp":"1675458720.0"},{"upvote_count":"1","timestamp":"1674113700.0","poster":"GCPpro","content":"B is the correct answer","comment_id":"780852"},{"content":"Selected Answer: B\nI vote on B\noption D does not answer where data is stored and it is pull with latency","comment_id":"761270","upvote_count":"3","poster":"Jackalski","timestamp":"1672332240.0"},{"content":"Selected Answer: B\nI would go with B as D is pull + 'give bid to user who processed first'. Since the requirement is to give bid on the basis of event and not processing ts, B is a close match","timestamp":"1671534060.0","comment_id":"750776","upvote_count":"2","poster":"Krish6488"},{"timestamp":"1671005820.0","upvote_count":"3","poster":"Sonika11","comment_id":"744865","content":"Selected Answer: D\nDataflow is needed to see which user sent bid first. Cannot track this with Pub/Sub alone."},{"upvote_count":"1","timestamp":"1670987040.0","content":"Selected Answer: C\nOption C look ok because in question mentioned You want to collate those bid events into a single location in real time to determine which user bid first","comment_id":"744673","poster":"DGames"},{"timestamp":"1670399760.0","comment_id":"737549","poster":"balajir27","upvote_count":"4","content":"Selected Answer: D\nIt is a global application hence cloud sql can not be an option"},{"poster":"ducc","comment_id":"653664","timestamp":"1661809560.0","content":"Selected Answer: B\nI voted for B","upvote_count":"3"},{"timestamp":"1661538300.0","upvote_count":"2","content":"Selected Answer: B\nthis is a bidding system. reading data periodically is not eficient.\nyou need to push data as soon as it is available, and storing it to CLoudSQL is fine. pubSub and Push are the key features here.","comment_id":"652314","poster":"MisuLava"},{"upvote_count":"1","content":"Selected Answer: B\nYou have to compare the timestamp before choosing the winner. There is latency to publish a message and to pull it. The first processed one by the dataflow is not always the first bid. You have to store all data to compare them via timestamp. Therefore not D, but B.","poster":"changsu","comments":[{"upvote_count":"2","timestamp":"1657981200.0","content":"That's not true, if you have a single partition in kafka is like a queue, so yes, the message will be ordered. In that case we can assume that the partition for the topic bids is 1, so only a single susbcriber = queue.\n\nThen point is if you have multiple partitions in pub/sub even using push, the messages can arrive at the wrong order, so the first is not necessary the first message to be pushed.\n\nDataflow can be considered as realtime if the window is seconds.","comment_id":"632206","poster":"alecuba16"}],"timestamp":"1656474660.0","comment_id":"624402"},{"content":"Selected Answer: B\nThis questions have ambigous answers, by the way, if we want to fit into the requirement, the answer that aproximates to it, is answer B because we have 2 facts \"they need to have a location where to RECEIVE the events in REAL TIME\". We have a push subscription, we have an endpoint that would be Cloud SQL, therefore, this gives the detail that they do not need to have automated the answer of which bid was the first to come, placing answer D in 2nd place and also, because it has a pull subscription, and in addition it doesnt mention where and which would be the sink. A and C aren't near to accomplish the goal to reach.","poster":"Preemptible_cerebrus","comment_id":"616804","upvote_count":"1","timestamp":"1655304600.0"},{"comments":[{"upvote_count":"1","poster":"snehalpatil","content":"it is proceed in a single region.","comment_id":"616522","timestamp":"1655262840.0"}],"upvote_count":"3","timestamp":"1654543800.0","comment_id":"612484","poster":"AmirN","content":"B is not correct. Cloud SQL is not a global system, and it is not scalable for use with pubsub.\nPush subscription to Cloud SQL is disaster because it wont be able to handle it. And also after the data gets into Cloud SQL what next? All the processing required to determine the fastest bid will cause more latency."},{"timestamp":"1646410440.0","upvote_count":"2","content":"Selected Answer: B\nB.\n- for realtime pub/sub push is critical. pull creates latency so not D","comment_id":"560885","poster":"Arkon88"},{"content":"B is correct. D is wrong as option clearly says \"give the bid that is processed first\".pub/sub can not guarantee ordering, therefore processing time is not the bar to identify bid.","upvote_count":"3","timestamp":"1646176680.0","poster":"rbeeraka","comment_id":"559067"},{"content":"Selected Answer: B\nb is making sense to me","timestamp":"1644721260.0","poster":"Derekliu","comment_id":"546201","upvote_count":"1"},{"timestamp":"1642190280.0","poster":"sraakesh95","comment_id":"523715","content":"Selected Answer: D\n@medeis_jar","upvote_count":"1"},{"timestamp":"1641986340.0","content":"Selected Answer: B\nThis is so tricky. A and C is out since it does not give the real-time feature. The battle is between B and D. \n\nIf we think about the business scenario we need to give the bid to who published it first. D is given to the who processed first. Technically it can be implemented bcz DataFlow is only support pull subscription with cloud pub-sub.\n\nIn answer B, events are pushed to the endpoint. Explicitly they haven't mentioned that it pushes to the \"Cloud Data Flow\". It may be a custom API endpoint. Some people have a misunderstanding about this point. There is no clue about that. It may be a REST API endpoint or application. \n\nThe event is consist of the timestamp field itself. So by inserting it into the DB we can find out who is the winner. \n\nBased on that we can Mark answer as B.","upvote_count":"8","poster":"Bhawantha","comment_id":"522075"},{"timestamp":"1641307920.0","poster":"medeis_jar","comment_id":"516737","content":"Selected Answer: D\n\"You want to collate those bid events into a single location in real time to determine which user bid first\"\n\nMultiple sources streaming -> Pub/Sub\n+\nReal-time first bid determining -> Cloud Dataflow","upvote_count":"3"},{"upvote_count":"4","content":"Answer is D.Application is globally distributed and how it gonna comply with Cloud SQL.And also pull does make sure all messages are acknowledged","comment_id":"511484","poster":"prasanna77","timestamp":"1640716800.0"},{"upvote_count":"2","comment_id":"504390","content":"Selected Answer: D\nD, realtime streaming - Pub/Sub + Dataflow","comments":[{"content":"https://stackoverflow.com/questions/53932773/right-google-cloud-product/53934435","comment_id":"504391","upvote_count":"2","timestamp":"1639852980.0","poster":"MaxNRG"}],"poster":"MaxNRG","timestamp":"1639852860.0"},{"content":"Correct B","comment_id":"501668","timestamp":"1639515480.0","upvote_count":"1","poster":"kreeaq"},{"timestamp":"1638367980.0","comment_id":"491707","upvote_count":"4","content":"B and D could be right. However, in question D indicates that the bid is won by the user whose bid event is \"processed first\", while it should be given to the bid event that is \"published first\". Hence, only B is valid. \nBy the way, D uses the timestamp of the message published in pub/sub (pub/sub automatically insert thetimestamp on published messages), while B can use either the timestamp of the bid arriving in the SQL database, either the timestap on published messages.","poster":"maurodipa"},{"timestamp":"1637924460.0","content":"Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: D","comment_id":"487277","poster":"JG123","upvote_count":"3"},{"upvote_count":"4","comments":[{"comment_id":"575143","poster":"godot","content":"https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub#streaming-pull-migration","upvote_count":"1","timestamp":"1648223160.0"}],"comment_id":"462527","poster":"iwanttogohome","content":"Can’t be B. I think is D.\nPubSub subscription with Dataflow, only Pull subscription are available\nhttps://stackoverflow.com/questions/62997414/push-vs-pull-for-gcp-dataflow","timestamp":"1634292720.0"},{"content":"CloudSQL in real time? I think 'B' is not a better choice, but 'D' can be.","poster":"anji007","upvote_count":"2","comments":[{"timestamp":"1634327520.0","comment_id":"462785","content":"After re-thinking, for me answer is B only.\nThough we have CloudSQL, main thing is identify first bid based on event time (time when bid was placed) and not on processing time (when message has reached dataflow).","poster":"anji007","upvote_count":"4"}],"timestamp":"1633266480.0","comment_id":"456590"},{"comment_id":"425377","poster":"ron123_aa","timestamp":"1629047640.0","upvote_count":"1","content":"no where i see that only use pull for realtime and not push. I think it's D","comments":[{"upvote_count":"2","comment_id":"466151","timestamp":"1634908260.0","poster":"Chelseajcole","content":"push for real-time not pull"}]},{"poster":"safiyu","upvote_count":"1","comment_id":"422692","timestamp":"1628596140.0","content":"the answer is B. Push makes it real time and pull does not.. simple as that"},{"timestamp":"1624828680.0","comment_id":"392369","content":"Vote for B","poster":"sumanshu","upvote_count":"2"},{"comments":[{"poster":"Ral17","content":"Can’t be D because you are giving advantage to the app server from which you do the pull first.","upvote_count":"1","comments":[{"upvote_count":"1","comment_id":"446986","content":"But there is only 1 pull subscription done in Dataflow","timestamp":"1631953260.0","poster":"yoshik"}],"timestamp":"1630895040.0","comment_id":"440061"}],"comment_id":"390831","poster":"moonlightbeamer","timestamp":"1624668900.0","upvote_count":"5","content":"I think is D. pub/sub is regional, global data publish into pub/sub will carry different delay. Dataflow being BEAM based, can deal with late arrivals to identify the true \"earliest bid got processed and associated with a timestamp\". I think the processed doesn't mean processed by dataflow, but processed by app server in global local time - timestamp."},{"content":"Can't be C, as they mentioned 'globally distributed auction application'... so D is correct --> Cloud pubsub with pull subscription.","upvote_count":"1","timestamp":"1624352940.0","poster":"vbondoo7","comment_id":"387783"},{"content":"Correct B","timestamp":"1615600380.0","upvote_count":"2","comment_id":"309335","poster":"BhupiSG"},{"timestamp":"1612085640.0","content":"I say B. Can’t be D because you are giving advantage to the app server from which you do the pull first. It should be about timestamps, not about \"event that is processed first\"","comment_id":"280460","upvote_count":"6","poster":"rosetta"},{"content":"Both Option D & B are correct. But we don't need to store the data persistently, we just need to process it. So, I would go with Option-D. Also if I use Cloud SQL, I will have to store then I need to run query to find the first user bid, then I will have to action. Whereas in DataFlow I can simply perform these actions immediately. I know push is immediate, but Pull mechanism is not bad, we can make near real time.","timestamp":"1610428020.0","comment_id":"265314","upvote_count":"10","poster":"StelSen"},{"content":"B. My reasoning is that the phrase \"Give the bid for each item to the user in the bid event that is processed first.\" in D is invalid. A bid should be given to a user who made it first based on the value of timestamp.","timestamp":"1609577760.0","upvote_count":"5","poster":"youngwjung","comment_id":"257353"},{"content":"I would go for B as push is suitable for real time processing..","timestamp":"1604771640.0","comment_id":"214766","poster":"arghya13","upvote_count":"3"},{"content":"D appears correct. A global application dealing with regional service like Cloud SQL is not sounding correct. PB and DF combination is ideal for many scenarios and both are Global services","timestamp":"1600862220.0","upvote_count":"3","comment_id":"185240","poster":"SureshKotla"},{"poster":"DeepakKhattar","content":"B - Its realtime because its push, ordering is taken care by using timestamp of bid event. co located on CLOUD SQL.","upvote_count":"2","comment_id":"184913","timestamp":"1600820400.0"},{"upvote_count":"2","content":"B should be the answer, D could have been correct if there is a push subscription instead of Pull , moreover in option B Cloud SQL will maintain proper transnational sequences.","comment_id":"175021","timestamp":"1599462600.0","poster":"Tanmoyk"},{"comment_id":"161165","timestamp":"1597795800.0","content":"D is correct","upvote_count":"1","poster":"haroldbenites"},{"timestamp":"1597297260.0","comment_id":"156996","content":"D as Cloud Pub/Sub with Cloud Dataflow can be used to buffer the bids and process them as per the order. Handles out of order and duplication of messages and hence get to know which user bid first.\nB is close but better to use Dataflow.","poster":"glachas","upvote_count":"5"},{"upvote_count":"2","timestamp":"1593943020.0","poster":"Rajuuu","comment_id":"126744","content":"B.\nPub-Sub along with Cloud SQL will allow message ordering."},{"poster":"serg3d","upvote_count":"4","content":"PubSub by itself does not guarantee a correct order of messages","comment_id":"102622","comments":[{"timestamp":"1594744500.0","upvote_count":"2","poster":"Rajuuu","comment_id":"135056","content":"Hence we need a Cloud SQL to get the correct sequence ."}],"timestamp":"1591301580.0"},{"poster":"itche_scratche","comment_id":"76127","content":"i'd go with B. push is the scenario to support realtime. but custom endpoint to cloudsql is not prefer. \nD is close, but since it's a pull it doesn't support realtime. i would do push with window session on expire time.","upvote_count":"3","timestamp":"1587230460.0"},{"comment_id":"68706","content":"Answer: B\nDescription: From Cloud SQL we can fetch the record on timestamp basis using where clause\nand it satisfies near real time","timestamp":"1585362540.0","upvote_count":"5","poster":"[Removed]"},{"poster":"[Removed]","timestamp":"1584782640.0","content":"Should be B","comment_id":"66448","upvote_count":"5"},{"comment_id":"65023","upvote_count":"3","timestamp":"1584421740.0","content":"I will go for B. C actually works but the master database will only have consolidated information with some delay, i.e. not real-time. D should be wrong since bid should not necessarily be given to user's request which is processed first as Pub/Sub does not guarantee messages to be delivered in order.","poster":"rickywck"}],"choices":{"B":"Have each application server write the bid events to Cloud Pub/Sub as they occur. Push the events from Cloud Pub/Sub to a custom endpoint that writes the bid event information into Cloud SQL.","A":"Create a file on a shared file and have the application servers write all bid events to that file. Process the file with Apache Hadoop to identify which user bid first.","D":"Have each application server write the bid events to Google Cloud Pub/Sub as they occur. Use a pull subscription to pull the bid events using Google Cloud Dataflow. Give the bid for each item to the user in the bid event that is processed first.","C":"Set up a MySQL database for each application server to write bid events into. Periodically query each of those distributed MySQL databases and update a master MySQL database with bid event information."},"url":"https://www.examtopics.com/discussions/google/view/16670-exam-professional-data-engineer-topic-1-question-54/","topic":"1","question_id":270}],"exam":{"isImplemented":true,"name":"Professional Data Engineer","isBeta":false,"lastUpdated":"15 Feb 2025","numberOfQuestions":319,"isMCOnly":true,"provider":"Google","id":10},"currentPage":54},"__N_SSP":true}